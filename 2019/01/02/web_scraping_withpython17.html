<!-- build time:Sat Jun 29 2019 20:19:08 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="google-site-verification" content="s0oCkquJSvsBetEUl3d8nDj5jYzNitxDJALA37MiIyM"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="python爬虫,Scrapy,豌豆荚爬虫"><link rel="alternate" href="/atom.xml" title="高级农民工" type="application/atom+xml"><meta name="description" content="使用 Scrapy 爬取豌豆荚全网 70000+ App。"><meta name="keywords" content="python爬虫,Scrapy,豌豆荚爬虫"><meta property="og:type" content="article"><meta property="og:title" content="以豌豆荚为例，用 Scrapy 爬取分类多级页面"><meta property="og:url" content="https://www.makcyun.top/2019/01/02/web_scraping_withpython17.html"><meta property="og:site_name" content="高级农民工"><meta property="og:description" content="使用 Scrapy 爬取豌豆荚全网 70000+ App。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://media.makcyun.top/19-1-3/91274061.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/97869343.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/93718642.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/68220890.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/4206503.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/90802300.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-2/26900812.jpg"><meta property="og:image" content="http://media.makcyun.top/18-12-14/2503305.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/13256282.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/48264516.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/7027912.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/14911632.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/62626697.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/23110694.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/86279055.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/63799914.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/48878130.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/26835054.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/22808039.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-3/1157763.jpg"><meta property="og:image" content="http://media.makcyun.top/19-1-1/31534576.jpg"><meta property="og:updated_time" content="2019-01-03T05:12:23.500Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="以豌豆荚为例，用 Scrapy 爬取分类多级页面"><meta name="twitter:description" content="使用 Scrapy 爬取豌豆荚全网 70000+ App。"><meta name="twitter:image" content="http://media.makcyun.top/19-1-3/91274061.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://www.makcyun.top/2019/01/02/web_scraping_withpython17.html"><title>以豌豆荚为例，用 Scrapy 爬取分类多级页面 | 高级农民工</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><meta name="baidu-site-verification" content="E65frtf6P6"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高级农民工</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Beginner's Mind</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首&emsp;&emsp;页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于博主</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归&emsp;&emsp;档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标&emsp;&emsp;签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分&emsp;&emsp;类</a></li><li class="menu-item menu-item-top"><a href="/top/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>最受欢迎</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://www.makcyun.top/2019/01/02/web_scraping_withpython17.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高级农民工"><meta itemprop="description" content=""><meta itemprop="image" content="http://media.makcyun.top/201901230951_146.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高级农民工"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">以豌豆荚为例，用 Scrapy 爬取分类多级页面</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-02T16:16:24+08:00">2019-01-02 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python爬虫/" itemprop="url" rel="index"><span itemprop="name">Python爬虫</span> </a></span></span><span id="/2019/01/02/web_scraping_withpython17.html" class="leancloud_visitors" data-flag-title="以豌豆荚为例，用 Scrapy 爬取分类多级页面"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">热度&#58;</span> <span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">5,123 字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">21 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>使用 Scrapy 爬取豌豆荚全网 70000+ App。</p><a id="more"></a><p><strong>摘要</strong>：使用 Scrapy 爬取豌豆荚全网 70000+ App，并进行探索性分析。</p><p><strong>写在前面</strong>：若对数据抓取部分不感兴趣，可以直接下拉到数据分析部分。</p><h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1 分析背景"></a>1 分析背景</h2><p>之前我们使用了 Scrapy 爬取并分析了酷安网 6000+ App，为什么这篇文章又在讲抓 App 呢?</p><p>因为我喜欢折腾 App，哈哈。当然，主要是因为下面这几点：</p><p><strong>第一、之前抓取的网页很简单</strong></p><p>在抓取酷安网时，我们使用 for 循环，遍历了几百页就完成了所有内容的抓取，非常简单，但现实往往不会这么 easy，有时我们要抓的内容会比较庞大，比如抓取整个网站的数据，为了增强爬虫技能，所以本文选择了「豌豆荚」这个网站。</p><p>目标是： <strong>爬取该网站所有分类下的 App 信息并下载 App 图标</strong>，数量在 <strong>70,000</strong> 左右，比酷安升了一个数量级。</p><p><strong>第二、再次练习使用强大的 Scrapy 框架</strong></p><p>之前只是初步地使用了 Scrapy 进行抓取，还没有充分领会到 Scrapy 有多么牛逼，所以本文尝试深入使用 Scrapy，增加随机 UserAgent、代理 IP 和图片下载等设置。</p><p><strong>第三、对比一下酷安和豌豆荚两个网站</strong></p><p>相信很多人都在使用豌豆荚下载 App，我则使用酷安较多，所以也想比较一下这两个网站的 App 特点。</p><p>话不多说，下面开始抓取流程。</p><h3 id="▌分析目标"><a href="#▌分析目标" class="headerlink" title="▌分析目标"></a>▌分析目标</h3><p>首先，我们先来了解一下要抓取的豌豆荚网页是什么样的，可以看到该网站上的 App 分成了很多类，包括：「应用播放」、「系统工具」等，一共有 14 个大类别，每个大类下又细分了多个小类，例如，影音播放下包括：「视频」、「直播」等。</p><p><img src="http://media.makcyun.top/19-1-3/91274061.jpg" alt=""></p><p>点击「视频」进入第二级子类页面，可以看到每款 App 的部分信息，包括：图标、名称、安装数量、体积、评论等。</p><p><img src="http://media.makcyun.top/19-1-3/97869343.jpg" alt=""></p><p>在之前的一篇文章中（见下方链接），我们分析了这个页面：采用 AJAX 加载，GET 请求，参数很容易构造，但是具体页数不确定，最后分别使用了 For 和 While 循环抓取了所有页数的数据。</p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ Python For 和 While 循环爬取不确定页数的网页</a></p><p>接着，我们可以再进入第三级页面，也就是每款 App 的详情页，可以看到多了下载数、好评率、评论数这几样参数，抓取思路和第二级页面大同小异，同时为了减小网站压力，所以 App 详情页就不抓取了。</p><p><img src="http://media.makcyun.top/19-1-3/93718642.jpg" alt=""></p><p>所以，<strong>这是一个分类多级页面的抓取问题，依次抓取每一个大类下的全部子类数据。</strong></p><p>学会了这种抓取思路，很多网站我们都可以去抓，比如很多人爱爬的「豆瓣电影」也是这样的结构。</p><p><img src="http://media.makcyun.top/19-1-3/68220890.jpg" alt=""></p><h3 id="▌分析内容"><a href="#▌分析内容" class="headerlink" title="▌分析内容"></a>▌分析内容</h3><p>数据抓取完成后，本文主要是对分类型数据的进行简单的探索性分析，包括这么几个方面：</p><ul><li>下载量最多 / 最少的 App 总排名</li><li>下载量最多 / 最少的 App 分类 / 子分类排名</li><li>App 下载量区间分布</li><li>App 名称重名的有多少</li><li>和酷安 App 进行对比</li></ul><h3 id="▌分析工具"><a href="#▌分析工具" class="headerlink" title="▌分析工具"></a>▌分析工具</h3><ul><li>Python</li><li>Scrapy</li><li>MongoDB</li><li>Pyecharts</li><li>Matplotlib</li></ul><h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2 数据抓取"></a>2 数据抓取</h2><h3 id="▌网站分析"><a href="#▌网站分析" class="headerlink" title="▌网站分析"></a>▌网站分析</h3><p>我们刚才已经初步对网站进行了分析，大致思路可以分为两步，首先是提取所有子类的 URL 链接，然后分别抓取每个 URL 下的 App 信息就行了。</p><p><img src="http://media.makcyun.top/19-1-3/4206503.jpg" alt=""></p><p>可以看到，子类的 URL 是由两个数字构成，前面的数字表示分类编号，后面的数字表示子分类编号，得到了这两个编号，就可以抓取该分类下的所有 App 信息，那么怎么获取这两个数值代码呢?</p><p>回到分类页面，定位查看信息，可以看到分类信息都包裹在每个 li 节点中，子分类 URL 则又在子节点 a 的 href 属性中，<strong>大分类一共有 14 个，子分类一共有 88 个</strong>。</p><p><img src="http://media.makcyun.top/19-1-3/90802300.jpg" alt=""></p><p>到这儿，思路就很清晰了，我们可以用 CSS 提取出全部子分类的 URL，然后分别抓取所需信息即可。</p><p>另外还需注意一点，该网站的 <strong>首页信息是静态加载的，从第 2 页开始是采用了 Ajax 动态加载</strong>，URL 不同，需要分别进行解析提取。</p><h3 id="▌Scrapy抓取"><a href="#▌Scrapy抓取" class="headerlink" title="▌Scrapy抓取"></a>▌Scrapy抓取</h3><p>我们要爬取两部分内容，一是 APP 的数据信息，包括前面所说的：名称、安装数量、体积、评论等，二是下载每款 App 的图标，分文件夹进行存放。</p><p>由于该网站有一定的反爬措施，所以我们需要添加随机 UA 和代理 IP，关于这两个知识点，我此前单独写了两篇文章进行铺垫，传送门：</p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ Scrapy 中设置随机 User-Agent 的方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython15.html">∞ Python 爬虫的代理 IP 设置方法汇总</a></p><p>这里随机 UA 使用 <strong>scrapy-fake-useragent </strong>库，一行代码就能搞定，代理 IP 直接上阿布云付费代理，几块钱搞定简单省事。</p><p>下面，就直接上代码了：</p><h4 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a><strong>items.py</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WandoujiaItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    cate_name = scrapy.Field() <span class="comment">#分类名</span></span><br><span class="line">    child_cate_name = scrapy.Field() <span class="comment">#分类编号</span></span><br><span class="line">    app_name = scrapy.Field()   <span class="comment"># 子分类名</span></span><br><span class="line">    install = scrapy.Field()    <span class="comment"># 子分类编号</span></span><br><span class="line">    volume = scrapy.Field()     <span class="comment"># 体积</span></span><br><span class="line">    comment = scrapy.Field()    <span class="comment"># 评论</span></span><br><span class="line">    icon_url = scrapy.Field()   <span class="comment"># 图标url</span></span><br></pre></td></tr></table></figure><h4 id="middles-py"><a href="#middles-py" class="headerlink" title="middles.py"></a><strong>middles.py</strong></h4><p>中间件主要用于设置代理 IP。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line">proxyServer = <span class="string">"http://http-dyn.abuyun.com:9020"</span></span><br><span class="line">proxyUser = <span class="string">"你的信息"</span></span><br><span class="line">proxyPass = <span class="string">"你的信息"</span></span><br><span class="line"></span><br><span class="line">proxyAuth = <span class="string">"Basic "</span> + base64.urlsafe_b64encode(bytes((proxyUser + <span class="string">":"</span> + proxyPass), <span class="string">"ascii"</span>)).decode(<span class="string">"utf8"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbuyunProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = proxyServer</span><br><span class="line">        request.headers[<span class="string">"Proxy-Authorization"</span>] = proxyAuth</span><br><span class="line">        logging.debug(<span class="string">'Using Proxy:%s'</span>%proxyServer)</span><br></pre></td></tr></table></figure><h4 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a><strong>pipelines.py</strong></h4><p>该文件用于存储数据到 MongoDB 和下载图标到分类文件夹中。</p><p>存储到 MongoDB：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MongoDB 存储</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,mongo_url,mongo_db)</span>:</span></span><br><span class="line">        self.mongo_url = mongo_url</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_url = crawler.settings.get(<span class="string">'MONGO_URL'</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_url)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        <span class="comment"># self.db[name].insert(dict(item))</span></span><br><span class="line">        self.db[name].update_one(item, &#123;<span class="string">'$set'</span>: item&#125;, upsert=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure><p>按文件夹下载图标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分文件夹下载</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagedownloadPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self,item,info)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'icon_url'</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'icon_url'</span>],meta=&#123;<span class="string">'item'</span>:item&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        name = request.meta[<span class="string">'item'</span>][<span class="string">'app_name'</span>]</span><br><span class="line">        cate_name = request.meta[<span class="string">'item'</span>][<span class="string">'cate_name'</span>]</span><br><span class="line">        child_cate_name = request.meta[<span class="string">'item'</span>][<span class="string">'child_cate_name'</span>]</span><br><span class="line">      </span><br><span class="line">        path1 = <span class="string">r'/wandoujia/%s/%s'</span> %(cate_name,child_cate_name)</span><br><span class="line">        path = <span class="string">r'&#123;&#125;\&#123;&#125;.&#123;&#125;'</span>.format(path1, name, <span class="string">'jpg'</span>)</span><br><span class="line">        <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self,results,item,info)</span>:</span></span><br><span class="line">        image_path = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok,x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_path:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'Item contains no images'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h4 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a><strong>settings.py</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">BOT_NAME = <span class="string">'wandoujia'</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">'wandoujia.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'wandoujia.spiders'</span></span><br><span class="line"></span><br><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'wandoujia'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否遵循机器人规则</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br><span class="line"><span class="comment"># 下载设置延迟 由于买的阿布云一秒只能请求5次，所以每个请求设置了 0.2s延迟</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>,</span><br><span class="line">    <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">100</span>, <span class="comment"># 随机UA</span></span><br><span class="line">    <span class="string">'wandoujia.middlewares.AbuyunProxyMiddleware'</span>: <span class="number">200</span> <span class="comment"># 阿布云代理</span></span><br><span class="line">    ）</span><br><span class="line">    </span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'wandoujia.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">'wandoujia.pipelines.ImagedownloadPipeline'</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment"># URL不去重</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy.dupefilters.BaseDupeFilter'</span></span><br></pre></td></tr></table></figure><h4 id="wandou-py"><a href="#wandou-py" class="headerlink" title="wandou.py"></a><strong>wandou.py</strong></h4><p>主程序这里列出关键的部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.cate_url = <span class="string">'https://www.wandoujia.com/category/app'</span></span><br><span class="line">        <span class="comment"># 子分类首页url</span></span><br><span class="line">        self.url = <span class="string">'https://www.wandoujia.com/category/'</span></span><br><span class="line">        <span class="comment"># 子分类 ajax请求页url</span></span><br><span class="line">        self.ajax_url = <span class="string">'https://www.wandoujia.com/wdjweb/api/category/more?'</span></span><br><span class="line">        <span class="comment"># 实例化分类标签</span></span><br><span class="line">        self.wandou_category = Get_category()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.cate_url,callback=self.get_category)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self,response)</span>:</span>    </span><br><span class="line">        cate_content = self.wandou_category.parse_category(response)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>这里，首先定义几个 URL，包括：分类页面、子分类首页、子分类 AJAX 页，也就是第 2 页开始的 URL，然后又定义了一个类 Get_category() 专门用于提取全部的子分类 URL，稍后我们将展开该类的代码。</p><p>程序从 start_requests 开始运行，解析首页获得响应，调用 get_category() 方法，然后使用 Get_category() 类中的 parse_category() 方法提取出所有 URL，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Get_category</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_category</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        category = response.css(<span class="string">'.parent-cate'</span>)</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'cate_name'</span>: item.css(<span class="string">'.cate-link::text'</span>).extract_first(),</span><br><span class="line">            <span class="string">'cate_code'</span>: self.get_category_code(item),</span><br><span class="line">            <span class="string">'child_cate_codes'</span>: self.get_child_category(item),</span><br><span class="line">        &#125; <span class="keyword">for</span> item <span class="keyword">in</span> category]</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取所有主分类标签数值代码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_category_code</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        cate_url = item.css(<span class="string">'.cate-link::attr("href")'</span>).extract_first()</span><br><span class="line">        pattern = re.compile(<span class="string">r'.*/(\d+)'</span>)  <span class="comment"># 提取主类标签代码</span></span><br><span class="line">        cate_code = re.search(pattern, cate_url)</span><br><span class="line">        <span class="keyword">return</span> cate_code.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有子分类名称和编码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_child_category</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        child_cate = item.css(<span class="string">'.child-cate a'</span>)</span><br><span class="line">        child_cate_url = [&#123;</span><br><span class="line">            <span class="string">'child_cate_name'</span>: child.css(<span class="string">'::text'</span>).extract_first(),</span><br><span class="line">            <span class="string">'child_cate_code'</span>: self.get_child_category_code(child)</span><br><span class="line">        &#125; <span class="keyword">for</span> child <span class="keyword">in</span> child_cate]</span><br><span class="line">        <span class="keyword">return</span> child_cate_url</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正则提取子分类编码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_child_category_code</span><span class="params">(self, child)</span>:</span></span><br><span class="line">        child_cate_url = child.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">        pattern = re.compile(<span class="string">r'.*_(\d+)'</span>)  <span class="comment"># 提取小类标签编号</span></span><br><span class="line">        child_cate_code = re.search(pattern, child_cate_url)</span><br><span class="line">        <span class="keyword">return</span> child_cate_code.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里，除了分类名称 cate_name 可以很方便地直接提取出来，分类编码和子分类的子分类的名称和编码，我们使用了 get_category_code() 等三个方法进行提取。提取方法使用了 CSS 和正则表达式，比较简单。</p><p>最终提取的分类名称和编码结果如下，利用这些编码，我们就可以构造 URL 请求开始提取每个子分类下的 App 信息了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'cate_name'</span>: <span class="string">'影音播放'</span>, <span class="string">'cate_code'</span>: <span class="string">'5029'</span>, <span class="string">'child_cate_codes'</span>: [</span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'视频'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'716'</span>&#125;, </span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'直播'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'1006'</span>&#125;, </span><br><span class="line">    ...</span><br><span class="line">	]&#125;, </span><br><span class="line">&#123;<span class="string">'cate_name'</span>: <span class="string">'系统工具'</span>, <span class="string">'cate_code'</span>: <span class="string">'5018'</span>, <span class="string">'child_cate_codes'</span>: [</span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'WiFi'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'895'</span>&#125;, </span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'浏览器'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'599'</span>&#125;, </span><br><span class="line">    ...</span><br><span class="line">	]&#125;, </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接着前面的 get_category() 继续往下写，提取 App 的信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self,response)</span>:</span>    </span><br><span class="line">        cate_content = self.wandou_category.parse_category(response)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> cate_content:</span><br><span class="line">            child_cate = item[<span class="string">'child_cate_codes'</span>]</span><br><span class="line">            <span class="keyword">for</span> cate <span class="keyword">in</span> child_cate:</span><br><span class="line">                cate_code = item[<span class="string">'cate_code'</span>]</span><br><span class="line">                cate_name = item[<span class="string">'cate_name'</span>]</span><br><span class="line">                child_cate_code = cate[<span class="string">'child_cate_code'</span>]</span><br><span class="line">                child_cate_name = cate[<span class="string">'child_cate_name'</span>]</span><br><span class="line">                </span><br><span class="line">                page = <span class="number">1</span> <span class="comment"># 设置爬取起始页数</span></span><br><span class="line">                <span class="keyword">if</span> page == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 构造首页url</span></span><br><span class="line">                    category_url = <span class="string">'&#123;&#125;&#123;&#125;_&#123;&#125;'</span> .format(self.url, cate_code, child_cate_code)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    params = &#123;</span><br><span class="line">                        <span class="string">'catId'</span>: cate_code,  <span class="comment"># 类别</span></span><br><span class="line">                        <span class="string">'subCatId'</span>: child_cate_code,  <span class="comment"># 子类别</span></span><br><span class="line">                        <span class="string">'page'</span>: page,</span><br><span class="line">                        &#125;</span><br><span class="line">                    category_url = self.ajax_url + urlencode(params)</span><br><span class="line">                dict = &#123;<span class="string">'page'</span>:page,<span class="string">'cate_name'</span>:cate_name,<span class="string">'cate_code'</span>:cate_code,<span class="string">'child_cate_name'</span>:child_cate_name,<span class="string">'child_cate_code'</span>:child_cate_code&#125;</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(category_url,callback=self.parse,meta=dict)</span><br></pre></td></tr></table></figure><p>这里，依次提取出全部的分类名称和编码，用于构造请求的 URL。由于首页的 URL 和第 2 页开始的 URL 形式不同，所以使用了 if 语句分别进行构造。接下来，请求该 URL 然后调用 self.parse() 方法进行解析，这里使用了 meta 参数用于传递相关参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(response.body) &gt;= <span class="number">100</span>:  <span class="comment"># 判断该页是否爬完，数值定为100是因为无内容时长度是87</span></span><br><span class="line">            page = response.meta[<span class="string">'page'</span>]</span><br><span class="line">            cate_name = response.meta[<span class="string">'cate_name'</span>]</span><br><span class="line">            cate_code = response.meta[<span class="string">'cate_code'</span>]</span><br><span class="line">            child_cate_name = response.meta[<span class="string">'child_cate_name'</span>]</span><br><span class="line">            child_cate_code = response.meta[<span class="string">'child_cate_code'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> page == <span class="number">1</span>:</span><br><span class="line">                contents = response</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                jsonresponse = json.loads(response.body_as_unicode())</span><br><span class="line">                contents = jsonresponse[<span class="string">'data'</span>][<span class="string">'content'</span>]</span><br><span class="line">                <span class="comment"># response 是json,json内容是html，html 为文本不能直接使用.css 提取，要先转换</span></span><br><span class="line">                contents = scrapy.Selector(text=contents, type=<span class="string">"html"</span>)</span><br><span class="line"></span><br><span class="line">            contents = contents.css(<span class="string">'.card'</span>)</span><br><span class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">                <span class="comment"># num += 1</span></span><br><span class="line">                item = WandoujiaItem()</span><br><span class="line">                item[<span class="string">'cate_name'</span>] = cate_name</span><br><span class="line">                item[<span class="string">'child_cate_name'</span>] = child_cate_name</span><br><span class="line">                item[<span class="string">'app_name'</span>] = self.clean_name(content.css(<span class="string">'.name::text'</span>).extract_first())  </span><br><span class="line">                item[<span class="string">'install'</span>] = content.css(<span class="string">'.install-count::text'</span>).extract_first()</span><br><span class="line">                item[<span class="string">'volume'</span>] = content.css(<span class="string">'.meta span:last-child::text'</span>).extract_first()</span><br><span class="line">                item[<span class="string">'comment'</span>] = content.css(<span class="string">'.comment::text'</span>).extract_first().strip()</span><br><span class="line">                item[<span class="string">'icon_url'</span>] = self.get_icon_url(content.css(<span class="string">'.icon-wrap a img'</span>),page)</span><br><span class="line">                <span class="keyword">yield</span> item</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 递归爬下一页</span></span><br><span class="line">            page += <span class="number">1</span></span><br><span class="line">            params = &#123;</span><br><span class="line">                    <span class="string">'catId'</span>: cate_code,  <span class="comment"># 大类别</span></span><br><span class="line">                    <span class="string">'subCatId'</span>: child_cate_code,  <span class="comment"># 小类别</span></span><br><span class="line">                    <span class="string">'page'</span>: page,</span><br><span class="line">                    &#125;</span><br><span class="line">            ajax_url = self.ajax_url + urlencode(params)</span><br><span class="line">            dict = &#123;<span class="string">'page'</span>:page,<span class="string">'cate_name'</span>:cate_name,<span class="string">'cate_code'</span>:cate_code,<span class="string">'child_cate_name'</span>:child_cate_name,<span class="string">'child_cate_code'</span>:child_cate_code&#125;</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(ajax_url,callback=self.parse,meta=dict)</span><br></pre></td></tr></table></figure><p>最后，parse() 方法用来解析提取最终我们需要的 App 名称、安装量等信息，解析完成一页后，page 进行递增，然后重复调用 parse() 方法循环解析，直到解析完全部分类的最后一页。</p><p>最终，几个小时后，我们就可以完成全部 App 信息的抓取，我这里得到 73,755 条信息和 72,150 个图标，两个数值不一样是因为有些 App 只有信息没有图标。</p><p><img src="http://media.makcyun.top/19-1-2/26900812.jpg" alt=""></p><p>图标下载：</p><p><img src="http://media.makcyun.top/18-12-14/2503305.jpg" alt=""></p><p>下面将对提取的信息，进行的数据分析。</p><h2 id="3-数据分析"><a href="#3-数据分析" class="headerlink" title="3 数据分析"></a>3 数据分析</h2><h3 id="▌总体情况"><a href="#▌总体情况" class="headerlink" title="▌总体情况"></a>▌总体情况</h3><p>首先来看一下 App 的安装量情况，毕竟 70000 多款 App，自然很感兴趣 <strong>哪些 App 使用地最多，哪些又使用地最少</strong>。</p><p><img src="http://media.makcyun.top/19-1-3/13256282.jpg" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">colors = <span class="string">'#6D6D6D'</span> <span class="comment">#字体颜色</span></span><br><span class="line">colorline = <span class="string">'#63AB47'</span>  <span class="comment">#红色CC2824  #豌豆荚绿</span></span><br><span class="line">fontsize_title = <span class="number">20</span></span><br><span class="line">fontsize_text = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载量总排名</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis_maxmin</span><span class="params">(data)</span>:</span></span><br><span class="line">    data_max = (data[:<span class="number">10</span>]).sort_values(by=<span class="string">'install_count'</span>)</span><br><span class="line">    data_max[<span class="string">'install_count'</span>] = (data_max[<span class="string">'install_count'</span>] / <span class="number">100000000</span>).round(<span class="number">1</span>)</span><br><span class="line">    data_max.plot.barh(x=<span class="string">'app_name'</span>,y=<span class="string">'install_count'</span>,color=colorline)</span><br><span class="line">    <span class="keyword">for</span> y, x <span class="keyword">in</span> enumerate(list((data_max[<span class="string">'install_count'</span>]))):</span><br><span class="line">        plt.text(x + <span class="number">0.1</span>, y - <span class="number">0.08</span>, <span class="string">'%s'</span> %</span><br><span class="line">                 round(x, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'安装量最多的 10 款 App ?'</span>,color=colors)</span><br><span class="line">    plt.xlabel(<span class="string">'下载量(亿次)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'App'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment"># plt.savefig('安装量最多的App.png',dpi=200)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>看了上图，有两个「<strong>没想到</strong>」：</p><ul><li><p><strong>排名第一的居然是一款手机管理软件</strong></p><p>对豌豆荚网上的这个第一名感到意外，一是，好奇大家都那么爱手机清理或者怕中毒么?毕竟，我自己的手机都「裸奔」了好些年；二是，第一名居然不是鹅厂的其他产品，比入「微信」或者「QQ」。</p></li><li><p><strong>榜单放眼望去，以为会出现的没有出现，没有想到的却出现了</strong></p><p>前十名中，居然出现了书旗小说、印客这些比较少听过的名字，而国民 App 微信、支付宝等，甚至都没有出现在这个榜单中。</p></li></ul><p>带着疑问和好奇，分别找到了「腾讯手机管家」和「微信」两款 App 的主页：</p><p>腾讯手机管家下载和安装量：</p><p><img src="http://media.makcyun.top/19-1-3/48264516.jpg" alt=""></p><p>微信下载和安装量：</p><p><img src="http://media.makcyun.top/19-1-3/7027912.jpg" alt=""></p><p>这是什么情况?</p><p>腾讯管家 3 亿多的下载量等同于安装量，而微信 20 多亿的下载量，只有区区一千多万的安装量，两组数据对比，大致反映了两个问题：</p><ul><li><p>要么是腾讯管家的下载量实际并没有那么多</p></li><li><p>要么是微信的下载量写少了</p></li></ul><p>不管是哪个问题，都反映了一个问题：<strong>该网站做得不够走心啊</strong>。</p><p>为了证明这个观点，将前十名的安装量和下载量都作了对比，发现很多 App 的安装量都和下载量是一样的，也就是说：<strong>这些 App 的实际下载量并没有那么多</strong>，而如果这样的话，那么这份榜单就有很大水分了。</p><p>难道，辛辛苦苦爬了那么久，就得到这样的结果?</p><p>不死心，接着再看看安装量最少的 App 是什么情况，这里找出了其中最少的 10 款：</p><p><img src="http://media.makcyun.top/19-1-3/14911632.jpg" alt=""></p><p>扫了一眼，更加没想到了：</p><p>「QQ 音乐」竟然是倒数第一，竟然只有 3 次安装量！</p><p><strong>确定这和刚刚上市、市值千亿的 QQ 音乐是同一款产品?</strong></p><p>再次核实了一下：</p><p><img src="http://media.makcyun.top/19-1-3/62626697.jpg" alt=""></p><p>没有看错，是写着 <strong>3人安装</strong>！</p><p>这是已经不走心到什么程度了? <strong>这个安装量，鹅厂还能「用心做好音乐」?</strong></p><p>说实话，到这儿已经不想再往下分析下去了，担心爬扒出更多没想到的东西，不过辛苦爬了这么久，还是再往下看看吧。</p><p>看了首尾，我们再看看整体，了解一下全部 App 的安装数量分布，这里去除了有很大水分的前十名 App。</p><p><img src="http://media.makcyun.top/19-1-3/23110694.jpg" alt=""></p><p>很惊讶地发现，竟然有 <strong>多达 67,195 款，占总数的 94% 的 App 的安装量不足 1万！</strong></p><p>如果这个网站的所有数据都是真的话，那么上面排名第一的手机管家，它 <strong>一款就差不多抵得上这 6 万多款 App 的安装量</strong>了！</p><p>对于多数 App 开发者，只能说：<strong>现实很残酷，辛苦开发出来的 App，用户不超过 1万人的可能性高达近 95% </strong>。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis_distribution</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = data.loc[<span class="number">10</span>:,:]</span><br><span class="line">    data[<span class="string">'install_count'</span>] = data[<span class="string">'install_count'</span>].apply(<span class="keyword">lambda</span> x:x/<span class="number">10000</span>)</span><br><span class="line">    bins = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>,<span class="number">1000</span>,<span class="number">10000</span>]</span><br><span class="line">    group_names = [<span class="string">'1万以下'</span>,<span class="string">'1-10万'</span>,<span class="string">'10-100万'</span>,<span class="string">'100-1000万'</span>,<span class="string">'1000万-1亿'</span>]</span><br><span class="line">    cats = pd.cut(data[<span class="string">'install_count'</span>],bins,labels=group_names)</span><br><span class="line">    cats = pd.value_counts(cats)</span><br><span class="line">    bar = Bar(<span class="string">'App 下载数量分布'</span>,<span class="string">'高达 94% 的 App 下载量低于1万'</span>)</span><br><span class="line">    bar.use_theme(<span class="string">'macarons'</span>)</span><br><span class="line">    bar.add(</span><br><span class="line">        <span class="string">'App 数量'</span>,</span><br><span class="line">        list(cats.index),</span><br><span class="line">        list(cats.values),</span><br><span class="line">        is_label_show = <span class="keyword">True</span>,</span><br><span class="line">        xaxis_interval = <span class="number">0</span>,</span><br><span class="line">        is_splitline_show = <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">    bar.render(path=<span class="string">'App下载数量分布.png'</span>,pixel_ration=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="▌分类情况"><a href="#▌分类情况" class="headerlink" title="▌分类情况"></a>▌分类情况</h3><p>下面，我们来看看各分类下 App 情况，不再看安装量，而看数量，以排出干扰。</p><p><img src="http://media.makcyun.top/19-1-3/86279055.jpg" alt=""></p><p>可以看到 14 个大分类中，<strong>每个分类的 App 数量差距都不大</strong>，数量最多的「生活休闲」是「摄影图像」的两倍多一点。</p><p>接着，我们进一步看看 88 个子分类的 App 数量情况，筛选出数量最多和最少的 10 个子类：</p><p><img src="http://media.makcyun.top/19-1-3/63799914.jpg" alt=""></p><p>可以发现两点有意思的现象：</p><ul><li><p><strong>「收音机」类别 App 数量最多，达到 1,300 多款</strong></p><p>这个很意外，当下收音机完全可以说是个老古董了，居然还有那么人去开发。</p></li><li><p><strong>App 子类数量差距较大</strong></p><p>最多的「收音机」是最少的「动态壁纸」近 20 倍，如果我是一个 App 开发者，<strong>那我更愿意去尝试开发些小众类的 App，竞争小一点</strong>，比如：「背单词」、「小儿百科」这些。</p></li></ul><p>看完了总体和分类情况，突然想到一个问题：<strong>这么多 App，有没有重名的呢?</strong></p><p><img src="http://media.makcyun.top/19-1-3/48878130.jpg" alt=""></p><p>惊奇地发现，叫「一键锁屏」的 App 多达 40 款，这个功能 App 很难再想出别的名字了么? 现在很多手机都支持触控锁屏了，比一键锁屏操作更加方便。</p><p>接下来，我们简单对比下豌豆荚和酷安两个网站的 App 情况。</p><h3 id="▌对比酷安"><a href="#▌对比酷安" class="headerlink" title="▌对比酷安"></a>▌对比酷安</h3><p>二者最直观的一个区别是在 App 数量上，豌豆荚拥有绝对的优势，达到了酷安的十倍之多，那么我们自然感兴趣：</p><p><strong>豌豆荚是否包括了酷安上所有的 App ?</strong></p><p>如果是，「你有的我都有，你没有的我也有」，那么酷安就没什么优势了。统计之后，发现豌豆荚 <strong>仅包括了 3,018 款，也就是一半左右</strong>，剩下的另一半则没有包括。</p><p>这里面固然存在两个平台上 App 名称不一致的现象，但更有理由相信 <strong>酷安很多小众的精品 App 是独有的，豌豆荚并没有。</strong></p><p><img src="http://media.makcyun.top/19-1-3/26835054.jpg" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">include = data3.shape[<span class="number">0</span>]</span><br><span class="line">notinclude = data2.shape[<span class="number">0</span>] - data3.shape[<span class="number">0</span>]</span><br><span class="line">sizes= [include,notinclude]</span><br><span class="line">labels = [<span class="string">u'包含'</span>,<span class="string">u'不包含'</span>]</span><br><span class="line">explode = [<span class="number">0</span>,<span class="number">0.05</span>]</span><br><span class="line">plt.pie(</span><br><span class="line">    sizes,</span><br><span class="line">    autopct = <span class="string">'%.1f%%'</span>,</span><br><span class="line">    labels = labels,</span><br><span class="line">    colors = [colorline,<span class="string">'#7FC161'</span>], <span class="comment"># 豌豆荚绿</span></span><br><span class="line">    shadow = <span class="keyword">False</span>,</span><br><span class="line">    startangle = <span class="number">90</span>,</span><br><span class="line">    explode = explode,</span><br><span class="line">    textprops = &#123;<span class="string">'fontsize'</span>:<span class="number">14</span>,<span class="string">'color'</span>:colors&#125;</span><br><span class="line">)</span><br><span class="line">plt.title(<span class="string">'豌豆荚仅包括酷安上一半的 App 数量'</span>,color=colorline,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">'包含不保包含对比.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>接下来，我们看看所包含的 App 当中，在两个平台上的下载量是怎么样的：</p><p><img src="http://media.makcyun.top/19-1-3/22808039.jpg" alt=""></p><p>可以看到，两个平台上 App 下载数量差距还是很明显。</p><p>最后，我面再看看豌豆荚上没有包括哪些APP：</p><p><img src="http://media.makcyun.top/19-1-3/1157763.jpg" alt=""></p><p>可以看到很多神器都没有包括，比如：RE、绿色守护、一个木函等等。豌豆荚和酷安的对比就到这里，如果用一句话来总结，我可能会说：</p><p><strong>豌豆荚太牛逼了， App 数量是酷安的十倍，所以我选酷安。</strong></p><p>以上，就是利用 Scrapy 爬取分类多级页面的抓取和分析的一次实战。</p><p>感兴趣的话可以找类似的网站练练手，如需本文的完整代码，可以加入我的知识星球：「<strong>第2脑袋</strong>」获得，里面有很多干货，可以扫描下方二维码预览下，觉得合适就入圈。</p><p><img src="http://media.makcyun.top/19-1-1/31534576.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ Python For 和 While 循环爬取不确定页数的网页</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython15.html">∞ Python 爬虫的代理 IP 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython14.html">∞ Python爬虫的随机 User-Agent 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">∞ pyspider 爬取并分析虎嗅网 5 万篇文章</a></p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>你一打赏，我就写得更来劲了</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.png" alt="高级农民工 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python爬虫/" rel="tag"><i class="fa fa-tag"></i> Python爬虫</a></div><div class="post-widgets"><div class="wp_rating"><div style="color:rgba(0,0,0,.75);font-size:13px;letter-spacing:3px">(&gt;你觉得这篇文章怎么样？&lt;)</div><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/01/05/weekly_sharing11.html" rel="prev" title="国外最牛逼的一套 PPT 作品分享给你"><i class="fa fa-chevron-left"></i> 国外最牛逼的一套 PPT 作品分享给你</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/12/31/life05.html" rel="next" title="原创精华 | 2018 年文章汇总">原创精华 | 2018 年文章汇总 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="http://media.makcyun.top/201901230951_146.jpg" alt="高级农民工"><p class="site-author-name" itemprop="name">高级农民工</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">97</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">43</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/makcyun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:johnny824lee@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-分析背景"><span class="nav-text">1 分析背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#▌分析目标"><span class="nav-text">▌分析目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#▌分析内容"><span class="nav-text">▌分析内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#▌分析工具"><span class="nav-text">▌分析工具</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-数据抓取"><span class="nav-text">2 数据抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#▌网站分析"><span class="nav-text">▌网站分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#▌Scrapy抓取"><span class="nav-text">▌Scrapy抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#items-py"><span class="nav-text">items.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#middles-py"><span class="nav-text">middles.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pipelines-py"><span class="nav-text">pipelines.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#settings-py"><span class="nav-text">settings.py</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#wandou-py"><span class="nav-text">wandou.py</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-数据分析"><span class="nav-text">3 数据分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#▌总体情况"><span class="nav-text">▌总体情况</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#▌分类情况"><span class="nav-text">▌分类情况</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#▌对比酷安"><span class="nav-text">▌对比酷安</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span> <span class="with-love" id="heart"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">高级农民工</span></div><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv">访客数：<span id="busuanzi_value_site_uv"></span>人次</span> <span class="post-meta-divider">|</span> <span id="busuanzi_container_site_pv">总访问量：<span id="busuanzi_value_site_pv"></span>次</span> <span class="post-meta-divider">|</span><div class="theme-info"><div class="powered-by"></div><span class="post-count">全站共：212.3k字</span></div><div style="width:300px;margin:0 auto;padding:10px 0"><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="" style="float:left"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备18144842号</p></a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("yps9pUuWWGeNG1MwVI2tVGys-gzGzoHsz","SNn7AhhHPrCndytXWSTwD5A2")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:12522,el:"wpac-rating",color:"E0943E"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html><!-- rebuild by neat -->