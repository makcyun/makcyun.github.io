<!-- build time:Fri Jun 28 2019 19:49:52 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="google-site-verification" content="s0oCkquJSvsBetEUl3d8nDj5jYzNitxDJALA37MiIyM"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="Python爬虫"><link rel="alternate" href="/atom.xml" title="高级农民工" type="application/atom+xml"><meta name="description" content="发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。"><meta name="keywords" content="Python爬虫"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）"><meta property="og:url" content="https://www.makcyun.top/2018/11/29/web_scraping_withpython10.html"><meta property="og:site_name" content="高级农民工"><meta property="og:description" content="发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://media.makcyun.top/18-11-29/13499186.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-29/57226641.jpg"><meta property="og:image" content="c:/Users/sony/AppData/Roaming/Typora/typora-user-images/1543460157799.png"><meta property="og:image" content="http://media.makcyun.top/18-11-30/64130730.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-30/61796640.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-30/67473511.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-30/67506230.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-30/16708510.jpg"><meta property="og:image" content="http://media.makcyun.top/18-12-1/95375420.jpg"><meta property="og:updated_time" content="2018-12-14T11:11:14.070Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）"><meta name="twitter:description" content="发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。"><meta name="twitter:image" content="http://media.makcyun.top/18-11-29/13499186.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://www.makcyun.top/2018/11/29/web_scraping_withpython10.html"><title>Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇） | 高级农民工</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><meta name="baidu-site-verification" content="E65frtf6P6"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高级农民工</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Beginner's Mind</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首&emsp;&emsp;页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于博主</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归&emsp;&emsp;档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标&emsp;&emsp;签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分&emsp;&emsp;类</a></li><li class="menu-item menu-item-top"><a href="/top/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>最受欢迎</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://www.makcyun.top/2018/11/29/web_scraping_withpython10.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高级农民工"><meta itemprop="description" content=""><meta itemprop="image" content="http://media.makcyun.top/201901230951_146.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高级农民工"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-29T16:16:24+08:00">2018-11-29 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python爬虫/" itemprop="url" rel="index"><span itemprop="name">Python爬虫</span> </a></span></span><span id="/2018/11/29/web_scraping_withpython10.html" class="leancloud_visitors" data-flag-title="Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">热度&#58;</span> <span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">5,059 字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">21 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。</p><a id="more"></a><p><strong>摘要：</strong> 如今移动互联网越来越发达，我们每个人的手机上至少都安装了好几十款 App，随着各式各样的 App 层出不穷，也就产生了优劣之分，而我们肯定愿意去使用那些良心佳软，而如何去发现这些 App 呢，本文使用 Scrapy 框架爬取了著名应用下载市场「酷安网」上的 6000 余款 App，通过分析，发现了各个类别领域下的佼佼者，这些 App 堪称真正的良心心之作，使用它们将会给你带来全新的手机使用体验。</p><h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1. 分析背景"></a>1. 分析背景</h2><h3 id="1-1-为什么选择酷安"><a href="#1-1-为什么选择酷安" class="headerlink" title="1.1. 为什么选择酷安"></a>1.1. 为什么选择酷安</h3><p>如果说 GitHub 是程序员的天堂，那么 <strong>酷安</strong> 则是手机 App 爱好者们（别称「搞机」爱好者）的天堂，相比于那些传统的手机应用下载市场，酷安有三点特别之处：</p><p>第一、可以搜索下载到各种 <strong>神器、佳软</strong>，其他应用下载市场几乎很难找得到。</p><p>比如之前的文章中说过的终端桌面「Aris」、安卓最强阅读器「静读天下」、RSS 阅读器 「Feedme」 等。</p><p>第二、可以找到很多 App 的破解版。</p><p>我们提倡「为好东西付费」，但是有些 App 很蛋疼，比如「百度网盘」，在这里面就可以找到很多 App 的破解版。</p><p>第三、可以找到 App 的历史版本。</p><p>很多人喜欢用最新版本的 App，一有更新就马上升级，但是现在很多 App 越来越功利、越更新越臃肿、广告满天飞，倒不如回归本源，使用体积小巧、功能精简、无广告的早期版本。</p><p>作为一名 App 爱好者，我在酷安上发现了很多不错的 App，越用越感觉自己知道的仅仅是冰山一角，便想扒一扒这个网站上到底有多少好东西，手动一个个去找肯定是不现实了，自然想到最好的方法——用爬虫来解决，为了实现此目的，最近就学习了一下 Scrapy 爬虫框架，爬取了该网 6000 款左右的 App，通过分析，找到了不同领域下的精品 App，下面我们就来一探究竟。</p><h3 id="1-2-分析内容"><a href="#1-2-分析内容" class="headerlink" title="1.2. 分析内容"></a>1.2. 分析内容</h3><ul><li>总体分析 6000 款 App 的评分、下载量、体积等指标。</li><li>根据日常使用功能场景，将 App 划分为：系统工具、资讯阅读、社交娱乐等 10 大类别，筛选出每个类别下的精品 App。</li></ul><h3 id="1-3-分析工具"><a href="#1-3-分析工具" class="headerlink" title="1.3. 分析工具"></a>1.3. 分析工具</h3><ul><li>Python</li><li>Scrapy</li><li>MongoDB</li><li>Pyecharts</li><li>Matplotlib</li></ul><h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2. 数据抓取"></a>2. 数据抓取</h2><p>由于酷安手机端 App 设置了反扒措施，使用 Charles 尝试后发现无法抓包， 暂退而求其次，使用 Scrapy 抓取网页端的 App 信息。抓取时期截止到 2018 年 11 月 23日，共计 6086 款 App，共抓取 了 8 个字段信息：App 名称、下载量、评分、评分人数、评论数、关注人数、体积、App 分类标签。</p><h3 id="2-1-目标网站分析"><a href="#2-1-目标网站分析" class="headerlink" title="2.1. 目标网站分析"></a>2.1. 目标网站分析</h3><p>这是我们要抓取的 <a href="https://www.coolapk.com/apk?p=1" target="_blank" rel="noopener">目标网页</a>，点击翻页可以发现两点有用的信息：</p><ul><li>每页显示了 10 条 App 信息，一共有610页，也就是 6100 个左右的 App 。</li><li>网页请求是 GET 形式，URL 只有一个页数递增参数，构造翻页非常简单。</li></ul><p><img src="http://media.makcyun.top/18-11-29/13499186.jpg" alt=""></p><p>接下来，我们来看看选择抓取哪些信息，可以看到，主页面内显示了 App 名称、下载量、评分等信息，我们再点击 App 图标进入详情页，可以看到提供了更齐全的信息，包括：分类标签、评分人数、关注人数等。由于，我们后续需要对 App 进行分类筛选，故分类标签很有用，所以这里我们选择进入每个 App 主页抓取所需信息指标。</p><p><img src="http://media.makcyun.top/18-11-29/57226641.jpg" alt=""></p><p><img src="C:\Users\sony\AppData\Roaming\Typora\typora-user-images\1543460157799.png" alt=""></p><p>通过上述分析，我们就可以确定抓取流程了，首先遍历主页面 ，抓取 10 个 App 的详情页 URL，然后详情页再抓取每个 App 的指标，如此遍历下来，我们需要抓取 6000 个左右网页内容，抓取工作量不算小，所以，我们接下来尝试使用 Scrapy 框架进行抓取。</p><h3 id="2-2-Scrapy-框架介绍"><a href="#2-2-Scrapy-框架介绍" class="headerlink" title="2.2. Scrapy 框架介绍"></a>2.2. Scrapy 框架介绍</h3><p>介绍 Scrapy 框架之前，我们先回忆一下 Pyspider 框架，之前一篇爬取分析 <a href="https://www.makcyun.top/web_scraping_withpython9.html">虎嗅网</a> 的文章，我们使用了它，它是由国内大神编写的一个爬虫利器， Github Star 超过 10K，但是它的整体功能还是相对单薄一些，还有比它更强大的框架么？有的，就是这里要说的 Scrapy 框架，Github Star 超过 30K，是 Python 爬虫界使用最广泛的爬虫框架，玩爬虫这个框架必须得会。</p><p>网上关于 Scrapy 的官方文档和教程很多，这里罗列几个。</p><blockquote><p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html" target="_blank" rel="noopener">Scrapy 中文文档</a></p><p><a href="https://cloud.tencent.com/developer/column/1108/tag-10756" target="_blank" rel="noopener">崔庆才的 Scrapy 专栏</a></p><p><a href="https://blog.csdn.net/hk2291976/article/details/51405052" target="_blank" rel="noopener">Scrapy 爬拉勾</a></p><p><a href="https://zhuanlan.zhihu.com/p/24769534" target="_blank" rel="noopener">Scrapy 爬豆瓣电影</a></p></blockquote><p>Scrapy 框架相对于 Pyspider 相对要复杂一些，有不同的处理模块，项目文件也由好几个程序组成，不同的爬虫模块需要放在不同的程序中去，所以刚开始入门会觉得程序七零八散，容易把人搞晕，建议采取以下思路快速入门 Scrapy：</p><ul><li><p>首先，快速过一下上面的参考教程，了解 Scrapy 的爬虫逻辑和各程序的用途与配合。</p></li><li><p>接着，看上面两个实操案例，熟悉在 Scrapy 中怎么写爬虫。</p></li><li><p>最后，找个自己感兴趣的网站作为爬虫项目，遇到不懂的就看教程或者 Google。</p></li></ul><p>这样的学习路径是比较快速而有效的，比一直抠教程不动手要好很多。下面，我们就以酷安网为例，用 Scrapy 来爬取一下。</p><h3 id="2-3-抓取数据"><a href="#2-3-抓取数据" class="headerlink" title="2.3. 抓取数据"></a>2.3. 抓取数据</h3><p>首先要安装好 Scrapy 框架，如果是 Windwos 系统，且已经安装了 Anaconda，那么安装 Scrapy 框架就非常简单，只需打开 Anaconda Prompt 命令窗口，输入下面一句命令即可，会自动帮我们安装好 Scrapy 所有需要安装和依赖的库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda pip scrapy</span><br></pre></td></tr></table></figure><h4 id="2-3-1-创建项目"><a href="#2-3-1-创建项目" class="headerlink" title="2.3.1. 创建项目"></a>2.3.1. 创建项目</h4><p>接着，我们需要创建一个爬虫项目，所以我们先从根目录切换到需要放置项目的工作路径，比如我这里设置的存放路径为：E:\my_Python\training\kuan，接着继续输入下面一行代码即可创建 kuan 爬虫项目：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换工作路径</span></span><br><span class="line">e:</span><br><span class="line">cd E:\my_Python\training\kuan</span><br><span class="line"><span class="comment"># 生成项目</span></span><br><span class="line">scrapy startproject kuspider</span><br></pre></td></tr></table></figure><p>执行上面的命令后，就会生成一个名为 kuan 的 scrapy 爬虫项目，包含以下几个文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrapy. cfg <span class="comment"># Scrapy 部署时的配置文件</span></span><br><span class="line">kuan <span class="comment"># 项目的模块，需要从这里引入</span></span><br><span class="line">_init__.py</span><br><span class="line">items.py <span class="comment"># 定义爬取的数据结构</span></span><br><span class="line">middlewares.py <span class="comment"># Middlewares 中间件</span></span><br><span class="line">pipelines.py <span class="comment"># 数据管道文件，可用于后续存储</span></span><br><span class="line">settings.py <span class="comment"># 配置文件</span></span><br><span class="line">spiders <span class="comment"># 爬取主程序文件夹</span></span><br><span class="line">_init_.py</span><br></pre></td></tr></table></figure><p>下面，我们需要再 spiders 文件夹中创建一个爬取主程序：kuan.py，接着运行下面两行命令即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd kuan <span class="comment"># 进入刚才生成的 kuan 项目文件夹</span></span><br><span class="line">scrapy genspider kuan www.coolapk.com  <span class="comment"># 生成爬虫主程序文件 kuan.py</span></span><br></pre></td></tr></table></figure><h4 id="2-3-2-声明-item"><a href="#2-3-2-声明-item" class="headerlink" title="2.3.2. 声明 item"></a>2.3.2. 声明 item</h4><p>项目文件创建好以后，我们就可以开始写爬虫程序了。</p><p>首先，需要在 items.py 文件中，预先定义好要爬取的字段信息名称，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KuanItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line"><span class="comment"># define the fields for your item here like:</span></span><br><span class="line">name = scrapy.Field()</span><br><span class="line">volume = scrapy.Field()</span><br><span class="line">download = scrapy.Field()</span><br><span class="line">follow = scrapy.Field()</span><br><span class="line">comment = scrapy.Field()</span><br><span class="line">tags = scrapy.Field()</span><br><span class="line">score = scrapy.Field()</span><br><span class="line">num_score = scrapy.Field()</span><br></pre></td></tr></table></figure><p>这里的字段信息就是我们前面在网页中定位的 8 个字段信息，包括：name 表示 App 名称、volume 表示体积、download 表示 下载数量。在这里定义好之后，我们在后续的爬取主程序中会利用到这些字段信息。</p><h4 id="2-3-3-爬取主程序"><a href="#2-3-3-爬取主程序" class="headerlink" title="2.3.3. 爬取主程序"></a>2.3.3. 爬取主程序</h4><p>创建好 kuan 项目后，Scrapy 框架会自动生成爬取的部分代码，我们接下来就需要在 parse 方法中增加网页抓取的字段解析内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KuspiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'kuan'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.coolapk.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.coolapk.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>打开主页 Dev Tools，找到每项抓取指标的节点位置，然后可以采用 CSS、Xpath、正则等方法进行提取解析，这些方法 Scrapy 都支持，可随意选择，这里我们选用 CSS 语法来定位节点，不过需要注意的是，Scrapy 的 CSS 语法和之前我们利用 pyquery 使用的 CSS 语法稍有不同，举几个例子，对比说明一下。</p><p><img src="http://media.makcyun.top/18-11-30/64130730.jpg" alt=""></p><p>首先，我们定位到第一个 APP 的主页 URL 节点，可以看到 URL 节点位于 class 属性为 <code>app_left_list</code> 的 div 节点下的 a 节点中，其 href 属性就是我们需要的 URL 信息，这里是相对地址，拼接后就是完整的 URL ：<a href="https://www.coolapk.com/apk/com.coolapk.market" target="_blank" rel="noopener">www.coolapk.com/apk/com.coolapk.market</a>。</p><p>接着我们进入酷安详情页，选择 App 名称并进行定位，可以看到 App 名称节点位于 class 属性为 <code>.detail_app_title</code> 的 p 节点的文本中。</p><p><img src="http://media.makcyun.top/18-11-30/61796640.jpg" alt=""></p><p>定位到这两个节点之后，我们就可以使用 CSS 提取字段信息了，这里对比一下常规写法和 Scrapy 中的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常规写法</span></span><br><span class="line">url = item(<span class="string">'.app_left_list&gt;a'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">name = item(<span class="string">'.list_app_title'</span>).text()</span><br><span class="line"><span class="comment"># Scrapy 写法</span></span><br><span class="line">url = item.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">name = item.css(<span class="string">'.detail_app_title::text'</span>).extract_first()</span><br></pre></td></tr></table></figure><p>可以看到，要获取 href 或者 text 属性，需要用 :: 表示，比如获取 text，则用 ::text。extract_first() 表示提取第一个元素，如果有多个元素，则用 extract() 。接着，我们就可以参照写出 8 个字段信息的解析代码。</p><p>首先，我们需要在主页提取 App 的 URL 列表，然后再进入每个 App 的详情页进一步提取 8 个字段信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        url = content.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">        url = response.urljoin(url)  <span class="comment"># 拼接相对 url 为绝对 url</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse_url)</span><br></pre></td></tr></table></figure><p>这里，利用 response.urljoin() 方法将提取出的相对 URL 拼接为完整的 URL，然后利用 scrapy.Request() 方法构造每个 App 详情页的请求，这里我们传递两个参数：url 和 callback，url 为详情页 URL，callback 是回调函数，它将主页 URL 请求返回的响应 response 传给专门用来解析字段内容的 parse_url() 方法，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    item = KuanItem()</span><br><span class="line">    item[<span class="string">'name'</span>] = response.css(<span class="string">'.detail_app_title::text'</span>).extract_first()</span><br><span class="line">    results = self.get_comment(response)</span><br><span class="line">    item[<span class="string">'volume'</span>] = results[<span class="number">0</span>]</span><br><span class="line">    item[<span class="string">'download'</span>] = results[<span class="number">1</span>]</span><br><span class="line">    item[<span class="string">'follow'</span>] = results[<span class="number">2</span>]</span><br><span class="line">    item[<span class="string">'comment'</span>] = results[<span class="number">3</span>]</span><br><span class="line">    item[<span class="string">'tags'</span>] = self.get_tags(response)</span><br><span class="line">    item[<span class="string">'score'</span>] = response.css(<span class="string">'.rank_num::text'</span>).extract_first()</span><br><span class="line">    num_score = response.css(<span class="string">'.apk_rank_p1::text'</span>).extract_first()</span><br><span class="line">    item[<span class="string">'num_score'</span>] = re.search(<span class="string">'共(.*?)个评分'</span>,num_score).group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">yield</span> item</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_comment</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    messages = response.css(<span class="string">'.apk_topba_message::text'</span>).extract_first()</span><br><span class="line">    result = re.findall(<span class="string">r'\s+(.*?)\s+/\s+(.*?)下载\s+/\s+(.*?)人关注\s+/\s+(.*?)个评论.*?'</span>,messages)  <span class="comment"># \s+ 表示匹配任意空白字符一次以上</span></span><br><span class="line">    <span class="keyword">if</span> result: <span class="comment"># 不为空</span></span><br><span class="line">        results = list(result[<span class="number">0</span>]) <span class="comment"># 提取出list 中第一个元素</span></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tags</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    data = response.css(<span class="string">'.apk_left_span2'</span>)</span><br><span class="line">    tags = [item.css(<span class="string">'::text'</span>).extract_first() <span class="keyword">for</span> item <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">return</span> tags</span><br></pre></td></tr></table></figure><p>这里，单独定义了 get_comment() 和 get_tags() 两个方法.</p><p>get_comment() 方法通过正则匹配提取 volume、download、follow、comment 四个字段信息，正则匹配结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">result = re.findall(<span class="string">r'\s+(.*?)\s+/\s+(.*?)下载\s+/\s+(.*?)人关注\s+/\s+(.*?)个评论.*?'</span>,messages)</span><br><span class="line">print(result) <span class="comment"># 输出第一页的结果信息</span></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">[(<span class="string">'21.74M'</span>, <span class="string">'5218万'</span>, <span class="string">'2.4万'</span>, <span class="string">'5.4万'</span>)]</span><br><span class="line">[(<span class="string">'75.53M'</span>, <span class="string">'2768万'</span>, <span class="string">'2.3万'</span>, <span class="string">'3.0万'</span>)]</span><br><span class="line">[(<span class="string">'46.21M'</span>, <span class="string">'1686万'</span>, <span class="string">'2.3万'</span>, <span class="string">'3.4万'</span>)]</span><br><span class="line">[(<span class="string">'54.77M'</span>, <span class="string">'1603万'</span>, <span class="string">'3.8万'</span>, <span class="string">'4.9万'</span>)]</span><br><span class="line">[(<span class="string">'3.32M'</span>, <span class="string">'1530万'</span>, <span class="string">'1.5万'</span>, <span class="string">'3343'</span>)]</span><br><span class="line">[(<span class="string">'75.07M'</span>, <span class="string">'1127万'</span>, <span class="string">'1.6万'</span>, <span class="string">'2.2万'</span>)]</span><br><span class="line">[(<span class="string">'92.70M'</span>, <span class="string">'1108万'</span>, <span class="string">'9167'</span>, <span class="string">'1.3万'</span>)]</span><br><span class="line">[(<span class="string">'68.94M'</span>, <span class="string">'1072万'</span>, <span class="string">'5718'</span>, <span class="string">'9869'</span>)]</span><br><span class="line">[(<span class="string">'61.45M'</span>, <span class="string">'935万'</span>, <span class="string">'1.1万'</span>, <span class="string">'1.6万'</span>)]</span><br><span class="line">[(<span class="string">'23.96M'</span>, <span class="string">'925万'</span>, <span class="string">'4157'</span>, <span class="string">'1956'</span>)]</span><br></pre></td></tr></table></figure><p>然后利用 result[0]、result[1] 等分别提取出四项信息，以 volume 为例，输出第一页的提取结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">item[<span class="string">'volume'</span>] = results[<span class="number">0</span>]</span><br><span class="line">print(item[<span class="string">'volume'</span>])</span><br><span class="line"><span class="number">21.74</span>M</span><br><span class="line"><span class="number">75.53</span>M</span><br><span class="line"><span class="number">46.21</span>M</span><br><span class="line"><span class="number">54.77</span>M</span><br><span class="line"><span class="number">3.32</span>M</span><br><span class="line"><span class="number">75.07</span>M</span><br><span class="line"><span class="number">92.70</span>M</span><br><span class="line"><span class="number">68.94</span>M</span><br><span class="line"><span class="number">61.45</span>M</span><br><span class="line"><span class="number">23.96</span>M</span><br></pre></td></tr></table></figure><p>这样一来，第一页 10 款 App 的所有字段信息都被成功提取出来，然后返回到 yied item 生成器中，我们输出一下它的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'酷安'</span>, <span class="string">'volume'</span>: <span class="string">'21.74M'</span>, <span class="string">'download'</span>: <span class="string">'5218万'</span>, <span class="string">'follow'</span>: <span class="string">'2.4万'</span>, <span class="string">'comment'</span>: <span class="string">'5.4万'</span>, <span class="string">'tags'</span>: <span class="string">"['酷市场', '酷安', '市场', 'coolapk', '装机必备']"</span>, <span class="string">'score'</span>: <span class="string">'4.4'</span>, <span class="string">'num_score'</span>: <span class="string">'1.4万'</span>&#125;, </span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'微信'</span>, <span class="string">'volume'</span>: <span class="string">'75.53M'</span>, <span class="string">'download'</span>: <span class="string">'2768万'</span>, <span class="string">'follow'</span>: <span class="string">'2.3万'</span>, <span class="string">'comment'</span>: <span class="string">'3.0万'</span>, <span class="string">'tags'</span>: <span class="string">"['微信', 'qq', '腾讯', 'tencent', '即时聊天', '装机必备']"</span>,<span class="string">'score'</span>: <span class="string">'2.3'</span>, <span class="string">'num_score'</span>: <span class="string">'1.1万'</span>&#125;,</span><br><span class="line">...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="2-3-4-分页爬取"><a href="#2-3-4-分页爬取" class="headerlink" title="2.3.4. 分页爬取"></a>2.3.4. 分页爬取</h4><p>以上，我们爬取了第一页内容，接下去需要遍历爬取全部 610 页的内容，这里有两种思路：</p><ul><li>第一种是提取翻页的节点信息，然后构造出下一页的请求，然后重复调用 parse 方法进行解析，如此循环往复，直到解析完最后一页。</li><li>第二种是先直接构造出 610 页的 URL 地址，然后批量调用 parse 方法进行解析。</li></ul><p>这里，我们分别写出两种方法的解析代码。</p><p>第一种方法很简单，直接接着 parse 方法继续添加以下几行代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        ...</span><br><span class="line">    </span><br><span class="line">    next_page = response.css(<span class="string">'.pagination li:nth-child(8) a::attr(href)'</span>).extract_first()</span><br><span class="line">    url = response.urljoin(next_page)</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse )</span><br></pre></td></tr></table></figure><p>第二种方法，我们在最开头的 parse() 方法前，定义一个 start_requests() 方法，用来批量生成 610 页的 URL，然后通过 scrapy.Request() 方法中的 callback 参数，传递给下面的 parse() 方法进行解析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        pages = []</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">610</span>):  <span class="comment"># 一共有610页</span></span><br><span class="line">            url = <span class="string">'https://www.coolapk.com/apk/?page=%s'</span>%page</span><br><span class="line">            page =  scrapy.Request(url,callback=self.parse)</span><br><span class="line">            pages.append(page)</span><br><span class="line">        <span class="keyword">return</span> pages</span><br></pre></td></tr></table></figure><p>以上就是全部页面的爬取思路，爬取成功后，我们需要存储下来。这里，我面选择存储到 MongoDB 中，不得不说，相比 MySQL，MongoDB 要方便省事很多。</p><h4 id="2-3-5-存储结果"><a href="#2-3-5-存储结果" class="headerlink" title="2.3.5. 存储结果"></a>2.3.5. 存储结果</h4><p>我们在 pipelines.py 程序中，定义数据存储方法，MongoDB 的一些参数，比如地址和数据库名称，需单独存放在 settings.py 设置文件中去，然后在 pipelines 程序中进行调用即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,mongo_url,mongo_db)</span>:</span></span><br><span class="line">        self.mongo_url = mongo_url</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_url = crawler.settings.get(<span class="string">'MONGO_URL'</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_url)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        self.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure><p>首先，我们定义一个 MongoPipeline(）存储类，里面定义了几个方法，简单进行一下说明：</p><p>from crawler() 是一个类方法，用 ＠class method 标识，这个方法的作用主要是用来获取我们在 settings.py 中设置的这几项参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'KuAn'</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'kuan.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>open_spider() 方法主要进行一些初始化操作 ，在 Spider 开启时，这个方法就会被调用 。</p><p>process_item() 方法是最重要的方法，实现插入数据到 MongoDB 中。</p><p><img src="http://media.makcyun.top/18-11-30/67473511.jpg" alt=""></p><p>完成上述代码以后，输入下面一行命令就可以开始整个爬虫的抓取和存储过程了，单机跑的话，6000 个网页需要不少时间才能完成，保持耐心。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl kuan</span><br></pre></td></tr></table></figure><p>这里，还有两点补充：</p><p>第一，<strong>为了减轻网站压力，我们最好在每个请求之间设置几秒延时</strong>，可以在 KuspiderSpider() 方法开头出，加入以下几行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"DOWNLOAD_DELAY"</span>: <span class="number">3</span>, <span class="comment"># 延迟3s,默认是0，即不延迟</span></span><br><span class="line">        <span class="string">"CONCURRENT_REQUESTS_PER_DOMAIN"</span>: <span class="number">8</span> <span class="comment"># 每秒默认并发8次，可适当降低</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>第二，为了更好监控爬虫程序运行，有必要 <strong>设置输出日志文件</strong>，可以通过 Python 自带的 logging 包实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(filename=<span class="string">'kuan.log'</span>,filemode=<span class="string">'w'</span>,level=logging.WARNING,format=<span class="string">'%(asctime)s %(message)s'</span>,datefmt=<span class="string">'%Y/%m/%d %I:%M:%S %p'</span>)</span><br><span class="line">logging.warning(<span class="string">"warn message"</span>)</span><br><span class="line">logging.error(<span class="string">"error message"</span>)</span><br></pre></td></tr></table></figure><p>这里的 level 参数表示警告级别，严重程度从低到高分别是：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，如果想日志文件不要记录太多内容，可以设置高一点的级别，这里设置为 WARNING，意味着只有 WARNING 级别以上的信息才会输出到日志中去。</p><p>添加 datefmt 参数是为了在每条日志前面加具体的时间，这点很有用处。</p><p><img src="http://media.makcyun.top/18-11-30/67506230.jpg" alt=""></p><p>以上，我们就完成了整个数据的抓取，有了数据我们就可以着手进行分析，不过这之前还需简单地对数据做一下清洗和处理。</p><h2 id="3-数据清洗处理"><a href="#3-数据清洗处理" class="headerlink" title="3. 数据清洗处理"></a>3. 数据清洗处理</h2><p>首先，我们从 MongoDB 中读取数据并转化为 DataFrame，然后查看一下数据的基本情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_kuan</span><span class="params">()</span>:</span></span><br><span class="line">    client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line">    db = client[<span class="string">'KuAn'</span>]</span><br><span class="line">    collection = db[<span class="string">'KuAnItem'</span>]</span><br><span class="line">    <span class="comment"># 将数据库数据转为DataFrame</span></span><br><span class="line">    data = pd.DataFrame(list(collection.find()))</span><br><span class="line">    print(data.head())</span><br><span class="line">    print(df.shape)</span><br><span class="line">    print(df.info())</span><br><span class="line">    print(df.describe())</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-11-30/16708510.jpg" alt=""></p><p>从 data.head() 输出的前 5 行数据中可以看到，除了 score 列是 float 格式以外，其他列都是 object 文本类型。</p><p>comment、download、follow、num_score 这 5 列数据中部分行带有「万」字后缀，需要将字符去掉再转换为数值型；volume 体积列，则分别带有「M」和「K」后缀，为了统一大小，则需将「K」除以 1024，转换为 「M」体积。</p><p>整个数据一共有 6086 行 x 8 列，每列均没有缺失值。</p><p>df.describe() 方法对 score 列做了基本统计，可以看到，所有 App 的平均得分是 3.9 分（5 分制），最低得分 1.6 分，最高得分 4.8 分。</p><p>下面，我们将以上几列文本型数据转换为数值型数据，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_processing</span><span class="params">(df)</span>:</span></span><br><span class="line"><span class="comment">#处理'comment','download','follow','num_score','volume' 5列数据，将单位万转换为单位1，再转换为数值型</span></span><br><span class="line">    str = <span class="string">'_ori'</span></span><br><span class="line">    cols = [<span class="string">'comment'</span>,<span class="string">'download'</span>,<span class="string">'follow'</span>,<span class="string">'num_score'</span>,<span class="string">'volume'</span>]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        colori = col+str</span><br><span class="line">        df[colori] = df[col] <span class="comment"># 复制保留原始列</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (col == <span class="string">'volume'</span>):</span><br><span class="line">            df[col] = clean_symbol(df,col)<span class="comment"># 处理原始列生成新列</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = clean_symbol2(df,col)<span class="comment"># 处理原始列生成新列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将download单独转换为万单位</span></span><br><span class="line">    df[<span class="string">'download'</span>] = df[<span class="string">'download'</span>].apply(<span class="keyword">lambda</span> x:x/<span class="number">10000</span>)</span><br><span class="line">    <span class="comment"># 批量转为数值型</span></span><br><span class="line">    df = df.apply(pd.to_numeric,errors=<span class="string">'ignore'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_symbol</span><span class="params">(df,col)</span>:</span></span><br><span class="line">    <span class="comment"># 将字符“万”替换为空</span></span><br><span class="line">    con = df[col].str.contains(<span class="string">'万$'</span>)</span><br><span class="line">    df.loc[con,col] = pd.to_numeric(df.loc[con,col].str.replace(<span class="string">'万'</span>,<span class="string">''</span>)) * <span class="number">10000</span></span><br><span class="line">    df[col] = pd.to_numeric(df[col])</span><br><span class="line">    <span class="keyword">return</span> df[col]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_symbol2</span><span class="params">(df,col)</span>:</span></span><br><span class="line">    <span class="comment"># 字符M替换为空</span></span><br><span class="line">    df[col] = df[col].str.replace(<span class="string">'M$'</span>,<span class="string">''</span>)</span><br><span class="line">    <span class="comment"># 体积为K的除以 1024 转换为M</span></span><br><span class="line">    con = df[col].str.contains(<span class="string">'K$'</span>)</span><br><span class="line">    df.loc[con,col] = pd.to_numeric(df.loc[con,col].str.replace(<span class="string">'K$'</span>,<span class="string">''</span>))/<span class="number">1024</span></span><br><span class="line">    df[col] = pd.to_numeric(df[col])</span><br><span class="line">    <span class="keyword">return</span> df[col]</span><br></pre></td></tr></table></figure><p>以上，就完成了几列文本型数据的转换，我们再来查看一下基本情况：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">comment</th><th style="text-align:center">download</th><th style="text-align:center">follow</th><th style="text-align:center">num_score</th><th style="text-align:center">score</th><th style="text-align:center">volume</th></tr></thead><tbody><tr><td style="text-align:center">count</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td></tr><tr><td style="text-align:center">mean</td><td style="text-align:center">255.5</td><td style="text-align:center">13.7</td><td style="text-align:center">729.3</td><td style="text-align:center">133.1</td><td style="text-align:center">3.9</td><td style="text-align:center">17.7</td></tr><tr><td style="text-align:center">std</td><td style="text-align:center">1437.3</td><td style="text-align:center">98</td><td style="text-align:center">1893.7</td><td style="text-align:center">595.4</td><td style="text-align:center">0.6</td><td style="text-align:center">20.6</td></tr><tr><td style="text-align:center">min</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1.6</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">25%</td><td style="text-align:center">16</td><td style="text-align:center">0.2</td><td style="text-align:center">65</td><td style="text-align:center">5.2</td><td style="text-align:center">3.7</td><td style="text-align:center">3.5</td></tr><tr><td style="text-align:center">50%</td><td style="text-align:center">38</td><td style="text-align:center">0.8</td><td style="text-align:center">180</td><td style="text-align:center">17</td><td style="text-align:center">4</td><td style="text-align:center">10.8</td></tr><tr><td style="text-align:center">75%</td><td style="text-align:center">119</td><td style="text-align:center">4.5</td><td style="text-align:center">573.8</td><td style="text-align:center">68</td><td style="text-align:center">4.3</td><td style="text-align:center">25.3</td></tr><tr><td style="text-align:center">max</td><td style="text-align:center">53000</td><td style="text-align:center">5190</td><td style="text-align:center">38000</td><td style="text-align:center">17000</td><td style="text-align:center">4.8</td><td style="text-align:center">294.2</td></tr></tbody></table><p>从中可以看出以下几点信息：</p><ul><li>download 列为 App 下载数量，下载量最多的 App 有 5190 万次，最少的为 0 (很少很少)，平均下载次数为 14 万次；</li><li>volume 列为 App 体积，体积最大的 App 达到近 300M，体积最小的几乎为 0，平均体积在 18M 左右。</li><li>comment 列为 App 评分，评分数最多的达到了 5 万多条，平均有 200 多条。</li></ul><p>以上，就完成了基本的数据清洗处理过程，下面一篇文章我们将对数据进行探索性分析。</p><h2 id="4-资源获取"><a href="#4-资源获取" class="headerlink" title="4. 资源获取"></a>4. 资源获取</h2><p>如需完整代码可以搜索加入我的「<strong>知识星球：第2脑袋</strong>」获取，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">pyspider 爬取并分析虎嗅网 5 万篇文章</a></p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>你一打赏，我就写得更来劲了</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.png" alt="高级农民工 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python爬虫/" rel="tag"><i class="fa fa-tag"></i> Python爬虫</a></div><div class="post-widgets"><div class="wp_rating"><div style="color:rgba(0,0,0,.75);font-size:13px;letter-spacing:3px">(&gt;你觉得这篇文章怎么样？&lt;)</div><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/12/02/data_analysis&mining02.html" rel="prev" title="Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（分析篇）"><i class="fa fa-chevron-left"></i> Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（分析篇）</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/11/24/weekly_sharing6.html" rel="next" title="有了它，你手机上的新闻资讯类 App 都可以卸了">有了它，你手机上的新闻资讯类 App 都可以卸了 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="http://media.makcyun.top/201901230951_146.jpg" alt="高级农民工"><p class="site-author-name" itemprop="name">高级农民工</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">96</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">43</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/makcyun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:johnny824lee@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-分析背景"><span class="nav-text">1. 分析背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-为什么选择酷安"><span class="nav-text">1.1. 为什么选择酷安</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-分析内容"><span class="nav-text">1.2. 分析内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-分析工具"><span class="nav-text">1.3. 分析工具</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-数据抓取"><span class="nav-text">2. 数据抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-目标网站分析"><span class="nav-text">2.1. 目标网站分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Scrapy-框架介绍"><span class="nav-text">2.2. Scrapy 框架介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-抓取数据"><span class="nav-text">2.3. 抓取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-创建项目"><span class="nav-text">2.3.1. 创建项目</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-声明-item"><span class="nav-text">2.3.2. 声明 item</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-爬取主程序"><span class="nav-text">2.3.3. 爬取主程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-4-分页爬取"><span class="nav-text">2.3.4. 分页爬取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-5-存储结果"><span class="nav-text">2.3.5. 存储结果</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-数据清洗处理"><span class="nav-text">3. 数据清洗处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-资源获取"><span class="nav-text">4. 资源获取</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span> <span class="with-love" id="heart"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">高级农民工</span></div><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv">访客数：<span id="busuanzi_value_site_uv"></span>人次</span> <span class="post-meta-divider">|</span> <span id="busuanzi_container_site_pv">总访问量：<span id="busuanzi_value_site_pv"></span>次</span> <span class="post-meta-divider">|</span><div class="theme-info"><div class="powered-by"></div><span class="post-count">全站共：211.8k字</span></div><div style="width:300px;margin:0 auto;padding:10px 0"><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="" style="float:left"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备18144842号</p></a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("yps9pUuWWGeNG1MwVI2tVGys-gzGzoHsz","SNn7AhhHPrCndytXWSTwD5A2")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:12522,el:"wpac-rating",color:"E0943E"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html><!-- rebuild by neat -->