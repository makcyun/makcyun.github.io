<!-- build time:Wed Jun 05 2019 16:36:06 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="google-site-verification" content="s0oCkquJSvsBetEUl3d8nDj5jYzNitxDJALA37MiIyM"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="Python爬虫,虎嗅网"><link rel="alternate" href="/atom.xml" title="高级农民工" type="application/atom+xml"><meta name="description" content="数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。"><meta name="keywords" content="Python爬虫,虎嗅网"><meta property="og:type" content="article"><meta property="og:title" content="pyspider 爬取并分析虎嗅网 5 万篇文章"><meta property="og:url" content="https://www.makcyun.top/2018/11/04/web_scraping_withpython9.html"><meta property="og:site_name" content="高级农民工"><meta property="og:description" content="数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://media.makcyun.top/18-11-4/22792947.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-4/38979674.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-5/33467436.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-5/80730129.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-5/65831925.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-5/51865737.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-5/80480010.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-5/13729703.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-6/80607479.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-6/36910200.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-6/47981964.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-8/86110562.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-8/9931236.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-7/59717401.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-6/85687626.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-6/93399033.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-8/44309942.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-8/24683879.jpg"><meta property="og:image" content="http://media.makcyun.top/18-11-7/78186420.jpg"><meta property="og:image" content="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg"><meta property="og:updated_time" content="2019-03-07T00:21:04.975Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="pyspider 爬取并分析虎嗅网 5 万篇文章"><meta name="twitter:description" content="数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。"><meta name="twitter:image" content="http://media.makcyun.top/18-11-4/22792947.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://www.makcyun.top/2018/11/04/web_scraping_withpython9.html"><title>pyspider 爬取并分析虎嗅网 5 万篇文章 | 高级农民工</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><meta name="baidu-site-verification" content="E65frtf6P6"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">高级农民工</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Beginner's Mind</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首&emsp;&emsp;页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于博主</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归&emsp;&emsp;档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标&emsp;&emsp;签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分&emsp;&emsp;类</a></li><li class="menu-item menu-item-top"><a href="/top/" rel="section"><i class="menu-item-icon fa fa-fw fa-signal"></i><br>最受欢迎</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>站内搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://www.makcyun.top/2018/11/04/web_scraping_withpython9.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="高级农民工"><meta itemprop="description" content=""><meta itemprop="image" content="http://media.makcyun.top/201901230951_146.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="高级农民工"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">pyspider 爬取并分析虎嗅网 5 万篇文章</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-04T16:16:24+08:00">2018-11-04 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python爬虫-Python分析-Python可视化/" itemprop="url" rel="index"><span itemprop="name">Python爬虫,Python分析,Python可视化</span> </a></span></span><span id="/2018/11/04/web_scraping_withpython9.html" class="leancloud_visitors" data-flag-title="pyspider 爬取并分析虎嗅网 5 万篇文章"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">热度&#58;</span> <span class="leancloud-visitors-count"></span> <span>℃</span></span><div class="post-wordcount"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">9,185 字 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">36 分钟</span></div></div></header><div class="post-body" itemprop="articleBody"><p>数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。</p><a id="more"></a><p><strong>摘要：</strong> 不少时候，一篇文章能否得到广泛的传播，除了文章本身实打实的质量以外，一个好的标题也至关重要。本文爬取了虎嗅网建站至今共 5 万条新闻标题内容，助你找到起文章标题的技巧与灵感。同时，分享一些值得关注的文章和作者。</p><h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1. 分析背景"></a>1. 分析背景</h2><h3 id="1-1-为什么选择虎嗅"><a href="#1-1-为什么选择虎嗅" class="headerlink" title="1.1. 为什么选择虎嗅"></a>1.1. 为什么选择虎嗅</h3><p>在众多新媒体网站中，「虎嗅」网的文章内容和质量还算不错。在「新榜」科技类公众号排名中，它位居榜单第 3 名，还是比较受欢迎的。所以选择爬取该网站的文章信息，顺便从中了解一下这几年科技互联网都出现了哪些热点信息。</p><p><img src="http://media.makcyun.top/18-11-4/22792947.jpg" alt=""></p><blockquote><p>「关于虎嗅」</p><p>虎嗅网创办于 2012 年 5 月，是一个聚合优质创新信息与人群的新媒体平台。该平台专注于贡献原创、深度、犀利优质的商业资讯，围绕创新创业的观点进行剖析与交流。虎嗅网的核心，是关注互联网及传统产业的融合、明星公司的起落轨迹、产业潮汐的动力与趋势。</p></blockquote><h3 id="1-2-分析内容"><a href="#1-2-分析内容" class="headerlink" title="1.2. 分析内容"></a>1.2. 分析内容</h3><ul><li>分析虎嗅网 5 万篇文章的基本情况，包括收藏数、评论数等</li><li>发掘最受欢迎和最不受欢迎的文章及作者</li><li>分析文章标题形式（长度、句式）与受欢迎程度之间的关系</li><li>展现近些年科技互联网行业的热门词汇</li></ul><h3 id="1-3-分析工具"><a href="#1-3-分析工具" class="headerlink" title="1.3. 分析工具"></a>1.3. 分析工具</h3><ul><li>Python</li><li>pyspider</li><li>MongoDB</li><li>Matplotlib</li><li>WordCloud</li><li>Jieba</li></ul><h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2. 数据抓取"></a>2. 数据抓取</h2><p>使用 pyspider 抓取了虎嗅网的主页文章，文章抓取时期为 2012 年建站至 2018 年 11 月 1 日，共计约 5 万篇文章。抓取 了 7 个字段信息：文章标题、作者、发文时间、评论数、收藏数、摘要和文章链接。</p><h3 id="2-1-目标网站分析"><a href="#2-1-目标网站分析" class="headerlink" title="2.1. 目标网站分析"></a>2.1. 目标网站分析</h3><p>这是要爬取的 <a href="https://www.huxiu.com/" target="_blank" rel="noopener">网页界面</a>，可以看到是通过 AJAX 加载的。</p><p><img src="http://media.makcyun.top/18-11-4/38979674.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-11-5/33467436.jpg" alt=""></p><p>右键打开开发者工具查看翻页规律，可以看到 URL 请求是 POST 类型，下拉到底部查看 Form Data，表单需提交参数只有 3 项。经尝试， 只提交 page 参数就能成功获取页面的信息，其他两项参数无关紧要，所以构造分页爬取非常简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huxiu_hash_code: <span class="number">39</span>bcd9c3fe9bc69a6b682343ee3f024a</span><br><span class="line">page: <span class="number">4</span></span><br><span class="line">last_dateline: <span class="number">1541123160</span></span><br></pre></td></tr></table></figure><p>接着，切换选项卡到 Preview 和 Response 查看网页内容，可以看到数据都位于 data 字段里。total_page 为 2004，表示一共有 2004 页的文章内容，每一页有 25 篇文章，总共约 5 万篇，也就是我们要爬取的数量。</p><p><img src="http://media.makcyun.top/18-11-5/80730129.jpg" alt=""></p><p>以上，我们就找到了所需内容，接下来可以开始构造爬虫，整个爬取思路比较简单。之前我们也练习过这一类 Ajax 文章的爬取，可以参考：</p><p><a href="https://www.makcyun.top/web_scraping_withpython4.html">抓取澎湃网建站至今 1500 期信息图栏目图片</a></p><h3 id="2-2-pyspider-介绍"><a href="#2-2-pyspider-介绍" class="headerlink" title="2.2. pyspider 介绍"></a>2.2. pyspider 介绍</h3><p>和之前文章不同的是，这里我们使用一种新的工具来进行爬取，叫做：pyspider 框架。由国人 binux 大神开发，GitHub Star 数超过 12 K，足以证明它的知名度。可以说，学习爬虫不能不会使用这个框架。</p><p>网上关于这个框架的介绍和实操案例非常多，这里仅简单介绍一下。</p><p>我们之前的爬虫都是在 Sublime 、PyCharm 这种 IDE 窗口中执行的，整个爬取过程可以说是处在黑箱中，内部运行的些细节并不太清楚。而 pyspider 一大亮点就在于提供了一个可视化的 WebUI 界面，能够清楚地查看爬虫的运行情况。</p><p><img src="http://media.makcyun.top/18-11-5/65831925.jpg" alt=""></p><p>pyspider 的架构主要分为 Scheduler(调度器)、Fetcher(抓取器)、Processer(处理器)三个部分。Monitor(监控器)对整个爬取过程进行监控，Result Worker(结果处理器)处理最后抓取的结果。</p><p><img src="http://media.makcyun.top/18-11-5/51865737.jpg" alt=""></p><p>我们看看该框架的运行流程大致是怎么样的：</p><ul><li>一个 pyppider 爬虫项目对应一个 Python 脚本，脚本里定义了一个 Handler 主类。爬取时首先调用 on_start() 方法生成最初的抓取任务，然后发送给 Scheduler。</li><li>Scheduler 将抓取任务分发给 Fetcher 进行抓取，Fetcher 执行然后得到 Response、随后将 Response 发送给 Processer。</li><li>Processer 处理响应并提取出新的 URL 然后生成新的抓取任务，然后通过消息队列的方式通知 Scheduler 当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待 Result Worker 处理。</li><li>Scheduler 接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回 Fetcher 进行抓取。</li><li>不断重复以上工作、直到所有的任务都执行完毕，抓取结束。</li><li>抓取结束后、程序会回调 on_finished() 方法，这里可以定义后处理过程。</li></ul><p>该框架比较容易上手，网页右边是代码区，先定义类（Class）然后在里面添加爬虫的各种方法（也可以称为函数），运行的过程会在左上方显示，左下方则是输出结果的区域。</p><p>这里，分享几个不错的教程以供参考：</p><p>GitHub 项目地址：<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a></p><p>官方主页：<a href="http://docs.pyspider.org/en/latest/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/</a></p><p>pyspider 中文网：<a href="http://www.pyspider.cn/page/1.html" target="_blank" rel="noopener">http://www.pyspider.cn/page/1.html</a></p><p>pyspider 爬虫原理剖析：<a href="http://python.jobbole.com/81109/" target="_blank" rel="noopener">http://python.jobbole.com/81109/</a></p><p>pyspider 爬淘宝图案例实操：<a href="https://cuiqingcai.com/2652.html" target="_blank" rel="noopener">https://cuiqingcai.com/2652.html</a></p><p>安装好该框架后，下面我们可以就开始爬取了。</p><h3 id="2-3-抓取数据"><a href="#2-3-抓取数据" class="headerlink" title="2.3. 抓取数据"></a>2.3. 抓取数据</h3><p>CMD 命令窗口执行：pyspider all 命令，然后浏览器输入：<a href="http://localhost:5000/" target="_blank" rel="noopener">http://localhost:5000/</a> 就可以启动 pyspider 。</p><p>点击 Create 新建一个项目，Project Name 命名为：huxiu，因为要爬取的 URL 是 POST 类型，所以这里可以先不填写，之后可以在代码中添加，再次点击 Creat 便完成了该项目的新建。</p><p><img src="http://media.makcyun.top/18-11-5/80480010.jpg" alt=""></p><p>新项目建立好后会自动生成一部分模板代码，我们只需在此基础上进行修改和完善，然后就可以运行爬虫项目了。现在，简单梳理下代码编写步骤。</p><p><img src="http://media.makcyun.top/18-11-5/13729703.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config:&#123;</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span></span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">3</span>): <span class="comment"># 先循环1页</span></span><br><span class="line">            print(<span class="string">'正在爬取第 %s 页'</span> % page)</span><br><span class="line">            self.crawl(<span class="string">'https://www.huxiu.com/v2_action/article_list'</span>,method=<span class="string">'POST'</span>,data=&#123;<span class="string">'page'</span>:page&#125;, callback=self.index_page)</span><br></pre></td></tr></table></figure><p>这里，首先定义了一个 Handler 主类，整个爬虫项目都主要在该类下完成。 接着，可以将爬虫基本的一些基本配置，比如 Headers、代理等设置写在下面的 crawl_config 属性中。（如果你还没有习惯从函数（def）转换到类（Class）的代码写法，那么需要先了解一下类的相关知识，之后我也会单独用一篇文章介绍一下。）</p><p>下面的 on_start() 方法是程序的入口，也就是说程序启动后会首先从这里开始运行。首先，我们将要爬取的 URL传入 crawl() 方法，同时将 URL 修改成虎嗅网的：<a href="https://www.huxiu.com/v2_action/article_list。由于" target="_blank" rel="noopener">https://www.huxiu.com/v2_action/article_list。由于</a> URL 是 POST 请求，所以我们还需要增加两个参数：method 和 data。method 表示 HTTP 请求方式，默认是 GET，这里我们需要设置为 POST；data 是 POST 请求表单参数，只需要添加一个 page 参数即可。</p><p>接着，通过 callback 参数定义一个 index_page() 方法，用来解析 crawl() 方法爬取 URL 成功后返回的 Response 响应。在后面的 index_page() 方法中，可以使用 PyQuery 提取响应中的所需内容。具体提取方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        content = response.json[<span class="string">'data'</span>]</span><br><span class="line">        <span class="comment"># 注意，在sublime中，json后面需要添加()，pyspider 中则不用</span></span><br><span class="line">        doc = pq(content)</span><br><span class="line">        lis = doc(<span class="string">'.mod-art'</span>).items()</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'title'</span>: item(<span class="string">'.msubstr-row2'</span>).text(),</span><br><span class="line">            <span class="string">'url'</span>:<span class="string">'https://www.huxiu.com'</span>+ str(item(<span class="string">'.msubstr-row2'</span>).attr(<span class="string">'href'</span>)),</span><br><span class="line">            <span class="string">'name'</span>: item(<span class="string">'.author-name'</span>).text(),</span><br><span class="line">            <span class="string">'write_time'</span>:item(<span class="string">'.time'</span>).text(),</span><br><span class="line">            <span class="string">'comment'</span>:item(<span class="string">'.icon-cmt+ em'</span>).text(),</span><br><span class="line">            <span class="string">'favorites'</span>:item(<span class="string">'.icon-fvr+ em'</span>).text(),</span><br><span class="line">            <span class="string">'abstract'</span>:item(<span class="string">'.mob-sub'</span>).text()</span><br><span class="line">            &#125; <span class="keyword">for</span> item <span class="keyword">in</span> lis ]   <span class="comment"># 列表生成式结果返回每页提取出25条字典信息构成的list</span></span><br><span class="line">        print(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>这里，网页返回的 Response 是 json 格式，待提取的信息存放在其中的 data 键值中，由一段 HTML 代码构成。我们可以使用 response.json[‘data’] 获取该 HTML 信息，接着使用 PyQuery 搭配 CSS 语法提取出文章标题、链接、作者等所需信息。这里使用了列表生成式，能够精简代码并且转换为方便的 list 格式，便于后续存储到 MongoDB 中。我们输出并查看一下第 2 页的提取结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由25个 dict 构成的 list</span></span><br><span class="line">[&#123;<span class="string">'title'</span>: <span class="string">'想要长生不老？杀死体内的“僵尸细胞”吧'</span>, <span class="string">'url'</span>: <span class="string">'https://www.huxiu.com/article/270086.html'</span>, <span class="string">'name'</span>: <span class="string">'造就Talk'</span>, <span class="string">'write_time'</span>: <span class="string">'19小时前'</span>, <span class="string">'comment'</span>: <span class="string">'4'</span>, <span class="string">'favorites'</span>: <span class="string">'28'</span>, <span class="string">'abstract'</span>: <span class="string">'如果有了最终疗法，也不应该是每天都需要接受治疗'</span>&#125;, </span><br><span class="line"> &#123;<span class="string">'title'</span>: <span class="string">'日本步入下流社会，我们还在买买买'</span>, <span class="string">'url'</span>: <span class="string">'https://www.huxiu.com/article/270112.html'</span>, <span class="string">'name'</span>: <span class="string">'腾讯《大家》©'</span>, <span class="string">'write_time'</span>: <span class="string">'20小时前'</span>, <span class="string">'comment'</span>: <span class="string">'13'</span>, <span class="string">'favorites'</span>: <span class="string">'142'</span>, <span class="string">'abstract'</span>: <span class="string">'我买，故我在'</span>&#125;</span><br><span class="line">...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>可以看到，成功得到所需数据，然后就可以保存了，可以选择输出为 CSV、MySQL、MongoDB 等方式，这里我们选择保存到 MongoDB 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>,<span class="number">27017</span>)</span><br><span class="line">db = client.Huxiu</span><br><span class="line">mongo_collection = db.huxiu_news</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_result</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            self.save_to_mongo(result)  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self,result)</span>:</span></span><br><span class="line">    df = pd.DataFrame(result)</span><br><span class="line">    <span class="comment">#print(df)</span></span><br><span class="line">    content = json.loads(df.T.to_json()).values()</span><br><span class="line">    <span class="keyword">if</span> mongo_collection.insert_many(content):</span><br><span class="line">        print(<span class="string">'存储到 mongondb 成功'</span>)</span><br><span class="line">        <span class="comment"># 随机暂停</span></span><br><span class="line">        sleep = np.random.randint(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">        time.sleep(sleep)</span><br></pre></td></tr></table></figure><p>上面，定义了一个 on_result() 方法，该方法专门用来获取 return 的结果数据。这里用来接收上面 index_page() 返回的 data 数据，在该方法里再定义一个存储到 MongoDB 的方法就可以保存到 MongoDB 中。关于数据如何存储到 MongoDB 中，我们在之前的 <a href="https://www.makcyun.top/web_scraping_withpython7.html">一篇文章</a> 中有过介绍，如果忘记了可以回顾一下。</p><p>下面，我们来测试一下整个爬取和存储过程。点击左上角的 run 就可以顺利运行单个网页的抓取、解析和存储，结果如下：</p><p><img src="http://media.makcyun.top/18-11-6/80607479.jpg" alt=""></p><p>上面完成了单页面的爬取，接下来，我们需要爬取全部 2000 余页内容。</p><p>需要修改两个地方，首先在 on_start() 方法中将 for 循环页数 3 改为 2002。改好以后，如果我们直接点击 run ，会发现还是只能爬取第 2 页的结果。这是因为，pyspider 以 URL的 MD5 值作为 唯一 ID 编号，ID 编号相同的话就视为同一个任务，便不会再重复爬取。由于 GET 请求的 分页URL 通常是有差异的，所以 ID 编号会不同，也就自然能够爬取多页。但这里 POST 请求的分页 URL 是相同的，所以爬完第 2 页，后面的页数便不会再爬取。</p><p>那有没有解决办法呢？ 当然是有的，我们需要重新写下 ID 编号的生成方式，方法很简单，在 on_start() 方法前面添加下面 2 行代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self,task)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> md5string(task[<span class="string">'url'</span>]+json.dumps(task[<span class="string">'fetch'</span>].get(<span class="string">'data'</span>,<span class="string">''</span>)))</span><br></pre></td></tr></table></figure><p>这样，我们再点击 run 就能够顺利爬取 2000 页的结果了，我这里一共抓取了 49,996 条结果，耗时 2 小时左右完成。</p><p><img src="http://media.makcyun.top/18-11-6/36910200.jpg" alt=""></p><p>以上，就完成了数据的获取。有了数据我们就可以着手分析，不过这之前还需简单地进行一下数据的清洗、处理。</p><h2 id="3-数据清洗处理"><a href="#3-数据清洗处理" class="headerlink" title="3. 数据清洗处理"></a>3. 数据清洗处理</h2><p>首先，我们需要从 MongoDB 中读取数据，并转换为 DataFrame。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'Huxiu'</span>]</span><br><span class="line">collection = db[<span class="string">'huxiu_news'</span>]</span><br><span class="line"><span class="comment"># 将数据库数据转为DataFrame</span></span><br><span class="line">data = pd.DataFrame(list(collection.find()))</span><br></pre></td></tr></table></figure><p>下面我们看一下数据的总体情况，可以看到数据的维度是 49996 行 × 8 列。发现多了一列无用的 _id 需删除，同时 name 列有一些特殊符号，比如© 需删除。另外，数据格式全部为 Object 字符串格式，需要将 comment 和 favorites 两列更改为数值格式、 write_time 列更改为日期格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">print(data.shape)  <span class="comment"># 查看行数和列数</span></span><br><span class="line">print(data.info()) <span class="comment"># 查看总体情况</span></span><br><span class="line">print(data.head()) <span class="comment"># 输出前5行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">(<span class="number">49996</span>, <span class="number">8</span>)</span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line">_id           <span class="number">49996</span> non-null object</span><br><span class="line">abstract      <span class="number">49996</span> non-null object</span><br><span class="line">comment       <span class="number">49996</span> non-null object</span><br><span class="line">favorites     <span class="number">49996</span> non-null object</span><br><span class="line">name          <span class="number">49996</span> non-null object</span><br><span class="line">title         <span class="number">49996</span> non-null object</span><br><span class="line">url           <span class="number">49996</span> non-null object</span><br><span class="line">write_time    <span class="number">49996</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">8</span>)</span><br><span class="line">    </span><br><span class="line">	_id	abstract	comment	favorites	name	title	url	write_time</span><br><span class="line"><span class="number">0</span>	<span class="number">5</span>bdc2	“在你们看到…	<span class="number">22</span>	<span class="number">50</span>	普象工业设计小站©	看了苹果屌	https://	<span class="number">10</span>小时前</span><br><span class="line"><span class="number">1</span>	<span class="number">5</span>bdc2	中国”绿卡”号称“世界最难拿”	<span class="number">9</span>	<span class="number">16</span>	经济观察报©	递交材料厚	https://	<span class="number">10</span>小时前</span><br><span class="line"><span class="number">2</span>	<span class="number">5</span>bdc2	鲜衣怒马少年时	<span class="number">2</span>	<span class="number">13</span>	小马宋	金庸小说陪	https://	<span class="number">11</span>小时前</span><br><span class="line"><span class="number">3</span>	<span class="number">5</span>bdc2	预告还是预警？	<span class="number">3</span>	<span class="number">10</span>	Cuba Libre	阿里即将发	https://	<span class="number">11</span>小时前</span><br><span class="line"><span class="number">4</span>	<span class="number">5</span>bdc2	库克：咋回事？	<span class="number">2</span>	<span class="number">3</span>	Cuba Libre	【虎嗅早报	https://	<span class="number">11</span>小时前</span><br></pre></td></tr></table></figure><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除无用_id列</span></span><br><span class="line">data.drop([<span class="string">'_id'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 替换掉特殊字符©</span></span><br><span class="line">data[<span class="string">'name'</span>].replace(<span class="string">'©'</span>,<span class="string">''</span>,inplace=<span class="keyword">True</span>,regex=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 字符更改为数值</span></span><br><span class="line">data = data.apply(pd.to_numeric,errors=<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># 更该日期格式</span></span><br><span class="line">data[<span class="string">'write_time'</span>] = data[<span class="string">'write_time'</span>].replace(<span class="string">'.*前'</span>,<span class="string">'2018-10-31'</span>,regex=<span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># 为了方便，将write_time列，包含几小时前和几天前的行，都替换为10月31日最后1天。</span></span><br><span class="line">data[<span class="string">'write_time'</span>] = pd.to_datetime(data[<span class="string">'write_time'</span>])</span><br></pre></td></tr></table></figure><p>下面，我们看一下数据是否有重复，如果有，那么需要删除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断整行是否有重复值</span></span><br><span class="line">print(any(data.duplicated()))</span><br><span class="line"><span class="comment"># 显示True，表明有重复值，进一步提取出重复值数量</span></span><br><span class="line">data_duplicated = data.duplicated().value_counts()</span><br><span class="line">print(data_duplicated) <span class="comment"># 显示2 True ，表明有2个重复值</span></span><br><span class="line"><span class="comment"># 删除重复值</span></span><br><span class="line">data = data.drop_duplicates(keep=<span class="string">'first'</span>)</span><br><span class="line"><span class="comment"># 删除部分行后，index中断，需重新设置index</span></span><br><span class="line">data = data.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#结果：</span></span><br><span class="line"><span class="keyword">True</span> </span><br><span class="line"><span class="keyword">False</span>    <span class="number">49994</span></span><br><span class="line"><span class="keyword">True</span>         <span class="number">2</span></span><br></pre></td></tr></table></figure><p>然后，我们再增加两列数据，一列是文章标题长度列，一列是年份列，便于后面进行分析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'title_length'</span>] = data[<span class="string">'title'</span>].apply(len)</span><br><span class="line">data[<span class="string">'year'</span>] = data[<span class="string">'write_time'</span>].dt.year</span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line">abstract        <span class="number">49994</span> non-null object</span><br><span class="line">comment         <span class="number">49994</span> non-null int64</span><br><span class="line">favorites       <span class="number">49994</span> non-null int64</span><br><span class="line">name            <span class="number">49994</span> non-null object</span><br><span class="line">title           <span class="number">49994</span> non-null object</span><br><span class="line">url             <span class="number">49994</span> non-null object</span><br><span class="line">write_time      <span class="number">49994</span> non-null datetime64[ns]</span><br><span class="line">title_length    <span class="number">49994</span> non-null int64</span><br><span class="line">year            <span class="number">49994</span> non-null int64</span><br></pre></td></tr></table></figure><p>以上，就完成了基本的数据清洗处理过程，针对这 9 列数据可以开始进行分析了。</p><h2 id="4-描述性数据分析"><a href="#4-描述性数据分析" class="headerlink" title="4. 描述性数据分析"></a>4. 描述性数据分析</h2><p>通常，数据分析主要分为四类： 「描述型分析」、「诊断型分析」「预测型分析」「规范型分析」。「描述型分析」是用来概括、表述事物整体状况以及事物间关联、类属关系的统计方法，是这四类中最为常见的数据分析类型。通过统计处理可以简洁地用几个统计值来表示一组数据地集中性（如平均值、中位数和众数等）和离散型(反映数据的波动性大小，如方差、标准差等)。</p><p>这里，我们主要进行描述性分析，数据主要为数值型数据（包括离散型变量和连续型变量）和文本数据。</p><h3 id="4-1-总体情况"><a href="#4-1-总体情况" class="headerlink" title="4.1. 总体情况"></a>4.1. 总体情况</h3><p>先来看一下总体情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(data.describe())</span><br><span class="line">             comment     favorites  title_length </span><br><span class="line">count  <span class="number">49994.000000</span>  <span class="number">49994.000000</span>  <span class="number">49994.000000</span>  </span><br><span class="line">mean      <span class="number">10.860203</span>     <span class="number">34.081810</span>     <span class="number">22.775333</span>  </span><br><span class="line">std       <span class="number">24.085969</span>     <span class="number">48.276213</span>      <span class="number">9.540142</span>  </span><br><span class="line">min        <span class="number">0.000000</span>      <span class="number">0.000000</span>      <span class="number">1.000000</span>  </span><br><span class="line"><span class="number">25</span>%        <span class="number">3.000000</span>      <span class="number">9.000000</span>     <span class="number">17.000000</span>  </span><br><span class="line"><span class="number">50</span>%        <span class="number">6.000000</span>     <span class="number">19.000000</span>     <span class="number">22.000000</span>  </span><br><span class="line"><span class="number">75</span>%       <span class="number">12.000000</span>     <span class="number">40.000000</span>     <span class="number">28.000000</span>  </span><br><span class="line">max     <span class="number">2376.000000</span>   <span class="number">1113.000000</span>    <span class="number">224.000000</span></span><br></pre></td></tr></table></figure><p>这里，使用了 data.describe() 方法对数值型变量进行统计分析。从上面可以简要得出以下几个结论：</p><ul><li>读者的评论和收藏热情都不算太高，大部分文章（75 %）的评论数量为十几条，收藏数量不过几十个。这和一些微信大 V 公众号动辄百万级阅读、数万级评论和收藏量相比，虎嗅网的确相对小众一些。不过也正是因为小众，也才深得部分人的喜欢。</li><li>评论数最多的文章有 2376 条，收藏数最多的文章有 1113 个收藏量，说明还是有一些潜在的比较火或者质量比较好的文章。</li><li>最长的文章标题长达 224 个字，大部分文章标题长度在 20 来个字左右，所以标题最好不要太长或过短。</li></ul><p>对于非数值型变量（name、write_time），使用 describe() 方法会产生另外一种汇总统计。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(data[<span class="string">'name'</span>].describe())</span><br><span class="line">print(data[<span class="string">'write_time'</span>].describe())</span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">count     <span class="number">49994</span></span><br><span class="line">unique     <span class="number">3162</span></span><br><span class="line">top          虎嗅</span><br><span class="line">freq      <span class="number">10513</span></span><br><span class="line">Name: name, dtype: object</span><br><span class="line">count                   <span class="number">49994</span></span><br><span class="line">unique                   <span class="number">2397</span></span><br><span class="line">top       <span class="number">2014</span><span class="number">-07</span><span class="number">-10</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">freq                      <span class="number">274</span></span><br><span class="line">first     <span class="number">2012</span><span class="number">-04</span><span class="number">-03</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">last      <span class="number">2018</span><span class="number">-10</span><span class="number">-31</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br></pre></td></tr></table></figure><p>unique 表示唯一值数量，top 表示出现次数最多的变量，freq 表示该变量出现的次数，所以可以简单得出以下几个结论：</p><ul><li>在文章来源方面，3162 个作者贡献了这 5 万篇文章，其中自家官网「虎嗅」写的数量最多，超过了 1 万篇，这也很自然。</li><li>在文章发表时间方面，最早的一篇文章来自于 2012年 4 月 3 日。 6 年多时间，发文数最多的 1 天 是 2014 年 7 月 10 日，一共发了 274 篇文章。</li></ul><h3 id="4-2-不同时期文章发布的数量变化"><a href="#4-2-不同时期文章发布的数量变化" class="headerlink" title="4.2. 不同时期文章发布的数量变化"></a>4.2. 不同时期文章发布的数量变化</h3><p><img src="http://media.makcyun.top/18-11-6/47981964.jpg" alt=""></p><p>可以看到 ，以季度为时间尺度的6 年间，前几年发文数量比较稳定，大概在1750 篇左右，个别季度数量激增到 2000 篇以上。2016 年之后文章开始增加到 2000 篇以上，可能跟网站知名度提升有关。首尾两个季度日期不全，所以数量比较少。</p><p>具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis1</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># # 汇总统计</span></span><br><span class="line">    <span class="comment"># print(data.describe())</span></span><br><span class="line">    <span class="comment"># print(data['name'].describe())</span></span><br><span class="line">    <span class="comment"># print(data['write_time'].describe())</span></span><br><span class="line">    </span><br><span class="line">    data.set_index(data[<span class="string">'write_time'</span>],inplace=<span class="keyword">True</span>)</span><br><span class="line">    data = data.resample(<span class="string">'Q'</span>).count()[<span class="string">'name'</span>]  <span class="comment"># 以季度汇总</span></span><br><span class="line">    data = data.to_period(<span class="string">'Q'</span>)</span><br><span class="line">    <span class="comment"># 创建x,y轴标签</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>,len(data),<span class="number">1</span>)</span><br><span class="line">    ax1.plot(x,data.values, <span class="comment">#x、y坐标</span></span><br><span class="line">        color = color_line , <span class="comment">#折线图颜色为红色</span></span><br><span class="line">        marker = <span class="string">'o'</span>,markersize = <span class="number">4</span> <span class="comment">#标记形状、大小设置</span></span><br><span class="line">        )</span><br><span class="line">    ax1.set_xticks(x) <span class="comment"># 设置x轴标签为自然数序列</span></span><br><span class="line">    ax1.set_xticklabels(data.index) <span class="comment"># 更改x轴标签值为年份</span></span><br><span class="line">    plt.xticks(rotation=<span class="number">90</span>) <span class="comment"># 旋转90度，不至太拥挤</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data.values):</span><br><span class="line">        plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors,fontsize=fontsize_text )</span><br><span class="line">        <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line">    <span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">    plt.title(<span class="string">'虎嗅网文章数量发布变化(2012-2018)'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.xlabel(<span class="string">'时期'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章(篇)'</span>)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘</span></span><br><span class="line">    plt.savefig(<span class="string">'虎嗅网文章数量发布变化.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="4-3-文章收藏量-TOP-10"><a href="#4-3-文章收藏量-TOP-10" class="headerlink" title="4.3. 文章收藏量 TOP 10"></a>4.3. 文章收藏量 TOP 10</h3><p>接下来，到了我们比较关心的问题：几万篇文章里，到底哪些文章写得比较好或者比较火？</p><table><thead><tr><th>序号</th><th>title</th><th>favorites</th><th>comment</th></tr></thead><tbody><tr><td>1</td><td>读完这10本书，你就能站在智商鄙视链的顶端了</td><td>1113</td><td>13</td></tr><tr><td>2</td><td>京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利</td><td>867</td><td>10</td></tr><tr><td>3</td><td>离职创业？先读完这22本书再说</td><td>860</td><td>9</td></tr><tr><td>4</td><td>货币如水，覆水难收</td><td>784</td><td>39</td></tr><tr><td>5</td><td>自杀经济学</td><td>778</td><td>119</td></tr><tr><td>6</td><td>2016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里</td><td>774</td><td>39</td></tr><tr><td>7</td><td>真正强大的商业分析能力是怎样炼成的？</td><td>746</td><td>18</td></tr><tr><td>8</td><td>腾讯没有梦想</td><td>705</td><td>32</td></tr><tr><td>9</td><td>段永平连答53问，核心是“不为清单”</td><td>703</td><td>27</td></tr><tr><td>10</td><td>王健林的滑铁卢</td><td>701</td><td>92</td></tr></tbody></table><p>此处选取了「favorites」(收藏数量)作为衡量标准。毕竟，一般好的文章，我们都会有收藏的习惯。</p><p>第一名「<a href="https://www.huxiu.com/article/123650.html" target="_blank" rel="noopener">读完这10本书，你就能站在智商鄙视链的顶端了</a> 」以 1113 次收藏位居第一，并且遥遥领先于后者，看来大家都怀有「想早日攀上人生巅峰，一览众人小」的想法啊。打开这篇文章的链接，文中提到了这几本书：《思考，快与慢》、《思考的技术》、《麦肯锡入职第一课：让职场新人一生受用的逻辑思考力》等。一本都没看过，看来这辈子是很难登上人生巅峰了。</p><p>发现两个有意思的地方。</p><p>第一，<strong>文章标题都比较短小精炼。</strong></p><p>第二，文章收藏量虽然比较高，但评论数都不多，猜测这是因为 <strong>大家都喜欢做伸手党</strong>？</p><h3 id="4-4-历年文章收藏量-TOP3"><a href="#4-4-历年文章收藏量-TOP3" class="headerlink" title="4.4. 历年文章收藏量 TOP3"></a>4.4. 历年文章收藏量 TOP3</h3><p>在了解文章的总体排名之后，我们来看看历年的文章排名是怎样的。这里，每年选取了收藏量最多的 3 篇文章。</p><table><thead><tr><th>year</th><th>title</th><th>favorites</th></tr></thead><tbody><tr><td>2012</td><td>产品的思路——来自腾讯张小龙的分享（全版）</td><td>187</td></tr><tr><td></td><td>Fab CEO：创办四家公司教给我的90件事</td><td>163</td></tr><tr><td></td><td>张小龙：微信背后的产品观</td><td>162</td></tr><tr><td>2013</td><td>创业者手记：我所犯的那些入门错误</td><td>473</td></tr><tr><td></td><td>马化腾三小时讲话实录：千亿美金这个线，其实很恐怖</td><td>391</td></tr><tr><td></td><td>雕爷亲身谈：白手起家的我如何在30岁之前赚到1000万。读《MBA教不了的创富课》</td><td>354</td></tr><tr><td>2014</td><td>85后，突变的一代</td><td>528</td></tr><tr><td></td><td>雕爷自述：什么是我做餐饮时琢磨、而大部分“外人”无法涉猎的思考？</td><td>521</td></tr><tr><td></td><td>据说这40张PPT是蚂蚁金服的内部培训资料……</td><td>485</td></tr><tr><td>2015</td><td>读完这10本书，你就能站在智商鄙视链的顶端了</td><td>1113</td></tr><tr><td></td><td>京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利</td><td>867</td></tr><tr><td></td><td>离职创业？先读完这22本书再说</td><td>860</td></tr><tr><td>2016</td><td>蝗虫般的刷客大军：手握千万手机号，分秒间薅干一家平台</td><td>554</td></tr><tr><td></td><td>准CEO必读的这20本书，你读过几本？</td><td>548</td></tr><tr><td></td><td>运营简史：一文读懂互联网运营的20年发展与演变</td><td>503</td></tr><tr><td>2017</td><td>2016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里</td><td>774</td></tr><tr><td></td><td>真正强大的商业分析能力是怎样炼成的？</td><td>746</td></tr><tr><td></td><td>王健林的滑铁卢</td><td>701</td></tr><tr><td>2018</td><td>货币如水，覆水难收</td><td>784</td></tr><tr><td></td><td>自杀经济学</td><td>778</td></tr><tr><td></td><td>腾讯没有梦想</td><td>705</td></tr></tbody></table><p><img src="http://media.makcyun.top/18-11-8/86110562.jpg" alt=""></p><p>可以看到，文章收藏量基本是逐年递增的，但 2015 年的 3 篇文章的收藏量却是最高的，包揽了总排名的前 3 名，不知道这一年的文章有什么特别之处。</p><p>以上只罗列了一小部分文章的标题，可以看到标题起地都蛮有水准的。关于标题的重要性，有这样通俗的说法：「<code>一篇好文章，标题占一半</code>」，一个好的标题可以大大增强文章的传播力和吸引力。文章标题虽只有短短数十字，但要想起好，里面也是很有很多技巧的。</p><p>好在，这里提供了 5 万个标题可以参考。<code>如需，可以在公众号后台回复「虎嗅」得到这份 CSV 文件。</code></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis2</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># # 总收藏排名</span></span><br><span class="line">    <span class="comment"># top = data.sort_values(['favorites'],ascending = False)</span></span><br><span class="line">    <span class="comment"># # 收藏前10</span></span><br><span class="line">    <span class="comment"># top.index = (range(1,len(top.index)+1)) # 重置index，并从1开始编号</span></span><br><span class="line">    <span class="comment"># print(top[:10][['title','favorites','comment']])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按年份排名</span></span><br><span class="line">    <span class="comment"># # 增加一列年份列</span></span><br><span class="line">    <span class="comment"># data['year'] = data['write_time'].dt.year</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(data)</span>:</span></span><br><span class="line">        top = data.sort_values(<span class="string">'favorites'</span>,ascending=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> top[:<span class="number">3</span>]</span><br><span class="line">    data = data.groupby(by=[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line">    print(data[[<span class="string">'title'</span>,<span class="string">'favorites'</span>]])</span><br><span class="line">    <span class="comment"># 增加每年top123列，列依次值为1、2、3</span></span><br><span class="line">    data[<span class="string">'add'</span>] = <span class="number">1</span> <span class="comment"># 辅助</span></span><br><span class="line">    data[<span class="string">'top'</span>] = data.groupby(by=<span class="string">'year'</span>)[<span class="string">'add'</span>].cumsum()</span><br><span class="line">    data_reshape = data.pivot_table(index=<span class="string">'year'</span>,columns=<span class="string">'top'</span>,values=<span class="string">'favorites'</span>).reset_index()</span><br><span class="line">    <span class="comment"># print(data_reshape)  # ok</span></span><br><span class="line">    data_reshape.plot(</span><br><span class="line">        <span class="comment"># x='year',</span></span><br><span class="line">        y=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">        kind=<span class="string">'bar'</span>,</span><br><span class="line">        width=<span class="number">0.3</span>,</span><br><span class="line">        color=[<span class="string">'#1362A3'</span>,<span class="string">'#3297EA'</span>,<span class="string">'#8EC6F5'</span>]  <span class="comment"># 设置不同的颜色</span></span><br><span class="line">        <span class="comment"># title='虎嗅网历年收藏数最多的3篇文章'</span></span><br><span class="line">        )</span><br><span class="line">    plt.xlabel(<span class="string">'Year'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章收藏数量'</span>)</span><br><span class="line">    plt.title(<span class="string">'历年 TOP3 文章收藏量比较'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line">    <span class="comment"># plt.savefig('历年 Top3 文章收藏量比较.png',dpi=200)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="4-4-1-最高产作者-TOP20"><a href="#4-4-1-最高产作者-TOP20" class="headerlink" title="4.4.1. 最高产作者 TOP20"></a>4.4.1. 最高产作者 TOP20</h4><p>上面，我们从收藏量指标进行了分析,下面，我们关注一下发布文章的作者（个人/媒体）。前面提到发文最多的是虎嗅官方，有一万多篇文章，这里我们筛除官媒，看看还有哪些比较高产的作者。</p><p><img src="http://media.makcyun.top/18-11-8/9931236.jpg" alt=""></p><p>可以看到，前 20 名作者的发文量差距都不太大。发文比较多的有「娱乐资本论」、「Eastland」、「发条橙子」这类媒体号；也有虎嗅官网团队的作者：发条橙子、周超臣、张博文等；还有部分独立作者：假装FBI、孙永杰等。可以尝试关注一下这些高产作者。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis3</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = data.groupby(data[<span class="string">'name'</span>])[<span class="string">'title'</span>].count()</span><br><span class="line">    data = data.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># pandas 直接绘制,.invert_yaxis()颠倒顺序</span></span><br><span class="line">    data[<span class="number">1</span>:<span class="number">21</span>].plot(kind=<span class="string">'barh'</span>,color=color_line).invert_yaxis()</span><br><span class="line">    <span class="keyword">for</span> y,x <span class="keyword">in</span> enumerate(list(data[<span class="number">1</span>:<span class="number">21</span>].values)):</span><br><span class="line">        plt.text(x+<span class="number">12</span>,y+<span class="number">0.2</span>,<span class="string">'%s'</span> %round(x,<span class="number">1</span>),ha=<span class="string">'center'</span>,color=colors)</span><br><span class="line">    plt.xlabel(<span class="string">'文章数量'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'作者'</span>)</span><br><span class="line">    plt.title(<span class="string">'发文数量最多的 TOP20 作者'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(<span class="string">'发文数量最多的TOP20作者.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="4-4-2-平均文章收藏量最多作者-TOP-10"><a href="#4-4-2-平均文章收藏量最多作者-TOP-10" class="headerlink" title="4.4.2. 平均文章收藏量最多作者 TOP 10"></a>4.4.2. 平均文章收藏量最多作者 TOP 10</h4><p>我们关注一个作者除了是因为文章高产以外，可能更看重的是其文章水准。这里我们选择「文章平均收藏量」（总收藏量/文章数）这个指标，来看看文章水准比较高的作者是哪些人。</p><p>这里，为了避免出现「某作者只写了一篇高收藏率的文章」这种不能代表其真实水准的情况，我们将筛选范围定在至少发布过 5 篇文章的作者们。</p><table><thead><tr><th>name</th><th>total_favorites</th><th>ariticls_num</th><th>avg_favorites</th></tr></thead><tbody><tr><td>重读</td><td>1947</td><td>6</td><td>324</td></tr><tr><td>楼台</td><td>2302</td><td>8</td><td>287</td></tr><tr><td>彭萦</td><td>2487</td><td>9</td><td>276</td></tr><tr><td>曹山石</td><td>1187</td><td>5</td><td>237</td></tr><tr><td>饭统戴老板</td><td>7870</td><td>36</td><td>218</td></tr><tr><td>笔记侠</td><td>1586</td><td>8</td><td>198</td></tr><tr><td>辩手李慕阳</td><td>11989</td><td>62</td><td>193</td></tr><tr><td>李录</td><td>2370</td><td>13</td><td>182</td></tr><tr><td>高晓松</td><td>889</td><td>5</td><td>177</td></tr><tr><td>宁南山</td><td>2827</td><td>16</td><td>176</td></tr></tbody></table><p>可以看到，前 10 名作者包括：遥遥领先的 <strong>重读</strong>、两位高产又有质量的 <strong>辩手李慕阳</strong> 和 <strong>饭统戴老板</strong> ，还有大众比较熟悉的 <strong>高晓松</strong>、<strong>宁南山 </strong>等。</p><p>如果你将这份名单和上面那份高产作者名单进行对比，会发现他们没有出现在这个名单中。相比于数量，质量可能更重要吧。</p><p>下面，我们就来看看排名第一的 <strong>重读</strong> 都写了哪些高收藏量文章。</p><table><thead><tr><th>order</th><th>title</th><th>favorites</th><th>write_time</th></tr></thead><tbody><tr><td>1</td><td>我采访出200多万字素材，还原了阿里系崛起前传</td><td>231</td><td>2018/10/31</td></tr><tr><td>2</td><td>阿里史上最强人事地震回顾：中供铁军何以被生生解体</td><td>494</td><td>2018/4/9</td></tr><tr><td>3</td><td>马云“斩”卫哲：复原阿里史上最震撼的人事地震</td><td>578</td><td>2018/3/15</td></tr><tr><td>4</td><td>重读一场马云发起、针对卫哲的批斗会</td><td>269</td><td>2017/8/31</td></tr><tr><td>5</td><td>阿里“中供系”前世今生：马云麾下最神秘的子弟兵</td><td>203</td><td>2017/5/10</td></tr><tr><td>6</td><td>揭秘马云麾下最神秘的子弟兵：阿里“中供系”的前世今生</td><td>172</td><td>2017/4/26</td></tr></tbody></table><p>居然写的都是清一色关于马老板家的文章。</p><p>了解了前十名作者之后，我们顺便也看看那些处于最后十名的都是哪些作者。</p><table><thead><tr><th>name</th><th>total_favorites</th><th>ariticls_num</th><th>avg_favorites</th></tr></thead><tbody><tr><td>于斌</td><td>25</td><td>11</td><td>2</td></tr><tr><td>朝克图</td><td>33</td><td>23</td><td>1</td></tr><tr><td>东风日产</td><td>24</td><td>13</td><td>1</td></tr><tr><td>董晓常</td><td>14</td><td>8</td><td>1</td></tr><tr><td>蔡钰</td><td>31</td><td>16</td><td>1</td></tr><tr><td>马继华</td><td>12</td><td>11</td><td>1</td></tr><tr><td>angeljie</td><td>7</td><td>5</td><td>1</td></tr><tr><td>薛开元</td><td>6</td><td>6</td><td>1</td></tr><tr><td>pookylee</td><td>15</td><td>24</td><td>0</td></tr><tr><td>Yang Yemeng</td><td>0</td><td>7</td><td>0</td></tr></tbody></table><p>一对比，就能看到他们的文章收藏量就比较寒碜了。尤其好奇最后一位作者 <strong>Yang Yemeng</strong> ，他写了 7 篇文章，竟然一个收藏都没有。</p><p>来看看他究竟写了些什么文章。</p><p><img src="http://media.makcyun.top/18-11-7/59717401.jpg" alt=""></p><p>原来写的全都是英文文章，看来大家并不太钟意阅读英文类的文章啊。</p><p>具体实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis4</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = pd.pivot_table(data,values=[<span class="string">'favorites'</span>],index=<span class="string">'name'</span>,aggfunc=[np.sum,np.size])</span><br><span class="line">    data[<span class="string">'avg'</span>] = data[(<span class="string">'sum'</span>,<span class="string">'favorites'</span>)]/data[(<span class="string">'size'</span>,<span class="string">'favorites'</span>)]</span><br><span class="line">    <span class="comment"># 平均收藏数取整</span></span><br><span class="line">    <span class="comment"># data['avg'] = data['avg'].round(decimals=1)</span></span><br><span class="line">    data[<span class="string">'avg'</span>] = data[<span class="string">'avg'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">    <span class="comment"># flatten 平铺列</span></span><br><span class="line">    data.columns = data.columns.get_level_values(<span class="number">0</span>)</span><br><span class="line">    data.columns = [<span class="string">'total_favorites'</span>,<span class="string">'ariticls_num'</span>,<span class="string">'avg_favorites'</span>]</span><br><span class="line">    <span class="comment"># 筛选出文章数至少5篇的</span></span><br><span class="line">    data=data.query(<span class="string">'ariticls_num &gt; 4'</span>)</span><br><span class="line">    data = data.sort_values(by=[<span class="string">'avg_favorites'</span>],ascending=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># # 查看平均收藏率第一名详情</span></span><br><span class="line">    <span class="comment"># data = data.query('name == "重读"')</span></span><br><span class="line">    <span class="comment"># # 查看平均收藏率倒数第一名详情</span></span><br><span class="line">    <span class="comment"># data = data.query('name == "Yang Yemeng"')</span></span><br><span class="line">    <span class="comment"># print(data[['title','favorites','write_time']])</span></span><br><span class="line">    print(data[:<span class="number">10</span>]) 	<span class="comment"># 前10名</span></span><br><span class="line">    print(data[<span class="number">-10</span>:])	<span class="comment"># 后10名</span></span><br></pre></td></tr></table></figure><h3 id="4-5-文章评论数最多-TOP10"><a href="#4-5-文章评论数最多-TOP10" class="headerlink" title="4.5. 文章评论数最多 TOP10"></a>4.5. 文章评论数最多 TOP10</h3><p>说完了收藏量。下面，我们再来看看评论数量最多的文章是哪些。</p><table><thead><tr><th>order</th><th>title</th><th>comment</th><th>favorites</th></tr></thead><tbody><tr><td>1</td><td>喜瓜2.0—明星社交应用的中国式引进与创新</td><td>2376</td><td>3</td></tr><tr><td>2</td><td>百度，请给“儿子们”好好起个名字</td><td>1297</td><td>9</td></tr><tr><td>3</td><td>三星S5为什么对凤凰新闻客户端下注？</td><td>1157</td><td>1</td></tr><tr><td>4</td><td>三星Tab S：马是什么样的马？鞍又是什么样的鞍？</td><td>951</td><td>0</td></tr><tr><td>5</td><td>三星，正在重塑你的营销观</td><td>914</td><td>1</td></tr><tr><td>6</td><td>马化腾，你就把微信卖给运营商得了！</td><td>743</td><td>20</td></tr><tr><td>7</td><td>【文字直播】罗永浩 VS 王自如 网络公开辩论</td><td>711</td><td>33</td></tr><tr><td>8</td><td>看三星Hub如何推动数字内容消费变革</td><td>684</td><td>1</td></tr><tr><td>9</td><td>三星要重新定义软件与内容商店新模式，SO?</td><td>670</td><td>0</td></tr><tr><td>10</td><td>三星Hub——数字内容交互新模式</td><td>611</td><td>0</td></tr></tbody></table><p>基本上都是和 <strong>三星</strong> 有关的文章，这些文章大多来自 2014 年，那几年 <strong>三星</strong> 好像是挺火的，不过这两年国内基本上都见不到三星的影子了，世界变化真快。</p><p>发现了两个有意思的现象。</p><p>第一，上面关于 <strong>三星</strong> 和前面 <strong>阿里</strong> 的这些批量文章，它们「霸占」了评论和收藏榜，结合知乎上曾经的一篇关于介绍虎嗅这个网站的文章：<a href="https://www.zhihu.com/question/20799239/answer/20698562" target="_blank" rel="noopener">虎嗅网其实是这样的</a> ，貌似能发现些微妙的事情。</p><p>第二，这些文章评论数和收藏数两个指标几乎呈极端趋势，评论量多的文章收藏量却很少，评论量少的文章收藏量却很多。</p><p>我们进一步观察下这两个参数的关系。</p><p><img src="http://media.makcyun.top/18-11-6/85687626.jpg" alt=""></p><p>可以看到，大多数点都位于左下角，意味着这些文章收藏量和评论数都比较低。但也存在少部分位于上方和右侧的异常值，表明这些文章呈现 「多评论、少收藏」或者「少评论、多收藏」的特点。</p><h3 id="4-6-文章标题长度"><a href="#4-6-文章标题长度" class="headerlink" title="4.6. 文章标题长度"></a>4.6. 文章标题长度</h3><p>下面，我们再来看看文章标题的长度和收藏量之间有没有什么关系。</p><p><img src="http://media.makcyun.top/18-11-6/93399033.jpg" alt=""></p><p>大致可以看出两点现象：</p><p>第一，<strong>收藏量高的文章，他们的标题都比较短</strong>（右侧的部分散点）。</p><p>第二，<strong>标题很长的文章，它们的收藏量都非常低</strong>（左边形成了一条垂直线）。</p><p>看来，文章起标题时最好不要起太长的。</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis5</span><span class="params">(data)</span>:</span></span><br><span class="line">    plt.scatter(</span><br><span class="line">        x=data[<span class="string">'favorites'</span>],</span><br><span class="line">        y =data[<span class="string">'comment'</span>],</span><br><span class="line">        s=data[<span class="string">'title_length'</span>]/<span class="number">2</span>,</span><br><span class="line">        )</span><br><span class="line">    plt.xlabel(<span class="string">'文章收藏量'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章评论数'</span>)</span><br><span class="line">    plt.title(<span class="string">'文章标题长度与收藏量和评论数之间的关系'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout() </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="文章标题形式"><a href="#文章标题形式" class="headerlink" title="文章标题形式"></a>文章标题形式</h3><p>下面，我们看看作者在起文章标题的时候，在标点符号方面有没有什么偏好。</p><p>可以看到，五万篇文章中，大多数文章的标题是陈述性标题。三分之一（34.8%） 的文章标题使用了问号「？」，而仅有 5% 的文章用了叹号「！」。通常，问号会让人们产生好奇，从而想去点开文章；而叹号则会带来一种紧张或者压迫感，使人不太想去点开。所以，<strong>可以尝试多用问号而少用叹号。</strong></p><p><img src="http://media.makcyun.top/18-11-8/44309942.jpg" alt=""></p><h3 id="4-7-文本分析"><a href="#4-7-文本分析" class="headerlink" title="4.7. 文本分析"></a>4.7. 文本分析</h3><p>最后，我们从这 5 万篇文章中的标题和摘要中，来看看虎嗅网的文章主要关注的都是哪些主题领域。</p><p>这里首先运用了 jieba 分词包对标题进行了分词，然后用 WordCloud 做成了词云图，因虎嗅网含有「虎」字，故选取了一张老虎头像。（关于 jieba 和 WordCloud 两个包，之后再详细介绍）</p><p><img src="http://media.makcyun.top/18-11-8/24683879.jpg" alt=""></p><p>可以看到文章的主题内容侧重于：互联网、知名公司、电商、投资这些领域。这和网站本身对外宣传的核心内容，即「关注互联网与移动互联网一系列明星公司的起落轨迹、产业潮汐的动力与趋势，以及互联网与移动互联网如何改造传统产业」大致相符合。</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis6</span><span class="params">(data)</span>:</span></span><br><span class="line">    text=<span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="string">'title'</span>].values:</span><br><span class="line">        symbol_to_replace = <span class="string">'[!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@，。?★、…【】《》？“”‘’！[\\]^_`&#123;|&#125;~]+'</span></span><br><span class="line">        i = re.sub(symbol_to_replace,<span class="string">''</span>,i)</span><br><span class="line">        text+=<span class="string">' '</span>.join(jieba.cut(i,cut_all=<span class="keyword">False</span>))</span><br><span class="line">    d = path.dirname(__file__) <span class="keyword">if</span> <span class="string">"__file__"</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> os.getcwd()</span><br><span class="line"></span><br><span class="line">    background_Image = np.array(Image.open(path.join(d, <span class="string">"tiger.png"</span>)))</span><br><span class="line">    font_path = <span class="string">'C:\Windows\Fonts\SourceHanSansCN-Regular.otf'</span>  <span class="comment"># 思源黑字体</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加stopswords</span></span><br><span class="line">    stopwords = set()</span><br><span class="line">    <span class="comment"># 先运行对text进行词频统计再排序，再选择要增加的停用词</span></span><br><span class="line">    stopwords.update([<span class="string">'如何'</span>,<span class="string">'怎么'</span>,<span class="string">'一个'</span>,<span class="string">'什么'</span>,<span class="string">'为什么'</span>,<span class="string">'还是'</span>,<span class="string">'我们'</span>,<span class="string">'为何'</span>,<span class="string">'可能'</span>,<span class="string">'不是'</span>,<span class="string">'没有'</span>,<span class="string">'哪些'</span>,<span class="string">'成为'</span>,<span class="string">'可以'</span>,<span class="string">'背后'</span>,<span class="string">'到底'</span>,<span class="string">'就是'</span>,<span class="string">'这么'</span>,<span class="string">'不要'</span>,<span class="string">'怎样'</span>,<span class="string">'为了'</span>,<span class="string">'能否'</span>,<span class="string">'你们'</span>,<span class="string">'还有'</span>,<span class="string">'这样'</span>,<span class="string">'这个'</span>,<span class="string">'真的'</span>,<span class="string">'那些'</span>])</span><br><span class="line">    wc = WordCloud(</span><br><span class="line">        background_color = <span class="string">'black'</span>,</span><br><span class="line">        font_path = font_path,</span><br><span class="line">        mask = background_Image,</span><br><span class="line">        stopwords = stopwords,</span><br><span class="line">        max_words = <span class="number">2000</span>,</span><br><span class="line">        margin =<span class="number">2</span>,</span><br><span class="line">        max_font_size = <span class="number">100</span>,</span><br><span class="line">        random_state = <span class="number">42</span>,</span><br><span class="line">        scale = <span class="number">2</span>,</span><br><span class="line">    )</span><br><span class="line">    wc.generate_from_text(text)</span><br><span class="line">    process_word = WordCloud.process_text(wc, text)</span><br><span class="line">    <span class="comment"># 下面是字典排序</span></span><br><span class="line">    sort = sorted(process_word.items(),key=<span class="keyword">lambda</span> e:e[<span class="number">1</span>],reverse=<span class="keyword">True</span>) <span class="comment"># sort为list</span></span><br><span class="line">    print(sort[:<span class="number">50</span>])  <span class="comment"># 输出前词频最高的前50个，然后筛选出不需要的stopwords，添加到前面的stopwords.update()方法中</span></span><br><span class="line">    img_colors = ImageColorGenerator(background_Image)</span><br><span class="line">    wc.recolor(color_func=img_colors)  <span class="comment"># 颜色跟随图片颜色</span></span><br><span class="line">    plt.imshow(wc,interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘</span></span><br><span class="line">    plt.savefig(<span class="string">'huxiu20.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>上面的关键词是这几年总体的概况，而科技互联网行业每年的发展都是不同的，所以，我们再来看看历年的一些关键词，透过这些关键词看看这几年互联网行业、科技热点、知名公司都有些什么不同变化。</p><p><img src="http://media.makcyun.top/18-11-7/78186420.jpg" alt=""></p><p>可以看到每年的关键词都有一些相同之处，但也不同的地方：</p><ul><li>中国互联网、公司、苹果、腾讯、阿里等这些热门关键词一直都是热门，这几家公司真是稳地一批啊。</li><li>每年会有新热点涌现：比如 2013 年的微信（刚开始火）、2016 年的直播（各大直播平台如雨后春笋般出现）、2017年的 iPhone（上市十周年）、2018年的小米（上市）。</li><li>不断有新的热门技术出现：2013 - 2015 年的 O2O、2016 年的 VR、2017 年的 AI 、2018 年的「区块链」。这些科技前沿技术也是这几年大家口耳相传的热门词汇。</li></ul><p>通过这一幅图，就看出了这几年科技互联网行业、明星公司、热点信息的风云变化。</p><h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><ul><li>本文简要分析了虎嗅网 5 万篇文章信息，大致了解了近些年科技互联网的千变万化。</li><li>发掘了那些优秀的文章和作者，能够节省宝贵的时间成本。</li><li>一篇文章要想传播广泛，文章本身的质量和标题各占一半，文中的5 万个标题相信能够带来一些灵感。</li><li>本文尚未做深入的文本挖掘，而文本挖掘可能比数据挖掘涵盖的信息量更大，更有价值。进行这些分析需要机器学习和深度学习的知识，待后期学习后再来补充。</li></ul><p>本文完。</p><p>文中的完整代码和素材可以在公众号后台回复「<strong>虎嗅</strong>」 或者在下面的链接中获取：</p><p><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython4.html">做 PPT 没灵感？澎湃网 1500 期信息图送给你</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython7.html">听说你想创业找投资？国内创业公司的信息都在这里了</a></p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎扫一扫识别关注我的公众号</center></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>你一打赏，我就写得更来劲了</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.png" alt="高级农民工 微信支付"><p>微信支付</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python爬虫/" rel="tag"><i class="fa fa-tag"></i> Python爬虫</a> <a href="/tags/Python可视化/" rel="tag"><i class="fa fa-tag"></i> Python可视化</a> <a href="/tags/Python分析/" rel="tag"><i class="fa fa-tag"></i> Python分析</a></div><div class="post-widgets"><div class="wp_rating"><div style="color:rgba(0,0,0,.75);font-size:13px;letter-spacing:3px">(&gt;你觉得这篇文章怎么样？&lt;)</div><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/11/10/weekly_sharing4.html" rel="prev" title="程序员是如何在 5 分钟内搞定公众号排版的"><i class="fa fa-chevron-left"></i> 程序员是如何在 5 分钟内搞定公众号排版的</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/11/02/weekly_sharing3.html" rel="next" title="安卓最好用的电子书阅读器">安卓最好用的电子书阅读器 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="http://media.makcyun.top/201901230951_146.jpg" alt="高级农民工"><p class="site-author-name" itemprop="name">高级农民工</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">84</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">43</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/makcyun" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:johnny824lee@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-分析背景"><span class="nav-text">1. 分析背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-为什么选择虎嗅"><span class="nav-text">1.1. 为什么选择虎嗅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-分析内容"><span class="nav-text">1.2. 分析内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-分析工具"><span class="nav-text">1.3. 分析工具</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-数据抓取"><span class="nav-text">2. 数据抓取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-目标网站分析"><span class="nav-text">2.1. 目标网站分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-pyspider-介绍"><span class="nav-text">2.2. pyspider 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-抓取数据"><span class="nav-text">2.3. 抓取数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-数据清洗处理"><span class="nav-text">3. 数据清洗处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-描述性数据分析"><span class="nav-text">4. 描述性数据分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-总体情况"><span class="nav-text">4.1. 总体情况</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-不同时期文章发布的数量变化"><span class="nav-text">4.2. 不同时期文章发布的数量变化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-文章收藏量-TOP-10"><span class="nav-text">4.3. 文章收藏量 TOP 10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-历年文章收藏量-TOP3"><span class="nav-text">4.4. 历年文章收藏量 TOP3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-1-最高产作者-TOP20"><span class="nav-text">4.4.1. 最高产作者 TOP20</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-2-平均文章收藏量最多作者-TOP-10"><span class="nav-text">4.4.2. 平均文章收藏量最多作者 TOP 10</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-文章评论数最多-TOP10"><span class="nav-text">4.5. 文章评论数最多 TOP10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-文章标题长度"><span class="nav-text">4.6. 文章标题长度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文章标题形式"><span class="nav-text">文章标题形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-文本分析"><span class="nav-text">4.7. 文本分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-小结"><span class="nav-text">5. 小结</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span> <span class="with-love" id="heart"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">高级农民工</span></div><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv">访客数：<span id="busuanzi_value_site_uv"></span>人次</span> <span class="post-meta-divider">|</span> <span id="busuanzi_container_site_pv">总访问量：<span id="busuanzi_value_site_pv"></span>次</span> <span class="post-meta-divider">|</span><div class="theme-info"><div class="powered-by"></div><span class="post-count">全站共：189.2k字</span></div><div style="width:300px;margin:0 auto;padding:10px 0"><a target="_blank" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="" style="float:left"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#939393">粤ICP备18144842号</p></a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("yps9pUuWWGeNG1MwVI2tVGys-gzGzoHsz","SNn7AhhHPrCndytXWSTwD5A2")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/javascript">wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:12522,el:"wpac-rating",color:"E0943E"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}()</script></body></html><!-- rebuild by neat -->