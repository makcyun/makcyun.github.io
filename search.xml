<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【机器学习10】可能是关于 kNN 算法最详细的总结]]></title>
    <url>%2F2019%2F06%2F23%2FMachine_learning12.html</url>
    <content type="text"><![CDATA[kNN 算法总结。摘要：kNN 算法总结。对于不少想学机器学习的初学者来说，可能听到「机器学习」四个字就有点发怵，觉得难学门槛高，既要会编程数学还要好，那些科班出身的研究生才干得了。这话也对也不对，要说它对是因为要学的内容的确不少也不简单，远不像数据分析、爬虫看着好学。其次，学习路径一定程度地被魔化了：很多人说学机器学习要先去啃《西瓜书》、《统计学习方法》这些大篇幅的数学公式算法书，很容易让新手放弃。要说它不对也有道理。一是可能你并没有真正想去学它，没发现它有用或者至少有趣的地方；二是机器学习真的没有那么难入门，前提是找到合适自己的方法。所以想清楚为什么学和从哪儿开始学很重要。看了很多教程后，找到了合适自己的机器学习入门方法，也觉得适合很多人，接下来会一点点分享出来。作为入门的第一课，kNN 算法是最好的选择，因为它是机器学习中最简单的算法，学会它几乎没有难度只需要会两点：高中数学和一点 Python 编程基础（远没有爬虫那么难）。虽然是最简单的算法，但我们还是花了大量篇幅（9 篇文章）来介绍它，为的是打好基础，今天这一篇做个总结。kNN 算法（K-Nearest Neighbor），也叫 K 近邻算法，在具体介绍它之前，记住它的两个特点。它有什么用途？主要有两种。一是可以解决分类问题，比如预测红酒属于哪一类，花属于哪一种，是良性肿瘤还是恶性肿瘤等。除了二分类，它还可以解决多分类问题。二是可以解决回归问题，比如预测房价多少、学生成绩多高等。kNN 算法是少数能同时解决分类和回归问题的算法，很实用。它的算法思想和实现难么？非常简单。kNN 算法思想可以用「近朱者赤近墨者黑」形容，谁离待预测值近，待预测值就属于哪一类，就这么简单。判断远近可以用中学学过的欧拉距离公式计算。知道特点后，我们通过一个酒吧猜酒的例子引入了 kNN算法：桌上倒的红酒属于两类，需要根据它们的颜色深度、酒精浓度值去预测新倒的酒属于哪一类：【机器学习01】Python 手写 kNN 算法具体如何预测呢？很简单，只需要两步：先把场景抽象为数学问题，然后将数学问题写成 Python 代码得到预测结果。抽象为数学问题就是把酒映射到坐标轴中，接着根据欧拉公式计算待预测酒（黄色点）同每杯样本酒（红绿色点）之间的距离，然后排序并取前 k 杯酒（k 个点），哪一类酒占比多，则新倒的酒就归属哪一类。理清数学思路后就可以手写代码，手写代码可以加深对算法的理解，同时也能搞懂 Sklearn 调包背后的真正含义。在这第一篇文章中，我们就手写了 kNN 算法，预测出新倒的酒（黄色点）属于绿色的赤霞珠。接下来为了作对比，我们又调用了 Sklearn 中的 kNN 算法包，仅 5 行代码就得到了相同的结果。初次接触 sklearn 的话，可能并不清楚代码背后的 fit、predict 做了什么，于是我们仿写了 Sklearn 的 kNN 算法包，加深了对 Sklearn 调包的理解：【机器学习02】sklearn 中的 kNN 封装算法实现通过以上两篇我们就初步学会了 kNN 算法。不过在对算法做预测时，使用了全部数据作为训练集，这样一来算法的预测准确率如何并不清楚。所以我们做了改进，将数据集拆成一大一小两部分，大的作为训练集，小的作为测试集。在训练集上训练好 kNN 模型后，把测试集喂给它得到预测结果，接着和测试集本身的真实标签值作比较，得到了模型的准确率。这一过程用到了 sklearn 的 train_test_split 方法，最终将一份有 178 个样本的葡萄酒数据集，按照 7:3 的比例随机划分出了 124 个训练样本和 54 个测试样本。【机器学习03】手写 Sklearn 的 train_test_split 函数具体如何计算模型的分类准确率呢？我们又用到了 sklearn 的 accuracy_score 方法，在葡萄酒数据集上达到了 76% 的准确率 。（54 个测试样本中，预测对了 41 个）【机器学习04】Sklearn 的 model.score 和 accuracy_score 函数为了巩固之前 4 节内容，我们又以两个机器学习中常见的数据集为例：鸢尾花和手写数字识别，初步走了一遍 kNN 分类算法。【机器学习05】kNN小结：解决鸢尾花和手写数字识别分类最后，模型在鸢尾花数据集上的准确率达到 97.8%（45 个花样本仅预测错 1 个），在手写数字数据集上的预测准确率达到了 98.7%。这是在使用默认参数而未调参的情况下得到的结果，可见 kNN 算法虽简单但效果很好。不过，我们又发现存在一个问题：kNN 模型有很多参数（叫超参数），我们使用的都是默认参数，这样建立得到的模型不一定是最好的，而我们期望找到最好的模型。如何做到呢？这就需要调参，于是我们了解了 kNN 算法几个重要的超参数，手动搭配组合之后找到了更好的模型，分类准确率进一步提升：【机器学习06】kNN 模型的几个超参数： Algorithm、n_neighbors、明科夫斯基距离但手动调整超参数很不方便，为此我们想到了 Sklearn 中专门实现调参功能的网格搜索方法（GridSearchCV），只需要指定超参数范围，它就会运行所有超参数组合建立模型，最后返回其中效果最好的一组，这比我们自己手写的方法方便地多。【机器学习07】使用网格搜索（GridSearchCV）快速找到 kNN 模型最佳超参数通过网格搜索我们进一步优化了 kNN 模型，这并没有完，还有很重要的一点工作：对数据做归一化处理。因为 kNN 模型跟样本距离有密切关系，如果特征之间的数量级相差很大的话，在计算距离时就容易产生偏差影响分类效果，所以最好建模前先去除数据量纲。常用的数据归一化方式有最值归一化和均值方差归一化，详细对比了两种方法的特点，最终建议使用均值方差归一化：【机器学习08】数据归一化 Fearture Scaling归一化处理后，之前葡萄酒模型的分类准确率从最初的 0.76 飙升到了 0.96。（54 个测试样本中，正确分类的样本数从 41 个上升到 52 个），可见数据归一化对 kNN 算法的重要性。以上我们主要介绍的是 kNN 算法解决分类问题，它还可以解决回归问题，方法大同小异。为了加深理解，我们以常见的回归数据集波士顿房价为例，运用 kNN 回归模型预测了房价，效果不错。【机器学习09】kNN 解决回归问题：以波士顿房价为例以上就是 kNN 算法的主要内容，最后总结一下 kNN 算法的特点：优点：可解决二分类和多分类问题（一些算法只能解决二分类问题）可解决回归问题（部分算法只能解决分类问题，解决不了回归问题）算法简单效果好缺点：计算耗时kNN 算法在计算样本距离时，每个样本都要遍历计算一篇全部训练样本，当数据量很大时，会很耗时，虽然可以用 KD 树、球树等改进方法但效率依然不高。对异常值敏感kNN 算法预测结果依赖最近的 K 个点，一旦 K 个点之中有异常值，很可能会分类错误。预测结果不具有可解释性对于预测的结果，并不能解释预测值为什么属于这个类别。不像线性回归算法具有可解释性。维数灾难随着维度的增加，“看似相近”的两个点，它们之间的距离会随着特征维数的增多而越来越大。比如有 2 个特征的数据集，两个点之间的距离是1.4，一旦特征数量增大到 10000 维时，距离就变成了 100。1维0-1的距离12维(0,0)到(1,1)的距离1.4143维(0,0,0)到(1,1,1)的距离1.73264维(0,0,…,0)到(1,1,…,1)的距离810000维(0,0,…,0)到(1,1,…,1)的距离100接下来我们将开始学习机器学习的第二个简单算法：线性回归。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习09】kNN 解决回归问题：以波士顿房价为例]]></title>
    <url>%2F2019%2F06%2F21%2FMachine_learning11.html</url>
    <content type="text"><![CDATA[kNN 既可以解决分类问题也可以解决回归问题。摘要：以波士顿房价数据集为例，使用 kNN 模型解决回归问题——预测房价。之前我们花了大量篇幅介绍使用 kNN 算法解决分类问题，其实 kNN 是少数机器学习算法中，既适合解决分类问题也适合解决回归问题的算法。在 sklearn 中使用 KNeighborsClassifier 类解决分类，回归问题则可以调用 KNeighborsRegressor类。先来回顾一下 kNN 是如何解决分类问题的。下图中，红绿色点表示两种类别，根据黄色点最近 K 个点的类别来评估其所属类别。假设 k 取 3 ，则黄色点大概率属于红色类别。当要解决回归问题时，我们面对的问题不再是判断样本属于哪个类别，样本值也不是离散值而是连续的具体值。比如预测学生的成绩具体是多少分。计算思路也很简单，主要分两步。第一步和分类算法一样，找到离待预测节点最近的 K 个点，第二步则是取这 K 个节点值的平均值作为待预测点的预测值。是不是很简单？下面我们就通过一个简单的数据集来熟悉一下 kNN 回归模型。训练集有 6 个红色样本点，每个样本点有 X Y 两个特征，点的标签值分别是 1-6。现在需要预测绿色样本点的标签值。假设 k 取 3，很容易就能得到左下角的三个红点是离绿色点最近的，则绿点的标签值等于（1+2+3）/3=2。 当 k 取 5 时，绿色点的标签值等于（1+2+3+4+5）/5=3。编码实现如下：下面我们再用 kNN 的回归模型解决一个实际案例：波士顿房价。波士顿房价是机器学习中很常用的一个解决回归问题的数据集。数据统计于 1978 年，包括 506个房价样本，每个样本包括波士顿不同郊区房屋的13 种特征信息，比如：住宅房间数、城镇教师和学生比例等。标签值则是每栋房子的房价（千美元）。所以这是一个小型数据集，有 506 * 14 维。我们通过这几步来预测房价：加载数据集并初步探索划分训练集和测试集对特征做均值方差归一化建立 kNN 回归模型并预测先加载数据集并做简单的初步探索。每个特征的含义如下：CRIM: 城镇人均犯罪率（%）ZN: 住宅用地所占比例（%）INDUS: 城镇中非住宅用地所占比例（%）CHAS: 0-1分类变量，是否靠近Charles River，靠近1，否则0NOX: 一氧化氮指数RM: 每栋住宅的房间数AGE: 1940 年以前建成的自住单位的比例（%）DIS: 距离 5 个波士顿的就业中心的加权距离RAD: 距离高速公路的便利指数TAX: 每一万美元的不动产税率（%）PTRATIO: 城镇中的教师学生比例（%）B: 关于黑人比例的一个参数（%）LSTAT: 地区中有多少房东属于低收入人群（%）MEDV: 自住房屋房价中位数（也就是均价,单位千美元)可以看到特征之间数值差异较大，所以最好先对数据做归一化再建立模型。样本一共有 13 个特征，建立模型时可以纳入全部特征也可以只纳入部分，我们选择后者。使用 SelectKBest 方法可以筛选出和标签最相关的 K 个特征，这里选择和房价最相关的 3 个特征：RMPTRATIOLSTAT特征选择好之后，接下来划分数据集并归一化，然后建模，代码如下：这样我们就计算出了房价预测值和相应的模型得分。房价预测值和实际值见下图，可以看到预测效果总体还不错。模型得分这里使用了均方根误差（RMSE）和 R2_score 来判断。上图中，这两个值分别为 4.58 和 0.74。均方根误差表示模型的偏离程度，越接近 0 越好。此处 4.58 的含义就是说 68% 的预测房价值和真实房价（均值为 22.53）之间的差值在 4.58（一个标准差） 之间，也就是 68% 的房价位于 [18,27] 之间（单位千美元）。 95% 的房价位于均值的两个标准差之间，也就是 [13.5,31.5]。R2_score 表示模型拟合数据集的好坏，越接近1 表示拟合效果越好。为了比较不同模型的均方根误差和 R2 值，我们还可以使用之前说的网格搜索方法进一步优化模型。可以看到均方根误差降低到了 4.35 ，R2 值提升到了 0.76，说明网格搜索建立的模型效果更好。之后我们学习其他算法（比如线性回归）的时候还会再对这个数据集建立模型并计算得分。本文的 jupyter notebook 代码，可以在公众号：「高级农民工」后台回复「kNN9」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习08】数据归一化 Fearture Scaling]]></title>
    <url>%2F2019%2F06%2F19%2FMachine_learning10.html</url>
    <content type="text"><![CDATA[数据归一化 Fearture Scaling。摘要：介绍最大最小值归一化和均值方差归一化。上一篇文章末我们说到了影响 kNN 模型的一个至关重要的因素，即距离（数据）的归一化问题。可拆解成 距离 和归一化两点。kNN 模型就是依靠样本点之间的距离来分类的，所以距离对 kNN 模型的分类效果很重要。至于归一化，来看两幅图就知道了。从下面的图中可以直观看到，黄色点离绿色点更近。接着，把纵坐标轴的单位年放大成天后得到下图。虽然点坐标和上图一样，但是 y 轴刻度是 x 轴的 50 倍，所以 y 轴主导了两点之间的距离。尽管黄色点和红色点的 x 轴距离比绿色点远，但是 y 轴距离要比绿色点近很多，这样一来，黄色点变得离红色点更近了。假设这个黄色点表示一个患肿瘤的病人，现在要用 kNN 模型判断该该病人患的是良性肿瘤还是恶性肿瘤，准确分类就变得非常重要。所以对 kNN 模型来说数值的单位对模型影响很大。为了消去距离的影响，可以消除数值的量纲，最好的方法就是做归一化处理，将所有数据映射到同一尺度。归一化方法有很多种，常用的是最大最小值归一化和均值方差归一化。最大最小值归一化 Normalization$$x_{\text { scale }}=\frac{x-x_{\text { min }}}{x_{\text { max }}-x_{\text { min }}}$$最值归一化很简单，就是将所有数据映射到 0-1 之间。它有两个特点。一个是适用于数据分布有明显边界的情况，比如学生考试成绩，分数是有边界的，通常在 0 到 100 分范围内。另外一个特点是容易极端值影响，比如一个公司员工和老板的收入，员工平均收入是两万元，而老板收入一百万元，对这样的数据归一化后，老板的值是 1，而员工数值基本是 0，数据呈现严重的有偏分布，对后续建模不利。根据上面的最大最小值公式对刚才的肿瘤数据归一化，绘制归一化后的分布图：坐标轴的刻度都是 0.2，黄色点离绿色点更近。那么，对没有明显边界的分布数据，或者有极端值的数据可以使用用：均值方差归一化。均值方差归一化 standardization这种方法是把所有数据映射到 均值为 0 方差为 1 的分布中。即使有极端值存在，也不会出现严重的数据有偏分布，便于后续建模。$$x_{\text { scale }}=\frac{x-x_{\text { mean }}}{s}$$根据这个公式编码绘制出归一化后的图：相比最值归一化，坐标轴起始点不是 0 -1，但数据基本沿两轴中心对称分布。所以，通常情况下建议使用用均值方差归一化。当然，还有别的归一化方式，以后再讲。下面，我们在 Sklearn 中使用归一化方法。Sklearn 最值归一化先来看一下最值归一化，仍然使用葡萄酒数据集。该数据集有十三个特征，特征的样本分布范围差别很大，有的达到上百倍只差。比如 Alcohol（酒精度） 和 Proline（脯氨酸） 两个特征样本。现在，将全部特征映射到 0-1 的范围后建模，预测模型得分效果如何，分三个步骤。第一步，将数据归一化。加载和划分数据集已经练习过很多遍了不用多说。Sklearn 中的最值归一化方法是 MinMaxScaler 类，fit 拟合之后调用 fit_transform 方法归一化训练集。解释一下 fit 、 fit_transform 和 transform 的作用和关系。在执行归一化操作时，一定要先 fit 然后才能调用 fit_transform。因为 fit 的作用是计算出训练集的基本参数，比如最大最小值等。只有当最大最小值有确定的数值时，才能通过 fit_tranform 方法的断言，进而才能运行并进行归一化操作。否则不先运行 fit 那么最大最小值的值是空值，就通不过 fit_transform 的断言，接着程序就会报错。虽然计算最大最小值和归一化这两个步骤不相关，但 Sklearn 的接口就是这样设计的。所以，要先 fit 然后 fit_transform。至于 transform，它的作用和 fit_transform 差不多，只是在不同的方法中应用。比如 MinMaxScaler 中是 fit_transform，等下要讲的均值方差归一化 StandardScaler 中则是 transform。第二步，建立 kNN 模型。这一步也做过很多遍了。第三步，测试集归一化并预测模型。这里一定要注意测试集归一化的方法：是在训练集的最大最小值基础归一化，而非测试集的最大最小值。为什么是在训练集上呢？其实很好理解，虽然我们划分出的测试集能很容易求出最大最小值，但是别忘了我们划分测试集的目的：来模拟实际中的情况。而在实际中，通常很难获得数据的最大最小值，因为我们得不到全部的测试集。比如早先说的案例：酒吧猜新倒的一杯红酒属于哪一类。凭这一杯葡萄酒它是没有最大最小值的，你可能说加多样本，多几杯红酒不就行了？但这也只是有限的样本，有限样本中求出的最大最小值是不准确的，因为如果再多加几杯酒，那参数很可能又变了。所以，测试集的归一化要利用训练集得到的参数，包括下面要说的均值方差归一化。这样求出模型得到了 0.907 的高分。而我们之前用未归一化的数据建立的模型仅得到 0.76 分。即使后来通过网格搜索调参的方式，得到的模型得分也远没有这个分数高。可见，建模前的数据归一化有多重要。说完均值归一化，下面再来介绍 Sklearn 中的均值方差归一化。Sklearn 均值方差归一化建模的步骤和上面一样，也是三步走。只是第二步使用的是 StandardScaler 类，使用该类会自动把训练集转变为均值为 0 方差 1 的数据分布。最后查看模型得分，达到 0.96，比刚才的得分还要高。将得分换算成测试集红酒的分类准确率的话，就是 54 个测试样本中，正确分类了其中的 52 个，仅错了 2个，而未归一化的 0.76 分，错了 13 个之多。最后小结一下，拿到一份数据集后，做基本的数据探索，查看特征数据分布是否差别很大，很大的话需要进行归一化，建议采用均值方差归一化。尤其是对 kNN 这种算法来说，模型好坏跟距离是直接相关的，对于后续会介绍的一些跟距离不相关的模型，比如决策树等，归一化就没有太大必要。本文的 jupyter notebook 代码，可以在公众号：「高级农民工」后台回复「kNN8」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习07】使用网格搜索（GridSearchCV）快速找到 kNN 模型最佳超参数]]></title>
    <url>%2F2019%2F06%2F17%2FMachine_learning09.html</url>
    <content type="text"><![CDATA[网格搜索 GridSearchCV。摘要：介绍 sklearn 的网格搜索（GridSearchCV）。上一篇文章我们说到了「网格搜索」，这是一种机器学习模型通常都会用到的模型调参方法，它能够返回最好的模型参数组合，采用这些参数建模便能得到最好的模型。为了解释网格搜索概念，我们还通过手写 for 循环遍历的方式模拟网格搜索思路，并找到了 kNN 模型的最好超参数组合。12345# 图上参数组合对应的 kNN 模型为：kNN_clf1 = KNeighborsClassifier(n_neighbors=1,weights='distance',p=1)kNN_clf2 = KNeighborsClassifier(n_neighbors=3,weights='distance',p=2)...kNN_clfn = KNeighborsClassifier(n_neighbors=n,weights='distance',p=n)但这种手写方式有个缺陷，就是不够智能需要手动调整。比如说，当我们的 kNN 模型在不考虑距离权重时，搜索超参数组合中的 weights 是 uniform，当超参数组合中有参数 p 的时候，就得将 uniform改成 distance，这很不方便，我们希望超参数在取不同的值时，能够自如地相互匹配。好在，在 Sklearn 中有一个网格搜索函数（GridSearchCV）能够很轻松解决这个问题。我们只需要添加超参数即可，它会自动匹配超参数之间的关系，让模型正确运行，最终返回最好的一组超参数组合。下面，我们就来介绍如何使用这个网格搜索函数，为了便于跟上一篇文章对比，这里仍采用葡萄酒数据集搭建 kNN 模型。先加载并划分数据集，顺便获得随机最佳超参数下的模型得分：0.76（上一篇文章中的结果）。定义超参数第一步，在 GridSearchCV 函数的 param_grid 参数中定义超参数。该参数是一个数组，数组中包括一个个字典，我们只需要把每一组超参数组合放置在该字典中即可。字典的键为超参数名称，键的值为超参数的搜索取值范围。比如，第一个字典中有两个超参数：n_neighbors 和 weights 。n_neighbors 取值范围是 1-20，weights 键值 uniform 表示不考虑距离，这组参数会循环搜索 20 次。第二个字典中 weights 变为 distance 表示要考虑距离，n_neighbors 和 p 两个超参数循环 20*10 共 200 次。两个字典加起来一共要循环搜索 220 次，后续就会建立 220 个模型中并根据得分返回最好的超参数组合。建模第二步，建模。先创建默认的 kNN 模型，把模型和刚才的超参数传进网格搜索函数中，接着传入训练数据集进行 fit 拟合，这时模型会尝试 220 组超参数组合，数据大的话会比较费时，可以使用 %%time 魔法命令记录运行时间。模型训练好之后可以通过它的方法查看训练结果。best_params_参数查看最佳超参数组合：1'n_neighbors': 4, 'p': 1, 'weights': 'distance'模型最高得分 best_score_ 0.83，这个得分不是 kNN 模型的真实得分，因为评分标准不一样（以后说）。 要想查看该参数组合下的模型真实得分可以传入该参数给 kNN 模型，最终计算出得分是 0.76。如果你有印象的话，我们在上一篇文章手动搜索出的最佳参数得分是 0.81，组合是：1'n_neighbors': 15, 'p': 1, 'weights': 'distance'这和上面使用网格搜索的结果并不一样，主要是网格搜索使用了更为复杂的计算方式。上面的 param_grid 是网格搜索中的一个参数，还有其他几个重要参数，比如 n_jobs，使用该参数可以让电脑多核并行计算以节省时间。更多网格搜索参数可以在官网了解它的 API，之后我们在讲交叉验证（Cross-Validation，GridSearchCV 的 CV）的时候还会再来介绍它。前后这两篇文章我们介绍了 kNN 模型和网格搜索的超参数问题，通过调参能进一步获得更好的 kNN 模型，但是影响 kNN 模型的好坏还有一个至关重要的因素，就是距离（数据）的归一化问题，下一篇文章来介绍。本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN7」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习06】kNN 模型的几个超参数： Algorithm、n_neighbors、明科夫斯基距离]]></title>
    <url>%2F2019%2F06%2F15%2FMachine_learning08.html</url>
    <content type="text"><![CDATA[了解 kNN 的超参数 。摘要：介绍 kNN 模型的几个超参数。上一篇文章我们说到了 kNN 模型中的超参数，今天来介绍几个重要的超参数。简而言之，超参数就是在模型运行前就要先确定的参数。比如指定的 k 个近邻值 k 就是一个超参数。为什么需要超参数呢，是因为使用不同的超参数就会建立不同的 kNN 模型最终也会得到不同的预测结果，我们尽可能想预测地更准确些，所以就需要尝试不同的超参数得到最好的模型。与超参数相对应的一个概念是模型参数，二者区别是：超参数是在模型运行前确定的模型参数是在算法运行过程自己学习的参数kNN 是最简单的机器学习算法，模型中只有超参数没有模型参数，之后我们会介绍的线性回归、逻辑回归等算法有模型参数，建立模型也会更加复杂。第一个超参数：algorithmalgorithm 即算法，意思就是建立 kNN 模型时采用什么算法去搜索最近的 k 个点，有四个选项：brute（暴力搜索）kd_tree（KD树）ball_tree（球树）auto（默认值，自动选择上面三种速度最快的）我们之前建立模型时没有设置这个参数，模型默认使用了 auto 方法，不过还是有必要了解一下这几种方法的区别。首先，我们知道 KNN 模型的核心思想是计算大量样本点之间的距离。第一种算法 brute 暴力搜索。意思就是计算预测样本和全部训练集样本的距离，最后筛选出前 K 个最近的样本，这种方法很蛮力，所以叫暴力搜索。当样本和特征数量很大的时候，每计算一个样本距离就要遍历一遍训练集样本，很费时间效率低下。有什么方法能够做到无需遍历全部训练集就可以快速找到需要的 k 个近邻点呢？这就要用到 KD 树和球树算法。第二种算法 KD 树（K-dimension tree缩写）。简单来说 KD 树是一种「二叉树」结构，就是把整个空间划分为特定的几个子空间，然后在合适的子空间中去搜索待预测的样本点。采用这种方法不用遍历全部样本就可以快速找到最近的 K 个点，速度比暴力搜索快很多（稍后会用实例对比一下）。至于什么是二叉树，这就涉及到数据结构的知识，稍微有些复杂，就目前来说暂时不用深入了解，sklearn 中可以直接调用 KD 树，很方便。之后会单独介绍二叉树和 KD 树，再来手写 KD 树的 Python 代码。目前只需要知道，什么样的数据集适合使用 KD 树算法假设数据集样本数为 m，特征数为 n，则当样本数量 m 大于 2 的 n 次方时，用 KD 树算法搜索效果会比较好。比如适合 1000 个样本且特征数不超过 10 个（2 的 10 次方为 1024）的数据集。一旦特征过多，KD 树的搜索效率就会大幅下降，最终变得和暴力搜索差不多。通常来说 KD 树适用维数不超过 20 维的数据集，超过这个维数可以用球树这种算法。第三种算法是球树（Ball tree）。对于一些分布不均匀的数据集，KD 树算法搜索效率并不好，为了优化就产生了球树这种算法。同样的，暂时先不用具体深入了解这种算法。下面，我们制造一个分类数据集，使用不同的算法来直观感受一下各自算法的运行速度。使用 datasets.make_classification 这种方法可以创建随机的分类数据集。第一个数据集是样本数 m 大于样本 2 的 n 次方，依次建立 kNN 模型并计算运行时间：可见当数据集样本数量 m &gt; 2 的 n 次方时，kd_tree 和 ball_tree 速度比 brute 暴力搜索快了一个量级，auto 采用其中最快的算法。第二个数据集是样本数 m 小于样本 2 的 n 次方时，各：当数据集样本数量 m 小于 2 的 n 次方 时，KD 树和球树的搜索速度大幅降低，暴力搜索算法相差无几。我们介绍了第一个超参数 algorithm，就 kNN 算法来说通常只需要设置 auto 即可，让模型自动为我们选择合适的算法。第二个超参数：n_neighborsn_neighbors 即要选择最近几个点，默认值是 5（等效 k )。回忆一下此前我们在建立 kNN 模型时传入的参数 n_neighbors 是随机选了一个 3，但这个 3 建立的模型不一定是最好的。举个例子，看下面一组对比图。红绿两种圆点代表不同类别，黄色点是待预测的点。左图中，k =3 时红绿点比例是 2:1，所以黄色点大概率属于红色类别。右图中 k =5 时，黄色点大概率又属于绿色类别了。到底应该选择哪一个 k 值，黄色才能正确分类呢？再以之前的葡萄酒数据集实际测试下，k 值分别选择 3 和 5 时的模型得分：加载、划分数据集和建模相信你已经很熟悉了，不再多说，重点看红色箭头处的模型得分。k = 3 时，模型得分 0.76，k=5 时模型得分只有 0.68。所以 k =3 是更好的选择，但可能还存在其他 k 值比 3 效果更好，怎么才能找到最好的 k 值呢？最简单的方式就是递归搜索，从中选择得分最高的 k 值。下面，我们设置一个 k 值范围，把刚才的代码封装到 for 循环中来尝试下：可以看到，当 k 取 1-10，建立的 10 个模型中，k =3 时的模型得分最高。这样我们就找到了合适的超参数 k。第三个超参数：距离权重刚才在划分黄色点分类时，最近的 3 个点中红绿色点比例是 2:1，所以我们就把黄色点划分为红色点了。这样做忽略了点与点之间的距离问题：虽然红点数量多于绿点，但显然绿点距离黄点更近，把黄点划分为绿色类可能更合适。因此，之前的模型效果可能并不好。所以，在建立模型时，还可以从距离出发，将样本权重与距离挂钩，近的点权重大，远的点权重小。怎么考虑权重呢，可以用取倒数这个简单方法实现。以上图为例，绿点距离是 1，取倒数权重还为 1；两个红点的距离分别是 3 和 4，去倒数相加后红点权重和为 7/12。绿点的权重大于红点，所以黄点属于绿色类。在 Sklearn 的 kNN 模型中有专门考虑权重的超参数：weights，它有两个选项：uniform 不考虑距离权重，默认值distance 考虑距离权重从这两个维度，我们可以再次循环遍历找到最合适的超参数：第四个超参数：p说到距离，还有一个重要的超参数 p。如果还记得的话，之前的模型在计算距离时，采用的是欧拉距离：$$\sqrt{\sum_{i=1}^{n}\left(X_{i}^{(a)}-X_{i}^{(b)}\right)^{2}}\$$$$变形：\\left(\sum_{i=1}^{n}\left|X_{i}^{(a)}-X_{i}^{(b)}\right|^{2}\right)^{\frac{1}{\gamma}}$$除了欧拉距离，还有一种常用的距离，曼哈顿距离：$$\sum_{i=1}^{n}\left|X_{i}^{(a)}-X_{i}^{(b)}\right|$$这两种距离很好理解，举个例子，从家到公司，欧拉距离就是二者的直线距离，但显然不可能以直线到公司，而只能按着街道线路过去，这就是曼哈顿距离，俗称出租车距离。这两种格式很像，其实他们有一个更通用的公式：$$\left(\sum_{i=1}^{n}\left|X_{i}^{(a)}-X_{i}^{(b)}\right|^{p}\right)^{\frac{1}{p}}$$这就是明可夫斯基距离（Minkowski Distance），曼哈顿距离和欧拉距离分别是 p 为 1 和 2 的特殊值。使用不同的距离计算公式，点的分类很可能不一样导致模型效果也不一样。在 Sklearn 中对应这个距离的超参数是 p，默认值是 2，也就是欧拉距离，下面我们尝试使用不同的 p 值建立模型，看看哪个 p 值得到的模型效果最好， 这里因为考虑了 p，那么 weights 超参数需是 distance，外面嵌套一个 for 循环即可：可以看到，找出的最好模型，超参数 k 值为 15 ，p 值为 1 即曼哈顿距离，模型得分 0.81。比我们最初采用的超参数： k=3、weights = uniform 得到的 0.76 分要好。但这里要注意 k=15 很可能是过拟合了，这样即使得分高这个模型也是不好的，以后会说。小结一下，本文介绍了 algorithm、n_neighbors、weights、p 等超参数，通过循环搜索超参数（也就是调参）获得了得分更高的模型。这种循环搜索的方式，有一个专门的名称叫：网格搜索（Grid Search），以 k 和 p 两个超参数为例，循环遍历就像是在网格中搜索参数组合。网格搜索是一个各种模型都很常用的模型调参方法，在 Sklearn 中专门封装了这样一个函数，只需要把想要调整的超参数都加进去运算之后，它会返回最好的一组超参数组合。比我们自己手写去找方便地多，我们在下一篇文章就来介绍一下网格搜索。kNN 模型还有一些超参数，可以在官方文档中了解。只有设置好合适的超参数才能建立更好的模型。sklearn.neighbors.KNeighborsClassifier本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN6」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习05】kNN小结：解决鸢尾花和手写数字识别分类]]></title>
    <url>%2F2019%2F06%2F10%2FMachine_learning07.html</url>
    <content type="text"><![CDATA[kNN 解决分类问题的套路。摘要：运用 kNN 解决鸢尾花和手写数字识别分类问题，熟悉 Sklearn 的一般套路。今天我们以两个常见的数据集鸢尾花和手写数字识别为例，练习 Sklearn 使用 kNN 算法解决机器学习分类问题，作为对之前四篇文章的小结。练习完这两个案例，相信会对 kNN 算法有一个比较全面的理解，同时能学会 Sklearn 处理机器学习的一些固定套路，为下一步继续 kNN 算法做准备。预测鸢尾花数据集分类鸢（yuan）尾花是 20 世纪 30 年代的一个经典数据集。该数据集包括三种花共 150 个样本，每个样本有 4 个数值型特征，分别是花萼长度(cm)、花萼宽度(cm)、花瓣长度(cm)、花瓣宽度(cm)。取数据集的前 5 行预览一下：sepal length (cm)sepal width (cm)petal length (cm)petal width (cm)Class05.13.51.40.2014.93.01.40.2024.73.21.30.2034.63.11.50.2045.03.61.40.20现在的任务是：随机给一些样本，要判定它们分别属于哪一种花。和葡萄酒数据集的问题很相似。所以我们同样可以用 kNN 算法来找到答案。思路很简单，加载数据集并划分训练集和测试集，在训练集上训练模型，然后把测试集应用到模型中，预测样本分别属于哪类花，最后计算分类的准确率。最后分类准确度达到了 97.8%，45 个测试花的样本仅预测错了 1 样本，准确率相当高。这还是在我们没有对模型做任何调优的情况下得到的。可见 kNN 算法的确是种效果很好的算法。接着，再来尝试一个相对大型点的分类数据集手写数字识别数据集，看看 kNN 算法性能如何。手写数字识别数据集预测这个数据集来源 1998年 的一个手写数字实验。包含 1797 个样本，每个样本有 64 个特征（由 8 * 8 构成的 64 个数字像素点）每个样本的标签分别是 0-9 自然数中的一个。现在的任务是：随机给一些数字样本，判定它们是哪个数字。这个任务 kNN 模型也能很好地完成，过程和刚才的鸢尾花一样，我们就直接贴代码：plt.imshow 是一个图像处理函数，详细使用可以参考：plt.imshow 教程%%time 是 jupyter book 的一个魔法命令，可以计算单元格执行的运算时间箭头处对比了我们手写的模型和 Sklearn 中的模型的运行时间：8.74s 和 105ms，我们的算法慢了 80 倍差距非常大，主要是我们使用的是最简单粗暴的算法，而 Sklearn 用了更优化的方法。这里也反映了 kNN 算法的运行时间会随着数据集维度增大而增大，所以得优化 kNN 算法才可以，比如使用 kd 树、球树等，我们放到后面再讲。到这儿，我们练习了三个实例，相信对 kNN 算法有了比较深的认识了，不过现在又有一个新的问题。我们在建立模型时，一直默认 k 参数（选择近邻样本点个数）为 3，这个参数对模型分类结果影响很大，它还可以是很多其他值，那是不是选 3 得到的模型就一定是最好的呢？答案显然不会这么绝对。另外，在第一篇文章中，我们计算模型距离时默认使用的是欧拉距离，而欧拉距离是不是对任何数据集都是最好的计算方法呢，答案显然也不绝对。所以这里就涉及到了 kNN 算法的超参数问题，上面的 k 和距离都是超参数。不同的超参数会得到不同的 kNN 模型，为了得到更好的 kNN 模型，我们就需要好好了解一下超参数，下一篇文章就来介绍它。本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN5」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习04】Sklearn 的 model.score 和 accuracy_score 函数]]></title>
    <url>%2F2019%2F06%2F09%2FMachine_learning06.html</url>
    <content type="text"><![CDATA[手写 kNN模型分类准确度。摘要：手写 kNN 模型分类准确度，理解 Sklearn 的 model.score 和 accuracy_score 函数。上一篇文章我们手写了划分数据集的函数，把 178 个葡萄酒数据集划分成了 124 个训练样本和 54 个测试样本。数据准备好之后，我们下面就使用 kNN 模型来训练这份数据集，最后通过模型得分来评价分类效果。调用 Sklearn 中的模型得分库先来调用 Skearn 中的 kNN 模型计算模型得分：只需要几行代码， Sklearn 就训练好了 kNN模型，并且给出了 0.76 的模型得分。也就是说这个 kNN 模型在 54 个红酒样本测试集上，正确分类出了 76% 的样本，剩下 24% 则预测错了，结果不坏但也算不上好。可以进一步优化模型，之后的文章我们再讲。现在我们想搞明白 Sklearn 如何计算模型得分的。上面 Sklearn 使用了两种方法计算模型得分，一种是调用包，即：from sklearn.metrics import accuracy_scoreaccuracy_score 函数利用 y_test 和 y_predict 计算出得分，这种方法需要先计算出 y_predict。而有些时候我们并不需要知道 y_predict ，而只关心模型最终的得分，那么可以跳过 predict 直接计算得分么，答案是可以的，就是第二种方法。一行代码就能计算出来，更为简洁：1kNN_clf.score(X_test,y_test)这行代码直接利用 X_test 和 y_test 就计算出得分，和第一种方法结果一样。下面，我们就来深入了解一下 Sklearn 是如何计算模型得分的，仍然选择手写实现。手写分类准确度算法这里为了多加练习，不使用 Sklearn 的 kNN 模型而使用之前手写的，调用 kNN_sklearn.py 中的 kNNCliassifier 类即可。先用 X_train 和 y_train 进行 fit 拟合，之后用 X_test predict 预测出新的标签值 y_predict。计算模型得分很简单，使用 np.sum 函数统计 y_predict 和 y_test 值相同的样本数再除以总测试样本数即可。模型得 0.74 分，和刚才 Sklearn 计算出的 0.76 略有出入，是因为 Sklearn 的使用的算法和我们写的不同更加复杂。下面，我们把计算得分的函数封装起来，像 Sklearn 中的 accuracy_score 函数一样进行调用。新建一个名为 metrics_score.py 程序保存在 train_test 文件夹下：文件目录层级如下：1234567kNN/ train_test_split.ipynb train_test/ __init__.py model_selection.py kNN_sklearn.py metrics_score.pymetrics_score.py 文件如下：1234import numpy as npdef accuracy_score(y_test, y_predict): assert y_test.shape[0] == y_predict.shape[0], "预测的y值和测试集的y值行数要一样" return sum(y_test == y_predict) / len(y_predict)调用该函数就可以得到一样的结果：1234from train_test.metrics_score import accuracy_scoreaccuracy_score(y_test,y_predict)[out]：0.7407407407407407Sklearn 的第二种方法是直接调用 model.score 方法得到模型分数，我们仍然可以尝试做到。打开之前手写的 kNN_sklearn.py 程序，添加一个 score 函数即可：1234from .metrics_score import accuracy_scoredef score(self,X_test,y_test): y_predict = self.predict(X_test) return accuracy_score(y_test,y_predict)这个函数通过调用自身的 predict 函数计算出 y_predict ，传入上面的 accuracy_score 函数中得到模型得分，然后调用 model 即可计算出：1kNN_clf.score(X_test,y_test)三种方法得到的结果是一样的，对 Sklearn 的 model.score 和 accuracy_score 两个计算模型得分的函数理解会更深刻。通过昨天和今天的两篇文章，我们初步学会了用 kNN 算法去解决葡萄酒分类问题并且计算出模型得分，对模型好坏有了大致判断。下一篇文章我们以两个常见的数据集鸢尾花和手写数字识别为例，完整地走一遍 kNN算法。本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN4」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习03】手写 Sklearn 的 train_test_split 函数]]></title>
    <url>%2F2019%2F06%2F08%2FMachine_learning05.html</url>
    <content type="text"><![CDATA[sklearn 划分训练集和测试集。摘要：手写 Sklearn 的 train_test_split 函数。之前两篇文章以酒吧的红酒故事引出了 kNN 分类算法，根据已倒好的酒（样本），预测新倒的酒（预测）属于哪一类，文章见文末。预测方法我们使用了两种，一种是根据欧拉公式逐步手写，思路清晰直观。另外一种方法是模仿 Sklearn 中的 kNN 算法，把代码封装起来以调用库的形式使用，更加精简。然而这样做忽略了一个重要的问题，我们把全部的红酒样本都拿来生成 kNN 模型进行预测。而模型预测的准确率怎么样，并不清楚也无从验证。这在实际运用中有很大问题，比如说，根据股市数据做了一个模型，然后就直接参照这个模型就去投资股票了，结果模型预测趋势跟实际走势大相径庭，那就惨了。所以我们希望生成一个模型后，能有一个测试集先来测试一下模型，看看效果怎么样，然后进一步地调整并生成一个尽可能好的模型。这样的话，就需要把原始样本分成一大一小两份，比如 70% 和 30%，大的一份用来训练（train）生成模型，一份用来最后测试（test）模型。因为测试集本身有标签参照，跟模型预测的标签一对比就能知道模型效果如何。下面我们仍然以葡萄酒数据集为例，把数据集划分为训练集和测试集建立模型，最后测试模型效果。这份葡萄酒数据集来源于1988 年意大利的葡萄酒产地，属于 sklearn 自带的分类数据集，Sklearn 自带好几个常用的分类和回归数据集，之后会一一拿来做例子。这份数据集包含 178 个样本，13 个数值型特征： alcohol酒精浓度、color_intensity 颜色深度等（我们在第一课就使用了这两个特征）。标签（y值） 是葡萄酒的 3 种分类，用 [0,1,2] 表示。加载数据集样本 X 是 178 行 13 列的矩阵，标签 y 是一个 178 元素的向量，接下来准备划分训练集和测试集。Sklearn 调包划分数据集先来调用 Sklearn 数据集划分函数 train_test_split ：test_size 是测试集比例，random_state 是一个随机种子，保证每次运行都能得到一样的随机结果Sklearn 只用两行代码就可以划分好数据集，虽然简单但是它背后是怎么划分的我们并不清楚，为此，下面来手写一下以更好理解。手写 train_test_split 函数观察到原始数据集的标签值是从 0 到 2 有序排列的，所以不能直接划分，要先把数据集打乱保证随机抽样。打乱可以用 numpy 的 permutation 函数，它会返回打乱后的数据集的索引，这个函数的妙处就在于根据索引就能同时匹配到 X 和 y。二者是一一对应的。数据集整体打乱后设置一个划分比例，比如 0.3，表示测试集和训练集的比例是 0.7:0.3，结合索引就分别能得到训练样本和标签的数量。因 Sklearn 中的 train_test_split 是向上取整，所以为了保持一致使用了向上取整的 ceil 函数，ceil (3.4) = 4；int 虽然也可以取整但它是向下取整的， int (3.4) = 3。在打乱数据之前，添加了一行 random.seed (321) 函数，它的作用是保证重新运行时能得到相同的随机数，若不加这句代码，每次得到的结果都会不一样，不便于相互比较。可以看到，手写函数比 Sklearn 调用的代码多很多，可不可以也像 Sklearn 那样只用几行代码完成数据集划分呢？可以的，只需要把上面的函数封装成一个库再调用即可。封装 train_test_split 函数封装很简单，只需要两步。首先，把代码写成一个函数存到一个 .py 程序：12345678910111213141516import numpy as npfrom math import ceildef train_test_split(X, y, test_ratio=0.3, seed= None): assert X.shape[0] == y.shape[0], 'X y 的行数要一样' if seed: np.random.seed(seed) shuffle_index = np.random.permutation(len(X)) test_size = ceil(len(X) * test_ratio) test_index = shuffle_index[:test_size] train_index = shuffle_index[test_size:] X_train = X[train_index] X_test = X[test_index] y_train = y[train_index] y_test = y[test_index] return X_train, X_test, y_train, y_test这里添加了一个 assert 断言保证 X y 的行数一致。train_test_split 函数有四个参数：test_ratio 是测试集比例，设置了一个默认值 0.3，意思就是当在 jupyter notebook 中调用这个函数的时候如果不指定该参数，它会默认设置为 0.3，如果想换一个比例了，手动设置这个参数就可以了。seed = None 的意思是提供了一个随机种子选项，如果对随机结果没有要求，那么使用默认的 None 就可以，如果想固定一个随机结果，那么就需要设置该参数，比如前面我们设置了一个 321 ，任何一个数都可以，每个数的得出的随机结果不一样。接着，将程序命名为 model_selection.py文件（跟 Sklearn 保持一致）存储到 .ipynb 目录的文件夹中，然后调用该函数即可。可以看到和 Sklearn 一样，也只需要两行代码就可以调用我们自己写的封装函数了。如果你有练习过一些机器学习的案例，就知道这两行代码非常常见，但以前你可能只是机械地完成这个操作，不清楚它背后具体是怎么划分的，现在手写这个函数后，对这两行代码的含义就会有更清晰地认识。另外说一个小知识点。如何在 jupyter notebook 中调用 .py程序，比较好的选择是把程序做成一个包再调用。方法很简单，只需要程序同级目录下新建一个 __init__.py的空白程序就可以了。文件目录层级如下：12345kNN/ train_test_split.ipynb train_test/ __init__.py model_selection.py__init__.py文件的作用是将程序所在的文件夹变成一个 Python 模块，事实上我们调用的 Python 模块，包中都有这样一个文件。深入理解 __init__.py 可以参考：__init__.py 作用详解__init__.py 作用详解2是不是不难理解？这样我们把模型划分好了，完成了准备工作，接下来传给 kNN 模型预测出结果 y_predict ，然后跟 y_test 对比，就可以计算出模型分类的准确率如何，即模型能准确预测出多要红酒分类正确。下一篇文章，用两个实际数据集介绍如何计算 kNN 模型分类的准确率。本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN3」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 年 5 月香港之旅：长洲、坪洲、石澳]]></title>
    <url>%2F2019%2F06%2F06%2Flife13.html</url>
    <content type="text"><![CDATA[轻松惬意。摘要：2019 年 5 月 7 天香港之旅。行程安排Day1 福田口岸-星光大道-TheOne EGG 配眼镜Day2 坪洲-香港仔-南丫岛Day3 长洲-半山闲走Day4 石澳龙脊徒步-石澳村《喜剧之王》Day5 香港历史博物馆-深水埗Day6 青旅休息做饭Day7 回京上周回深圳办理落户，搞定后便跑去香港偷闲几天。旅行体验很棒，分享一二，如果你也同样爱旅行，那就更好了。从小到大都生活在内陆，对香港的印象基本来自 90 年代的电影碟片。几年前到广东后才第一次去香港，一到香港便马不停蹄地赶往逛尖沙咀、旺角、铜锣湾一带，把自己置身于熙熙攘攘的人流和缤纷跃动的霓虹灯中，才觉得算是真正来到了这里。其实，这只不过是香港最普通大众的一面，看多了也会腻。所以这次没有过多在市区逗留，到了后直奔郊区和离岛，想看看不一样的香港，果然没叫人失望。一周的时间足够充裕，不用赶场，慢慢地走慢慢地逛，船、电车跟巴士坐了个遍；海，看了个够。Day1 福田口岸-星光大道-TheOne 配眼镜中午坐地铁到福田口岸，过关前在二楼「生活书店」取了香港电话卡，无限流量上网还可免费打电话。去香港上网有两种方式，一是支付宝上买香港出境流量，比较贵，1 天 200MB 要 9 元。二是淘宝买张电话卡，便宜，7 天不过 30 元。So，去的天数多就买电话卡，时间短就买流量，差不了几块钱，也省得换 SIM卡。过了关就是落马洲地铁站，去往香港只有地铁一种交通方式。时间很多，所以坐到上水后就出站改乘巴士前往市区，坐在巴士上层第一排，可以看看新界九龙的风景，这点比地铁好很多。270A 巴士一个半小时左右到达终点站尖东。上水巴士线路查询：上水巴士總站下午四点多到达终点站「尖东」，离星光大道很近便过去看了看。星光大道在维港海堤上，来回走了两遍发现几个有意思的现象：最右边的手印是谢霆锋，最左边的则是麦兜不是全部人都有手印，除去已逝的不少在世的也没有，比如星爷最受欢迎的明星大概是刘德华，因为他的手印掉漆掉地最厉害，合影的游客很多。走走停停，累了就坐在椅子上吹吹维港上的海风，享受不受时间约束的下午。临近傍晚，回住处尚早，打算去尖沙咀的 EGG 眼镜店配下眼镜，香港的进口镜片比内地划算不少。戴眼镜已超过十年，却从没有重视过如此重要的东西。以前配眼镜只关心镜框好不好看、价格便不便宜，结果本末倒置。人大抵如此，常把注意力放在美不美、划不划算这些表面上，而对自己真正重要的东西却习惯性忽略。舍得买贵重的护肤化妆品却不勤换几支牙刷；舍得买一堆鞋却不买一双跑步鞋；舍得吃喝玩乐看电影却不愿买一份医疗保险。来之前做了些眼镜方面的功课，所以离开星光大道后，直奔目标 TheONE 商城五楼的 EGG 眼镜店。EGG 是香港的平价连锁眼镜店，比 Optical88 便宜。提前在京东上买了一副 99 元的小米眼镜框，所以只需配一对镜片。每天对着电脑很久，一副防蓝光的镜片很有必要，虽然近视度数不高（350 度），还是选择了更轻薄的 1.60 折射率镜片。最后选择了豪雅的 stellify 1.60 防蓝光镜片，700 港币。如果镜框也在店里配，镜片只需要 500 港币。淘宝上同样规格的 stellify 镜片要 1000 多。把钱花在镜片上才是购买眼镜的正确方式。眼镜当场拿不了，需 2 -3 天，如果待不了那么久可以让店家邮寄。EGG 眼镜店各分店地址：https://www.eggoptical.com/zh-hans/eGG眼镜京东旗舰店：https://mall.jd.com/index-754186.html参考的眼镜购买攻略：香港购眼镜记配完眼镜出来天已黑，附近吃了饭后坐巴士回住处，住的地方是一家叫「摩星嶺」的国际青年旅社，位于港岛最西边的半山上，可以在坚尼地地铁站 C 出口换乘青旅的巴士到达。巴士位置在地铁站 C 口左手边的街道，距离 200 米左右，有一个青旅的站牌标志就是了，对面是一家烧烤店。一定要提前到，司机师傅很准时一到点就开车，一分钟也不会多等，如果错过了那几分钟就需要等下一个小时。到青旅的时候下起了大雨，淋了一身，这样特殊的欢迎方式也是难忘。青旅由几栋白色房屋连接，两边对称。搬张椅子坐下来，海风阵阵吹来很是凉快，安静地只能听到两种声音：树林里的蛐蛐叫和天空中飞机的轰鸣声。漆黑的海面上飘着三三两两的船只，缓慢穿梭于市区和离岛。前方是青马大桥，再远一点的海对面，是灯火通明连城一线的九龙。左手边则是长洲、坪洲等多个离岛，右手边可以看到维港天际线的一部分。就这样坐上一两个钟也不会觉得无趣。这么一家相对冷门的住处如找到的呢。源于五年前住过一家很棒的外滩国际青旅老船长国际青年旅社，那之后到外地旅行都会先找找有没有青旅。「国际青年旅社」简称 YHA（Youth Hostel Association），起源于 100 多年前的德国。相较于普通的酒店或连锁宾馆，青旅的房多为宿舍型床位房，价格便宜，可以遇到来自天南海北的人，是一种不错的旅行体验。住国际青年旅社有一个条件是要办一张会员卡，没有会员卡不能入住。淘宝有便宜的，50 块就可以买一张有效期一年的会员卡。有了会员卡，可以任意住全世界的青旅。YHA 国际青年旅社官网YHA 中国官网早上醒来四处走走看看，旅社设施相当完善，有很大的休闲娱乐房、干净的冲凉房卫生间，还有可以做饭的厨房。最棒的当属天台，碧海蓝天映入眼前，可以倒杯咖啡在椅子上懒散地欣赏无敌海景。这样的海景房，只要 140 港币一晚，在香港应该很难再找出这样性价比的地方。青旅老板是个小哥，港普讲得蛮好，问他为什么这么好的房子拿来做青旅。他说以前打仗的时候这座房屋是军营炮台，和平年代后逐渐荒废，政府为了保护好这座房子便同意拿出来做青旅。Day2 坪洲岛-香港仔-南丫岛收拾好准备下山坐船去坪洲岛。从青旅下山有两种方式，一是走路，二是坐巴士。走路好在可以随时出发，一路沿着林荫小道下坡，脚步轻快不费力，时不时还能从树丛间望到大海，半小时就能到坚尼地地铁站，如果时间多又喜欢徒步，可以试试。坐巴士只要十分钟，但班次有限受时间限制，每班车间隔至少 1 小时。下山最早一班是 08:20，上市最晚是 22:45，中间隔一到若干个小时，具体发车时间可以参见旅社官网：YHA賽馬會摩星嶺青年旅舍摩星嶺专车巴士时间表到了坚尼地地铁站去往中心市区很方便，除了地铁、巴士可以选择，更推荐坐电车（叮叮车），地铁站不远就是电车起点站。去坐电车的路上，在士美菲路跟吉席街的交界处，有一家叫「麵食坊」的牛杂店，花 27 港币就能吃一碗地道的牛杂面，每天早上都会去那里吃一碗。吃完出门右拐到吉席街，100 远处就是电车的第一站。如果想坐视野好的第一排，可以向后走三百米到始发站上车。电车班次查询：https://www.hktramways.com/sc/攻略：2019 香港叮叮车攻略，叮叮车路线图半小时后就到了中环，在 E25 毕打街下车左拐上天桥去往中环码头。如果错过了这一站也没关系，电车每站之间的距离都很近，在 E25 和 E31 之间下车都可以。中环 6 号码头去往坪洲，班次密集，45 分钟一班，有普通船和高速船两种，走到码头刚好赶上 10 点钟的普通船。喜欢待在少有人坐的货物舱，没有了玻璃的遮挡能感受到海的咸味，内地的长大的人大都爱坐船吧。船费有便宜和贵两种，普通船 16.8 港币，航行 35 分钟左右；高速船 29.6 港币，25 分钟左右。到周日和节假日价格会贵一些。买票可以刷八达通。中环码头轮船时间价格表维港逐渐被抛在船后，坪洲越来越近。35 分钟就可以从繁华的闹市切换到质朴的风情海岛。坪洲岛只有 0.98 平方公里，岛上游玩预留两三个小时足矣，徒步或者租单车是环岛游最好的方式。岛上值得一逛的街道是「永安街」，窄而短，半个小时能逛完。如果肚子饿了可以去两家网红小食店：以虾多士出名的「祺森冰室」和酥饼很棒的「面包铺」。吃饱喝足，从祺森冰室出来，附近有一家单车店，租上一辆单车开始环岛游。两种路线，顺时针和逆时针走都可以。选择了顺时针，骑不多久能看到一座桥连接的小岛，名「大利岛」，长桥上有几位垂钓大叔。从小爱钓鱼，羡慕他们可以天天海钓。单车租车首小时 30 港币，之后每小时 10 港币，需要提供证件登记（通行证就可以）。大利岛不大，十分钟就逛一圈，从长桥返回继续向左走，可以看到一块路牌写着「坪愉径」，这条小路就是环岛路线。一路上见不到几个人，会经过两段沙滩，沙子很细，海水很蓝。虽是初夏正午，海风不时吹来倒也不热。一个小时左右便能走到最高峰凤萍亭，和一般的亭子没有什么不同，风很大，扛单车上坡的一身汗很快就被吹干，向远望去正好对着维港。歇好出发，一路下山，沿途是岛上村民的农家田地，十五分钟能走回码头，看了下表不到下午 3 点，就这样吧，下次再来。回到中环后临时决定坐巴士去港岛南面的避风塘转转，半个来小时就到，没有想象中的壮观，一个港口里停靠着些大大小小的渔船，恰好看到有去南丫岛的渡船，又决定去看看发哥的故乡。随性决定去一个地方，大概是旅行中最有意思的事。出发已是近 6 点，太阳渐落山，光线变得很柔软。船上有许多老外，到了岛上后发现南丫岛基本成了外国岛。主街是一排排酒吧，没有多少特色，快速走了一圈，便往码头走，趁太阳落山之前赶最近的航班回中环。回到青旅已是晚上九点，看台边搬个椅子歇歇，结束暴走的一天。Day3 长洲-半山徒步第 3 天接着坐船，去长洲岛，网上不少游客将它列为香港离岛之首，多了几分兴奋。去长洲在中环 5 号码头坐船，航行近 1 小时。如果说坪洲小众的话，位于它南面的长洲岛则热门不少，面积有 3 平方公里，旅游业开发地很完善，预留半天就够，如果喜欢可以住一晚。下了码头，对面是一字排开的海鲜大排档，绵延数百米，很壮观。游览路线可以从右手边开始，沿着滨海路一路走下去，到后面游客越来越少，偶尔能碰到几个当地的居民，走累了可以找个椅子坐下，对着大海发发呆，感受一下真正的长洲。走到尽头原路返回，穿过中心街道到岛的另一面是一片海滩，许多游客在里面玩。靠在栏杆上吹吹海风，远处能望到南丫岛和港岛。海滩左边有两家旅社，直面大海，一晚房价在 350 - 900 港币，下次来会在岛上留宿一晚。走回码头的路上，发现一家隐秘的牛杂店，老板是一位阿婆，做的牛杂很好吃。待到临近傍晚才坐船离开，回到中环 7 点左右。时间还不晚，决定走走半山斜道。沿着与半山扶梯平行的小道向上走一直到坚道。小道里，繁华不再，取而代之的是市井小店，廉价衣服铺、瓜果蔬菜店，香港的另一面。最后走回坚尼地城，附近有家麦当劳，点个套餐，空调很足，吃得很爽。回到青旅，找出《喜剧之王》，第一次看。Day4 石澳龙脊-石澳村位于石澳的龙脊是香港最美的登山径，全长 8 公里左右，徒步大概 2 小时。石澳在港岛的最东面，青旅在最西面，时间充裕可坐电车从始发站坚尼地城一直坐到终点站筲箕灣，穿过整个港岛，全程 100 分钟。想快一点就坐地铁，半小时能到。筲箕灣终点站后，朝左手边走 300 米到公交总站，转 9 路巴士半小时左右在「土地湾」站下车，就到了龙脊的起点，从这儿开始登山。一路景色很美，山顶更棒，远眺是望不到边的大海，近处的脚下是石澳村。两三个小时后，下山搭巴士去石澳，十分钟左右车程。石澳很小，1 个小时能逛完。有两处主要景点，一个是海滩，许多人在那里玩，另一处是《喜剧之王》取景点，比如天后庙、养老院（养老院近期施工，已关闭）。石澳旅行攻略：石澳之旅Day5 香港历史博物馆-深水埗郊外逛了几天，有点腻，决定这一天去市区转转。去了香港最好的博物馆：香港历史博物馆。免费参观，参观时间 10 点到下午 6 点左右，周二休馆。对面是科技馆，门票 20 港币，旁边是香港理工大学。这座 1975 开放的博物馆相当不错，8 个展厅，介绍了香港从远古到回归，八个时期的变化，了解了许多此前不知道的。博物馆出来，时间还早，决定坐巴士去深水埗逛逛街。去了就后悔，周末人太多，街道也没什么意思，堆积的都是廉价物品，快速转了几圈便搭车回去。Day6 青旅休息做饭最后一天，睡个懒觉，下山去地铁站旁边的菜市场买点菜回来做顿饭。菜市场在坚尼地城地铁站旁边，蔬菜瓜果很新鲜，价格不便宜。厨房餐具一应俱全，想做什么都没问题，几天都没有好好吃个饭了。还好会做点饭，不至于下不了厨房。剩下的大半天，在房间里找几部以前的老港片出来看看，弥补下错过的时光。第二天早上，坐 A10 路巴士去机场回京。A10 巴士也在吉席街上，离电车站不远，车费 33 港币，比地铁便宜，一个小时左右到。完美旅行，生活不易，及时行乐。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习02】sklearn 中的 kNN 封装算法实现]]></title>
    <url>%2F2019%2F06%2F05%2FMachine_learning04.html</url>
    <content type="text"><![CDATA[解析Sklearn 中 的 kNN 算法封装。摘要：Python 一步步写出 Sklearn 中的 kNN 封装算法。昨天通过一个酒吧猜红酒的故事，介绍了机器学习中最简单的一个算法：kNN （K 近邻算法），并用 Python 一步步实现这个算法。同时为了对比，调用了 Sklearn 中的 kNN 算法包，仅用了 5 行代码。两种方法殊途同归，都正确解决了二分类问题，即新倒的红酒属于赤霞珠。文章传送门：Python 手写机器学习最简单的 kNN 算法 （可点击）虽然调用 Sklearn 库算法，简单的几行代码就能解决问题，感觉很爽，但其实我们时处于黑箱中的，Sklearn 背后干了些什么我们其实不明白。作为初学者，如果不搞清楚算法原理就直接调包，学的也只是表面功夫，没什么卵用。所以今天来我们了解一下 Sklearn 是如何封装 kNN 算法的并自己 Python 实现一下。这样，以后我们再调用 Sklearn 算法包时，会有更清晰的认识。先来回顾昨天 Sklearn 中 kNN 算法的 5 行代码：12345from sklearn.neighbors import KNeighborsClassifier kNN_classifier = KNeighborsClassifier(n_neighbors=3)kNN_classifier.fit(X_train,y_train )x_test = x_test.reshape(1,-1)kNN_classifier.predict(x_test)[0]代码已解释过，今天用一张图继续加深理解：可以说，Sklearn 调用所有的机器学习算法几乎都是按照这样的套路：把训练数据喂给选择的算法进行 fit 拟合，能计算出一个模型，模型有了就把要预测的数据喂给模型，进行预测 predict，最后输出结果，分类和回归算法都是如此。值得注意的一点是，kNN 是一个特殊算法，它不需要训练（fit）建立模型，直接拿测试数据在训练集上就可以预测出结果。这也是为什么说 kNN 算法是最简单的机器学习算法原因之一。但在上面的 Sklearn 中为什么这里还 fit 拟合这一步操作呢，实际上是可以不用的，不过 Sklearn 的接口很整齐统一，所以为了跟多数算法保持一致把训练集当成模型。随着之后我们学习更多的算法，会发现每个算法都有一些特点，可以总结对比一下。把昨天的手写代码整理成一个函数就可以看到没有训练过程：1234567891011import numpy as npfrom math import sqrtfrom collections import Counterdef kNNClassify(K, X_train, y_train, X_predict): distances = [sqrt(np.sum((x - X_predict)**2)) for x in X_train] sort = np.argsort(distances) topK = [y_train[i] for i in sort[:K]] votes = Counter(topK) y_predict = votes.most_common(1)[0][0] return y_predict接下来我们按照上图的思路，把 Sklearn 中封装的 kNN 算法，从底层一步步写出那 5 行代码是如何运行的：1234567891011121314import numpy as npfrom math import sqrtfrom collections import Counterclass kNNClassifier: def __init__(self,k): self.k =k self._X_train = None self._y_train = None def fit(self,X_train,y_train): self._X_train = X_train self._y_train = y_train return self首先，我们需要把之前的函数改写一个名为 kNNClassifier 的 Class 类，因为 Sklearn 中的算法都是面向对象的，使用类更方便。如果你对类还不熟悉可以参考我以前的一篇文章：Python 类 Class 的理解（可点击）在__init__函数中定义三个初始变量，k 表示我们要选择传进了的 k 个近邻点。self._X_train 和 self._y_train前面有个 下划线_ ，意思是把它们当成内部私有变量，只在内部运算，外部不能改动。接着定义一个 fit 函数，这个函数就是用来拟合 kNN 模型，但 kNN 模型并不需要拟合，所以我们就原封不动地把数据集复制一遍，最后返回两个数据集自身。这里要对输入的变量做一下约束，一个是 X_train 和 y_train 的行数要一样，一个是我们选的 k 近邻点不能是非法数，比如负数或者多于样本点的数， 不然后续计算会出错。用什么做约束呢，可以使用 assert 断言语句：123456def fit(self,X_train,y_train): assert X_train.shape[0] == y_train.shape[0],"添加 assert 断言是为了确保输入正常的数据集和k值，如果不添加一旦输入不正常的值，难找到出错原因" assert self.k &lt;= X_train.shape[0] self._X_train = X_train self._y_train = y_train return self接下来我们就要传进待预测的样本点，计算它跟每个样本点之间的距离，对应 Sklearn 中的 predict ，这是算法的核心部分。而这一步代码就是我们之前写的函数，可以直接拿过来用，加几行断言保证输入的变量是合理的。1234567891011def predict(self,X_predict): assert self._X_train is not None,"要求predict 之前要先运行 fit 这样self._X_train 就不会为空" assert self._y_train is not None assert X_predict.shape[1] == self._X_train.shape[1],"要求测试集和预测集的特征数量一致" distances = [sqrt(np.sum((x_train - X_predict)**2)) for x_train in self._X_train] sort = np.argsort(distances) topK = [self._y_train[i] for i in sort[:self.k]] votes = Counter(topK) y_predict = votes.most_common(1)[0][0] return y_predict到这儿我们就完成了一个简易的 Sklearn kNN 封装算法，保存为kNN_sklearn.py文件，然后在 jupyter notebook 运行测试一下：先获得基础数据：1234567891011121314151617181920# 样本集X_raw = [[13.23, 5.64], [13.2 , 4.38], [13.16, 4.68], [13.37, 4.8 ], [13.24, 4.32], [12.07, 2.76], [12.43, 3.94], [11.79, 3. ], [12.37, 2.12], [12.04, 2.6 ]]X_train = np.array(X_raw)# 特征值y_raw = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]y_train = np.array(y_raw)# 待预测值x_test= np.array([12.08, 3.3])X_predict = x_test.reshape(1,-1)注意：当预测变量只有一个时，一定要 reshape(1,-1) 成二维数组不然会报错。在 jupyter notebook 中运行程序可以使用一个魔法命令 %run：1%run kNN_Euler.py这样就直接运行好了 kNN_Euler.py 程序，然后就可以调用程序中的 kNNClassifier 类，赋予 k 参数为 3，命名为一个实例 kNN_classify 。1kNN_classify = kNNClassifier(3)接着把样本集 X_train，y_train 传给实例 fit ：1kNN_classify.fit(X_train,y_train)fit 好后再传入待预测样本 X_predict 进行预测就可以得到分类结果了：1234y_predict = kNN_classify.predict(X_predict)y_predict[out]:1答案是 1 和昨天两种方法的结果是一样的。是不是不难？再进一步，如果我们一次预测不只一个点，而是多个点，比如要预测下面这两个点属于哪一类：那能不能同时给出预测的分类结果呢？答案当然是可以的，我们只需要稍微修改以下上面的封装算法就可以了，把 predict 函数作如下修改：1234567891011121314def predict(self,X_predict): y_predict = [self._predict(x) for x in X_predict] # 列表生成是把分类结果都存储到list 中然后返回 return np.array(y_predict)def _predict(self,x): # _predict私有函数 assert self._X_train is not None assert self._y_train is not None distances = [sqrt(np.sum((x_train - x)**2)) for x_train in self._X_train] sort = np.argsort(distances) topK = [self._y_train[i] for i in sort[:self.k]] votes = Counter(topK) y_predict = votes.most_common(1)[0][0] return y_predict这里定义了两个函数，predict 用列表生成式来存储多个预测分类值，预测值从哪里来呢，就是利用 _predict 函数计算，_predict 前面的下划线同样表明它是封装的私有函数，只在内部使用，外界不能调用，因为不需要。算法写好，只需要传入多个预测样本就可以了，这里我们传递两个：12X_predict = np.array([[12.08, 3.3 ], [12.8,4.1]])输出预测结果：1234y_predict = kNN_classify.predict(X_predict)y_predict[out]：array([1, 0])看，返回了两个值，第一个样本的分类结果是 1 即赤霞珠，第二个样本结果是 0 即黑皮诺。和实际结果一致，很完美。到这里，我们就按照 Sklearn 算法封装方式写出了 kNN 算法，不过 Sklearn 中的 kNN 算法要比这复杂地多，因为 kNN 算法还有很多要考虑的，比如处理 kNN 算法的一个缺点：计算耗时。简单说就是 kNN 算法运行时间高度依赖样本集有和特征值数量的维度，当维度很高时算法运行时间就极速增加，具体原因和改善方法我们后续再说。现在还有一个重要的问题，我们在全部训练集上实现了 kNN 算法，但它预测的效果和准确率怎么样，我们并不清楚，下一篇文章，来说说怎么衡量 kNN 算法效果的好坏。本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN2」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习01】Python 手写 kNN 算法]]></title>
    <url>%2F2019%2F06%2F02%2FMachine_learning03.html</url>
    <content type="text"><![CDATA[Python 手写 kNN 算法。摘要：Python 手写 K 近邻算法。今天开始，我打算写写机器学习教程。说实话，相比爬虫，掌握机器学习竞争力更强些。目前网上大多这类教程对新手都不友好，要么直接调用 Sklearn 包，要么满篇抽象枯燥的算法公式文字，看这些教程你很难入门，而真正适合入门的手写 Python 代码教程寥寥无几。最近看了慕课网 bobo 老师的机器学习课程后，大呼过瘾，最好的机器学习教程没有之一。我打算以他的教程为基础并结合自己的理解，从零开始更新机器学习系列推文。第一篇推文先不扯诸如什么是机器学习、机器学习有哪些算法这些总结性的文章，在你没有真正知道它是什么之前，这些看了也不会有印象反而会增加心理负荷。我将长驱直入从一个算法实战开始，就像以前爬虫教程一样，当你真正感受到它的趣味性后，才会有想去学它的欲望。下面就从一个场景故事开始。01 场景代入在一个酒吧里，吧台上摆着十杯几乎一样的红酒，老板跟你打趣说想不想来玩个游戏，赢了免费喝酒，输了付 3 倍酒钱，赢的概率有 50%。你是个爱冒险的人，果断说玩。老板接着道：你眼前的这十杯红酒，每杯略不相同，前五杯属于「赤霞珠」，后五杯属于「黑皮诺」。现在，我重新倒一杯酒，你只需要根据刚才的十杯正确地告诉我它属于哪一类。听完你有点心虚：根本不懂酒啊，光靠看和尝根本区分辨不出来，不过想起自己是搞机器学习的，不由多了几分底气爽快地答应了老板。你没有急着品酒而是问了老板每杯酒的一些具体信息：酒精浓度、颜色深度等，以及一份纸笔。老板一边倒一杯新酒，你边疯狂打草稿。很快，你告诉老板这杯新酒应该是「赤霞珠」。老板瞪大了眼下巴也差点惊掉，从来没有人一口酒都不尝就能答对，无数人都是反复尝来尝去，最后以犹豫不定猜错而结束。你神秘地笑了笑，老板信守承诺让你开怀畅饮。你微醺之时，老板终于忍不住凑向你打探是怎么做到的。你炫耀道：无他，但机器学习熟尔。02 kNN 算法介绍接下来，我们就要从这个故事中开始接触机器学习了，机器学习给很多人的感觉就是「难」，所以我编了上面这个故事，就是要引出机器学习的一个最简单算法：kNN 算法（K-Nearest Neighbor），也叫 K 近邻算法。别被「算法」二字吓到，我保证你只要有高中数学加上一点点 Python 基础就能学会这个算法。学会 kNN 算法，只需要三步：了解 kNN 算法思想掌握它背后的数学原理（别怕，你初中就学过）最后用简单的 Python 代码实现在说 kNN 算法前说两个概念：样本和特征。上面的每一杯酒称作一个「样本」，十杯酒组成一个样本集。酒精浓度、颜色深度等信息叫作「特征」。这十杯酒分布在一个多维特征空间中。说到空间，我们最多能感知三维空间，为了理解方便，我们假设区分赤霞珠和黑皮诺，只需利用：酒精浓度和颜色深度两个特征值。这样就能在二维坐标轴来直观展示。横轴是酒精浓度值，纵轴是颜色深度值。十杯酒在坐标轴上形成十个点，绿色的 5 个点代表五杯赤霞珠，红色的 5 个点代表五杯黑皮诺。可以看到两类酒有明显的界限。老板新倒的一杯酒是图中黄色的点。记得我们的问题么？要确定这杯酒是赤霞珠还是黑皮诺，答案显而易见，通过主观距离判断它应该属于赤霞珠。这就用到了 K 近邻算法思想。该算法首先需要取一个参数 K，机器学习中给的经验取值是 3，我们假设先取 3 ，具体取多少以后再研究。对于每个新来的点，K 近邻算法做的事情就是在所有样本点中寻找离这个新点最近的三个点，统计三个点所属类别然后投票统计，得票数最多的类别就是新点的类别。上图有绿色和红色两个类别。离黄色最近的 3 个点都是绿点，所以绿色和红色类别的投票数是 3:0 ，绿色取胜，所以黄色点就属于绿色，也就是新的一杯就属于赤霞珠。这就是 K 近邻算法，它的本质就是通过距离判断两个样本是否相似，如果距离够近就觉得它们相似属于同一个类别。当然只对比一个样本是不够的，误差会很大，要比较最近的 K 个样本，看这 K 个 样本属于哪个类别最多就认为这个新样本属于哪个类别。是不是很简单？再举一例，老板又倒了杯酒让你再猜，你可以在坐标轴中画出它的位置。离它最近的三个点，是两个红点和一个绿点。红绿比例是 2:1，红色胜出，所以 K 近邻算法告诉我们这杯酒大概率是黑皮诺。可以看到 K 近邻算法就是通过距离来解决分类问题。这里我们解决的二分类问题，事实上 K 近邻算法天然适合解决多分类问题，除此之外，它也适合解决回归问题，之后一一细讲。02 数学理论K 近邻算法基本思想我们知道了，来看看它背后的数学原理。该算法的「距离」在二维坐标轴中就是两点之间的距离，计算距离的公式有很多，一般常用欧拉公式，这个我们中学就学过：$$\sqrt{\left(x^{(m)}-x^{(n)}\right)^{2}+\left(y^{(m)}-y^{(n)}\right)^{2}}$$解释下就是：空间中 m 和 n 两个点，它们的距离等于 x y 两坐标差的平方和再开根。如果在三维坐标中，多了个 z 坐标，距离计算公式也相同：$$\sqrt{\left(x^{(m)}-x^{(n)}\right)^{2}+\left(y^{(m)}-y^{(n)}\right)^{2}+\left(z^{(m)}-z^{(n)}\right)^{2}}$$当特征数量有很多个形成多维空间时，再用 x y z 写就不方便，我们换一个写法，用 X 加下角标的方式表示特征维度，这样 n 维 空间两点之间的距离公式可以写成：$$\sqrt{\left(X_{1}^{(m)}-X_{1}^{(n)}\right)^{2}+\left(X_{2}^{(m)}-X_{2}^{(n)}\right)^{2}+\ldots+\left(X_{n}^{(m)}-X_{n}^{(n)}\right)^{2}}$$公式还可以进一步精简：$$\sqrt{\sum_{i=1}^{n}\left(X_{i}^{(m)}-X_{i}^{(n)}\right)^{2}}$$这就是 kNN 算法的数学原理，不难吧？只要计算出新样本点与样本集中的每个样本的坐标距离，然后排序筛选出距离最短的 3 个点，统计这 3 个点所属类别，数量占多的就是新样本所属的酒类。根据欧拉公式，我们可以用很基础的 Python 实现。03 Python 代码实现首先随机设置十个样本点表示十杯酒，我这里取了 Sklearn 中的葡萄酒数据集的部分样本点，这个数据集在之后的算法中会经常用到会慢慢介绍。12345678910111213import numpy as npX_raw = [[13.23, 5.64], [13.2 , 4.38], [13.16, 4.68], [13.37, 4.8 ], [13.24, 4.32], [12.07, 2.76], [12.43, 3.94], [11.79, 3. ], [12.37, 2.12], [12.04, 2.6 ]]y_raw = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]X_raw 的两列值分别是颜色深度和酒精浓度值，y_raw 中的 0 表示黑皮诺，1 表示赤霞珠。新的一杯酒信息：1x_test = np.array([12.8,4.1])在机器学习中常使用 numpy 的 array 数组而不是列表 list，因为 array 速度快也能执行向量运算，所以在运算之前先把上面的列表转为数组：12X_train = np.array(X_raw)y_train = np.array(y_raw)有了 X Y 坐标就可以绘制出第一张散点图：1234567891011121314import matplotlib.pyplot as plt plt.style.use('ggplot')plt.figure(figsize=(10,6)) plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],s=100,color=color_g,label='赤霞珠') plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],s=100,color=color_r,label='黑皮诺') plt.scatter(x_test2[0],x_test2[1],s=100,color=color_y) # x_testplt.xlabel('酒精浓度')plt.ylabel('颜色深度')plt.legend(loc='lower right')plt.tight_layout()plt.savefig('葡萄酒样本.png')接着，根据欧拉公式计算黄色的新样本点到每个样本点的距离：123456789101112131415from math import sqrtdistances = [sqrt(np.sum((x - x_test)**2)) for x in X_train] # 列表推导式distances[out]：[1.7658142597679973, 1.5558920271021373, 2.6135799203391503, 1.9784084512557052, 1.5446682491719705, 0.540092584655631, 0.7294518489934753, 0.4172529209005018, 1.215113163454334, 0.7011419257183239]上面用到了列表推导式，以前的爬虫教程中经常用到，如果不熟悉可以在公众号搜索「列表推导式」关键字复习。这样就计算出了黄色点到每个样本点的距离，接着找出最近的 3 个点，可以使用 np.argsort 函数返回样本点的索引位置：1234sort = np.argsort(distances)sort[out]：array([7, 5, 9, 6, 8, 4, 1, 0, 3, 2], dtype=int64)通过这个索引值就能在 y_train 中找到对应酒的类别，再统计出排名前 3 的就行了：12345K = 3 topK = [y_train[i] for i in sort[:K]]topK[out]：[1, 1, 1]可以看到距离黄色点最近的 3 个点都是绿色的赤霞珠，与刚才肉眼观测的结果一致。到这里，距离输出黄色点所属类别只剩最后一步，使用 Counter 函数统计返回类别值即可：12345678from collections import Countervotes = Counter(topK)votes[out]：Counter(&#123;1: 3&#125;)predict_y = votes.most_common(1)[0][0]predict_y[out]：1最后的分类结果是 1 ，也就是新的一杯酒是赤霞珠。我们使用 Python 手写完成了一个简易的 kNN 算法，是不是不难？如果觉得难，来看一个更简单的方法：调用 sklearn 库中的 kNN 算法，俗称调包，只要 5 行代码就能得到同样的结论。04 sklearn 调包1234567from sklearn.neighbors import KNeighborsClassifier kNN_classifier = KNeighborsClassifier(n_neighbors=3)kNN_classifier.fit(X_train,y_train )x_test = x_test.reshape(1,-1)kNN_classifier.predict(x_test)[0][out]：1首先从 sklearn 中引入了 kNN 的分类算法函数 KNeighborsClassifier 并建立模型，设置最近的 K 个样本数量 n_neighbors 为 3。接下来 fit 训练模型，最后 predict 预测模型得到分类结果 1，和我们刚才手写的代码结果一样的。你可以看到，sklearn 调包虽然简单，不过作为初学者最好是懂得它背后的算法原理，然后用 Python 代码亲自实现一遍，这才是入门机器的正确姿势。下一篇推文来看看 sklearn 是如何封装 kNN 算法的，并用 Python 手写一遍。本文的 jupyter notebook 代码，可以在我公众号：「高级农民工」后台回复「kNN1」得到，加油！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量下载 B 站视频]]></title>
    <url>%2F2019%2F06%2F01%2Fweekly_sharing30.html</url>
    <content type="text"><![CDATA[B 站学习。这里是「每周分享」的第 30 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：下载 B 站视频。最近在公众号看到不少推文的标题很有意思，大意是：你在 B 站刷剧，别人却在上面学习。B 站上最多的就是那些二次元动漫，不过年纪大了也搞不懂有意思在哪里。其实，B 站除了娱乐真可以干正事——学习。上个月央视爸爸还专门力挺 B 站：2018 年有近 2000 万人在 B 站学习，相当于去年高考人数的 2 倍！B 站正在成为年轻人学习的首要阵地。B 站有不少优质学习资源，有些还是全网独家，甚至还有一些其他网站的付费视频。我在上面看过不少 Python 、机器学习方面的视频。相对于在线看，我更喜欢把视频下载下来配合 Potplayer 倍速播放，无边框沉浸式观看能让我更专心。说到下载 B 站视频，之前尝试了不少方法：硕鼠下载yout-get 库下载修改 url 后缀无一例外，这几种方法都不太好使：硕鼠貌似用不了了，you-get 库虽然便捷但下载速度慢，命令行操作对新手也有一定挑战，修改 url 的方法难以批量下载。最后终于找到一个完美批量下载工具：「唧唧」，从名字到软件界面都很二次元，如果不喜欢封面可以自定义。这款软件只有几 MB，不需要注册登录，下载下来就能用，速度相当快。视频下载很简单，只需要两步，这里以林轩田老师的「机器学习」课程为例，说说怎么批量下载视频。第一步，打开软件浏览器复制视频 URL：第二步，回到软件中会自动粘贴 URL，回车点击「批量下载」再选择想要的清晰度就可以批量下载了。下载完成使用 Potplayer 打开就可以心无旁骛地学习了，是不是还不错？软件下载地址在这儿：http://client.jijidown.com/为了更方便你，我也下载好了后台回复「B站」就可以得到。如果你想下载其他网站视频，那么这款软件就无能为力，不过你可以使用刚才说的 yot-get 库，这个库下载视频方法也很简单，只需要两步。第一步，一句命令安装好这个库：1pip install you-get第二步，安装好之后就可以下载了。这里仍以下载上面的机器学习视频为例，使用一行命令批量下载视频：1you-get -playlist https://www.bilibili.com/video/av12463015?from=search&amp;seid=5913559342423660559you-get 还有许多选项，比如选择下载清晰度：1you-get -i https://www.bilibili.com/video/av12463015?from=search&amp;seid=5913559342423660559除了下载视频，you-get 库还可以下载图片、音乐等形式多媒体，可以说非常强大了，感兴趣安装下来试试。以上就介绍了 B 站批量视频下载方法。彩蛋经常看到微信群里有人分享「微信读书组队抽奖」链接，其实这不是最好的方法，组队速度慢，对微信读书不感兴趣的人来还是一种信息干扰。说一种好方法，既可以快速组队抽奖又不用分享群或者朋友圈。打开这个链接，扫码参与就可以了：https://weread.qnmlgb.tech/]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 年 5 月的小结思考]]></title>
    <url>%2F2019%2F05%2F31%2Flife14.html</url>
    <content type="text"><![CDATA[及时总结。摘要：几点小事。辞职一年今天是 5 月的最后一天，去年的今天我在深圳上完了最后一天班。时间过得好快，辞职已有一年，一年前还做着并不喜欢的传统水利行业工作，一年后，在转行路上，还需要更努力才是。好在接下来家事会轻松些，能有更多时间学习并输出文章。落好深圳户口上周拿到了深圳户口。几经辗转，身份从新疆人、四川人、东莞人终于又变成了深圳人。迁过这么多次户口，深圳落户效率最高、流程手续也最简单。二十分钟拿到户口本，一周后拿到身份证。相对于北京、上海，深圳落户相当容易。为什么要落户深圳呢，不同的人可能有不同的想法，我个人主要是考虑到目前处于待业期，将来也大概率不会回老家，趁着深圳落户还方便，先占个位再说，哪怕以后不在深圳工作，户口在那儿也没什么不好。5 月份都在办理落户这个事，全程很顺利，不过中途还是遇到不少坑，特意梳理出来写了一篇 5000 字的「2019 毕业两年内大学生落户深圳指南」。如果你也有落户深圳的打算，可以参考一二：链接：https://zhuanlan.zhihu.com/p/67609618专栏课更新完毕3 月底我出了一门只要 19.9 元的《Python 入门爬虫和数据分析实战》专栏课，这周已全部更新完成。不知道你有没有订，订了有没有看。课程介绍在这里：《Python 入门爬虫和数据分析实战》当时有同学反映课程只能在手机上查看，也不能复制，不方便。后来，我在知识星球中给出了教程源文件：如果你原先买过这门课，想要这些教程继续学习的话，可以免费加入我的知识星球（微信私聊我： qqguai001 ）。如果你现在想订阅，可以加入我的知识星球（99 元）学习，微信扫描二维码或者知识星球搜索「高级农民工」。更新机器学习熟悉我的粉丝知道，我早先写爬虫类文章比较多，公众号推文也主要是爬虫这块儿。但始终觉得光会爬虫好像不太够，从一些招聘网站就可以发现，纯爬虫类的岗位不多，比较多的是机器学习、数据挖掘之类的，待遇也要更高些。所以，我不打算一直深钻爬虫，我的建议也是能会一些基础的爬虫就可以了，后期需要现学就行。所以，接下来会写一些更有用的入门机器学习方面的文章。网上机器学习的教程一抓一大把，但总觉得那些教程对零基础的人不够友好。有的不系统，有的则是直接调包没有基础的 Python 代码实现，各种各样的教程和书看一圈下来，依然觉得难入门机器学习，我这几个月也是这么过来的。下周会从零开始，系统地更新一系列机器学习教程，用基础的 Python 代码实现各种算法，尽量做到浅显易懂。以上就是 5 月份的一些小结。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[毕业两年内落户深圳攻略]]></title>
    <url>%2F2019%2F05%2F31%2Flife12.html</url>
    <content type="text"><![CDATA[深圳落户二三事。摘要：大学生深圳落户攻略。上周回深圳办完了落户的最后一道手续，几经辗转，身份从新疆人、四川人、东莞人终于又变成了深圳人。转过这么多次户口，深圳落户效率最高、流程手续最简单。20 分钟拿到户口本，一周后拿到身份证。下面试着梳理下落户流程，分享给有同样需求的人。网上关于深圳落户的攻略已有不少，个人建议看最新和符合自身实际情况的攻略，以免走弯路。先简单说下个人情况，如果发现跟你差别很大，建议就不要作参考了。本人 2017 年 6 月硕士毕业，大学期间户口迁到了学校，毕业后到东莞工作，报到证、档案、户口都跟着迁到了东莞。深圳落户主要分两种，应届生和在职落户，应届生落户相对容易，所以我打算走应届生落户。深圳对于应届生的定义是：即将毕业和毕业两年内的都算应届生，所以我即将 2 年到期，再不办就不能走应届生落户了。2019 年 5 月份开始着手办理，5 月 21 号顺利完成落户。由于我人在北京，所以全部事务都是找人代办，只有 21 号去深圳派出所是自己去的。去派出所是必须本人亲自到场办理。户口性质是派出所集体户口，没有时间期限，挂多久都可以，只要你自己不迁出来，派出所是不会主动撵你走的。先简单说下落户流程。落户深圳不难，时间快，需要的材料也不多。正常情况下半个月能搞好，也只需要做两件事：通过资格审核、办理落户材料。怎么样才能通过资格审核呢？很简单，只要你有下面这四样东西就可以了：身份证毕业证教育部《学历证书电子注册备案表》《毕业生接收申请表》身份证和毕业证是手上就有的，《学历证书电子注册备案表》花几分钟就能在学信网上下载下来。这样就只剩下《毕业生接收申请表》了，这个表是落户深圳必须要填写的一个表，表的内容就是一些个人基本情况以及想落户到深圳哪个区哪个派出所，半个小时差不多也就能填好。有了这四项材料，上传到系统中随即就能通过系统审核。这样就算完成了第一件事，一天的时间就能搞定。审核通过表示有落户资格，接下来就只需做第二件事：准备带去派出所办理落户的几项材料。具体需要哪几项材料呢，也只有寥寥这几样：身份证结婚证（已婚的话）《毕业生接收申请表》身份证照相回执户口迁移证不需要一寸证件照照片、也不需要毕业证和学位证。身份证和结婚证也是现成的，《毕业生接收申请表》第一步已经有了，打印出来带上即可。身份证照相回执也很容易获得，去照相馆花几十块前就能拿到，还有一种既便宜又省事的方法（后续说）。插一句，身份证照相回执照片就是你之后的身份证头像，尽量弄好看些，毕竟这张身份证会跟你 20 年。最后就只剩下「户口迁移证」这一样证件，这个证是办理户口最难拿到的东西，落户的大部分时间就是为了得到它，后面会详说。到这里，就得到了落户全部需要的材料，把这些材料带到派出所办理，快的话半个小时就能成深圳人了。是不是不难？下面展开说说如何实操。仍然从「资格审核」和「办理派出所落户材料」这两方面来说。要想通过资格审核需要做这几件事：第一步：注册账号第二步：填写《毕业生接收申请表》第三步：打印扫描申请表（后面几个地方会用到）第四步：上传材料在开始之前，最重要的事就是：用 IE 浏览器！用 IE 浏览器！用 IE 浏览器！，不然会出现很多 Bug。通过资格审核第一步：注册账号打开 深圳人力资源和社会保障局网 这个网站，选择：人才引进（毕业生、在职人才引进）测评与申报系统此时会跳转到到广东政务网，填写账号密码登陆，如果还没有注册就注册一下。账号和密码一定要记住！因为后续会经常要登陆这个网站。选择「接收毕业生」：（另外可以看到，如果走「在职落户」，也是在这里面操作）第二步：填写《毕业生接收申请表》填写必须要填的红色 * 号的内容即可。个人信息没什么说的，重点注意落户地点，需要选择落户到哪个区和该区上具体的派出所。落到哪个区跟后面能拿多少补贴是直接挂钩的，因为不同的区补贴政策不同。如果你不在意补贴的话，落户哪个区都没有关系了，自行斟酌。我落的是：福田区莲花派出所。确定好区之后，具体落哪个派出所则没有区别。推荐莲花派出所，离地铁站近，日后办事方便，办事效率挺高。在深圳十区中，南山区实力最强，IT 企业和人才都多，所以没有补贴。福田区次之，本科生没有补贴，硕士有 12,500 元的补贴。排名靠后的其他区诸如宝安、龙华、龙岗等，硕士补贴则为 25,000 元，本科生也有补贴，具体多少没太关注，可以关注《深圳本地宝》公众号上查，之后补贴内容会详细讲。第三步：打印《毕业生接收申请表》《毕业生接收申请表》填写好之后，右键另存为 PDF 保存下来，找个打印店打印两份，签上自己名字再扫描。打印出来的两份表之后会用到。扫描件需要上传，见下一步。注意红色箭头处：一旦你按照上面的步骤注册填写好了这份申请表，那么 90 天之内必须要完成下一步的证件上传，否则就失效了，需要重新申请。第四步：上传材料接下来就要在系统中上传四份材料，分别是：《毕业生接收申请表》身份证正反面毕业证（不是学位证，办户口全程都用不到学位证）教育部学历证书电子注册备案表前三项资料你有现成的了，最后一个电子注册备案表在学信网上下载下来就行，有效期尽可能选 180 天。到这里就可以提交材料了，审核通过的话，很快（我是半个小时）就会收到一条来自深圳公安的短信：意思就是通过系统审核了，即上面说的第一件事。给你发了一个指标卡编号，有这个编号才能去派出所落户，有效期到年底，也就是最晚要在 12 月 30 之前去到派出所办理落户，这之间哪天去都行。另外还一个就是去办理之前，需要关注「深圳公安」公众号进行取号预约，一般头天预约第二天就行，如果是想预约周末，那可能要很早预约。办理派出所落户材料到这里就要准备前面说过的派出所落户材料：身份证结婚证（已婚的话）《毕业生接收申请表》户口迁移证身份证照相回执前三个已有，只剩下「户口迁移证」和「身份证照相回执」。先说如何办理「户口迁移证」。以我户口所在地东莞为例，去公安局领取户口迁移证需要这几项材料：《毕业生接收申请表》户口本身份证改派到深圳的报到证同样，前三样是已有的，只剩下一个「改派到深圳的报到证」没有。当初毕业时学校发了报到证，到东莞工作后交给了当地的人才管理办公室，现在要迁到深圳，那就需要一个新的报到证。第一步：改派报到证怎么获得新的报到证呢，最好直接联系学校就业办，我们学校是叫「就业指导服务中心」，问过学校老师后说，要想开新的报到证需要两样材料：原来的报到证离职证明也就是说，得拿回原来的旧报到证才能开新的报到证，另外还需要离职证明。于是打电话给当地人才办，说明我的情况（从原先公司离职，要迁户口到深圳）后，问如何拿回报到证，工作人员说需要这两项材料：离职证明人事代理协议还好我保留着当时公司的离职证明，不然就很难办了。由于我的档案是保管在人才办的，所以当时签了人事代理协议以让他们代为保管档案。把这两项材料交给他们就可以拿回报到证，把报到证再寄回学校，让同学去就业指导中心就能拿到新的报到证，也就是改派的报到证。这时，需要填写报到证抬头，填写：深圳市人力资源和社会保障局就行了。这里注意几点：报到地址是默认填写为罗湖区的，这个跟要迁的户口所在区没关系，不用管报到证有效期是两个月，两个月没有用就作废有了报到证其实落户就完成了一半。第二步：办户口迁移证拿到改派《报到证》后连同这三项材料的原件和复印件就可以去户口所在公安局办理户口迁移证了。《毕业生接收申请表》户口本身份证工作人员会问你户口迁往地址是哪里，填你所要落的派出所地址就行，比如我的是：深圳市福田区莲花派出所。很快当场就能拿到户口迁移证，注意一点，户口迁移证原则上是要本人办理的，如果本人办不了那就只有两类人可以代办。一类是和你在一个户口本上的亲属，一类是夫妻有结婚证。第三步：身份证照相回执有了户口迁移证就只需要办理最后一样证件：身份证照相回执，有两种方法可以拿到。第一种是去照相馆。到深圳随便找家照相馆说要办理身份证，让他照个相给个回执就可以了，加急一般当天能拿到。还一种更好的方法是自己制作，只需花 1.5 元很快就可以做好。打开这个网站：证件数码相片质量检测系统，注册填写基本信息，然后上传照片，照片要符合下面两个要求才能通过审核：大小为 14K 至28 K，小于14K 或大于 28K 都过不了审核长宽分别为 358 * 441 像素通过审核之后就可以保存回执再打印出来就可以了，类似下面这样：插一句，此处有商机，在淘宝上办这个照相回执要 5 块钱，利用信息差赚钱。到这儿我们就有了办理户口所需要的全部资料，最后一步就是带着这些资料去落户。第四步：派出所迁入户口去派出所落户之前，确保已经早先已经收到了深圳公安发的短信。然后微信关注「深圳公安」微信公众号，依次选择：业务办理—业务预约申请—普通户政业务—“毕业生入户（不需本市准迁）”。预约办理时间，然后按照指定时间提前 15 分钟去派出所办理就行。工作人员会当场给两张户口页，一张是自己的，一张是集体户口首页。保存好，万一丢了需要本人到派出所补办，费用 20 元。身份证不能当场拿到，需要十天左右，可以到派出所领，也可以选择邮寄，邮寄只能邮寄到深圳本地地址，可以找在深圳的朋友代领，否则就只能本人回派出所领。后续事项到这里落户口这件事就算完成，不过之后还有几件重要的事：报到迁移档案申请市区补贴这几件事如果有时间就尽快办，如果暂时没时间晚点办也没关系。比如我暂时不在深圳，便打算晚点回去办。报到报到的意思就是需要把《就业报到证》和《毕业生介绍信》交到档案保管单位，他们会存入个人档案。《就业报到证》已经有了，《毕业生介绍信》还没有，在哪里获取呢，很简单。当天办完落户后，就可以到申报系统里打印《毕业生介绍信》，大致是这个样子：有了材料就可以交到档案保管单位了，这个单位又在哪儿呢，很简单。在《毕业生介绍申请表》下面的小字中有写：打电话给福田区人力资源服务中心（0755-82918158），咨询报到材料交到哪里就可以了。迁移档案户口迁到深圳后，档案也要跟着迁过去。迁到哪里呢，上面的红色箭头处就是。我落的福田区，所以档案保管在福田人力资源中心。如何迁移档案呢？咨询两个单位：档案保管单位和接收单位。先打电话给档案保管单位，也就是我原户口所在地东莞的人才办，工作人员说需要两项材料：「调档函」和档案迁移地址。「调档函」需要档案接收单位开具。接着又打电话给档案接收单位：福田人力资源中心，工作人员说需要两项材料：《毕业生介绍信》和《毕业生接收申请表》，这两样手上是有的，去现场办理就可以拿到调档函。拿着调档函回档案接收单位，他们会把档案寄给接收单位。不需要自己操心。额外说一下，户口办好之后，有时间就顺便把档案调了，如果没时间那晚点调也没关系，我不在深圳就打算晚点回去再调。到这儿就完成了报到和迁档案这两件事。领取市、区两级补贴如果你目前有在深圳工作，公司给你缴纳了社保（缴纳多久没有限制，建议至少一个月，且是公司缴纳，个人缴纳无效），就可以计划申请深圳市的「新引进人才租房和生活补贴」。补贴分市里和区里两级补贴。先申请到市里补贴再申请区级补贴。市级补贴所有人是一样的：全日制本科15000元/人全日制硕士25000元/人全日制博士30000元/人市级补贴申请资格条件注意这里的市级补贴分两种：「人才租房补贴」和「人才租房和生活补贴」。后者补贴多，学历要求高。具有全日制普通高等教育本科及以上学历，学历以办理引进手续时申报的为准具有深圳户籍在深圳缴纳了社保，申请是社保仍在深圳市本人未享受过购房优惠政策、未正在租住公租房；新调入的在职人才除符合上述规定条件外，还应当符合下列年龄条件：本科的未满 30 周岁、硕士的未满 35 周岁、博士未满 40 周岁。申请时间限制申请市级补贴是有时间限制的，也就是《毕业生介绍信》签发之日起一年内。超过这个时间就领不了补贴了，一定要注意。那就需要搞清楚「《毕业生介绍信》签发之日起」是什么时候。通过上面的《毕业生介绍信》截图可以看到，时间是 2019 年 5 月 15 日。我不太清楚为什么是这个日期，但应该跟两件事有关。第一个，我通过资格审核是 5 月 15 日，也是这一天收到了深圳公安的短信。第二个，我 5 月 21 日去派出所办理的落户，这一天才有的《毕业生介绍信》。符合上面两点后，就可以申请补贴了。补贴申请网址深圳市新引进人才租房和生活补贴Tips尽管我自认上面说的清楚，但你可能还会遇到其他问题，可以选择下面这三种方式咨询，得到的答案是最权威的。深圳社保电话：0755-12333 ，注意了拨通后默认是智能语音，想转接人工服务，依次选择 1-2-2-0 就可以。深圳户政电话：0755-84465000工作人员QQ：1840886088另推荐一个公众号和一篇攻略。一个公众号：深圳本地宝这个公众号有对落户、补贴等事项的详细教程。一篇文章：2019 年应届毕业生深圳户口自助落户攻略这篇文章是答主亲身落户经历，内容非常详尽。好，以上就是一点深圳落户心得。仔细看完上面说的，落户应该没问题。还有问题，我也木办法。想交流可以加我微信：qqguai001 ，备注（落户）。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonchallenge 一个边玩游戏边学 Python 的通关网站]]></title>
    <url>%2F2019%2F05%2F17%2FPython_learning05.html</url>
    <content type="text"><![CDATA[Python 趣味学习网站。这里是「每周分享」的第 28 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：一个学习 Python 的趣味网站 。最近在网上看到一个非常有意思的 Python 游戏通关网站叫 ：pythonchallenge。一共有 33 关，每一关都需要利用 Python 知识解题找到答案，然后进入下一关。很考验对 Python 的综合掌握能力，比如有的闯关需要用到正则表达式，有的要用到简单的爬虫。平常学 Python 都是按章节顺序、包或者模块来学，容易前学后忘。正好可以拿这个网站来综合测试一下对 Python 的掌握情况，以便查缺补漏。来说说这个网站怎么玩。这是网站主页面，很有历史感对吧，诞生了已有十几年了。但千万不要因为看着像老古董而小瞧它。我们来玩玩看，点击「get challenged」开始挑战。第 0 关是 Warming up 热身环节：这一关要求是修改 URL 链接，给的提示是电脑上的数学表达式： 2 的 38 次方，所以大概就是需要计算出数值，然后修改url 进入下一关。所以这关就是考Python 的基本数值运算，你知道怎么算么？随意打开个 IDE，比如 Python 自带终端，只要一行代码就能计算出结果：把原链接中的 0替换为 274877906944回车就会进入下一关：游戏这就正式开始了。图片中的笔记本给了三组字母，很容易发现规律：前面的字母往后移动两位就是后面的字母。那么需要做的就是根据这个规律把下面的提示字符串，做位移解密得到真正的句子含义：这道题考察字符串编码和 for 循环相关知识，代码实现如下：1234567891011121314151617text = '''g fmnc wms bgblr rpylqjyrc gr zw fylb. rfyrq ufyr amknsrcpq ypc dmp. bmgle gr gl zw fylb gq glcddgagclr ylb rfyr'q ufw rfgq rcvr gq qm jmle. sqgle qrpgle.kyicrpylq() gq pcamkkclbcb. lmu ynnjw ml rfc spj.'''text_translate = ''for i in text: if str.isalpha(i): n = ord(i) if i &gt;= 'y': n = ord(i) + 2 - 26 else: n = ord(i) + 2 text_translate += chr(n) else: text_translate += iprint(text_translate)得到结果：1234i hope you didnt translate it by hand. thats what computers are for. doing it in by hand is inefficient and that&apos;s why this text is so long. using string.maketrans()is recommended. now apply on the url.作者很风趣，当然不能手动去一个推算了，推荐用 string.maketrans() 这个方法解决，我们上面采取的是比较直接的方法，官方给出了更为精简的方法：1234import stringl = string.lowercaset = string.maketrans(l, l[2:] + l[:2])print (text.translate(t))然后把 url 中的 map 改为ocr回车就来到了第 2 关：作者接着说过关的提示可能在书里（当然不可能了）也可能在网页源代码里。那就右键查看源代码往下拉看到绿色区域，果然找到了问题：意思就是：要在下面这一大串字符里找到出现次数最少的几个字符考察了这么几个知识点：正则表达式提取字符串list 计数条件语句如果是你，你会怎么做？来看下，十行代码怎么实现的：12345678910111213141516171819import requestsurl = 'http://www.pythonchallenge.com/pc/def/ocr.html'res = requests.get(url).texttext = re.findall('.*?&lt;!--.*--&gt;.*&lt;!--(.*)--&gt;',res,re.S)# list转为str便于遍历字符str = ''.join(text)lst = []key=[]#遍历字符for i in str: #将字符存到list中 lst.append(i) #如果字符是唯一的，则添加进key if i not in key: key.append(i)# 将list列表中的字符出现字数统计出来for items in key: print(items,lst.count(items))首先，用 Requests 请求网页然后用正则提取出字符串，接着 for 循环计算每个字符出现的次数。12345678910111213141516171819202122232425% 6104$ 6046@ 6157_ 6112^ 6030# 6115) 6186&amp; 6043! 6079+ 6066] 6152* 6034&#125; 6105[ 6108( 6154&#123; 6046e 1q 1u 1a 1l 1i 1t 1y 1可以看到出现次数最少的就是最后几个字符，合起来是「equality」，替换 url 字符就闯过过了第 2 关进入下一关继续挑战。是不是有点意思？后面每一关都需要用到相关的 Python 技巧解决，比如第 4 关：这一关作者弄了个小恶作剧，需要手动输入数值到 url 中然后回车，你以为这样就完了么？并没有它有会不断重复弹出新的数值让你输入，貌似没完没了。到这儿就能发现肯定不能靠手动去完成，要用到 Python 实现自动填充修改 url 回车跳转到新 url，循环直到网页再也无法跳转为止。如果是你，你会怎么做？其实一段简单的爬虫加正则就能搞定。思路很简单，把每次网页中的数值提取出来替换成新的 url 再请求网页，循环下去，代码实现如下：12345678910111213141516171819202122232425import requestsimport reimport os# 首页urlresp = requests.get( 'http://www.pythonchallenge.com/pc/def/linkedlist.php?nothing=12345').texturl = 'http://www.pythonchallenge.com/pc/def/linkedlist.php?nothing='# 计数器count = 0while True: try: # 提取下一页动态数值 nextid = re.search('\d+', resp).group() count = count + 1 nextid = int(nextid) except: print('最后一个url为：%s' % nexturl) break # 获取下一页url nexturl = url + str(nextid) print('url %s:%s' % (count, nexturl)) # 重复请求 resp = requests.get(nexturl).text输出结果如下：gif可以看到，最终循环了 85 次找到了最后一个数字16044，输入到 url 中就闯关成功。33 关既有趣又能锻炼使用 Python 解决问题的技巧，感兴趣的话去玩玩看。网址：http://www.pythonchallenge.com/如果遇到不会做的题，可以在这里找到参考答案：中参考文教程：https://www.cnblogs.com/jimnox/archive/2009/12/08/tips-to-python-challenge.htmlhttps://blog.csdn.net/Jurbo/article/details/52136323官方参考教程：http://garethrees.org/2007/05/07/python-challenge/]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信朋友圈的一些新奇玩法]]></title>
    <url>%2F2019%2F05%2F11%2Fweekly_sharing27.html</url>
    <content type="text"><![CDATA[新鲜有趣。这里是「每周分享」的第 27 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：微信朋友圈的一些高（装）级（逼）玩法。大多数人每天都要刷朋友圈，但很多人可能觉得朋友圈只有三个主要功能：发图文、视频以及转发动态。其实，朋友圈还有不少新奇玩法，下面来说道说道。01 朋友圈文字防止被折叠文字被折叠是很多人发朋友圈会遇到的问题，而大部分人会选择在评论区重发一遍，其实没有必要。用下面的方法基本可以保证文字不被折叠。安卓手机复制好想发的文字 →进入朋友圈编辑界面 →随便输入几个字符并全选 →覆盖粘贴再发布。苹果手机朋友圈编辑页面粘贴好文字→点击左上角「取消」并保留草稿→重新进入编辑界面再发布。02 朋友圈发九宫格图片你有时可能会见到别人发出这样的九宫格图片，一张照片被切割成了九部分，其实做起来很简单。微信搜索「九宫格照片」小程序，添加想要切割的图片然后保存，发朋友圈按照顺序选择就可以了。除了正方形，还可以切割成其他多种形状。03 朋友圈图片评论通常我们评论朋友圈都是文字/表情评论，如果你想用表情包或者图片评论的话就不行，其实可以采用曲线方式评论，比如像下面这样：别人打开链接就能看到你发的图片，这种操作实际上就是把表情/图片上传到图床再生成链接得到，实现起来也简单：安卓手机搜索「图床神器」小程序，上传图片复制 http 链接，觉得链接长可以用「短网址生成」小程序生成短连接，最后复制到朋友圈评论中就可以了。苹果手机安装 「快捷指令」或者「捷径」App，参考这个教程就可以了：https://sharecuts.cn/shortcut/1133需要注意一点，图片是上传到公共图床上的，所以不要上传一些隐私图片。04 朋友圈发九宫格动态图片/视频上面的九宫格图片还有另外一种更高级的玩法，就是图片是动态的，像下面这样：实现起来也很简单，使用「Cinepic」款 App 就可以了。依次选择：背景图片、图片、背景音乐，然后保存到本地视频。发到朋友圈就可以了。除了动态图片还可以发送视频，适合那些看现场演唱会爱连发朋友圈的人。05 朋友圈发语音你大概见过很多人在朋友圈分享音乐链接，但朋友圈其实还可以发语音。实现方法也很简单，微信中打开这个链接，选择录音就可以了，最长 1 分钟。https://flhd.maikaolin.club/huodong/wxvoice/最后，分享一个旧版本微信。自从微信升级到 7.0 之后，你可能会觉得界面使用起来很别扭，比如说公众号订阅列表，变得很乱，不再像以前那么干净，比如像我还在用的 6.7 版本：在我公众号：「高级农民工」后台回复「微信」就可以得到旧版本安装包和上面的 Cinepic App。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫威 DC 宇宙英雄综合实力可视化对比分析]]></title>
    <url>%2F2019%2F05%2F06%2Fdata_analysis%26mining05.html</url>
    <content type="text"><![CDATA[一生漫威粉。摘要：数据采集、分析实战。昨天借最近持续火爆的的《复联4》说了说漫威电影宇宙票房话题，今天票房就上升到了全球第二，超越保持 20 多年记录的《泰坦尼克号》，有生之年能见到也是难得了。另外，文末卖了个关子：那么多英雄到底谁最强？今天就来用 Python 对比分析一下各位英雄的综合实力，结果绝对超出你预料。先说说下漫威电影和漫威漫画的关系。我们看的电影叫「漫改电影」，意思就是从漫画中改编过来搬上荧幕的。这些电影出现不过十年，而漫画五十年前就出现了，大多数数角色由斯坦·李创造，所以你可以看到每部漫威电影他都有客串。电影中为了呈现更好的视觉效果以刺激观众感官，会刻意加强或者弱化某些英雄的能力，尤其是精彩的打斗场景，让我们以为这就是他们的真实实力。比如：美队跟谁都能五五分最强之人是灭霸正面对决猩红女巫能手撕灭霸惊奇队长貌似是唯一一个能单挑不怵灭霸的而在漫画中的实际情况并不完全是这样。漫画里对每个角色都设定了能力值，能力值包括六个方面，分别是Intelligence / 智力Power / 能量Strength / 力量Speed / 速度Durability / 耐力Combat / 格斗技比如钢铁侠的能力值是这样的：可以看到他的智力和能量值是满分，很贴合电影中 Tony Stark 演的钢铁侠形象。而速度和格斗技巧不过刚及格，可电影中给我们看到的钢铁侠上天入地速度杠杠的，打斗也很强。唯一的解释就是，电影作了美化。在权威漫画人物网站：superherodb上，给每位角色都标出了能力值。孰强孰弱一对比就一目了然。不只是上面这些热门角色，该网站拥有包括漫威 、DC 在内的上百家漫画公司的数千位漫画角色详细信息，可以说是非常强大。当然，一个个去对比很麻烦而且很难发现深层次关系，这时候就需要 Python 出场了。首先需要获取这些数据，怎么获取呢？当然是爬虫。鉴于以前爬过类似的网站，这里就不爬了感兴趣可以自行尝试。还有一种更为取巧的方法就是找现成的 API 接口然后调用即可。网上找了一圈，最终找到了 superheroapi 这个网站。该网站上提供了 700 多位角色的详细信息，数量虽不多但也够用。API 返回结果是 JSON 格式，包括能力值、身高体重等信息，例如：钢铁侠的信息如下：1234567891011121314151617&#123; "response": "success", "results-for": "Iron Man", "results": [ &#123; "id": "346", "name": "Iron Man", "powerstats": &#123; # 能力值 "intelligence": "100", "strength": "85", "speed": "58", "durability": "85", "power": "100", "combat": "64" &#125;,...&#125;数据采集下面我们用 Python 先获取网站全部 700 多位角色信息然后保存到本地数据库。代码见文末，几分钟就可以下载好结果如下：简单的清洗处理后就可以着手分析。可视化分析先看漫威复联系列。说起复联最重要的人物自然是六位初代英雄。初代六人组实力对比凭电影中的印象对这六人的实力排序的话，你会怎么排？按图上从左到右的顺序来看看六人的实际实力。雷神通过雷达图可以看到雷神很全面，多项数据都是满分，几乎没有弱点，然而智力这块儿只有中等水平。如果你看过雷神系列就会知道他的智商的确很捉急。黑寡妇寡姐身为六人组里唯一的女性，不会飞也没有什么道具，最拿手的就是肉搏，《钢铁侠2》首次出场表现就奠定了她的风格。鹰眼箭筒里永远射不完箭的鹰眼在《雷神1》中首次亮相，实力差不多是最弱的，感觉《复联1》中演反派更厉害。绿巨人终于出现个和雷神实力匹配的对手，三项能力满分，格斗技和速度中等，浩克的确格斗能力没那么强，在《复联3》开头分分钟被灭霸给收拾了。智商比雷神高，毕竟是拥有 7 个博士头衔的人。美国队长整个系列一共说了三次「I can do this all day.」 的美队给人最大的错觉就是：和谁都能五五开，然而毕竟肉身，实际没有那么强。钢铁侠最后是最帅气最聪明的托尼了，感觉没有电影中想的那么强大，主要他演得好印象分高。上面的雷达图绘制代码如下：12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif'] = ['SimHei']plt.style.use('ggplot')plt.figure(figsize=(5, 30))names = ['Thor','Iron Man','Captain America','Hulk','Black Widow','Hawkeye']names_cn = ['雷神','钢铁侠','美队','绿巨人','黑寡妇','鹰眼'] #标签title = ['智力', '能量', '力量', '速度', '耐力', '格斗技'] # 标签# 从data查询 A6 数据scores = pd.DataFrame(&#123;'name':names&#125;)scores = scores.merge(data,how='left',on='name')[['intelligence','power','strength','speed','durability','combat']]# 转换为listscores = scores.values.tolist()zipped = zip(scores,names_cn)# for循环生成A6成员雷达图for i,(value, name) in enumerate(zipped): ax= 'ax%s'%i theta = np.linspace(0, 2*np.pi, len(value), endpoint=False) # 将圆根据标签的个数等比分 theta = np.concatenate((theta, [theta[0]])) # 闭合 value = np.concatenate((value, [value[0]])) # 闭合 # 这里要设置为极坐标格式 ax = plt.subplot2grid(shape=(6,1), loc=(i,0),polar=True) ax.plot(theta, value, lw=2, alpha=1,label=name) # 绘图 ax.fill(theta, value, alpha=0.25) # 填充 ax.set_thetagrids(theta*180/np.pi,title) #替换标签 ax.set_ylim(0,110) #设置极轴的区间 ax.set_theta_zero_location('N') #设置极轴方向 ax.set_title('%s战力'%name,fontsize = 15) #添加图描述 plt.tight_layout(h_pad=5.0)plt.savefig('A6单人战力.jpg',dpi=200)来个汇总看得更清楚，初代六人组孰强孰弱这下有答案了吧？代码实现如下：1234567891011121314151617for value, name in zipped: theta = np.linspace(0, 2*np.pi, len(value), endpoint=False) # 将圆根据标签的个数等比分 theta = np.concatenate((theta, [theta[0]])) # 闭合 value = np.concatenate((value, [value[0]])) # 闭合 ax = fig.add_subplot(111, polar=True) ax.plot(theta, value, lw=2, alpha=1,label=name) # 绘图# ax.fill(theta, value, alpha=0.25) # 填充ax.set_thetagrids(theta*180/np.pi,title) #替换标签ax.set_ylim(0,110) #设置极轴的区间ax.set_theta_zero_location('N') #设置极轴方向ax.set_title('复联六位初代英雄战力对比',fontsize = 20) #添加图描述 plt.legend(loc='lower center',ncol=6)plt.tight_layout()fig.savefig('A6.jpg',dpi=200)十位重要英雄实力除了六位初代英雄，陆陆续续还出现了很多其他英雄，挑选十位露脸最多的来看看。洛基有「锤」必有「基」，虽然电影中洛基饰演的是反派，但其实不坏，跟雷神相爱相杀带来不少笑料，所以重要人物中必须「Loki」的名字。惊奇队长很多人都说惊奇队长应该是《复联》中最牛逼的人，在《复联4》打了个酱油。战斗力的确很惊人，但 DC 中还有一个比她还厉害的男性「惊奇队长」，一会儿说。绯红女巫不得不说绯红女巫是又美又能打，差点把灭霸撕了。我不会告诉你他们俩早在另外一部电影《老男孩》里也上演了一出别样「大战」。幻视《复联2》中诞生就拥有心灵宝石的幻视着实牛逼，把奥创打得满地找牙，但到了后面怎么就沦落到被保护的境地了。奇异博士卷福饰演的奇异博士还是很牛逼，有时间宝石、有斗篷还有酷炫的阿戈摩托之眼。《复联4》最后对着托尼竖起了一根手指，大概是说：「福尔摩斯，只能有一个。」蚁人 &amp; 蜘蛛侠蚁人和蜘蛛侠差不多，飞来飞去。蚁人是复联少数几个绝顶聪明的人，可以说《复联4》能够逆转，蚁人功劳很大。蜘蛛侠实力均衡，早在《钢铁侠》系列中就出场了，虽然身为托尼的小跟班，但漫画中蜘蛛侠是漫威最大的 IP。黑豹 &amp; 冬兵要问谁比钢铁侠还有钱，那必然是「振金王国」瓦坎达的国王黑豹了。在去年的独立电影中大放异彩，复联中到没有太多施展拳脚的机会。要论复联有哪几对相爱相杀组合，除了雷神和洛基，就是美队和冬兵了，《美队1》中还是挺感人的。星爵最后隆重出场的是星爵，也是我本人最喜欢的复联英雄。《银河护卫队1》打养父，《银河护卫队2》打生父，《复联34》打岳父，他才是最牛逼的「灭爸」。现实中的岳父是位了不得的人物：施瓦辛格。虽然综合实力不怎么样但银河系尬舞天团的能力不是吹的。来听听这首星伴随着 Walkman 尬舞的歌。以上就介绍了十位重要英雄。去掉四位稍弱人物，来对比下六人组综合实力。惊队除了智商稍微弱点，其他基本无敌，这点和雷神很像，二者综合实力也差不多，可以说是新老成员中最厉害的了。灭霸正派说完轮到大 BOSS 灭霸出场了。看到灭霸就会想起电影中被他那宝石手套支配的恐惧，五一终于理解灭霸的初心了。而灭霸真实的实力如何呢，来看看他和雷神、惊奇队长三人对比。可以看到灭霸的优缺点非常明显，优点是和托尼一样绝顶聪明，缺点就是速度慢，难怪电影中要靠宝石跑到地球来。综合来看，三者单挑的话基本五五开，灭霸戴上手套的话就另算了。漫威和 DC 英雄比作为两大漫画巨头，漫威和 DC 一直在明争暗斗，早些年 DC 要比漫威混得好，漫威这十几年才起来。两大公司手上都握有大量漫画角色，对比一下这两家当家英雄应该会很有意思。DC 比较熟知的就是超人了，这里来拿雷神和超人对比下看看。可以看到超人近乎完美，比雷神聪明速度也更快，除了格斗稍弱雷神，总体来说是碾压雷神的。DC 其他英雄又如何呢，把 700 多位英雄六项能力值汇总得到综合实力，然后取前十名来看看。标红色的是 DC 家的浅灰色是其他公司的深黑色的是漫威家的完全没有想到，综合实力最强的 10 位竟然有 8 位都来自 DC，漫威完全被碾压，唯一登榜的是超越者（Beyonder），雷神都上不了榜。然而问题来了，拥有如此众多实力超强的英雄，DC 近些年为什么风头全被漫威压住了？榜单上排名第一得到了 600 满分无敌了，来揭晓下 TA 是谁。就是这位 Man of Miracles，别名 Mother of Existence 宇宙的创造者，上帝是他儿子。其他有意思的最高的人很多漫威英雄五大三粗，就来扒一扒哪些角色最高。排第一的是 「Ymir」超过 300 米，他是冰霜巨人的祖先，即洛基的祖先。范迪塞尔配音的 Groot 在银护中非常高，也仅能排第 10。numnameheighttotal_score0Ymir304.80403.01Godzilla108.00418.02Giganta62.50352.03Anti-Monitor61.00528.04King Kong30.50424.05Bloodwraith30.5072.06Utgard-Loki15.20386.07Fin Fang Foom9.75399.08Galactus8.76533.09Groot7.01427.0最壮的人复联中浩克、灭霸都很壮，可在诸多大神面前就是小巫见大巫了。排第一的就是熟悉的哥斯拉，重达九万吨，不得不说日本人脑洞真大。第二的金刚也有九千吨。nameweighttotal_score0Godzilla90000000418.01King Kong9000000424.02Utgard-Loki58000386.03Fin Fang Foom18000399.04Galactus16000533.05Groot4000427.06Iron Monger2000379.07Sasquatch900291.08Juggernaut855441.09Darkseid817566.0以上就是对宇宙英雄的一些简单分析，感兴趣的话可以自己试试。Python 源代码和数据在公众号后台回复：「漫威」就可以得到。数据采集部分的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# -*- coding: utf-8 -*-"""Created on Mon May 5 12:57:10 2019@author: 高级农民工"""import requestsimport pandas as pdimport refrom requests.exceptions import RequestExceptionimport timefrom multiprocessing import Process, Poolimport pymongoimport os# https://superheroapi.com/，facebook 登陆即可自动获取tokentoken = '输入你的token' # 如果获取不到，可以微信找我提供给你def getapi(i): url = 'https://superheroapi.com/api/%s/%s' % (token, i) data = requests.get(url).json() return datadef parseapi(item): lst = &#123; 'id': item['id'], 'name': item['name'], # 提取人物战斗力值 'intelligence': item['powerstats']['intelligence'], 'strength': item['powerstats']['strength'], 'speed': item['powerstats']['speed'], 'durability': item['powerstats']['durability'], 'power': item['powerstats']['power'], 'combat': item['powerstats']['combat'], # 提取人物特征 'gender': item['appearance']['gender'], 'race': item['appearance']['race'], 'height': item['appearance']['height'][1], # 取cm 'weight': item['appearance']['weight'][1], # 取kg # 提取人物头像url 'image': item['image']['url'], 'publisher': item['biography']['publisher'], # 出版方 Marvel/DC 'alignment': item['biography']['alignment'] # 正派/反派 &#125; # 写入csv write_csv(lst) # 或者写入MongoDB # write_mongodb(lst) # # 下载图片拼图片墙 image = item['image']['url'] save(image)def write_mongodb(lst): client = pymongo.MongoClient('localhost', 27017) db = client.marvel mongo_collection = db.marvel_stats if mongo_collection.update_one(lst, &#123;'$set': lst&#125;, upsert=True): pass else: print('存储失败') print('id:%s 存储完成' % lst['id'])def write_csv(lst): content = pd.DataFrame([lst]) content.to_csv('./marvel.csv', mode='a', encoding='utf_8_sig', index=False, header=None)def save(image): # 获取头像编号 num = re.search('https:.*\/(.*?).jpg', image).group(1) dir = os.getcwd() + '\\marvel\\' if not os.path.exists(dir): os.mkdir(dir) file_path = '&#123;0&#125;\\&#123;1&#125;.&#123;2&#125;'.format(dir, num, 'jpg') try: response = requests.get(image) if response.status_code == 200: with open(file_path, 'wb') as f: f.write(response.content) print('编号：%s下载完成' % num) except exceptions: passdef main(i): data = getapi(i) parseapi(data)if __name__ == '__main__': start = time.time() pool = Pool() for i in range(1, 732): # 多进程 pool.apply_async(main, args=[i, ]) pool.close() pool.join() end = time.time() print('总共用时&#123;&#125;s'.format((end - start)))最后，欢迎加入我的知识星球，还有更多干货。]]></content>
      <categories>
        <category>Python数据分析</category>
      </categories>
      <tags>
        <tag>Python数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫威电影宇宙，宇宙票房收割机]]></title>
    <url>%2F2019%2F05%2F05%2Fdata_analysis%26mining04.html</url>
    <content type="text"><![CDATA[漫威电影宇宙已经无敌，霸占全球前十票房中的五席。摘要：看的不是电影是青春。《复仇者联盟4：终局之战》上映已十天有余，刷了两遍 IMAX，在家又重温了之前的 21 部电影，感觉就是一个字：爽，两个字：难舍。入坑漫威不早也不算晚，2011 年大一的时候偶然看了《钢铁侠》和《绿巨人》后被圈粉。12 年复联上映时和室友一起去了学校附近的影城看，晚上看完点个铁板烧边吃边讨论剧情。八九年一晃而过，电影迎来了终局，人也从大一新生变成了社会狗。其实，追漫威跟追星没什么区别，看电影跟看演唱会也没什么区别。你要很早开始追周杰伦，就会期待他的新专辑和演唱会，曲子一出你可能就知道是什么歌名，出自哪张专辑，会想去看他的演唱会。如果你没有追过或者很晚才追他，那真的很遗憾，因为你错过的不仅是他，更是自己的青春。多年下来，才发现一直不变的是唯有 NBA 和漫威电影。然而今年是悲伤的一年，喜欢的球星退役，喜欢的电影迎来终局。一部「着看着就笑了，笑着笑着就泪了的电影」值得说道几句。自上映之日起，这部电影就不断刷爆国内外各种记录，相信下映之时必将名留史册。在国内，上映 12 天票房超过 38 亿，飙升到中国电影历史总票房第三位，离前不久才上升到第二位 的《流浪地球》只有 8 亿只差，目测最终二者排名会交换。《复联4》上映之前，国内前五名都是国产电影，进入前十名的只有两部，由速度与激情系列包揽，影片一上映很快就飙到了前三甲。国际上，目前票房超过了 19 亿美元，上升到第五名，大概率会超过 21 亿的《泰坦尼克号》，勇夺全球票房亚军，甚至能挑战票房纪录保持了十年之久的《阿凡达》。作为漫威电影宇宙布局 11 年、22 部电影的最后一部，以这样的成绩结束太完美了。这 22 部电影很恐怖，票房最高的 5 部进入了全球历史总票房的前 10 名，占据半壁江山。排名电影票房(亿美元)发行商1阿凡达27.88二十世纪福斯2泰坦尼克号21.87派拉蒙影业 / 二十世纪福斯3星球大战：原力觉醒20.68华特迪士尼影业4复仇者联盟3：无限战争20.48华特迪士尼影业5复仇者联盟4：终局之战17.86华特迪士尼影业6侏罗纪世界16.72环球影业7复仇者联盟15.19华特迪士尼影业8速度与激情715.16环球影业9复仇者联盟2：奥创纪元14.05华特迪士尼影业10黑豹13.47华特迪士尼影业条形图看着更清楚：上面的条形图为了突出漫威电影和非漫威电影，设置了两种颜色。而通常的图形是默认一种颜色，或者通过 cmap 光谱参数各一种颜色。有两种方法可以实现自定义设置柱状颜色。一种是通过barh[i].set_color(colors)单独设置，适合要设置的颜色比较少的情况，当数量比较多的时候，可以采取第二种方法就是先设置好颜色。这里，使用了第二种方法，代码实现如下：123456789101112131415161718192021222324252627282930# 全球票房 plt.style.use('ggplot')movies = ['阿凡达','泰坦尼克号','星球大战：原力觉醒','复仇者联盟3','复仇者联盟4','侏罗纪世界','复仇者联盟','速度与激情7','复仇者联盟2','黑豹']income = [27.88,21.87,20.68,20.48,19.14,16.72,15.19,15.16,14.05,13.47,]data_movies = pd.DataFrame(&#123;'movie':movies,'income':income&#125;)[::-1]#倒序# 绘制票房条形图# https://stackoverflow.com/questions/37447056/different-colors-for-rows-in-barh-chart-from-pandas-dataframe-pythonfig, ax = plt.subplots(figsize=(8, 6))grey = '#969696' # 深灰red = '#E24A33' # 红色# 批量设置颜色color = [red, red, grey, red, grey, red, red, grey, grey, grey]barh = ax.barh(np.arange(10),data_movies['income'],color=color)# 或者单独设置颜色# barh[2].set_color = [grey]for y, x in enumerate(data_movies['income'].values.tolist()): ax.text(x-1, y-0.1, '%s' % round(x, 1), ha='right', size=15, color='#FFFFFF')ax.set_yticks(np.arange(10)) # 设置y轴标签数量ax.set_yticklabels(data_movies['movie'].tolist(), size=15) # 设置y轴标签ax.set_xlim(0, 32)ax.legend(['票房(亿美元)'], loc='best')plt.tight_layout()plt.savefig('global.jpg', dpi=200)22 部电影总票房超过 200 亿美元，这个数字有多牛逼，用国产电影的数据来对比一下就知道了，过去 2015-2018 这四年一共上映了超过 1400 部国产电影，总票房是 1277 亿元。条形图：年份国产电影票房(亿元）国产电影数量2015297.02782016288.43832017312.13752018379.3393汇总1276.81429所以，漫威的 22 部电影票房等于过去四年全部国产电影票房的总和。上面条形图设置的代码如下：1234567891011121314151617181920212223242526272829303132333435year = ['2015年','2016年','2017年','2018年'] income = [297.0,288.4,312.1,379.3]quantity= [278,383,375,393]movie_cn = pd.DataFrame(&#123;'year':year,'income':income,'quantity':quantity&#125;)width = 0.3fig, ax = plt.subplots(figsize=(8, 6))barh = ax.barh(np.arange(4), movie_cn['income'], width, color=grey )barh2 = ax.barh(np.arange(4)+0.3, movie_cn['quantity'], width, color=red )# 设置添加数值def autolabel(bars): for bar in bars: width = bar.get_width() ax.text(width*0.97, bar.get_y() + bar.get_height()/2, '%d' % int(width), ha='right', va='center', color='#FFFFFF', size=15)ax.set_yticks(np.arange(4)+0.2)ax.set_yticklabels(movie_cn['year'].tolist(), size=15)ax.set_xlim(0, 430)ax.legend(['票房(亿元)', '影片数量(部)'], loc='best')autolabel(barh)autolabel(barh2)无数的英雄出现在了这些电影中，钢铁侠、雷神、浩克、惊奇队长、灭霸等人各显神通，个人最喜欢星爵。英雄一多就出现了一个很有意思的问题：这些人中到底谁最厉害？答案仁者见仁，智者见智。不过在漫画中每位英雄都给出了详细的战斗力指数，之后会写一篇 Python 分析宇宙英雄数据，对比英雄综合实力。资料：猫眼 2018 票房分析报告中国电影票房分析]]></content>
      <categories>
        <category>Python数据分析</category>
      </categories>
      <tags>
        <tag>Python数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一台 Macbook Pro]]></title>
    <url>%2F2019%2F04%2F30%2Flife09.html</url>
    <content type="text"><![CDATA[Macbook Pro 开箱及使用心得。摘要：Macbook Pro 开箱及使用心得。转眼间，Macbook Pro 入手已一个月有余，忘了写写购买和使用心得，那就现在开始吧。正式介绍 Mac 之前先说说一直用 Windows 的经历。2011 年，大一下学期开学买了人生第一台笔记本，不到 4,000 块的华硕。一直不懂电脑，干的最多的事就是用用 Office 、上上网，一直持续到 2017 年。之后，电脑性能越来越差干什么都卡，开个机要两三分钟。后来换上了 SSD 和内存条，老机焕发新春。苹果电脑老早就听过，印象中一直是设计师的专属或者拿来装逼用的，加上有贵，从来没想过买一台来用用。直到去年接触编程圈后，几乎无一例外都说过「搞编程，用 Mac」、「用了 Mac 不想再回 Windows」之类的话。这才对苹果电脑产生了兴趣，网上了解到 Mac 和 Windows 使用起来根本不同，行云流水般的操作，顿生好感。3 月份决定买一台，上官网看了看，比一般电脑贵多了，越喜欢的越贵，最后想既然打算剁手就干脆多狠点，一次到位省得以后纠结惦记。最后看上了 2018 款、15 英寸、512GB SSD 的配置，正常价格是 21,988 元，可以分 12 期免息。3 月份正值开学季，大学生教育优惠价是 20,388 元，仍然很贵。上京东看了下价格一样，会额外送一个售价 2,288 元 的 Beats Solo3 耳机，图案是很丑的迪士尼，二手能卖 1000 算不错。一时头脑发热准备下手，由于已经毕业便找到了还在学校的师弟帮忙，通过京东的学生认证后就可以下单。这时又想起港版可能会便宜些，上香港官网看了下，港币价格是一样的 20,388 ，那几天的汇率是 0.85，算下来差不多可以便宜 3,000 块。之后找到了一位在香港上大学的朋友，使用学生证以教育优惠价代买了，实付价 17,500 元。想享受香港教育优惠需要注意几点：只有在香港读书的大学生才能享受教育优惠，内地的学生证不行。付款可以现金或刷卡，如果是刷卡一定是刷学生本人的卡，不能用你的卡，因为苹果要登记到系统里，付现金就没有事。想现金买，最好在银行提前换好港币带过去，不然在香港银联取现金的话，很多银行卡有取现金额限制，一般都是不能超过 10000 港币，而且还有很高的手续费。各银行的手续费见下面。由于我是头天下午突然决定第二天就让朋友买，所以来不及换钱，刚好我用的是浦发卡，手续费最低，让朋友去到香港后取，结果在 ATM 取钱发现 10000 都取不出，十分不方便。最后还是刷了学生朋友本人的卡，之后转账给她才成功。买好以后拆了外包装让朋友带到深圳然后顺丰寄到家。担心快递损坏，买了 2 万的报价，保费是千分之一，也就是 200 很贵了，然后在外面又钉了个木框，双保险。拿到电脑后，查了下充电次数，只有 1 次，这机也太新了。充电次数能判断这台电脑是否是新机，太多的话就要注意了。还可以通过查询序列号验证是否是正品，查询网址：在此。有些人会纠结要不要花上千块额外多买一年的 Apple Care ，个人觉得没必要。新机苹果免费全球保修 1 年，香港买的可以在内地保，保存好发票和三包凭据就行了。下面就上两张开箱图。Mackbook Pro 2018内存：16GBSSD： 512GBChangelog新建博文：2019/5/5]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比百度更好用的软件下载网站]]></title>
    <url>%2F2019%2F04%2F27%2Fweekly_sharing26.html</url>
    <content type="text"><![CDATA[下载软件有门道。摘要：下载软件有门道。这里是「每周分享」的第 26 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：Windows 和 Andriod 软件下载。之前很多期分享推荐了不少软件，却从没分享过有哪些不错的软件下载网站，尤其在听到有人说下载软件很简单，「电脑上百度，手机上应用市场」这样的话后，决定这一期来说说。百度或者应用市场不是不好，但你值得更好的。下面来推荐几个我常用的软件下载网站。Windows大眼仔http://www.dayanzai.me/自从 ZD423 站长跑路后，大眼仔 就成了我下载软件的首选，一直未变过。这个网站有大量免费的汉化和破解软件，堪称业界仅存良心。网站界面干净，没有乱七八糟的广告，博主还是个很有情怀的人。当 ZD423 、殁漂遥这些网站都一一消失时，它还能一直坚挺，且用且珍惜吧。异次元https://www.iplaysoft.com这个网站软件资源非常丰富，从 Win、Mac 到 Linux ，从 Andriod 到 iOS 都有。此外还有详细的软件使用教程，授人以鱼也授人以渔。I Tell Youhttps://msdn.itellyou.cn/很多人常常为不知在哪儿下 Office、操作系统这类软件而发愁，百度上搜了十页八页结果都不行，如果你知道这个网站，那就不会再有这些烦恼。在上面能找到很多专业软件，版本齐全，比如从 Office 95 到 Office 2019，从 Windows 98 到 Windows 10 的每一个版本都有。这些软件都是官方版本，提供 SHA1 验证，不用再担心下载的是被人动过手脚的软件。吾爱破解https://www.52pojie.cn/forum.php说起软件网站，一定不能没有大名鼎鼎的 52。不要因为这个网站界面看起来有点乱就小瞧它，实际上很多全网一手资源都是在这个网站发布的。当你下次想要找软件资源时，不妨在站内搜搜看，也许就有惊喜发现。我收藏夹里还有很多 Windows 软件网站，但觉得这四个基本就够了，多了反而不知道用哪个好。下面来说说下载安卓 App 的好去处。安卓软件酷安下载安卓软件，首选必然是「酷安」，我此前也说过很多次。你如果习惯了到手机自带的各种应用市场下载软件，那使用酷安后一定会觉得好像发现了新世界，感叹原来还有这么多神奇好用的 App。手机乐园https://soft.shouji.com.cn/在酷安可以找到各种各样的 App，但是你如果想下载旧版本 App 来用，那「手机乐园」就是你的乐园。这个网站上的软件都提供了非常全历史版本供下载。你可能诧异为什么软件不用最新版，毕竟新版本又好看，功能又多。事实上，现在绝大多数软件都是越更新越臃肿，加入很多用不着的功能，甚至还会有广告。所以我手机上基本没有最新版的 App，都到这个网站上去找到能用的旧版本。艾薇百科https://www.aiweibk.com/sjrj/看到这个名字不要想歪，是个下载软件的地方。手机发烧友http://htcui.com/这个网站也不错。好就介绍这么多网站，不多不少，8 个，用好上面网站足以解决软件下载难的问题。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最后一把键盘：HHKB Pro2]]></title>
    <url>%2F2019%2F04%2F26%2Flife11.html</url>
    <content type="text"><![CDATA[HHKB Pro2 开箱和使用心得。摘要：HHKB Pro2 开箱和使用心得。美西的牛仔会将死去的马留在原地，继续扛着马鞍长途跋涉，穿越一望无垠的沙漠。因为马是消耗品，而马鞍却是与人体融合在一起的「知己」。电脑是消耗品，键盘却是传递情感，相伴一生的「挚友」。—— 男人也应该享有「剁手」的理 jie 由 kou。昨天一早，顺丰小哥敲门送上了心心念念的 HHKB Pro2，距离亚马逊下单刚好一周，决定买它也不过一周。发现慢慢养成了一种果断的消费理念，值得的东西越早买越好，因为：买了，心疼一阵，不买，一直心疼。下面就来写写这款键盘的开箱心得和使用体验吧。分别在京东、淘宝和亚马逊上看了价格。京东最贵，自营要 1900，非自营 1700 ；淘宝近 1700 不过送几样配件，官方授权店：军曹の宅窝亚马逊最便宜，1368 加上 130 左右的税费，1500 拿下。本来是有运费的，但使用了免费的 Prime 会员，可以免邮。下单后，霓虹直邮，一周就到速度挺快。购买链接：中亚 HHKB Pro2开箱图：很多人说这是一把只适合程序员玩的键盘。因为只有 60 个键，少了方向键、数字键，Vim 或者 Emacs 党用会觉得很顺手。也有人说适合 Mac 用户，Windows 用户会差很多。总之，我目前是属于这些人口中的「不适合的那一类」：不会用 Vim、 Emacs，刚开始 Mac 不久不熟练。相信不少朋友都像属于这一类，如果你不属于那也就没必要看此文了。等后续用熟了再来写感受。Changelog2019/4/26：晒购买体验]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个快速搜索筛选豆瓣好书的网站]]></title>
    <url>%2F2019%2F04%2F20%2Fweekly_sharing25.html</url>
    <content type="text"><![CDATA[解决找书时间。摘要：多看书，看好书。这里是「每周分享」的第 25 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：读书和买书。前天给各位争取了「图书日」在当当买书送优惠码的福利，能省点是点，你要打算买还没买可以去瞧瞧。传送门：一直以为我国人均读书量应该不少，但实际上跟其他国家比起来，差了老远。中国每人年均读书 4.66 本韩国每人年均 11 本法国每人年均14本日本每人年均 40 本德国每人年均 47 本俄罗斯每人年均 55 本犹太每人年均 64 本以前不爱看书，这两年自学一些东西后才养成主动去找书看的习惯。才知道看书花时间，找书更花时间。国内书评做得比较好的也就豆瓣了，可以根据大家对一本书的评分、打分人数和评论去选择要不要买/看这本书，不过书找起来仍然挺费时间。最近发现了一个神网站，一大佬爬了豆瓣上 300 多万本书后，搭建出来这么个搜索网站。可以在上面快速筛选好书，比豆瓣页面简洁、搜索功能更强大。网站上有三个搜索选项：书名关键字、分数和打分人数，设置好选项后就可以搜索了，适合去找某一类别下的书。举个例子，比如想找 Python 方面的好书，可以设置筛选范围在 9 分以上、打分人数超过 100 人，然后就会搜出下面这些书来：质量都不错，要想了解每本书的详情可以点击进入豆瓣的主页查看。再比如想看看写作方面有哪些好书，可以设置评分 8.5 分以上，打分人数超过 500 人，就会搜到这些书：怎么样，是不是比豆瓣更好用？书找起来快多了，虽说不绝对权威，但可以做很好的参考。网站地址：http://sobook.lanbing510.info/另外，作者还很 nice 地提供了各类别下的书单文件，方便在 Excel 中搜索筛选。GitHub 库地址：https://github.com/lanbing510/DouBanSpider有这两个网址找书再也不用发愁了，感兴趣可以去搜搜看，不错的书可以下单。老规矩，我下载了全部书单文件，如需可以在后台回复：「豆瓣书」得到。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 一键制作微信好友图片墙]]></title>
    <url>%2F2019%2F04%2F19%2FPython_learning04.html</url>
    <content type="text"><![CDATA[wxpy、pyinstalller 库的使用。上午发了张我微信近 2000 位好友的头像拼图，让大伙儿看能不能快速找到自己的头像，没想到反响很强烈，引得阵阵惊呼与膜拜，没有料到。有没有犯密集恐惧症？这并不震撼，如果你有 5000 位好友的话，做出来的图看着会更刺激些。看完了图，你可能想知道这个图咋做出来的，不会是我闲着无聊把把好友头像一个个保存下来再用 PS 拼的吧？自然不是了，Python 做的，是不是觉得没有 Python 干不了的事儿。其实，这种图很早就有人玩过了，不过下面还是来说说怎么做出来，这样你也可以做一个自己微信好友的图片墙。有两种方法，一种简单的，不用接触 Python 代码，一种稍微复杂点，需要写代码。先说简单的方法，只需要两步：运行程序然后扫微信二维码就行了。剩下的交给程序自己蹦跶，泡杯茶在电脑前等待几分钟左右就可以得到图片，具体的等待时间视微信好友数量而不同，我近 2000 好友，用时 10 分钟左右。一个简单的操作示意图：几分钟后就可以得到上面的图片了。其实到这儿就完了，是不是很简单。你要感兴趣怎么实现的，可以往下看用 Python 代码怎么实现的，代码不长，60 行就可以搞定。核心是利用三个个库：wxpy 库，用于获取好友头像然后下载Pillow 库，用于拼接头像Pyinstaller 库，用来打包 Python 程序成 exe 文件程序通过三个函数实现，第一个 creat_filepath 函数生成图片下载文件路径，第二个 save_avatar 函数循环获取微信好友头像然后保存到本地，第三个 joint_avatar 函数就是把头像拼接成一张大图。完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# -*- coding: utf-8 -*-from wxpy import *import mathfrom PIL import Imageimport os# 创建头像存放文件夹def creat_filepath(): avatar_dir = os.getcwd() + "\\wechat\\" if not os.path.exists(avatar_dir): os.mkdir(avatar_dir) return avatar_dir# 保存好友头像def save_avatar(avatar_dir): # 初始化机器人，扫码登陆 bot = Bot() friends = bot.friends(update=True) num = 0 for friend in friends: friend.get_avatar(avatar_dir + '\\' + str(num) + ".jpg") print('好友昵称:%s' % friend.nick_name) num = num + 1# 拼接头像def joint_avatar(path): # 获取文件夹内头像个数 length = len(os.listdir(path)) # 设置画布大小 image_size = 2560 # 设置每个头像大小 each_size = math.ceil(2560 / math.floor(math.sqrt(length))) # 计算所需各行列的头像数量 x_lines = math.ceil(math.sqrt(length)) y_lines = math.ceil(math.sqrt(length)) image = Image.new('RGB', (each_size * x_lines, each_size * y_lines)) x = 0 y = 0 for (root, dirs, files) in os.walk(path): for pic_name in files: # 增加头像读取不出来的异常处理 try: with Image.open(path + pic_name) as img: img = img.resize((each_size, each_size)) image.paste(img, (x * each_size, y * each_size)) x += 1 if x == x_lines: x = 0 y += 1 except IOError: print("头像读取失败") img = image.save(os.getcwd() + "/wechat.png") print('微信好友头像拼接完成!')if __name__ == '__main__': avatar_dir = creat_filepath() save_avatar(avatar_dir) joint_avatar(avatar_dir)可以直接在运行程序文件，也可以用 Pyinstaller 文件打包后运行。这里额外说一下 pyinstaller 打包的方法和闭坑指南。不要直接在系统中用 pyinstaller 打包，否则打包出来的 exe 文件会很大。建议在虚拟环境中打包，打包出来的 exe 文件会小很多， 10MB 左右。虚拟环境创建很简单，简单说一下步骤：1 安装 pipenv 和 pyinstaller 包，用于后续创建虚拟环境和打包程序：12pip install pipenvpip install pyinstaller # 已安装就不用安装了2 选择一个合适的目录作为 Python 虚拟环境，运行：12pipenv install # 创建虚拟环境pipenv shell # 创建好后，进入虚拟环境3 安装程序引用的库，上面程序引用了四个库：wxpy、math、os 和 PIL，一行代码就可以完成安装。1pipenv install wxpy math os4 这里要额外注意 PIL 的安装，现在不用 PIL 库，而是用 Pillow 库取代，所以安装 Pillow 库就行。但不要安装最新的 6.0.0 版本，否则可能会遇到各种错误，例如：PIL 无法识别下载的 jpg 头像文件。1OSError: cannot identify image file &lt;ImageFieldFile: images正确的安装方法是安装低版本，经尝试安装 4.2.1 版本没有问题，安装命令：1pipenv install Pillow==4.2.15 然后打包程序就可以了：123pyinstaller -F C:\Users\sony\Desktop\wechat_avatar.py # 程序路径要改成你电脑上的路径# -F 表示生成单个 exe 文件，方便运行运行如下：运行命令，1 分钟左右若显示 successfully 字样表示程序打包成功：接着在程序目录下找到 wechat_avatar.exe 文件，然后按照第一种方法那样运行就行了。以上就是用 Python 制作微信好友图片墙的方法。完整代码和 exe 文件可以在我的公众号：高级农民工，后台回复「微信好友」得到。参考：wxpy 库Pipenv – 超好用的 Python 包管理工具pyinstaller打包后程序体积太大，如何解决？]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[截图、录 GIF、录视频的佳软推荐]]></title>
    <url>%2F2019%2F04%2F13%2Fweekly_sharing24.html</url>
    <content type="text"><![CDATA[款款好用。有了它们，我把所有截图、录 GIF、录屏软件都卸了这里是「每周分享」的第 24 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：截图、录 GIF、录视频。平时写公众号，常需要添加个截图作辅助解释，截图不形象就录个 GIF 或者视频。不只是做公众号，日常工作也经常会有这些需求。今天就来跟大家推荐几款非常好用的软件。01 截图Faststone Capturewww.faststone.org/FSCaptureDetail.htm这是一款我天天都在用的老牌截图软件，体积小巧，平时隐藏在窗口端，需要截图的时候鼠标一点就可以截图，自定义快捷键则可以更快。它提供多种截图方式：全屏、窗口截屏、矩形截屏等，还有一个实用功能是「滚动截屏」。图截好可以顺带编辑，比如添加、箭头、文字、马赛克。除了截图，它还可以录制视频，不过不支持后期编辑，用得比较少。我在用了 MAC 之后第一时间想下的软件就是它，遗憾地是，目前只有 Windows 版。如果你想用一款同时支持 Win 和 Mac 的截图软件，那首推下面这款。Sinpastezh.snipaste.com/Sinpaste 更确切地说是一款「贴图」软件，其次是截图工具。贴图的意思就是，截好图后可以置顶在屏幕跟前，这个功能很实用。我们经常需要在不同窗口切换查找输入信息，一般操作就是窗口来回切换，眼睛都晃晕，用了这个贴图功能后，可以直接把多个窗口截图并排放在一起，方便许多。软件用起来很简单，F1 键截屏，F3 贴图。除了这两样还有很多其他功能，专业版更强大一些。02 录 GIFScreenToGifwww.screentogif.com/之前遇到很多人在问怎么录制 GIF，要是只推荐一款 GIF 录制软件的话，那必然是这款 ScreenToGif 。它不到 1MB 大，功能极简，打开软件就能录制。除了这些优点外，我更满意的是它提供了强大的后期编辑功能。很多 GIF 软件只能录而不能编辑，比如不少人推荐的 LICEcap，其实一点都不实用，因为你很难一次性就录好 GIF，要在微信中插入 GIF 的话 ，体积超过 2MB 也插入不了。我们知道 GIF 其实就是由一帧帧的图像组成的，编辑 GIF 就是编辑帧 。它提供了删减帧、加速减速帧的功能，以去除不想要的画面减小 GIF 体积。还有添加字幕、马赛克、过渡这些功能，可以说只要你想， GIF 就能玩出花来。如果想录制更长一点的动画， GIF 就不合适了，需要录制视频。03 录视频FlashBackwww.flashbackrecorder.com说起录视频，可能很多人会推荐大名鼎鼎的 Camtasia，我倒觉得，一般情况下其实用不着体积庞大的 Camtasia，FlashBack 足矣。这款软件体积比 Camtasia 小多了，不管是录制还是后期编辑，速度都够快。后期编辑功能很多，添加字幕、加速/减速帧、缩放平移这些都有，此外还可以添加音频。视频编辑好，还可以导出各种形式的文件。以上，就介绍这几款我一直在用的佳软。感兴趣，可以搜来试试。你要不想搜，欢迎加入我的知识星球中，或者可以打赏，我看到了也会发给你链接。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比百度网盘更好的临时文件传输方法]]></title>
    <url>%2F2019%2F04%2F06%2Fweekly_sharing23.html</url>
    <content type="text"><![CDATA[实用技能。这里是「每周分享」的第 23 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：文件传输。说起文件传输，很多人首先会想到某度网盘，上传好之后给对方发送一个链接就可以了，大部分时候都是可行的。不过，少数情况下就行不通了，因为这种上传方式有几个限制：对方也要有网盘账号4GB 以内的文件非敏感文件，比如不可描述工具所以如果出现这样的情况：有人说他没用过网盘、要传输的文件很大或者网盘文件受限制，那就用不了网盘。你可能会想到用微信、QQ 也照样可以传啊，但如果对方不是好友怎么办，关系不错倒可以加好友，就是稍微麻烦点，如果关系要一般你可能并不想加，或者你就根本不想多费这档子事。另外，还有一种常见需求，要传输的文件很重要或者很隐私，只希望对方一人或者小范围接收。这一点，网盘链接就不合适了，因为发给对方链接后，他可能转手就发给别人了。你可能觉得上面说的这些有点扯或者概率很低，这年头谁还没个网盘账号，谁没事会转发扩散资源。但这些确是我经历过的，后来找到了几个神器完美解决了这些痛点，下面来介绍下。FireFox Send网址：https://send.firefox.com/Firefox Send 是火狐浏览器一款产品，使用它的网页版可以传送 1GB 以内的文件，上传好后把链接发送给对方就可以。这看起来和网盘分享方式并无不同，但其实大不同。首先，对方接收到这个链接可以直接下载，不需要任何账号。其次，可以选择分享次数和时间限制。比如，文件只能下载 1 次，那么对方下载文件后链接就失效，别人不能再下载。适合私人之间的单次分享。如果文件泄露出去了，你就会知道肯定是对方做的。分享次数和时间很灵活，既可以选择 1 到 100 次分享次数，也可以选择 5 分钟到 7 天的时间有效期，过期就作废。你可能会觉得在网页上这样传输是否安全，答案是绝对安全。另外如果觉得网页版不是很方便，想使用软件，那就推荐下面这一款神器。SendAnywhere网址：https://send-anywhere.com/SendAnywhere 是一款全平台专业分享利器，手机端、PC 端、网页版、Chrome 插件都支持。其传输能力，如它名字一样出色。下面以网页版为例介绍一下使用方式。使用起来很简单，可以选择四种传输方式：6 位数字二维码链接邮箱使用 6 位数字密码和二维码的直接传输方式，文件只有十分钟有效期，适合紧急或者小文件传输。把 6 位密码或者二维码发送给对方就可以下载。需要注意的是，下载次数和上面的 FireFox Send 不同，可以多次下载，只要没过期。如果觉得 10 分钟太短，可以注册个账号，文件有效期可以达到 48 小时。它还有一个优点是文件上传大小几乎没有限制，因为一次性最大上传量高达 100G。相较之下，网盘单个文件体积限制 4GB 就显得小儿科了。虽然很少会传输超过 4G 的文件，但总会有这样的需求存在。以上，就是两款临时文件传输神器，可以选择性使用。另外，还忘了说一点上传下载速度比网盘快不少，用来备份照片文件是个不错的方法。本文完。参考：免费全平台的文件分享利器：SendAnywhere只能用 1 次的网盘]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我们说着「中式英语」写着「英式中文」]]></title>
    <url>%2F2019%2F04%2F01%2Fweekly_sharing22.html</url>
    <content type="text"><![CDATA[好好写作。这里是「每周分享」的第 22 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：写作。最近为了出 Python 专栏课，重新整理了以前写的文章，发现很多地方写得不好，犯了不少错误。一直以为英语没学好，没想到汉语也给带坏了，说着中式英语，写着英式中文。上学那阵英语很差，逮住机会就想找老外练口语。有次学校碰到一个老外，他先开的口：Can you speak english?条件反射地答到：My english is poor.后面又偶遇了一次，他又主动问道：How are you？我说：I‘m fine，thank you，and you?就这样好歹算是认识了，口语进步明显，张口说话的胆子也大了，碰到不会的就问他：How to say this in English?被他纠正过许多回，便想请他吃个饭略表谢意。找了家学校附近的餐馆，拿给他菜单让他随意点菜，他看了半天指着一处问我这是什么菜：一看，对饭店老板的英语水平顿生敬意，委婉同老外说不要在意，这道菜不贵：The price is very suitable.嗯，以上是我编的，英语说成这样大概率是交不到老外朋友的，不过，相信像我这种水平的应该还大有人在。在网上看到过个段子，讲几个在国外旅行的中国人恰巧遇到一起车祸，这样向警察描述过程：「one car come, one car go , two cars pengpeng ,the people die.」这些都是一目了然的「中式英语」，想纠正并不难，难的是那些我们并不认为是「中式英语」的地方，比如学校的英文课本、老师出的试卷、学生写的作文。拿写英语作文来说，以前最喜欢用一些短语或者同学不知道的单词，既高大上又能凑字数，关键老师给分还高。比如：Make great efforts toMake an improvementPay attention toTry our best toaccelerate the pace of economic reformoverwhelming majority明明一个词或者更简单的词可以说明，偏要写这些在 The Economist 里几乎见不到的句法。不久前，看到一本叫《中式英语之鉴》的书，作者是 Joan Pinkham 女士，她为国内官方机构翻译润稿多年，这本书中她纠正了很多中式英语问题。豆瓣 8.8 分，五六百页，京东上却只要十几块钱，想必不是很火。如果当年学校能给老师和学生发上一本作为辅导书，想必我的英语水平不至于是现在这个样子。所幸，大部分人这辈子并不出国，英语使用的场景也不多，所以「中式英语」问题倒也没什么太大影响。然而，多数人地道的英语没学到，却把不地道的英语带到了中文写作中，写出来的都是「英式中文」。不信，举一些例子，看看你有没有这样写过。名词主语习惯说：流浪汉的爆红，背后有什么秘密？他的收入的减少改变了他的生活方式。却不会说：流浪汉爆红，背后有什么秘密？他因为收入减少而改变生活方式。英文常用名词做主语，而中文是常用一件事 (一个短句)做主语 。万恶的「作出」「进行」「造成」习惯说：校友为母校作出了重大贡献。专家们对该问题已经进行了详细的探讨。埃塞俄比亚航空公司客机坠毁，造成一百五十七人死亡。却不会说：校友对母校贡献很大。专家们详细探讨了该问题。埃塞俄比亚航空公司客机坠毁，死了一百五十七人。「作出」「进行」「造成」这些是弱动词，很多人找不到合适的动词就喜欢用这些代替。这正是英文蹩脚的人的特色，把简单明了的动词分解成「弱动词+抽象名词」的组合。故作高深的「性」「度」「力」习惯说：这本书可读性高么？他在国内 IT 领域享有很高的知名度。一个人应该有良好的自控力。却不会说：这本书值得看。他在国内很有名。一个人应该管好自己。很多人文章中好用「性」「度」「力」这类抽象名词，说白了就是故作高深，奥威尔将这类词汇称为「语言的义肢」，能砍就砍。模糊的… 之一习惯说：这款 App 是最好用的笔记软件之一。他是世界上最有钱的人之一。李白是中国最伟大的诗人之一。却不会说：这是一款好用的笔记 App。他是世界顶级富豪。李白是中国的大诗人。「之一」和「One of ..」一脉相承，不确定范围就不要说「之一」，你觉得精确，其实恰恰相反。画蛇添足的「有关」「关于」「由于」习惯说：今天我们讨论有关 Python 的学习方法。关于他的方案，你看过没有？由于他近视，所以看不清东西。却不会说：今天我们讨论 Python 的学习方法。你看过他的方案没有？他近视，看不清东西。英语常用 about、concerning、with regard to 这些词表明逻辑，交代事物因果关系。而多数中文语境自带逻辑关系，不需用这些泛滥的介词。啰嗦的「成功地」习惯说：我们成功地把该网页数据爬取下来了。却不会说：我们把网页数据爬取下来了。爬取下来就是成功，不用再重复，类似这样的副词还有不少。形容词必用「的」习惯说：晴朗的天气。这篇文章的阅读量是最高的。我见到一个长得像你兄弟说话也有点像他的陌生男人。却不会说：天气晴朗。这篇文章阅读量最高。我见到一个陌生男人，长得像你兄弟，说话也有点像他。很多人碰到形容词就爱用「的」，其实大多数的「的」都可以删掉。另外，英语习惯把所有形容词都放在名词前面，也就是前置。而中文这样用就会显得冗长，其实更适合后置，短小精练，突出重点。以上这些毛病，你有中招么？上面的内容参照总结了余光中老先生在 1987 年发表的《论中文的常态与变态》一文。他三十多年前就指出了多数人爱犯的中文写作毛病，今天仍然那么多人没有注意到。到底是我们没有学好，还是没有学过？网上有很多所谓的爆款写作课，教你怎么写出十万+ 文章，我倒觉得，老师的心思都放在运营技巧上了，哪里还会教你写作。想好好学写作，不妨看看余老的《翻译乃大道》，让你知道什么是中文。最后，缅怀敬爱的余光中先生，他的《乡愁》至今仍记忆犹新。推荐阅读：論中文的常態與變態余光中文章精选马英九评论余光中阳志平《平克写作八原则》王烁《有效写作十三篇》]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>写作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降最直观的理解]]></title>
    <url>%2F2019%2F03%2F28%2FMachine_learning02.html</url>
    <content type="text"><![CDATA[通俗易懂。最近在学习机器学习，入门第一课就是「梯度下降」。看了不少教程都没有很好地理解，直到看到下面这篇文章，算是通俗易懂地理解了。作者：六尺帐篷链接：https://www.jianshu.com/p/c7e642877b0e来源：简书本文将从一个下山的场景开始，先提出梯度下降算法的基本思想，进而从数学上解释梯度下降算法的原理，最后实现一个简单的梯度下降算法的实例！梯度下降的场景假设梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。image.png我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！梯度下降梯度下降的基本过程就和下山的场景很类似。首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释)所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？接下来，我们从微分开始讲起微分看待微分的意义，可以有不同的角度，最常用的两种是：函数图像中，某点的切线的斜率函数的变化率几个微分的例子：上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分梯度梯度实际上就是多变量微分的一般化。下面这个例子：我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用&lt;&gt;包括起来，说明梯度其实一个向量。梯度是微积分中一个很重要的概念，之前提到过梯度的意义在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！梯度下降算法的数学解释上面我们花了大量的篇幅介绍梯度下降算法的基本思想和场景假设，以及梯度的概念和思想。下面我们就开始从数学上解释梯度下降算法的计算过程和思想！此公式的意义是：J 是关于Θ的一个函数，我们当前所处的位置为Θ0 点，要从这个点走到 J 的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1 这个点！下面就这个公式的几个常见的疑问：α是什么含义？α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！为什么要梯度要乘以一个负号？梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号。梯度下降算法的实例我们已经基本了解了梯度下降算法的计算过程，那么我们就来看几个梯度下降算法的小实例，首先从单变量的函数开始单变量函数的梯度下降我们假设有一个单变量的函数函数的微分初始化，起点为学习率为根据梯度下降的计算公式我们开始进行梯度下降的迭代计算过程：如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底多变量函数的梯度下降我们假设有一个目标函数：现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！我们假设初始的起点为：初始的学习率为：函数的梯度为：进行多次迭代：我们发现，已经基本靠近函数的最小值点梯度下降算法的实现下面我们将用 python 实现一个简单的梯度下降算法。场景是一个简单的线性回归的例子：假设现在我们有一系列的点，如下图所示我们将用梯度下降法来拟合出这条直线！首先，我们需要定义一个代价函数，在此我们选用均方误差代价函数：此公示中m 是数据集中点的个数½是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响y 是数据集中每个点的真实 y 坐标的值h 是我们的预测函数，根据每一个输入 x，根据Θ 计算得到预测的 y 值，即我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python 中计算矩阵是非常方便的，同时代码也会变得非常的简洁。为了转换为矩阵的计算，我们观察到预测函数的形式：我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点 x 增加一维，这一维的值固定为 1，这一维将会乘到Θ0 上。这样就方便我们统一矩阵化的计算：然后我们将代价函数和梯度转化为矩阵向量相乘的形式：coding time首先，我们需要定义数据集和学习率123456789101112131415161718import numpy as np# Size of the points dataset.m = 20# Points x-coordinate and dummy value (x0, x1).X0 = np.ones((m, 1))X1 = np.arange(1, m+1).reshape(m, 1)X = np.hstack((X0, X1))# Points y-coordinatey = np.array([ 3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12, 11, 13, 13, 16, 17, 18, 17, 19, 21]).reshape(m, 1)# The Learning Rate alpha.alpha = 0.01接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度123456789def error_function(theta, X, y): '''Error function J definition.''' diff = np.dot(X, theta) - y return (1./2*m) * np.dot(np.transpose(diff), diff)def gradient_function(theta, X, y): '''Gradient of the function J definition.''' diff = np.dot(X, theta) - y return (1./m) * np.dot(np.transpose(X), diff)最后就是算法的核心部分，梯度下降迭代计算12345678def gradient_descent(X, y, alpha): '''Perform gradient descent.''' theta = np.array([1, 1]).reshape(2, 1) gradient = gradient_function(theta, X, y) while not np.all(np.absolute(gradient) &lt;= 1e-5): theta = theta - alpha * gradient gradient = gradient_function(theta, X, y) return theta当梯度小于 1e-5 时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！完整的代码如下1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as np# Size of the points dataset.m = 20# Points x-coordinate and dummy value (x0, x1).X0 = np.ones((m, 1))X1 = np.arange(1, m+1).reshape(m, 1)X = np.hstack((X0, X1))# Points y-coordinatey = np.array([ 3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12, 11, 13, 13, 16, 17, 18, 17, 19, 21]).reshape(m, 1)# The Learning Rate alpha.alpha = 0.01def error_function(theta, X, y): '''Error function J definition.''' diff = np.dot(X, theta) - y return (1./2*m) * np.dot(np.transpose(diff), diff)def gradient_function(theta, X, y): '''Gradient of the function J definition.''' diff = np.dot(X, theta) - y return (1./m) * np.dot(np.transpose(X), diff)def gradient_descent(X, y, alpha): '''Perform gradient descent.''' theta = np.array([1, 1]).reshape(2, 1) gradient = gradient_function(theta, X, y) while not np.all(np.absolute(gradient) &lt;= 1e-5): theta = theta - alpha * gradient gradient = gradient_function(theta, X, y) return thetaoptimal = gradient_descent(X, y, alpha)print('optimal:', optimal)print('error function:', error_function(optimal, X, y)[0,0])运行代码，计算得到的结果如下：所拟合出的直线如下：小结至此，我们就基本介绍完了梯度下降法的基本思想和算法流程，并且用 python 实现了一个简单的梯度下降算法拟合直线的案例！最后，我们回到文章开头所提出的场景假设:这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。可以看到场景假设和梯度下降算法很好的完成了对应！Further readingGradient Descent lecture notes from UD262 Udacity Georgia Tech ML Course.An overview of gradient descent optimization algorithms.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的爬虫&数据分析课]]></title>
    <url>%2F2019%2F03%2F26%2Fweb_scraping_withpython22.html</url>
    <content type="text"><![CDATA[7 万字、26 个知识点，13 个爬虫&amp;数据分析实战项目。我开了一门只要 19.9 元的爬虫课，叫《Python 入门爬虫与数据分析》。这是一门够便宜的课19.9 元，中间没有误打个「.」号。19.9 元就可以买下我大半年的时光，7 万字、26 个知识点，13 个爬虫&amp;数据分析实战项目，一套精选Python 电子书和 Pyhton 视频参考教程。这是一门友好的零基础入门课半年前，我决定转行，从零开始学习 Python。过程很痛苦，常常被很小的一个问题卡很久，全靠一点点 Google 找到答案，再写到教程中去。一路从小白过来，深知在学习过程中可能会遇到的种种问题，所以文章中我都尽可能地写得直白友好、详略得当，让你少走些弯路。这是一门可以学到不少东西的课你可以学到快速爬取百万行上市公司财务报表：你可以学到怎么爬取图片：你可以学到怎么爬取整个网站的 App：你可以学到如何做一些好看的图：还有很多，都在二维码里。觉得值得，就扫码入手吧。另外，说一个福利。加入我知识星球的星友可以免费得到这门课，有任何问题都可以找我，欢迎加入。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主流网站 Python 爬虫模拟登陆方法汇总]]></title>
    <url>%2F2019%2F03%2F19%2Fweb_scraping_withpython21.html</url>
    <content type="text"><![CDATA[微信、知乎、新浪、京东、淘宝等大型网站的模拟登陆方法。摘要：介绍微信、知乎、新浪等一众主流网站的模拟登陆爬取方法。网络上有形形色色的网站，不同类型的网站爬虫策略不同，难易程度也不一样。从是否需要登陆来说，一些简单网站不需要登陆就可以爬，比如之前爬过的猫眼电影、东方财富网等。有一些网站需要先登陆才能爬，比如知乎、微信等。这类网站在模拟登陆时需要处理验证码、js 加密参数这些问题，爬取难度会大很多。费很大力气登陆进去后才能爬取想要的内容，很花时间。是不是一定要自己动手去实现每一个网站的模拟登陆方法呢，从效率上来讲，其实大可不必，已经有前人替我们造好轮子了。最近发现一个神库，汇总了数十个主流网站的模拟登陆方法：知乎微信网页版登录并获取好友列表BilibiliFacebook无需身份验证即可抓取Twitter前端API微博网页版QQZoneCSDN淘宝Baidu果壳JingDong 模拟登录163mail拉钩豆瓣Baidu2猎聘网Github爬取图虫相应的图片网易云音乐糗事百科这些网站基本采用是直接登录或者使用 selenium+webdriver 的方式。每一个网站都有完整的模拟登陆代码，拿来就可以用到自己的爬虫中。下面我们来测试一下。先说说很难爬的「知乎」，假如我们想爬取知乎主页的 HTML 内容，就必须要先登陆才能爬，不然看不到这个界面。下面来简单梳理一下流程。知乎需要手机号才能注册登陆。为了方便测试，可以随便找个手机号，手机号到哪儿去找呢，我上周写的那篇文章就发挥作用了。文章里介绍了一个免费电话号码网站，用上面的手机号可以成功注册。文章传送门：两个神网站保护你的隐私顺利登录后就可以进入主页了。下面，我们用这个库提供的代码来模拟登陆，输出主页 HTML 内容作测试。操作很简单，只需要输入手机号、密码和验证码就可以了。成功登陆后，接下来就可以做一些有意思的事了。比如曾有人爬取所有知乎账号的信息，分析了知乎用户群体画像。是不是有点意思。再来看看微信。用上面的微信代码可以把全部微信好友信息爬取下来，比如：昵称、性别、地域、个性签名。接着可以分析一下你的朋友圈是什么样的，应该会很有趣。还可以爬 B 站：还可以爬链家租房信息：还有很多实用有趣的内容，就不一一罗列了，感兴趣的话可以试试，最后放上 GitHub 库地址：https://github.com/CriseLYJ/awesome-python-login-model不要闷头造轮子，多抬抬头会发现你在做/想做的东西，别人早已经弄好了，拿来用或者参考学习都是件好事。本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向优秀的人靠近]]></title>
    <url>%2F2019%2F03%2F16%2Flife09.html</url>
    <content type="text"><![CDATA[长久的利他必然利己。摘要：长久的利他必然利己。昨天读者群里的一位朋友告诉我，说刘志军（公众号 Python 之禅作者）在文章里推荐了我的公众号。听了很纳闷儿，寻思并没有跟他约公众号互推啊，赶紧去他公众号看了看，还真的是。推荐语，有意思、也实在。正如他所说，这个推荐纯粹是出于「推荐不错原创号」的初衷。感到很意外，有点小激动，有人说是我面子大，我并不认同。其实我和军哥并无多少交集，微信也没聊过几句，主要是觉得还不够分量去跟他混熟。可是他怎么就推荐我了呢。跟背后的几件小事有点关系，如果你想勾搭大佬，不妨看看。第一件事。你可以看到截图上，打赏最多的人中有我的头像。这就不难理解了吧，我打赏的次数多，他对我有印象了，说明我有一直在关注他的公众号，再顺便翻一下我的公众号，觉得还不错就友情推荐一下，也算是给我的一点福利。当然，我是赚大了，基本上没花什么成本，就收获很多粉丝。意外、惊喜。我之前常送书，看到很多人说为什么总中不了奖，你跟上千个人去拼运气，中奖概率当然很低了。要想增大中奖概率，打赏、留言绝对是既划算、赢面又大的方式，每篇推文打赏个一两块，十次也才一二十块，几乎没有人能够打赏这么多次，你要做到了，在我这里的印象分就是独一档，中奖概率自然会高许多。这只是第一件事，我并不是平白无故地就打赏他的。很多人也是这样，关注的公众号那么多，要持续打赏一个公众号，必然是有理由的。我是因为第二件事。第二件事。上两周我在一个几百位大佬的微信群求助了一个问题，最终，只有他私信帮了我。就因为这件事，他在我心里的印象分也变得很高了，那之后他发文我基本上都看，顺便打赏。你以为完了么，并没有，他私信帮我也是有事铺垫在先的。第三件事。前阵他做了一个网站，邀请大家去注册体验一下，我看到后就马上微信他，说想注册体验下。我支持他的产品，背后也是有原因的，就是想再次勾搭大佬。为什么说再次呢，这就跟第四件事有关了。第四件事半年前，我还是一个刚学 Python 的菜鸟，那会儿军哥在我眼里就是高不可攀的大神，当然现在也是。发现他公众号后，果断关注，还加了他微信。有次他发了一条朋友圈，想混个脸熟，就在下面留言评论了下，然而他并没有回复。看吧，我以前也有被大佬忽略的辛酸史。再来顺着理一下这几件事背后的逻辑。我刚入门 Python ，想勾搭认识一下大佬，没想评论朋友圈被忽略，之后依然主动去他注册他的产品，继续勾搭，然后被他熟悉，在群里帮我忙，我打赏他文章，他推荐我公众号。你看，并不是我有面子，我只是做了少有人做的事情。很多人想抱大佬腿，很多人也是玻璃心，如果我也是的话，那他朋友圈不回复我评论，我就会觉得不爽：这人咋这么高高在上，取关。如此的话，也就不会有后面的事。你是个菜鸟，就最好放低姿态，他不回复你，并不是针对你，大佬每天那么忙，不回复也很正常，何况他根本不熟悉你。除了放低姿态，就是要主动些。持续关注他动态，公众号留言打赏，能做的就做，你既然想勾搭他，为什么不去做这些简单而少有人做的事情呢。做的多了，他自然就会关注到你，如果没有关注到，说明别人比你做的更多。当然，这里并不是教你去一味地做大佬舔狗。先做好自己，你没有拿得出手的东西，大佬就算想推你也推荐不了。我是持续地在公众号输出了一些原创文章，才慢慢混到大佬的圈层，当你跟大佬在一个共同的圈子，勾搭的机会才会多。我知道关注我的部分粉丝，会觉得我是大佬，我要谦虚地说我只是菜鸟，没什么意思，对你也没什么用。因为我们站在的高度是不一样的，你说我是大佬，我说军哥是大佬，军哥还会说 stormzhang 是大佬。这么多大佬，你应该多勾搭离你最近的大佬。级别太高的大佬，你费很大劲也难够着，更好的方式是一级一级地让大佬带你。想向大佬靠拢，真得用心去付出一点，长久的利他必然利己。本文完。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 两款神器：Wox 快速查询启动和 QuickLook 空格预览]]></title>
    <url>%2F2019%2F03%2F16%2Fweekly_sharing20.html</url>
    <content type="text"><![CDATA[摆脱鼠标，提升效率。摘要：推荐两款 Windows 神器，远离鼠标，释放右手。这里是「每周分享」的第 21 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：释放右手。下面要介绍的内容针对和我一样的 Windows 用户。有一阵子，每天都要重复大量的 Ctrl C / V 操作，键盘上的这三个按键都擦出了油渍，锃亮无比。后来受不了剁手入了 G502，不是 502 胶水，是上面那款鼠标。（非广告）不得不说，是真好用，复制粘贴、返回关闭这些操作再也不用键盘了。鼠标使用多了，新问题也随之而来，一是手指关节不舒服，二是其他操作还是需要「鼠键结合」，比如打开文件/夹、浏览器、切换窗口这些，纯鼠标无法完成，效率仍不高。难道只能这样么？快捷键操作慢慢学会了一些快捷键，利用 Ctrl 、Tab、Win 加几个字母键完成很多快捷操作。比如 Win + Tab 键切换窗口：觉得眼花缭乱，可以 Alt + Tab。Win + D 可以秒回桌面，不用一个个去关或者最小化窗口。在浏览器中，不需要用鼠标点击去切换页面，Ctrl + Tab 就可以了，想关闭页面，按下 Ctrl + M 搞定：另外，再配合 Vimium 这款插件更是如虎添翼，基本上，浏览器这块儿就能告别鼠标了。这些快捷键给我节省了不少时间，不过发现有些地方还是取代不了鼠标，比如打开软件、文件/ 文件夹，还是要在开始菜单或者桌面中去找出来打开。难道没有更好的方法了么？ALT + 空格 搞定一切这样的操作习惯持续了很久，直到发现了 Wox 这款神器，便再也不用浪时间去找东西。Alt + 空格键快速启动 Wox，接着输入想要打开的文件/软件名就 OK，打开、切换如行云流水般丝滑：打开软件这款软件可以说是 Everything 的加强版，除了能搜索电脑中的东西，还有一些功能丰富强大的插件可用，下面举几个例子。网页搜索利用网页搜索插件，可以随便切换 Google 、Bing 、百度搜索引擎，比打开浏览器再去切换、搜索方便多了：还可以直接 StackOverflow：查天气可以查天气：翻译有道、谷歌翻译都可以用：命令行不用再 Win + R 了：觉得牛逼么？功能还远不止这些。如果你电脑上的软件文件很多或着杂乱无章的话，绝对值得一试。不过还有一个痛点没解决，怎么样可以不用打开文件就能查看内容。这就要介绍另一款神器。完美的「一指禅」有时候要找一些文件，只能打开看了才知道是不是要找的，一启动一关闭，时间都浪费了。图片还可以通过设置「大图」预览查看，文件就没办法了。不得不说 Mac 用户很幸福， Finder 中有一个「一指禅」功能，只需要按一下空格就可以预览查看文件内容。其实 Windows 也可以实现同样的效果，就是利用这款神器：Quicklook 。选中文件按一下空格就可以预览，再按一下关闭预览，找到想要的文件后，敲下「回车」就可以打开，像下面这样：图片、PDF、Markdown、代码、GIF 都能完美打开。这两款神器节省了我不少时间，大部分时间能够做到手不离开键盘。感兴趣的话可以自行搜索下载下来试试，老规矩，为了更方便你可以在公众号后台回复：「wox」得到。时间是海绵里的水，只要愿意，挤一挤总是有的。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两个神网站：在线接收短信和邮件]]></title>
    <url>%2F2019%2F03%2F16%2Fweekly_sharing20.html</url>
    <content type="text"><![CDATA[日常实用网站，保护隐私。摘要：在外不要随便打开 WIFI，另分享两个神网站：在线接收短信和邮件。这里是「每周分享」的第 20 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的话题是：保护隐私。前天的 315 晚会上，曝光了几家制造机器人骚扰电话的公司，其中一家宣称一年打出去了 40 亿个骚扰电话。以前，我们接到的骚扰电话基本都是真人给你打的，你可能还会礼貌地听几句再拒绝。而现在，很多电话是都是机器人打给你的，你敢信？真人一天顶多打三五百个电话，而这些机器人一天可以打五千个。这么多骚扰电话不算可怕，平均到全中国每个人身上数量也不多。可怕之处在于，这些机器人似乎对你很了解，知道你的性别、年路、收入，有没有购房、贷款需求等等。很纳闷对吧，你好像从来没有告诉过谁。其实，他们获取的方法很简单。在商场、便利店放置一种探针盒子，只要你手机打开 Wi-Fi ，它就能识别出你手机的 Mac 地址转换成 IMEI 号，再和他们的大数据库匹配，最终查询到你的手机号和各种信息。所以出门在外，别为了省那点流量钱就到处蹭 WIFI。他们的大数据系统是从哪里来的呢，主要就是从你使用的那些 App 中获取到的。很多无良 App 安装时会提示你开启各种不相关的权限，比如通讯录、短信。很多人看都不看就默认同意了，一旦你同意，它就开始收集你信息，存储到数据库中，然后卖给中原地产这些广告商。所以，尽可能地不要用垃圾 App，手机权限能不开就不开。这条黑色产业链经这么一曝光，不知道会不会就此销声匿迹，但你能做的就是主动保护自己隐私。下面说说怎么保护。现在很多网站都需要注册，要么用手机号要么用邮箱，接着会给你发确认的短信验证码或者链接，以保证你用的是自己的。重要的网站，可以用自己的，那些不重要或者很少使用的网站就没有必要用。然而，很多人不管什么网站，统统用自己的手机号或邮箱去注册，一是没有隐私保护意识，二是没有空闲手机号和邮箱。今天这个网站注册下，明天那个注册下，长时间下来，自己都不清楚到底注册了多少网站，这是个很大的隐患。其实，那些不重要的网站，你可以不用自己的手机号和邮箱，用别人的就可以。这样的话，既能满足一时之需，又不用泄露自己手机号。你可能不信，哪有那么好的人会把自己的拿给你注册。别说，还真有。下面，就来说两个神网站，一个专门接收短信验证码，一个专门接收邮件。这个网站提供很多虚拟手机号码，注册网站时随意填写一个手机号，接收网站发来的短信验证码就能注册。简单实用。不过要注意的是，注册那些无关紧要的网站可以，重要的网站别用这个。这些号码是公用的，短信对所有人开放，你能注册登录一个网站，别人也能通过找回短信验证码的方式，登录你的网站看到各种信息。接下来说接收邮箱验证信息。很多网站要你用邮箱注册，它会给你发你一封邮件里面是验证链接，以确认是你的邮箱。如果你不想邮箱塞满垃圾邮件的话，那就可以用下面这个网站，它提供免费临时邮箱。这两个网站我用了很久，推荐给你。鉴于前不久，写了一篇分享网盘资源网站的文章，不久后该网站就挂掉了，心痛。所以这次不再公开这两个网站，你可以自己搜，或者在我公众号后台回复：「知识星球」，我看到了就私发你。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速在中文和英文数字之间添加空格]]></title>
    <url>%2F2019%2F03%2F10%2Fweekly_sharing19.html</url>
    <content type="text"><![CDATA[写作排版必备技能：快速在中文和英文数字之间添加空格。摘要：快速在中文和英文数字之间添加空格。这里是「每周分享」的第 19 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期来说说写作排版。为什么想起这个主题呢，因为发现一些迷之尴尬的现象，很多人每天都在写文，有的还专门传授写作技巧，却连基本的文案排版都不会，最简单的一点就是「不会在中文和英文数字之间添加空格」。你可以验证一下，看看有多少文章是这样的，我敢说绝大部分都是。这么个小细节也算回事儿么？先拿同样一段文字对比一下：不会空格的人这样写：第八代Intel Core处理器将MacBook Pro的运算性能提升到了新的高度。15英寸机型现在可配备六核Intel Core i9处理器，处理速度最快可比上一代提升70%。会空格的人这样写：第八代 Intel Core 处理器将 MacBook Pro 的运算性能提升到了新的高度。15 英寸机型现在可配备六核 Intel Core i9 处理器，处理速度最快可比上一代提升 70%。哪段文字更好看？你要是说前一个或者不知道，那就不用往下看了，这篇文章不是为你准备的。明显后者看起来更舒服。事实上，很多优秀的网站或者作者都是这样排版的，比如苹果、少数派、stormzhang、阳志平等。我以前从未意识到「适当添加空格」这件事的重要性，直到看到一篇《中文文案排版指北》的教程，里面这么调侃那些不喜欢添加空格的人：有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。那之后就刻意遵守这个排版规则，仅这一点小改变，文章观感就明显比以前更舒适。不过这样做有一个缺点就是打字速度会降低，因为要常切换添加空格，有时可能还会忘记反而会让文章看起来不统一。于是开始找有没有可以自动添加空格的方法，最近找到了完美解决方案。使用「纯纯写作」这款笔记 App，它里面有一个「在中英文之间插入空格」的功能，有了它写文时可以先不管空不空格，写好后一次性添加所有空格，效率更高。不过这款 App 没有电脑版，无法直接实现这个功能，可以曲线借助坚果云网盘，在电脑端和手机端同步使用「拉取」和「上传」功能添加空格，效果像下面这样：添加之前：添加之后：想要这款 App 可以网上搜，也可以我的公众号后台回复：「笔记App」 得到。最后想说，写作好不好是能力，排版好不好是态度。觉得不错想看更多的文案排版规则的话，可参见下文：这个 GitHub 库能拯救你的文章排版本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 Python 官方年度报告学习 PPT 设计]]></title>
    <url>%2F2019%2F03%2F01%2Fweekly_sharing18.html</url>
    <content type="text"><![CDATA[分享几点专业 PPT 制作技巧。摘要：分享几点专业 PPT 制作技巧，另送一套珍藏模板。这里是「每周分享」的第 18 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。上周的一篇推文分享了一份官方的 Python 2018 年度报告，这份报告做得相当专业，我看了好几遍。文章传送门：说它专业，一个是内容本身专业，另一个是报告设计地专业，20 多页的报告仅使用了一些简单的元素组合搭配，却处处体现了设计美感。说到「设计」，很多人觉得这是设计师的专业领域，一般人不会也不用会。其实「设计」离我们每个人都很近，你常使用的 PPT 就是设计工具 ，只不过很多人没有把它当作设计工具来用，自然做出的 PPT 也就很一般。其实只要遵循一些简单的设计规则和技巧，我们都可以做出水准不错的 PPT。今天就来说说，从这份报告中可以学到哪些 PPT 制作技巧，当学会这些技巧了，相信你的 PPT 水平也会上一个台阶。初看这份报告最直观的感觉就是风格统一，简洁专业，看着舒服，我从下面这几点来说说：用色统一克制整套报告的前后页颜色统一，没有乱入的颜色，且用色很克制，只使用了三种颜色，符合多数专业报告的标准。以上这两点看似简单，但很多人却做不到。有的人做 PPT 完全不顾整体，前几张幻灯片用这种颜色，后几张又换成了其他颜色，最后一套 PPT 看起来乱七八糟。其实 PPT 新手通常驾驭不好多种颜色的使用，最安全保险的选择是只使用不超过三种颜色或者使用一类单色，这样的话 PPT 看起来至少风格是统一的，不会乱。具体做法很简单，在做 PPT 之前就选好配色方案，这样做的好处就是能够保证只从这一组颜色中去选，不会选到其他颜色，保证了整套 PPT 的颜色统一。通过左侧的图你可以看到，我设置了很多种配色方案，这个好处在于可以随时切换整套 PPT 的色彩搭配，只需要做好一套 PPT 就可以生成很多套不同风格的 PPT 出来。这也是网上卖的那些 PPT 模板，可以给你提供多种配色方案所采用的方法。你可能会问，这种专业的配色方案是我自己组合搭配出来的么，当然不是，我是从别处「偷」过来的。如果你不太懂色彩理论知识，也不会搭搭配颜色，那可以去专业的配色网站上寻找好的配色方案，比如 Color Hunt ，记录色彩方案的 RGB 颜色值再保存到 PPT 中，就可以作为自己的方案了。色彩饱和度适中这份报告使用了两种主色，黄色和蓝色，适中的色彩饱和度让报告看起来舒服不刺眼，而很多新手则爱用 PPT 中默认的高饱和度色彩，这是大忌。饱和度是指色彩纯度，数值范围是在 0-255，常说的大红大紫就是说颜色饱和度数值很高，看起来刺眼。新手不太懂饱和度的概念，只会用会用系统默认的高饱和度色彩，后果就是做出来的 PPT 看着很 LOW。事实上，大多数专业报告和 PPT 使用的都是适中的饱和度色彩。黑金万能搭配色报告还使用了一种万能的色彩搭配组合：黑色和金（黄）色，这两种颜色可以说是百搭，好看不俗气，如果你不知道哪两种颜色搭配好，就选这一组吧。除了这种组合，还有两种万能搭配色，分别是红黑和红金。上面说的都是配色方面，下面说一下字体。字体选择报告的标题字体很好看，没有使用烂大街的宋体、黑体和微软雅黑，使用的是「文悦后现代体」。字体也是影响 PPT 美观的一个重要因素，如果你想让你的 PPT 看起来不一样，那就不要用系统自带的字体，去网上下载一些好看的字体。字体下载网站如果推荐一个的话，我会首推「字客网」，这个网站有很多漂亮的中英文字体，大多都支持下载，唯一需要注意的是字体能否商用。图表制作接下来再来说说这份报告的图表。在 PPT 中绘制图表并不方便，通常是利用 Excel 或者一些形状元素绘制，效率不高。如果能够借助插件的话，图表绘制起来会快很多，比如我使用了一款叫 iSlide 的插件可以快速绘制下面的饼图：http://media.makcyun.top/%E9%A5%BC%E5%9B%BE.gif这样画图是不是很快？有些人做 PPT 很慢，往往是不会用或者没利用好 PPT 插件。对这款插件感兴趣的话可以下载下来体验下，也可以在我公众号后台回复：18期 得到这款插件。3 月份，很多人要开始准备做晋升述职类的 PPT 了，想到这个呢，今天就送各位一套珍藏已久的 PPT 模板。个人觉得模板不在于多而在于精，很多公众号动不动送几十上百套烂模板，学不到东西还占地方，我每次只送一套有质量的模板，数量少你不会有压力，也有时间慢慢去学习研究。来看看这套模板的部分预览图：你如果需要，可以在公众号后台回复 18 期 得到。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些不错的 Python 开源电子书]]></title>
    <url>%2F2019%2F02%2F28%2Fpython_learning02.html</url>
    <content type="text"><![CDATA[Python 各个领域的精品。摘要：我学习 Python 收藏的一些开源电子书网站。昨天给大伙儿送了书，留言区的篇幅占了整篇文章的一半，看来大家都想好（把）好（我）学（掏）习（空）。今天不送纸质书了，分享几个我学 Python 一路以来收藏的几个开源电子书网站。说到电子书，你可能会说「我有大把的电子书，还用得着你推荐？」注意，我这里说的不是烂大街的 PDF，是专门放电子书的网站。这种网站相比 PDF 电子书有什么好处呢，就是排版更优美。复制粘贴保存到自己的学习笔记中也很方便。说到保存学习笔记，昨天偶然发现印象笔记的 Windows 版支持 Markdown 了，你要也是 Windows 重度用户，感觉升级尝试下吧。你要是没用过 Markdown ，可以看我之前的写的这篇文章，保你相见恨晚：文章传送门：5 分钟内搞定公众号排版拐回来，下面就来介绍下这些电子书网站，不同的领域都介绍一个：Python 入门《A Byte of Python（简明 Python 教程）》豆瓣评分：8.7一句话介绍：Python 初学者的极佳教材。网址：https://wizardforcel.gitbooks.io/a-byte-of-python/content/Python 进阶《Python Cookbook》豆瓣评分：9.3一句话介绍：有很多高级技巧，想了解 Python 底层的工作原理就看这本。网址：https://python3-cookbook.readthedocs.io/zh_CN/latest/Python 数据分析《利用 Python 进行数据分析》豆瓣评分：8.6一句话介绍：学习 Python 基础库最好的书。网址：https://seancheney.gitbook.io/python-for-data-analysis-2nd/机器学习《Scikit-Learn与TensorFlow机器学习实用指南》豆瓣评分：9.6一句话介绍：机器学习书中理论结合实战最好的书。网址：https://hand2st.apachecn.org/#/READMEPython 数据结构算法《problem-solving-with-algorithms-and-data-structure-using-python》豆瓣评分：9.3一句话介绍：Python 数据结构与算法相关的书很少，这本堪称最好。网址：https://facert.gitbooks.io/python-data-structure-cn/最后来一个大杂烩，最全的在线手册，各种教程手册汇集到一起，哪个不会点哪个。网址：https://docs.pythontab.com以上就是我一直珍藏的几个电子书资源，配合纸质书学习，完美。本文完。ChangeLog2019/2/28 version 1.0]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python 入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 年大学毕业生薪酬排行榜]]></title>
    <url>%2F2019%2F02%2F25%2Fdata_analysis%26mining03.html</url>
    <content type="text"><![CDATA[简单分析 2018 年大学毕业生薪酬。摘要： 看看你的学校和专业 钱途 怎么样，你的工资有没有被平均。这两天刷微博看到一个热门话题，叫：2018 年中国大学毕业生薪酬排行榜 TOP 200。这份榜单罗列了各高校毕业生的薪资排名。名单出来后评论区炸了，有吐槽有调侃。有人说前三名应该是北电、中戏和上戏，有人说自己从毕业到现在赚了两个亿，一个失忆一个回忆，还有的惊叹自己专业薪资那么低（高）。我是一谈「钱」就激动，工作后同事之间的薪资是严格保密的，一般很难打听到，越神秘越好奇，总想跟别人比比看自己的工资是高还是低。高了，优越感油然而生；低了，再不跟这个人来往。赶紧戳开看了看，看完放心了，都比我高。一起来看看，先是按学校排名：然后是不同专业的薪资排名：大概看出这么几点事儿：工资最高的是「清北」这一点都不意外，2017 届毕业也就是工作一年的大学生，平均薪资在 9000 左右，工作三年后工资达到 1 万 1，五年后则能达到 1 万 3 左右。排前 40 的大部分是「985」和「211」高校是谁说学校不重要来着，薪资不会撒谎。北京外国语、对外经贸、外交学院这些大学薪资很高这些学校听得比较少，没想到工资这么高，为什么那么多人说学外语没前途？理学和工学类专业薪酬水平较高比如软件工程、材料物理、汽车类这些。农学、法学和管理学较低，大多数专业平均薪酬不到 3000 元。你看完是什么感觉？我是不太信这份榜单的，因为翻了几遍没发现我的学校。不过这份榜单说是由中国薪酬网发布的，他们调研了 39 所 「985」和 112 所「211」高校的 280 万学生计算出来的，比较权威。暂且不管吧，下面来用 Python 稍微深入地分析一下这份榜单，我分析了这么几点：哪些学校薪资有竞争力？筛选出来了工作一年和五年后，薪资最高（低）的 10 所高校：rankuniversity2017university20130清华大学9065北京大学137901北京大学9042复旦大学135942北京外国语大学9020外交学院126693上海交通大学9010清华大学126144对外经济贸易大学8998同济大学136165外交学院8956国际关系学院127866复旦大学8842上海外国语大学125877浙江大学8810中国人民大学122588同济大学8784对外经济贸易大学123169中央财经大学8771广东外语外贸大学12229rankuniversity2017university2013190兰州理工大学3616兰州理工大学5206191青岛科技大学3584延边大学5072192天津师范大学3574重庆工商大学5253193华北电力大学(保定）3542青岛科技大学5032194山东中医药大学3528南京信息工程大学4974195江苏科技大学3513山东中医药大学4871196武汉工程大学3460江苏科技大学4934197延边大学3437华北电力大学(保定）4931198南京信息工程大学3426武汉工程大学4734199西安建筑科技大学3394西安建筑科技大学4925可以看到：清华毕业一年薪资最高，但五年后就不如北大、复旦甚至外交学院。前十的不少是语言贸易类大学，羡慕。工程科技类大学薪资很低，有点没想到。哪些地方薪资高？薪资最高的还是上海、北京、广东这些一线城市，看来想拿高工资还是得去一线。另外，西北一片白是因为一所学校都没有统计到，身为一个新疆人很气愤。是理工类、综合类还是其他类型学校薪资高？国内大学按照学校的类型，可以分为下面十大类。薪资最高的不是理工类，也不是综合型大学，而是语言、艺术类学校，完全没想到啊。看来，今天薪资这么低是因为选错了学校类型。学校重不重要？出来工作后很多人鼓吹学校不重要，能力才重要。个人觉得这话对一半，要有本事的话，出身好能力又强不更好？来看「985 」「211 」和「双非」这三类高校不同工作年限的薪资情况。可以看到，不管是工作一年、三年还是五年，「985 」学校的薪资都高于「211 」学校，远高于双非学校。所以能上好学校还是尽量上。以上就是一个的简单的分析，源码在我的知识星球。参考：http://www.sohu.com/a/250611376_508571]]></content>
      <categories>
        <category>Python数据分析</category>
      </categories>
      <tags>
        <tag>Python数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[珍藏已久的网盘资源搜索网站和下载神器]]></title>
    <url>%2F2019%2F02%2F23%2Fweekly_sharing17.html</url>
    <content type="text"><![CDATA[推荐几个精品网盘资源搜索网站和下载软件。摘要：分享几个网盘资源搜索网站和下载神器。这是「每周分享」的第 17 期。最近有不少新朋友关注，所在再介绍一下这个栏目，每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、Ps 等几个方面。往期内容你可以在公众号界面中的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。这一期的主题是网盘资源资源搜索和下载。很多人在网上搜索电影、书籍、课程这些资源的时候，会直接在百度中去搜，往往要花很长时间才能搜到想要的东西。懂点门道的会选择专门的网盘资源网站，比如：去转盘、胖次搜索、网盘007 这些，但这些网站的资源基本差不多，你有的我也有，我没有的你也没有。接下来分享两个搜索资源的神级网站，在这两个网站里，我找到了很多其他地方找不到的资源。云盘精灵网址：www.yunpanjingling.com这个网站的特点是能搜到很多付费的东西，比如网课、图书、刚上映的电影等等，具体能搜哪些，自己可以对比其他网站试试，好的话过来留言告诉我。不过，该网站的资源是需要充值付费的，下载一个资源 2 分钱，等于不要钱。如果你不喜欢付费的，推荐第 2 个网站给你。爱搜资源网址：www.aisouziyuan.com这个网站可以说是「云盘精灵」的翻版，对比搜索了很多东西后，发现两个网站资源差不多丰富，总体来说，云盘精灵的资源更全面一些。有了获取资源的地方，就需要下载工具了。某度的网盘会员既收费还不好用，随便下载个大点的文件，必须得打开网盘才能下，不方便速度也慢。这里推荐几个下载工具，可以加速下载网盘资源。先来解决怎么不启动网盘直接下载大文件。你可以看到，我的下载界面多了个插件，可以选择普通下载或者复制链接等模式。点普通下载就可以用浏览器直接下载。这种网盘插件很多，在 Greak Fork 网站能找到很多，安装上就能用。普通下载模式速度比较慢，想加快的话可以选择「复制链接」到 IDM 这款下载神器中下载。想要这款精巧的神器，可以在公众号后台回复：「网盘下载」得到。如果你觉得速度还不够快，可以使用下面这款神器。速盘网址：www.speedpan.com这个一款不限速的网盘下载工具，我没有调到最快，速度还是杠杠的。保险起见，不要用常用的网盘下载，花两三块买个备用网盘，专门用来下载东西，就算被封了也没大碍。要是不知道在哪里买，私信我。以上主要是针对 Windows 平台的下载软件，如果你用的是 Mac，可以使用 BaiduNetdiskPlugin 这款插件，速度也很快。下载地址和使用教程，见下面这个 GitHub 库：https://github.com/CodeTips/BaiduNetdiskPlugin-macOS好，以上就是这一期分享的几个网盘资源搜索和下载的实用工具。善用佳软，提高生产力。本文完。/今日留言主题/你常用网盘么，还有什么网盘搜索下载的技巧或者网站？(留言格式：Day01：blahblah)]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>网盘搜索下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美国 IT 大牛建议：2019 年如何学习数据科学]]></title>
    <url>%2F2019%2F02%2F21%2Ftranslating01.html</url>
    <content type="text"><![CDATA[初学 / 转行 Python 的困惑，这里都有答案。摘要：初学、转行 Python 的困惑，这里都有答案。作者 | Thomas Nield来源 | How it feels to learn data science in 2019去年我决定从传统水利行业跨行到 Python 领域的时候，满脑子都是困惑与担心，犹豫放弃所学多年的专业知识值不值得，担心万一转行失败怎么办，纠结实际工作比想象中的难怎么办。没遇到指点迷津的大佬，只好网上各种搜，众说纷纭，最后在「要不要转行」这个问题上浪费了很长时间。在跨过这个坎之后，回头来看以前那些问题，思路清晰很多。其实，在开始阶段，相比具体的专业知识，更重要的是大方向把握。好比，你告诉我旅途上的风景有多么多么美，但我想先知道是哪条路。最近看到一篇叫「2019 年学习数据科学是什么感受」的文章，深有感触。作者是Thomas Nield，美国西南航空公司的商务顾问，著有《Getting Started with SQL (O’Reilly) 》等书，经验丰富的 IT 大牛。文章中他以一问一答的形式，给那些想要踏上数据科学之路的人，提了一些中肯的建议。里面有些观点很有价值，特翻译成文，这里分享给你。背景：假设你是一名「表哥」，平常工作主要使用 Excel，数据透视表、制图表这些。最近了解到未来很多工作岗位会被人工智能会取代，甚至包括你现在的工作。你决定开始学习数据科学、人工智能和机器学习，在 Google 搜索了「如何成为数据科学家」找到了下面这样一份学习路线图，然后你就开始向作者大牛请教了。Q：我是否真的必须掌握这个图表中的所有内容，才能成为数据科学家？成为一名自信数据科学家的必须技能（截至2013年）A：简单说，不需要全部。这是 2013 年的路线图，有点过时了，里面连 TensorFlow 都没有，基本没有人再参考。完全可以划掉这个图中的一些路径，前几年「数据科学」划分地过于分散，采用其他方法会更好。Q：听你这样说就不那么紧张了，那么我应该回到学校继续深造，然后获得一个数据科学硕士学位吗？ 我看很多数据科学家至少都是硕士。A：天哪，你为什么这样做？不要被「数据科学」这些高大上的术语给唬住了，这些术语主要是用来重新定义一些业务分类。事实上，学校教授的东西基本都是过时的技术，不如选择 Coursera 或 Khan Academy 这些在线自学网站。Q：那么我该如何开始自学呢？LinkedIn上的人说应该先学习 Linux ，Twitter 的人建议先学习 Scala，而不是 Python 或 RA：不要信那些人的话。Q：好的，R怎么样？不少人喜欢它。A：R 擅长数学建模，但 Python 能做的更多，比如数据处理和搭建 Web 服务，总之比 R 的学习投资回报率高。Q：R 在 Tiobe上的排名仍然很高，而且拥有大量的社区和资源，学它有什么不好？如果你只是对数学感兴趣，使用 R 完全没问题，配合 Tidyverse 包更是如虎添翼。但数据科学的应用范围远超数学和统计学。所以相信我，Python 在 2019 年更值得学，学它不会让你后悔。Q：Python 难学么？A：Python 是一种简单的语言，可以帮你可以自动完成许多任务，做一些很酷的事情。不过数据科学不仅仅是脚本和机器学习，甚至不需要依赖 Python 。Q：什么意思？A：Python 这些只是工具，使用这些工具可以从数据中获取洞察力，这个过程有时会涉及到机器学习，但大部分时间没有。简单地来说，创建图表也可以算是数据科学，所以你甚至不必学习 Python，使用 Tableau 都行，他们宣称使用他们的产品就可以「成为数据科学家」。Q：好吧，但数据科学应该不仅仅是制作出漂亮的可视化图表，Excel 中都可以做到，另外学习编程应该很有用，告诉我一些 Python 方面的知识吧A：学习 Python，你需要学习一些库，比如用于操作 DataFrame 的 Pandas 、制作图表的 Matplotlib，实际上更好的选择是 Plotly，它用了 d3.js。Q：我能懂一些，但什么是 DataFrame？A：它是一种有行和列的数据结构，类似 Excel 表，使用它可以实现很酷的转换、透视和聚合等功能。Q：那 Python 与 Excel 有什么不同？A：大不相同，你可以在 Jupyter Notebook 中完成所有操作，逐步完成每个数据分析阶段并可视化，就像你正在创建一个可以与他人分享的故事。毕竟，沟通和讲故事是数据科学的重要组成部分。Q：这听起来和 PowerPoint 没什么区别啊？A：当然有区别，Jupyter Notebook 更自动简洁，可以轻松追溯每个分析步骤。有些人不太喜欢它，因为代码不是很实用。如果你想做一款软件产品，更好的方法是使用其他工具模块化封装代码。Q：那么数据科学跟软件工程也有关系么？A：也可以这么说，但不要走偏，学习数据科学最需要的是数据。初学的最佳方式是网络爬虫，抓取一些网页，使用 Beautiful Soup 解析它生成大量非结构化文本数据下载到电脑上。Q：我以为学习数据科学是做表格查询而不是网页抓取的工作，所以我刚学完一本 SQL 的书，SQL 不是访问数据的典型方式吗？A：好吧，我们可以使用非结构化文本数据做很多很酷的事情。比如对社交媒体帖子上的情绪进行分类或进行自然语言处理。NoSQL 非常擅长存储这种类型的数据。Q：我听说过 NoSQL 这个词，跟 SQL 、大数据有什么关系？A：大数据是 2016 年的概念，已经有点过时了，现在大多数人不再使用这个术语。NoSQL 是大数据的产物，今天发展成为了像 MongoDB 一样的平台。Q：好的，但为什么称它为 NoSQL？A：NoSQL 代表不仅是 SQL，它支持关系表之外的数据结构，不过 NoSQL 数据库通常不使用 SQL，有专门的查询语言，简单对比一下 MongoDB 和 SQL 查询语言：Q：这太可怕了，你意思是每个 NoSQL 平台都有自己的查询语言？SQL 有什么问题？A：SQL 没有任何问题，它很有价值。不过这几年非结构化数据是热潮，用它来做分析更容易。需强调的是，尽管 SQL 难学，但它是一种非常通用的语言。Q：好的，我可以这样理解么： NoSQL 对数据科学家来说不像 SQL 那么重要，除非工作中需要它？A：差不多，除非你想成为一名数据工程师。Q：数据工程师？A：数据科学家分为两个职业。数据工程师为模型提供可用的数据，机器学习和数学建模涉及比较少，这些工作主要由数据科学家来做。如果你想成为一名数据工程师，建议优先考虑学习 Apache Kafka 而不是 NoSQL，Apache Kafka 现在非常热门。如果想成为「数据科学家」，可以看看这张数据科学维恩图。简单来说，数据工程师是一个多领域交叉的岗位，你需要懂数学/统计学、编程以及你专业方面的知识。Q：好吧，我不知道我现在是想成为数据科学家还是数据工程师。回过头来，为什么要抓维基百科页面呢？A：抓取下来的页面数据，可以作为自然语言处理的输入数据，之后就可以做一些事情，如创建聊天机器人。Q：我暂时应该不用接触自然语言处理、聊天机器人、非结构化文本数据这些吧？A：不用但值得关注，像 Google 和 Facebook 这些大公司，目前在处理大量非结构化数据（如社交媒体帖子和新闻文章）。除了这些科技巨头，大部分人仍然在使用关系数据库形式的业务运营数据，使用着不是那么前沿的技术，比如 SQL。Q：是的，我猜他们还在做挖掘用户帖子、电子邮件以及广告之类的事情。A：是的，你会发现 Naive Bayes 有趣也很有用。获取文本正文并预测它所属的类别。先跳过这块，你目前的工作是处理大量表格数据，是想做一些预测或统计分析么？Q：对的，我们终于回到正题上了，就是解决实际问题，这是神经网络和深度学习的用武之地吗？A：不要着急，如果想学这些，建议从基础开始，比如正态分布、线性回归等。Q：明白，但这些我仍然可以在 Excel 中完成，有什么区别？A：你可以在 Excel中 做很多事情，但使用程序你可以获得更大的灵活性。Q：你说的编程是像 VBA 这样的么？A：看来我需要从头说了。Excel 确实有很好的统计运算符和不错的线性回归模型。但如果你需要对每个类别的项目进行单独的正态分布或回归，那么使用 Python 要容易得多，而不是创建一长串的公式，比如下面这样，这会让看公式的人无比痛苦。除此之外，Python 还有功能强大的 scikit-learn 库，可以处理更多的回归和机器学习模型。Q：这需要涉及到数学建模领域是吧，我需要学习哪些数学知识？A：从线性代数开始吧，它是许多数据科学的基础。你会处理各种矩阵运算、行列式、特征向量这些概念。不得不说，线性代数很抽象，如果你想要得到线性代数的直观解释，3Blue1Brown 是最棒的。（这和我之前写的一篇文章观点不谋而合：最棒的高数和线代入门教程）Q：就是作大量的线性代数运算？这听起来毫无意义和无聊，能举个例子么？A：好吧，机器学习中会用到大量的线性代数知识，比如：线性回归或构建自己的神经网络时，会使用随机权重值进行大量矩阵乘法和缩放。Q：好吧，矩阵与 DataFrame 有什么关系？感觉很相似。A：实际上，我需要收回刚才说的话，你可以不用线性代数。Q：真的吗？那我还要不要学习线性代数？A：就目前而言，你可能不需要学习线性代数，直接使用机器学习库就行，比如 TensorFlow 和 scikit-learn 这些库，它们会帮助你自动完成线性代数部分的工作。不过你需要对这些库的工作原理有所了解。Q：说到机器学习，线性回归真的算是机器学习吗？A：是的，线性回归是机器学习的敲门砖。Q：真棒，我一直在 Excel 中这样做，那我是不是也可以自称「机器学习从业者」？A：技术上来说是的，不过你需要扩大知识面。机器学习通常有两个任务：回归或分类。从技术上讲，分类是回归。决策树、神经网络、支持向量机、逻辑回归以及线性回归，这些算法都在做某种形式的曲线拟合，每种算法各有优缺点。Q：所以机器学习只是回归？它们都有效地拟合了曲线？A：差不多，像线性回归这样的一些模型清晰可解释，而像神经网络这样更先进的模型定义是复杂的，并且难以解释。神经网络实际上只是具有一些非线性函数的多层回归。当你只有 2-3 个变量时，它可能看起来不那么令人印象深刻，但是当你有数百或数千个变量时它就开始变得有趣了。Q：那图像识别也只是回归？A：是的，每个图像像素基本上变成具有数值的输入变量。你必须警惕维度的诅咒，变量（维度）越多，需要的数据越多，以防变得稀疏。这是机器学习如此不可靠和混乱的众多原因之一，并且需要大量你没有的标记数据。Q：机器学习能解决安排员工、交通工具、数独所有这些问题吗？A：当你遇到这些类型的问题时，有些人会说这不是数据科学或机器学习而是运筹学。Q：这对我来说似乎是实际问题。运营研究与数据科学无关？A：实际上，存在相当多的重叠。运筹学已经提供了许多机器学习使用的优化算法。它还为常见的 AI 问题提供了许多解决方案。Q：那么我们用什么算法来解决这些问题呢？A：绝对不是机器学习算法，很少有人知道这一点。几十年前就有更好的算法，树搜索、元启发式、线性规划和其他运算研究方法已经使用了很长时间，并且比机器学习算法对这些类别的问题做得更好。Q：那为什么每个人都在谈论机器学习而不是这些算法呢？A：因为很长一段时间里，这些优化算法问题已经有了令人满意的解决方案，但自那时起就一直没有成为头条新闻。几十年前就出现了这些算法的 AI 炒作周期。如今，AI 炒作重新点燃了机器学习及其解决的问题类型：图像识别、自然语言处理、图像生成等。Q：所以使用机器学习来解决调度问题，或者像数独一样简单的事情时，这样做是错误的吗？A：差不多，机器学习，深度学习这些今天被炒作的任何东西通常都不能解决离散优化问题，至少不是很好，效果非常不理想。Q：如果机器学习只是回归，为什么每个人都对机器人和人工智能，这么忧心忡忡，认为会危害我们的工作和社会？我的意思是拟合曲线真的那么危险吗？AI 在进行回归时有多少自我意识？A：人们已经找到了一些巧妙的回归应用，例如在给定的转弯上找到最佳的国际象棋移动（离散优化也可以做）或者计算自动驾驶汽车的转向方向。但是大多都是炒作，回归只能干这些事。Q：好吧，我要散个步慢慢消化下。我目前的 Excel 工作感觉也算「数据科学」，但数据科学家这个名头有点虚幻。A：也许你应该关注一下 IBM。本文完。参考：∞ How it feels to learn data science in 2019]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回顾当初是怎么跳出 Excel 转学 Python 的]]></title>
    <url>%2F2019%2F02%2F21%2Fpython_learning01.html</url>
    <content type="text"><![CDATA[保持兴趣，才能发现更多。摘要：回顾当初是怎么跳出 Excel 转学 Python 的 。昨天的文章，说了一位用 Excel 做数据分析的「表哥」，在感受到人工智能所带来的行业冲击后，担心未来自己的工作会被取代，决定学一些数据科学和人工智能方面的技能，向 IT 大牛 Thomas Nield 请教之后，大牛告诉他可以学 Python、机器学习这些。文章传送门：∞ 2019 年怎么学习数据科学「表哥」提了一些有点傻的问题，认为 Python 中的数据处理、绘制图表、线性回归这些功能，Excel 也都能做，甚至连编程 Excel 也有 VBA。总之觉得 Excel 不比 Python 差，自己称得上是「数据科学工程师」。他提的这些问题，很典型也有意思，没有接触过编程的人会这么天真地对比。很理解他，因为看到了自己的影子，确切地说，他就是前一两年的我。以前在学校，做软件模型实验会产生大量的数据，基本都用 Excel 处理，操作基本都是手工粘贴复制、拖拉填拽完成，效率低地可怜，后来跟着同学慢慢学会编公式，尝到了便利的甜头，原来 Excel 还能这样玩。软件用了那么多年，只会那 5% 的基础功能，剩下的基本没点过。之后决定好好学习一下 Excel，下载了不少实战教程书看，比如：数据处理分析、函数公式、图表绘制、VBA 这些。如果你现在经常用到 Excel 的话，这里推荐 ExcelHome 论坛，他们出版的一系列书，系统详细，我看完了一整套。送个福利，公众号后台回复：「Excel」可以得到全部电子版 PDF。Excel 系统学了一阵之后，思路开阔很多，以前遇到的难题，很快能知道解决方法，顺手拈来，往往还不止一种思路。对数据分析越来越有兴趣，不断尝试 Excel 的各种功能，甚至还用 Power BI 做过一次爬虫数据分析。接触网络爬虫后，发现 Excel 的爬虫功能有限，就开始搜爬虫用什么比较好，偶然就了解到了 Python，说是很热门的语言能干很多事，爬虫、数据分析比 Excel 牛逼多了。一时兴起想看看到底怎么个牛逼法，以前没有接触过编程，所以一时半会儿也没搞懂 Python 能干什么，只是觉得比 Excel 厉害。之后决定不能再处在 Excel 舒适区里，得去学 Python。虽决定要学 Python，但真学起来的时候发现难度比想象中大，光是思维转换就很困难。一直习惯 Excel 中直观的手动操作，现在却要全部靠编写代码实现。简单的几行代码，报错不断。画一个简单图表、处理一张表格这些用 Excel 几分钟就能搞定的操作，写 Python 用了半个钟，一度怀疑要不要放弃，根本不觉得 Python 比 Excel 效率高。强迫自己学了几个月，写了一些爬虫案例后才慢慢找到感觉，体验到学编程的好处。很明显的一点区别就是可复用性，Excel 中的大部分操作是一次性的，操作完一次，下一次只能再手动重复一遍，效率极低，而 Python 写的代码可以无限复用。如果一些操作只需要做一次，那用 Excel 没有什么问题，但重复的操作用 Python 能够一劳永逸了，节省很多时间。最直观的一个例子，可以看看之前写的一篇文章：∞ 5 行代码入门爬虫以上是个人过去的一点经验，如果你目前还在大量用 Excel，不妨多尝试写写 Python 代码，一个不错的方法是用 Excel 和 Python 分别实现同样的目标。本文完。]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python 入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[求职必看 | 国内外独角兽公司排行榜]]></title>
    <url>%2F2019%2F02%2F16%2Fhotspot02.html</url>
    <content type="text"><![CDATA[世界第 3 大独角兽公司滴滴裁员，求职寒冬有必要了解一下独角兽公司。摘要：求职别只盯着 BAT，来看看独角兽公司潜力排行榜。年基本过完，寒冬依然没过去。这两天「滴滴亏损 109亿，裁员 2000 人」的新闻一爆出，这个冬天瞬间更冷了。滴滴是什么公司？你可能说，不就是一家做网约车生意的公司嘛。事实上，滴滴可能比你想象地牛逼。根据权威网站「CBInsights」对全世界独角兽公司的最新统计（截止2019 年 1 月），滴滴是世界第 3 大独角兽公司，估值达到 560 亿美金，近 4000 亿人民币，比一大堆上市公司都值钱。除了第 3 的滴滴，排名第一和第二的分别是中美两国独角兽「扛把子」：今日头条和 Uber。全球独角兽公司估值 TOP 20 排名RankCompanyCountryValuation (亿美元)1Toutiao (Bytedance)China7502UberUnited States7203Didi ChuxingChina5604WeWorkUnited States4705AirbnbUnited States2906SpaceXUnited States2207StripeUnited States2008JUUL LabsUnited States1509Epic GamesUnited States15010PinterestUnited States12011Bitmain TechnologiesChina12012SamumedUnited States12013LyftUnited States12014GrabTaxiSingapore11015Palantir TechnologiesUnited States11016Global SwitchUnited Kingdom11017InforUnited States10018DJI InnovationsChina10019One97 Communications (operates Paytm)India10020Go-JekIndonesia100完整名单可以在公众号后台回复：「独角兽」得到。你可能听过独角兽公司，但并不太清楚和一般公司、上市公司有什么区别，简单介绍下：独角兽公司（Unicorn Companies）是指：成立不到 10 年，估值在 10 亿美元以上，且未在股票市场上市的科技创业公司。超级独角兽公司则是估值在 100 亿美元以上。独角兽公司在多轮融资后就会谋划上市，公司市值也会随之大涨一番。比如去年上市之前的独角兽公司：小米、美团、拼多多，上市后市值分别增长了 400、1200、1300 亿元。回过来说，滴滴这么牛逼的公司都没能抗住这个寒冬，举起了裁员的大刀，看来这个冬天真的是既冷又长。实际上，从去年冬天开始，不少公司都被爆出了「裁员缩编」的现象，滴滴只不过是新年里的第一个重磅炸弹而已。求职环境越来越恶劣，以前找工作挑公司，现在能找到都算不错。好在马上就要到三月了，难得的「金三银四」黄金求职季。如果你打算找工作，就努力抓紧机会，如果你没找工作计划，那恭喜你不用面临残酷的竞争，至少该认清下形势，把握好时间努力提升自己。不少人找工作很被动。有些人只看大厂招聘，有些人则看到有公司招人，岗位匹配就投递简历。前者选择范围有限，后者网撒地又太大，要是好一阵子都找不到工作很容易陷入焦虑。其实，找工作之前很重要的一点是锁定目标公司。别只盯着 BAT，不妨关注一些有潜力的独角兽公司，这样选择面更广，目标也清晰，找工作也能更主动。下面来了解下独角兽公司。先看看全球情况。下面这张图是全球独角兽公司的分布，目前一共有 326 家独角兽公司，分布在 26 个国家，总估值超过 10,000 亿美元，大部分集中在中美两国。从各国具体的数据来看，美国基本占了全球独角兽公司数量和估值的 50%，其次是中国，占了 30% 左右，剩下的二十来个国家只占 20%。可以说，全球只有中美两国在「玩」独角兽公司。简单对比一下两国估值排名前十的独角兽公司。可以看到美国除了 Uber，比较熟悉的还有：短期房屋出租网站 Airbnb、伊隆·马斯克的 SpaceX 、图片分享网站 Pinterest 等。中国排名前十的则有今日头条、滴滴、大疆等。总体来看，美国的独角兽公司估值比中国均匀一些。另外，上面的公司名单中没有出现「蚂蚁金服」公司，这可是个估值超过 1500 亿美金的「大家伙」，有了蚂蚁金服，中国在账面上至少是不输美国了。相较于全球独角兽公司 ，我们可能更关心本土公司的情况。好在，胡润研究院也经常发布独角兽公司报告。该机构 1 月 24 日发布了《2018胡润大中华区独角兽指数》报告，将「蚂蚁金服」列入了独角兽企业估值榜单的第一位，估值达到 10,000 亿人民币。这份报告中显示，大中华区一共有 186 家独角兽公司上榜，总估值超过 5 万亿人民币，体量相当大，平均每个公司估值 270 亿。来看看估值前二十名的公司有哪些：完整名单可以在公众号后台回复：「独角兽」得到前二十名公司中，有多少你此前是不知道的？这些公司就是不错的求职目标。先来总体看一下 186 家公司都分布在哪些城市：独角兽公司主要集中在京津、长江经济带和珠三角三大区域。其中，北京「一枝独秀」，不得不说，北京的科创公司环境氛围是真的好。挑选图中比较突出的五个城市作对比，四个一线城市加上杭州。北京的独角兽公司整体估值水平远高于其他四大城市，上海和广州较弱，深圳和杭州旗鼓相当，马老板的「蚂蚁金服」让杭州很突出。放大城市范围，看看大中华区各大城市估值排名前三的独角兽公司有哪些：CityRankCompanyValuation(亿元)Industry北京1今日头条5000文化娱乐2滴滴出行3000互联网服务3京东数字科技1000互联网服务上海1陆金所2500互联网金融2平安医保科技500医疗健康3金融壹账通500互联网金融广州1小鹏汽车300汽车交通2云从科技200人工智能3名创优品150新零售深圳1微众银行1500互联网金融2大疆1000机器人3柔宇科技300智能硬件杭州1蚂蚁金服10000互联网金融2菜鸟网络1000物流服务3微医400医疗健康南京1苏宁金服500互联网金融2满帮400物流服务3汇通达200电子商务天津1神州优车400互联网服务2纳恩博100智能硬件苏州1信达生物100医疗健康2基石药业70医疗健康香港1客路旅行70互联网服务2亚洲医疗70医疗健康成都11919酒类直供70新零售无锡1华云数据70大数据与云计算武汉1斗鱼200文化娱乐绍兴1电咖汽车100汽车交通重庆1猪八戒网100互联网服务金华1零跑汽车70汽车交通长沙1芒果TV100文化娱乐青岛1日日顺100物流服务台北1辉能科技70新能源只有 19 座城市拥有独角兽公司，城市基本是一二线，且一多半的城市只上榜了一家公司，比如成都、重庆。看来，想去独角兽公司工作，得身处一二线城市。接着对比一下各个行业的独角兽公司情况：互联网金融行业拥有绝对的优势，占了全部公司估值 5 万亿的三分之一。其次是互联网服务行业，看来互联网行业的工作机会要大很多。具体地看看各行业估值前三名的公司是哪些：IndustryRankCompanyValuation(亿元)互联网服务1滴滴出行30002京东数字科技10003车好多500互联网金融1蚂蚁金服100002陆金所25003微众银行1500人工智能1商汤科技4002Face++2003地平线机器人200区块链1比特大陆5002嘉楠耘智1503亿邦国际70医疗健康1平安医保科技5002微医4003联影医疗300大数据与云计算1金山云1002盘石股份1003华云数据70房产服务1链家4002小猪短租1003V领地70教育1VIPKID2002猿辅导2003作业帮200文化娱乐1今日头条50002快手10003博纳影业200新能源1辉能科技70新零售1名创优品1502瑞幸咖啡15031919酒类直供70智能硬件1柔宇科技3002寒武纪科技1503纳恩博100机器人1大疆10002优必选300汽车交通1威马汽车3002小鹏汽车3003奇点汽车200游戏1英雄互娱100物流服务1菜鸟网络10002京东物流8003满帮400电子商务1美菜网5002每日优鲜2003小红书200这里面又有多少公司，你此前是不知道的？ 你不知道意味着很多人也不知道，现在你知道了，就掌握了一定的主动权。最后，感兴趣的话可以去了解下，为了更方便你，我已下载好全球 326 家和国内 186 家的独角兽公司数据，在公众号后台回复：「独角兽」就可以得到。2019 年，撸起袖子加油干。本文完。参考：∞ January 2019: research-unicorn-companies∞ 2018胡润大中华区独角兽指数本文完。]]></content>
      <categories>
        <category>热点分享</category>
      </categories>
      <tags>
        <tag>热门事件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 年的第一场雪]]></title>
    <url>%2F2019%2F02%2F14%2Flife08.html</url>
    <content type="text"><![CDATA[好些年没看到雪了。摘要：流水账一篇。自从去年 8 月来到北京这边，就盼望着冬天能下一场雪。从新疆到广东后的的几年里，冬天一直是当夏天过的，一个新疆人快记不清冬天是什么感觉。然而你越盼它越不来，十一二月没下，一月也没下，年都快过完了还是没有下。不免遗憾，这个冬天是看雪概率最大的一次，错过不知下次是什么时候，在东莞的那几个冬天，唯一遇到的一次小雪时隔了一百多年。多年之前，在新疆念中学，每天骑车去上学。冬天冷下雪更冷，天亮地比内地迟两小时，早上学都是摸黑出门，路灯光线暗。下了一整晚的雪，被来往的车压过之后会变成冰，走在上面稍不留神就会摔跤。经常看到骑车的人骑着骑着就摔到马路中间去了，后面的出租车赶紧急刹或者转方向盘，很险。所以下雪的时候，不再骑车，改为推自行车跑步去四公里之外的学校，持续整个冬天。那时冬天就是雪，雪就是冬天。本来不再抱期望，没想到前天中午突然飘起了小雪，像柳絮满天飞，短短一会儿就停了下来，落到地上还没积起来就化地一干二净，虽不算完整意义上的「下雪」，但好歹是下了。看了看天气预报，说最近几天都会下，很是期待。结果昨天晴了一整天，回头再查天气预报，却显示未来几天都不会再下雪。不知从什么时候开始，下雪变成了一件奢侈的事情。中午在电脑前码字，馍馍大声喊「下雪了」，头歪向窗外，果然。家里人看了看天说估计会下大，不太信，事实证明又想错了。随后，雪越下越大，没有要停下来的意思，到最后如鹅毛大雪一般，小区楼下开始渐渐泛白。雪落在了地上、树枝上、凉亭上，整个小区都变成了白色。记不起上一次遇到这么大的雪是哪一年了，可能是 2002 年的第一场雪。馍馍兴奋地不行，他在南方出生，这辈子是第一次见到大雪。兴奋地不行，一会儿冲到阳台边，一会儿喊大人去看，生怕他们没看到。透过窗户，小区里的大人和小孩儿纷纷走了出来。问馍馍想不想下去玩，他马上说好，给他「全副武装」好后一起下了楼，一走出来，密密麻麻的雪从天而落，速度如雨滴那般快，比在楼上隔着玻璃看地清楚。馍馍一会儿望着天上问：「雪从哪里来，它们要去哪里」，一会儿看着地下，不停用脚踩来踩去。在小区逛了一圈，到处都是人，比过年还热闹，有堆雪人的、打雪仗的还有视频直播的。看到那些小朋友带着铲子、手套、雨伞，忽然觉得我们很惨，什么都没带，雪落了一身，反而觉得暖和。给馍馍捏了几个雪球，手就快要冻僵，如《雪国列车》那般冷。他手拿了一会儿后，也冻得不敢再拿，说：「太凉了」。看来忘记了教他「冻」这个词。顺手就把雪球轻轻砸向他的肚子，裂成几瓣掉在了地上，他开心地不行，结果乐极生悲，脚一滑，四脚朝天结结实实地摔了一跤，吓得哇哇大哭。见状，赶紧掏出手机给他拍了几张，他哭得更厉害了。扶他起来哄了两句，他看到别的小朋友在堆雪人，说也想玩，没办法只好尝试着用手把周围的雪堆成一圈，结果几十秒钟不到，手冻得没知觉，最后雪人没堆起来，只滚出一个大雪球，不舍得扔带回了家，给馍馍说是他的「晚餐」，他听罢张嘴就准备咬。把馍馍放在家之后，去了附近的一条河边。夏天来过一次，那会儿水面波光粼粼，两岸郁郁葱葱，岸边还有人钓鱼。到了之后，眼前只有茫茫一片白，分不清河跟岸。捡起一块石头用力向远处扔去，听到很闷的回响，确定冰面很厚可以站人。朝河中间走去，一个脚印也没有，伴着大雪纷飞，耳边仿佛响起了《难念的经》，朝远处望去又好似看到《情书》的场景。你多久没有近距离地接触过雪了？七夕快乐。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《流浪地球》导演的微博粉丝不到 10 万，却能一打七]]></title>
    <url>%2F2019%2F02%2F11%2Flife07.html</url>
    <content type="text"><![CDATA[新年开工第一天，借《流浪地球》打鸡血。摘要：新年开工第一天，借《流浪地球》打鸡血。一周的春节年假转眼间就过去了，很多朋友今天开始上班了，终于开工了，太好了。我在家里祝你们工作顺利，哈哈。过了个年，你长了几斤膘？见了多少亲朋好友？耍了多少地方？我呢，春节没胖，没见亲朋好友，哪里也没去，除了电影院。不过，这个春节过得挺充实。先是年三十的春晚，不得不说，今年的节目水准比前几年好多了，往年春晚没有完整看完过，一个节目也没记住，而今年的语言类节目有不少亮点。到今天年初七，还能记得起几个有意思的节目，比如：佟掌柜闫妮的《办公室故事》、第一次上春晚的葛优带来的《儿子来了》、开心麻花团队的《占位子》、老戏骨林永健的《演戏给你看》、贾玲的《啼笑皆非》以及阔别多年重回春晚舞台的刘谦表演的魔术《魔壶》。一台春晚能有这么几个不错的节目，难得。看来一个好导演还是很重要啊，今年的春晚导演叫「刘真」，第一次导演春晚，一位啥都敢说的「真」导演。就冲他这态度，我希望明年的春晚导演还是他。导演虽不像主持人、演员那么为观众所熟知，但他们发挥的作用绝对是最大的，除了上面说的春晚导演刘真，还包括下面要说的电影导演。看完春晚也就过了除夕，正式迎来 2019 年，今年的大年初一如果用一个词来形容的话，就是「屌炸天」。为啥？因为这一天有八部电影上映，每一部电影搁到往年都算是大片，堪称「史上最牛逼春节档」，这一天的票房几乎等于 2004 年全年票房。2018 年，我没有去过电影院，也基本没有在线上看过电影。春节看到了这些电影，感觉就是「饥不择食」，年初一中午吃了碗泡面，速抢了张电影票就奔去电影院了，平常看电影是挑座位，那一天只能挑还有哪个电影院有票。你猜我买的是上面哪部电影？你可能猜不到，我买的排名第四的《流浪地球》。今天，你估计会觉得我很会挑，其实不是，是因为前三部火爆地一票难求，所以就买了这部电影，说实话，春节之前根本不知道这部电影。两个小时后，看完出了电影院，回答了知乎上的一个问题：一个礼拜后的今天来看，这个预言应该不会错，现在票房已经 20 多亿了，保二望一指日可待（第一是 56 亿的《战狼2》）。看了《流浪地球》，很容易想起 2015 年看过的《大圣归来》，都是忍不住想再去看一遍，所以没两天就又去了家有 IMAX 版本的电影院。去完电影院，开始在知乎、微博、豆瓣、B站上不停地刷这部电影的信息，感觉一年的娱乐时间都提前透支完了。了解了电影中很多细节、看了各种好的坏的评论、也看了全部宣传片和MV。关于这部电影的评价讨论，经过这些天网络上的各种刷屏，你应该也了解不少了，什么耗时 4 年、拍摄遭遇撤资、空手套《战狼》等等。还不太了解的话，推荐看下面这一篇文章：《流浪地球》诞生记：拍了这个电影，我能吹一辈子牛逼众多评价中，有一个不可忽略的现象，是网上对这部电影的口碑评价呈两极化趋势，尤其是在豆瓣上，出现了大量的一星评论，评分从刚开始的 8.4 分也降到了现在的 8 分以下。那些给这部电影打一星两星的人，说的头头是道，那些看不过去的人看了很不爽，双方就开始互怼，电影里的战场估计都没有这么猛。要我给这部电影打分的话，我只想说：「满分是多少就打多少」。因为，想看到用心的导演拍出更多用心的电影，这些年受够了洋垃圾、流量明星、强行植入中国元素的电影。说一点网上没有人说过的，也是偶然发现的，就是电影导演郭帆的微博粉丝数，只有区区几万！而且是在这部电影已经火了好几天的情况下，在这之前恐怕只有两三万。虽然不玩微博，但觉得很多草根网红的粉丝数恐怕都不只这么点，更别说那些大碗了，比如下面这几位，一条微博，留言点赞几十万，转发几百万。然而，就是这位只有几万粉丝的导演，拍出了几十亿的电影，票房从第四变成「一打七」，还是吊打。起先觉得是郭帆导演太低调了，才这么点粉丝，就又随便搜了另外一位导演文牧野的粉丝数，「文牧野」这个名字你可能不熟悉，但说一部他导演的电影你就知道了，就是那部 票房 31 亿，豆瓣 100 万人评分 9.0 的《我不是药神》。他的粉丝数是多少呢，23 万。这两组数字对比下来，心痛伴随着开心，心痛就不说了，开心的是：中国从不缺认认真真做电影的人！最后，套用《流浪地球》电影中的一句台词作为结尾：无论中国的电影最终走向何方，我选择希望！今天新年开工第一天，无论 2019 年会怎么样，愿你今年充满希望。本文完。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐几个看最新电影，下无损音乐的网站]]></title>
    <url>%2F2019%2F02%2F02%2Fweekly_sharing15.html</url>
    <content type="text"><![CDATA[看最新电影，下无损音乐。摘要：看最新电影，下无损音乐，这 5 个网站就够了 。这是「每周分享」的第 14 期，也是农历 2018 年的最后一期。最近有不少新朋友关注，这里就再介绍一下这个栏目，每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号界面中的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。过年的气息越来越浓，后天就是除夕了，如果你是学生党应该早已开启年假模式，如果你是上班族也差不多到家或者在回家的路上了。一年到头，终于可以好好放松几天，和家人朋友在一起吃吃喝喝，小娱小乐，想想都惬意。说起娱乐，很多人会选择看电影听音乐，有空就去电影院，今年春节有很多不错的电影，没空的话，在家看看电影或者听听歌也不错。这一期的分享，就来推荐 5 个我珍藏许久的看电影和听音乐的网站，给你的春节添一点乐趣。首先是 3 个电影网站。能看电影的网站有很多，个人觉得一个网站好不好，主要看新片更新的速度和质量，毕竟一般在网上看电影就是为了看新片。下面推荐的 3 个网站最大特点就是新片更新速度快，甚至快过爱奇艺、优酷。这里，选了三部最新的大片作测试。网站 1：新 6V 电影（www.66s.cc/）推荐指数：★★★★★这个网站强烈推荐，片源多，新片更新速度快。上面三部电影在这个网站上都能搜到，提供了很多资源下载链接，还可以在线观看，良心。网页端：手机端：网站 2：两个 BT （www.bttwo.com/ ）推荐指数：★★★★这个网站片源丰富，上面三部大片，搜索到了两部，可以在线播放也可以下载。网站 3：疯狂影视搜索 （ifkdy.com/）推荐指数：★★★这个网站能直接搜索电影，有片源就可以在线看。不过，上面的三部大片这里只搜到了一部。说完网站，下面推荐两个听音乐网站。网站 1： 音乐搜索神器 （lai.yuweining.cn/music）推荐指数：★★★★这个网站可以下载各大付费平台的音乐，以周董的《告白气球》为例，可以在线听也可以下载。网站 2 ：51ape （http://www.51ape.com/）推荐指数：★★★★★这个网站可以下载无损音质的音乐，如果你对音质比较挑剔，这个网站能够满足你，一首歌几十 M 的体积，注意手机内存就是了。一个小福利，下载了部分张国荣和周杰伦的歌，你要刚好也喜欢的话，可以后台回复：「音乐」 得到。最后，这是农历 2018 年的最后一更，好好过个年，就不骚扰你了。不过，想到过年期间你玩手机的频率可能会更高，考虑到你可能会有无聊的时候，所以打算推出一个新栏目：「过年七天乐之年度最佳文章回顾」，每天转载其他公众号的一篇年度最佳文章，年初一开始，每天晚上 19：00 更新。你可能问为什么是这个时间，因为「苏克1900」 啊。本文完。推荐阅读：Python 告诉你绝不知道的1983-2018 春晚]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>电影音乐网站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习必备 | 最棒的高数和线代入门教程]]></title>
    <url>%2F2019%2F01%2F31%2FMachine_learning01.html</url>
    <content type="text"><![CDATA[这门高数和线代教程，绝对刷新你三观。摘要：最牛逼的高数和线代教程分享给你 。前阵，推荐了一个很棒的 Python 入门视频教程，很多朋友喜欢，觉得是良心推荐，传送门：∞ 入门 Python 最好的视频说实话有点意外，因为这门课一点都不小众，在我推荐之前已经非常火了，我当初也是看了别人的推荐才知道的。尽管如此，从大家的反应来看还是有许多人第一次知道，很开心帮助到了这些朋友，这也让我今后更有动力去分享。很多时候，我们不愿意主动去分享的一个重要原因，是自己先否定了自己，觉得「这个大家都知道，没什么稀罕的」，这样想就偏了，张哥说过一句话我觉得有道理：「在中国，你以为很普通的常识，也至少有一亿人不知道。」所以，主动分享利他利己，说来就来，今天继续分享干货。最近在学习机器学习，涉及到不少高数和线代知识，大一学了这两门课，没怎么学懂，考试只是低空飘过，一直不怎么喜欢这两门课，开心直到现在都没再接触过。近期不得不重新捡回来，很是头大，毕业当废纸论斤卖掉的课本，在网上又高价买了回来。快速过了一遍当初的教材，感觉依然是「知道怎么回事，但不理解为什么」，这一方面归结于自己领悟能力差，另一方面就要吐槽国内的教育理念了，教材不人性，老师教法也有问题。记得当时教线代的是一位刚博士毕业不久的老师，喜欢自 High ，经常在黑板上手推公式，推到最后自己都不会了，站在那儿傻乐，台下早已睡到一片。曾一直认为学高数、线代，上面这两本教材是最好的选择，直到后来看了别人推荐的国外课本，才发现原来最好的教材在国外，后悔不已，为什么当初学的时候没有用到国外的教材，这之中的原因就不细说了。下面就推荐我认为入门学习高数、线代最棒的教材和视频。教材没有打任何广告，单纯就是觉得这两本书不错，甩国内教材不知道几条街，文末会提供电子书，可以看看。下面，是重头戏。推荐一个学习高数、线代最棒的入门视频，如果你有被它们支配过的恐惧，那绝对该看看，因为它会颠覆你的三观。视频这门视频教程是一位斯坦福大学的数学系学生 Gran 制作的，发布在油管的「3BlueBrown」频道上，也搬运到了国内的 B 站上。这门系列课最大的特点就是通过 Python 制作的各种酷炫动画，帮助你理解深奥的数学概念。如果说，国内老师是教你怎么算，Gran 则是教你为什么这么算。比如，x 的立方求导，你知道为什么等于 3x 的平方么？换个角度再来理解下：Sin(x) 求导为什么等于 Cos(x)？除此之外，高数部分还有很多其他内容，微分、积分、泰勒级数这些。再来看看线性代数，最头疼的莫过于矩阵、行列式相关的运算和各种性质了，但 Gran 很好地帮你理解这些问题。比如，矩阵的运算，最正确地理解是把它当成一种特定的空间变换。对于行列式，一直觉得就是一堆数字倒来倒去地计算，其实更好的理解也是放到空间中去。怎么样，有没有觉得你大学上的是假的高数和线代？以上只是这门系列课的冰山一角，还有很多有意思的内容，可以到 B 站上看看，传送门：https://space.bilibili.com/88461692/video系列课很短，两三个小时就能看完，时间短不代表内容少，实际上内容非常丰富，你可能需要经常暂停下来思考一下，慢慢就会颠覆你此前对高数线代的认知。光看视频可能不够，我在 GitHub 上找到了热心网友做过的笔记，感兴趣的话可以 Star 一下：3Blue1Brown 教程笔记另外，如果你对视频中酷炫动画感兴趣，想学学看，作者提供了源码，可以试试看，仓库地址：https://github.com/3b1b/manim动画制作教程：https://www.bilibili.com/read/mobile/17444老规矩，为了更方便你，我下载了高数和线代两门课的视频教程以及文章两本教材的电子书，如需，可以在公众号后台回复：3blue 得到。本文完。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 告诉你所不知道的春晚 1983 -2018]]></title>
    <url>%2F2019%2F01%2F29%2Fweb_scraping_withpython20.html</url>
    <content type="text"><![CDATA[万万没想到。摘要：用 Python 分析过往 36 年春晚节目数据，发现一些趣事。马上就要过年了，距离 2019 己亥猪年的除夕已不足一个礼拜，提起除夕，多数人马上想到「春节联欢晚会」这道丰盛的「年夜大餐」。看过那么多春晚，哪一年、哪些节目、哪些人你还留有深刻印象呢。记忆中，只完整地看过 2005 年到 2015 年十年春晚。05 年之前，还很小，看不懂；15 年之后，长大了，也看不懂了。如今，距离第一届春晚 1983 年，整整过去了 36 年，3 轮的「十二生肖」年。趁今年春晚还没到，来回顾一下过往 36 届春晚的一些有趣数据。分析内容接下来，通过 Python 数据分析，会回答下面这些问题，在知道答案之前，你可以先猜猜看：谁导演春晚次数最多？谁主持春晚次数最多？哪两年的除夕刚好是同一天？谁上春晚次数最多，堪称「钉子户」？港台明星上春晚次数对比歌曲、小品、相声类节目数量对比数据获取网上搜了挺久都没有找到齐全的春晚节目数据，连春晚官网也没有，结果在维基百科上找到了。右侧信息表有导演、主持人、除夕当天日期这几项数据。节目单表是每一年春晚上表演的节目，包括：节目类型、节目名、演员名这几项数据。Python 抓取这类表格数据，方法简单，几行代码就能搞定，修改 URL 的 page 参数，可以循环遍历抓取 1983 到 2018 年所有的节目数据。数据抓取代码如下：12345678910111213141516171819import requestsimport pandas as pdfrom urllib.parse import quotedef get_content(year): keywords = quote('年中国中央电视台春节联欢晚会') url = 'https://zh.wikipedia.org/wiki/&#123;&#125;&#123;&#125;'.format(year,keywords) # 1 节目单； 0 节目信息 if year != 2014: response = pd.read_html(url)[1] else: response = pd.read_html(url)[3] response['year'] = year response.drop([0],inplace=True) #删除首行 response.to_csv('chinese_newyear.csv',mode='a',encoding='utf_8_sig',index=0,header=0)if __name__ == '__main__': for year in range(1983,2018): get_content(year)抓取下来的节目信息：抓取下来的节目数据：抓取下来的数据是脏数据，用 Python 清洗处理一下就可以分析，这些不是重点，所以下面直接进入分析环节，来一探究竟。数据分析▌谁导演春晚次数最多？导演是春晚的总负责人，好比厨师，厨师决定了春晚大餐好不好吃。36 年间，有很多导演负责过春晚，比如近些年的哈文、朗昆，你可能想知道他们是不是导演次数最多的，下面就来看看导演次数最多的十大导演：导演次数最多的是黄一鹤和朗昆导演，两个人都导演了 5 次。黄一鹤导演对于 80 后之后的人来说，不算熟悉，因为他导演春晚的时候是在 80 年代，很多人都没有出生。朗坤则相对熟悉些，最近一次导演是 2009 年。那一年的春晚，是印象最深刻的一届，因为诞生了赵本山最棒的小品《不差钱》（个人之见）。哈文一共导演了 3 次，都在 2010 年之后，其他的导演就不那么熟悉了，相比于主持人、演员，他们是幕后工作者。代码实现如下，关键点在于 For 循环绘制子图：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def get_director(): data = pd.read_csv('chinese_newyear3.csv',encoding='utf_8_sig') # 筛选导演主持人 data = data[data['category'] == '导演'] # data = data[data['category'] == '主持'] data2 = data['content'].str.split('、',expand=True) # 统计出现次数 data2 = data2.apply(pd.value_counts) data2['col_num'] = data2.sum(axis=1) data2.sort_values(by='col_num',ascending=False,inplace=True) data2 = data2['col_num'][:10][::-1] return data,data2def analysis6(data,data2): data = data.set_index('year') data2.sort_values(ascending=False,inplace=True) lst = list(data2.index)[:10] lst_num = list(data2)[:10] colorsall = [color1,color2,color3,color4,color5,color1,color2,color3,color4,color5] for i,name in enumerate(lst): data3 = data['content'].str.contains(name,na=False).astype('int') data3 = pd.DataFrame(data3[data3.values == 1]) data3['year'] = data3.index axs = fig.add_subplot(1, 10, 1+i) data3.plot( ax=axs, x='content', y = 'year', kind = 'scatter', subplots=True, sharey=True, color=colorsall[i], ) new_ticks = np.linspace(1980,2020,41) plt.yticks(new_ticks) plt.tick_params(direction='in') #标签朝里 plt.tick_params(which='major',length=0) # 不显示刻度标签长度 plt.xticks([]) #去掉坐标标签 plt.xlabel('%i' %(lst_num[i]),fontsize=10) plt.xlim(0,2) plt.title(name,color=color5,fontsize=10) plt.tight_layout() fig.subplots_adjust(hspace=0,wspace=0) # 调整子图间距为0 plt.savefig('导演次数最多的主持 TOP 10.png',dpi=200) plt.show()▌谁主持春晚次数最多？导演过后就是主持人了，他们堪称春晚的门面，大家也更熟悉些，不管是早年的赵忠祥、倪萍，中生代的朱军、董卿，还是近年新晋的康辉、李思思以及我们新疆大帅锅尼格买提，只要一提名字，你马上就能对号入座。可你知道主持界的「钉子户」是谁？「常青树」又是哪些人？ 来看看春晚主持次数最多的 TOP 10 名单：一眼望去，十个人每一个都很熟悉。排第一的是 主持了 21 年的朱军，称得上劳模了，从 1997 年连续不间断地住持到 2017 年，远超其他主持人。排第二的是周涛，主持了 14 年，2011 年之后退居幕后， 2016 年又复出了一次。对她的印象，莫过于 2003 年和冯巩合作过的一个小品《马路情歌》：排第三的是董卿，从 05 年开始主持，只在 2018 年缺席了一年，大家都喜欢她，所以她缺席也成为去年春晚的一大讨论话题。好消息是，今年的春晚她会继续主持。董卿春晚上的亮点太多了，印象最深的是和刘谦三度搭档过的魔术节目。刘谦技术够神，董卿配合地也神。另一个好消息是，自 13 年阔别春晚舞台之后，刘谦今年也会再度登台，二人是否还会继续搭档呢，拭目以待。再往后是一对著名搭档，赵忠祥和倪萍老师，可惜印象不深，找了找他们曾主持过的春晚照片，满满的历史感。（1992 年春晚）（1996 年春晚）再之后是李咏，刚刚离开不久，喜欢他的风格，希望天堂也有话筒，也有春晚。说完主持人，下面说一说除夕的日期。▌哪两年的除夕刚好是同一天？小时候一直没有搞懂中国的农历，想不明白为什么每年除夕的日子都不一样，为什么过年不是在元旦。后来才知道这是我们老祖宗的智慧，因为农历比阳历更加准确。每年除夕的日期都在变化，那你是否好奇过某两年的除夕是在同一天这个问题？来看看：以从下往上，从左往右的顺序看上面这张图，会发现几件有意思的事：除夕最早的一年是 2004 年， 1 月 21 日。除夕最晚的一年是 1985 年， 2 月 19 日，最早最晚差了近一个月。2019 年除夕是 2 月 4 日，和 2000 年是一样的。有不少年份的除夕在同一天。比如：1993 和 2012 年都在 1 月 22 日，1998 和 2017 年的 1 月 27 日，1987 和 2006 年的 1 月 28 日。很有意思对吧，但至今最多只有两年的除夕是在同一年，没有出现过「三年除夕都在同一天」这种现象，也许以后会。代码实现如下：123456789101112131415161718192021222324252627282930313233343536# 分析除夕日期def get_date(): fig = plt.figure(figsize=(5,8)) ax = fig.add_subplot(111) data = pd.read_csv('chinese_newyear3.csv',encoding='utf_8_sig') data = data[data['category'] == '播出日期'] data = data['content'].str.extract(r'.*?年(.*?)月(.*?)日.*?') data = data.reset_index(drop=True) data.columns = ['month','day'] data['year'] = np.arange(1983,2019) # int 转 string data = data.applymap(str) data['year2'] = '1900' data['date'] = data['year2'].str.cat([data['month'],data['day']],sep='/') data = data.apply(pd.to_numeric,errors='ignore') data['date'] = pd.to_datetime(data['date']) ax.plot( data['date'], data['year'], ) new_yticks = np.linspace(1980,2020,41) date_format = mpl.dates.DateFormatter("%m-%d") ax.xaxis.set_major_formatter(date_format) plt.yticks(new_yticks) plt.tick_params(direction='in') #标签朝里 content = list(zip(data['date'],data['year'])) for x, y in content: x2 = '%s' %x.strftime('%m/%d') # 只显示月日格式 # print(x,'\n',y) plt.text(x, y+0.2,x2, ha='center', color=color4) plt.title('历年农历除夕日期变化',color=color4,fontsize=14) plt.tight_layout() plt.savefig('除夕日期变化.png',dpi=200,) plt.show()下面，来看看更有意思的。▌谁上春晚次数最多，堪称「钉子户」？很多人梦想着这辈子能上一次春晚，但绝大多数人都只能坐在电视跟前，而有些人则天生是上春晚的，上春晚就像家常便饭，一上就是几十年。你能猜到这 36 年谁上春晚次数最多么？来看看：春晚表演次数最多的是相声演员冯巩大叔，你猜对了么。春晚总共 36 年历史，他就登台了 35 次，十足的「钉子户」。前 5 名除了李谷一老师是歌手以外，其他人都是相声或小品演员，也都是你熟悉的面孔：姜昆、蔡明和黄宏。相声和小品为春晚贡献了很多人才。5 - 10 名歌唱家占多，有宋祖英，还有一位你知道的，没想到还上了这么多次吧。另外还有两名小品演员本山大叔和黄宏。可惜，不见二人表演已多年。10 - 20 名还有很多熟悉的人，比如郭冬临、巩汉林、潘长江等。来具体看一下他们都上了哪些年的春晚：（注：部分演员表演次数和图中点数不一致，是因为某些年 TA 不只表演了一个节目）冯巩老师从 86 年至今，雷打不动地从未缺席过春晚。他上的最多，每年的开场白都是那句「我想死你们了」，对春晚和观众的热爱之情可见一斑。他的作品太多了，相声小品通吃，多的让人记不住。姜昆老师，登台最早，第一年春晚就上台了。 这些年，消失几年后又出现在春晚舞台，也许是为了以免大家太过于想他。最喜欢他 1987 年的《虎口遐想》相声，30 年后的 2017 年又再度带来了《新虎口遐想》。岁月在他的脸上仿佛没有留下痕迹。（1987 年《虎口遐想》）（2017 年《新虎口遐想》）蔡明老师，是女性中上春晚最多的。她和许多人合作过，和谁合作效果都好。（1991 年《陌生人》）黄宏老师一连登台 25 年，小品中扮演过很多角色，可惜 2012 年之后就没有上台了。李谷一老师，春晚演出时间跨度最大，1983 年有她，2018 年还有她。对她的印象莫过于每年春晚尾声的那曲《难忘今宵》。实际上，李谷一老师曾一人撑起了早期的春晚。1983 年的春晚，她一人连唱 7 首歌，前无古人，也后无来者。赵本山老师，对他的评价只有一句：赵本山之后再无春晚。他在就是压轴，每一部作品都看过很多遍，现在也只有靠怀念，众多作品中，最喜欢两部：（2005 年《功夫》）（2009 年《不差钱》）▌港台明星上春晚次数对比不知你发现没有，前面这些人员都来自内地，港台及海外明星并没有出现，但不可忽略的是，他们的演出让春晚更精彩。来看看都有哪些港台明星上过春晚，上了几次：（注：这里选的 10 位明星是第一时间想到的，全凭个人主观印象）周董上过 5 次春晚，你猜对了么？最近一次是去年的《告白气球》，当时朋友圈都刷爆了。除此之外，他其他几次登台，都惊艳无比，04 年的《龙拳》，08 年的《青花瓷》，09 年的《本草纲目》以及 11 年的《兰亭集序》。15 年，偶像换了一波又一波，他却依旧是那个光芒四射的周杰伦。成龙大哥同样上了 5 次，一身正气、中国功夫。刘德华是四大天王中上春晚最多、最早的，2005 年唱了那首《恭喜发财》之后，每年过年在大街小巷你都能听到。王力宏上了 4 次，2012 年龙年，再次唱了那首家喻户晓的《龙的传人》。王菲是港台明星中，上春晚次数最多的女星之一，98 年和那英的《相约九八 》影响力空前。其他几次《传奇》、《因为爱》和《岁月》也空灵好听。▌歌曲、小品、相声等节目数量对比最后，再来看看春晚各类节目构成比例。印象中，最多的节目就是歌曲，其次是小品、相声。可以看到，歌曲类节目基本占了所有节目的一半，小品占了 15%，相声是 9%。其他类型则是一些杂技、戏剧、舞蹈这些，未作统计。代码实现如下：12345678910111213141516171819202122232425262728293031323334353637# 各节目数量def analysis3(data): fig = plt.figure(figsize=(8,5)) num_all = data.shape[0] # 歌曲节目数量 num_song = data[data['category'].str.contains('歌|尾|开场')].shape[0] # 小品数量 num_sketch = data[data['category'].str.contains('小品')].shape[0] # 相声数量 num_crosstalk = data[data['category'].str.contains('相声')].shape[0] # 其他节目数量 other = num_all - sum([num_song,num_sketch,num_crosstalk]) lst = [num_song,num_sketch,num_crosstalk,other] sizes = [num_song,other,num_crosstalk,num_sketch] labels = ['歌曲','其他','相声','小品'] colors_pie = [color1,color4,color3,color2] explode = [0.05,0,0,0] plt.pie( sizes, autopct='%.1f%%', labels=labels, colors=colors_pie, shadow=False, startangle=270, explode=explode, textprops=&#123;'fontsize':14,'color':colors&#125; ) plt.title('1983-2018 共 36 年春晚节目类型数量比较',color=colorstitle,fontsize=fontsize_title) plt.tight_layout() plt.axis('equal') plt.axis('off') plt.legend(loc='upper right') plt.savefig('1983-2018 共 36 年间各类节目类型数量比较.png',dpi=200) plt.show()以上，就是对过去 36 年春晚节目的简单分析。你对春晚的了解有增加么？如需完整代码，可以加入我的知识星球，都是干货。本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你根本不会用百度]]></title>
    <url>%2F2019%2F01%2F25%2Fweekly_sharing14.html</url>
    <content type="text"><![CDATA[正确使用百度的方法。摘要：正确使用百度的方法。想必你的朋友圈这两天应该被《搜索引擎百度已死》这篇文章刷屏了吧，随着文章持续发酵，百度股价也大跌 6%，对于很多深受百度搜索之痛的网友来说，这篇文章和这个结果够解气，没办法谁让百度恨铁不成钢。一直以来，百度都是中国绝对的门面网站。在 Alexa 世界 TOP 500 网站排名中，百度位居世界第四，中国第一 。如此牛逼的网站做出的搜索引擎，和远在大洋那头的谷歌一比，让无数网友都感到汗颜，只能庆幸老外不识中文，不用百度，不然脸更是打地啪啪响。于是，懂技术一点的网友纷纷弃百度投谷歌，不懂技术的就只能继续受着了，有人善意支了个招，说用微软的 Bing 搜索吧，于是 Bing 的访问量这两天陡增。万万没想到，这家国内最后一个西方搜索引擎居然很快就无法访问了，你说百度厉不厉害。百度这么牛逼，那只能以壮士断腕的勇气去直面它了。当然，正刚不过它，这里就给你支几招怎么 正确使用百度搜索。上面那篇文章举了多个例子说百度的首页搜索结果，几乎都被广告、百家号及其他自家网站的内容占据了，很难搜到想要的内容。这里，我们也再来测试一下，首先打开手机端浏览器，输入百度网址进入主页面。呵，这主页够花里胡哨啊，不能给一个干净清爽点的搜索界面么，这可是你百度的脸啊。如果你不喜欢看到这些乱七八糟的内容，想干净一点，方法很简单，比如下面这个样子，只需要替换一下网址就行了。把 www.baidu.com 替换为下面的这一串就行了：https://m.baidu.com/?from=1013843a&amp;pu=sz%401321_480&amp;wpo=btmfast这样再去搜索是不是就爽很多了。接着，随便输入一个关键词，比如「 python学习」看看，嚯，果真清一色的广告啊。看到这些广告你可能就烦死了，有段子说正确使用百度的方式是，搜索结果后直接从第 2 页开始看，因为第一页都是广告或者是百家号的内容。其实，也不用这么麻烦，直接把广告给它干掉不就行了？方法很简单，开启强力广告拦截模式就行了。你可能会说：我的浏览器怎么没有这个功能？为了避免打广告，公众号后台回复：浏览器 就可以得到我用的这款浏览器。这下看着就舒服多了，虽然第一条还是百家号的内容，不过其他内容还算正常。以上是手机端使用百度的正确方式，下面再看看网页端怎么使用，这里仍然你「Python学习」作为关键词。首先，在百度搜索就有个很蛋疼的体验，就是它默认会实时展现你搜索的内容，什么意思呢，就是说你还没输完内容它就跳转到搜索页了，不会等你在主页输完再跳转，就像下面这样。如果你想改变，可以在右上角的设置中，关闭实时预览。搜索出来的结果又是满屏的广告，无力吐槽。怎么干掉这些广告呢？很简单，就是使用大名鼎鼎的油猴浏览器插件了（如果你用的是 Chrome 的话）。把这些广告、百家号内容统统干掉，还原出一个干净的搜索引擎界面。怎么做呢，分两步。第一步，安装油猴（TemperMonkey） 插件。第二步，在 Greasy Fork 上面搜索百度相关的插件，你可以看到有各种各样的插件，去广告的、移除百家号的，全都安装上就行了。再次重复刚才的搜索，你会看到截然不同的搜索界面，是不是爽很多。你可能很兴奋地想居然有这么多开挂的插件，马上安起，不过我觉得既然你都能用这些插件了，其实你一个插件也都不需要了。你会用百度了么？本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今天，公众号达到了一个小里程碑]]></title>
    <url>%2F2019%2F01%2F23%2Flife06.html</url>
    <content type="text"><![CDATA[记录我的写作历程。摘要：回顾过去 5 个月写公众号的点滴。今天，我的公众号迎来了一个小小的里程碑，粉丝数达到了 5000，感谢正在看此文的你，没有你的关注，我现在也达不到这个小目标，最想说的话还是那句：「你终于来了，这可能是一生中最让我感动的事。」5000 这个数对于很多公众号来说，根本不值一提，但对我来说意味着很多，所以今天给第 5000 位粉丝发了 50 元的红包。发红包不是因为我钱太多，事实上我穷得一批。之所以发红包，一是算做对自己一直在努力坚持的一个正向激励，同时也回馈一下粉丝，我发得开心，收到的人也觉得惊喜意外，何乐而不为。事实上，这个惯例从第 1000 位粉丝就开始做了，也希望能够一直做下去，我希望自己的公众号能和别人有一点不同。从去年 8 月底开始再次更新，当时的公众号粉丝数是 200 左右，而这个数字是过去 4 年积累下来的，几乎是冷启动。到现在 5 个月过去了，时间说长不长，说短也不短，这 5 个月堪称我人生的最低谷，不过好在这件事上是坚持下来了，现在越来越理解「触底反弹」是什么意思。一直以来，我把公众号当成是自己的一个私人世界，几乎所有的文章都是原创，哪怕一篇文章要写一个礼拜才能写出来，也没有转发过别人的文章，也从没接过广告，唯一的一点收入来自于一些朋友的赞赏。承认自己还是挺倔强的，虽然涨粉速度比别人慢很多，但也没有太着急。每一篇文章会自恋地看好几遍，感叹写得真好，排版真漂亮。幻想着发出去后，会有很多人看，会有人点赞。文章发出去之后，会反复看有多少阅读量，新增了多少粉丝。而现实则比较残酷，经常是一篇文章只有几十个阅读量，有时一天都没有一个新粉丝。有那么几次，是想放弃的，但一想自己已经在悬崖边上了，再不逼自己一把，就真的掉下去爬不上来了，所以一次次地又坐回电脑跟前继续写。前几个月写的文章只发布在了自己的公众号，没敢向别的公众号投稿，觉得自己还没有什么积累，别人关注了也很可能会取关，一直到在写了十几篇文章后，才慢慢尝试着去投稿。就这样，粉丝数从 200 到 1000，花了 3 个月才达到，很多做公众号的人都挺着急，我想如果他们面对这样的结果，很可能早就放弃了。好在，有了前面的积累，状况开始慢慢变好，从粉丝方面来说，从 1000 到 2000 位粉丝用了 5 周时间，2000 到 3000 用了 3 周，3000 到 5000 则只用了 2 周。除了得到粉丝认可以外，也结识了不少圈内伙伴，增长了不少见识，这一切是都当初在写公众号前没有预想过的。回过头来看，我很庆幸一路坚持了下来，也越来越有自信，现在觉得：不管是小到做公众号，还是大到人生转折，考虑好了就只管去放手一搏，不要抱有太多期待和幻想，这些是阻止你开始的东西。新的一年希望我们都能有更大的进步，另外从今天开始，也欢迎你随手帮我点点广告吧，算是对我的支持和肯定。本文完。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[做好一套 PPT 的核心：注重内容逻辑与视觉化表达]]></title>
    <url>%2F2019%2F01%2F20%2Fweekly_sharing13.html</url>
    <content type="text"><![CDATA[GraphicRiver 上销量排名第 2 的 PPT 作品。摘要：GraphicRiver 上销量排名第 2 的 PPT 作品 。这是「每周分享」的第 13 期， 鉴于最近新关注的很多朋友还不太了解这个栏目，所以我这里再介绍一下。顾名思义，就是会在每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、摄影、Ps 等几个方面。你可以在我公众号界面中的 「不务正业」菜单里集中查看，Python 文章则在另一个「不误正业」菜单中。上两期，分享了一套 GraphicRiver 上销量第 1 的 PPT 作品，大伙儿反应很是强烈，光公众号后台回复「ppt模板」的消息就有好几百条，完全超出我的预料，看来大家都想做出一手有逼格的 PPT 啊。好东西不嫌多，今天我再次拿出压箱底的一套 PPT 作品送给你，这套作品就是 GraphicRiver 上销量排名第 2 的 PPT。虽然是第 2 名，但这套作品从水准和实用性上，我觉得比排名第 1 的还要棒，随便看几张就知道了。这套作品非常专业，使用了专业的配色、丰富的逻辑关系图、形象的图标、好看的字体，整体视觉效果非常棒，如果你能把这些都学以致用，那你的 PPT 绝对能甩周围的同学同事几条街。在送你这套模板之前，我想先和你分享几点经验，能帮助你充分利用它。我相信所有人都想把 PPT 做好，但不幸地是，绝大多数人都不得要领，做出的 PPT 离专业水平差老远，归结原因无非这么几点，要么是见得太少没有灵感，要么收藏地太多却不归类总结，要么是过多地关注于奇技淫巧，总之就是走偏了。如何不走偏呢，很简单，注重内容逻辑与视觉化表达，这就是做出一套好 PPT 的核心。展开一点说就是，把自己的 PPT 文稿，按逻辑关系有条理地组织，配以视觉图形呈现出来就行了。文字之间的逻辑关系是什么？很简单，我们从小就学，天天都在用，比如：「因为 Python 很火，所以我要去学」，这就是因果逻辑关系；「学了 Python，可以做爬虫、开发、机器学习」，这就是并列逻辑关系。例子无穷尽，但 常见的逻辑关系只有 5 种，即：因果关系、递进关系、主次关系、总分关系和并列关系。把这些逻辑关系运用到 PPT 中，你的 PPT就成功了一半。很多人做 PPT 从没思考过处理逻辑关系，猛往幻灯片里面堆文字和数据，殊不知，做 PPT 最忌讳的就是没有逻辑性和大篇幅文字（这里主要指演讲型 PPT），你可能会说，我是做 PPT 又不是写作文，搞清这些逻辑关系有什么用？你如果现在真是这么想，那我要恭喜你，因为你终于找到了做 PPT 不得要领的原因。下面就以这套模板为例，来说说如何采用「内容逻辑与视觉化表达」的方法来做好一套 PPT。先说不内容逻辑与视觉化表达的 PPT 是什么样的，典型的就是这种长篇大段式的 Word 型 PPT，没有做任何内容逻辑和视觉效果处理，不分哪些是需要自己讲的，哪些需要读者看的，全都揉搓在一起丢给读者，效果可想而知。那如何注重内容逻辑和视觉化表达呢，简单，两步走。首先，分析文稿中的内容逻辑，提取和重组关键字，删除无用的字句，这些字句口头表述就可以了，不需要出现在幻灯片中。然后，确定关键字之间的逻辑关系属于上面 5 种逻辑关系中的哪一种，然后选取适合的视觉图形进行呈现就行了。好，下面来看一下这套模板是采用了哪些「内容逻辑和视觉化表达」的制作方法 。▌因果关系图▌递进关系图年终总结中常会用到时间线型的幻灯片，回顾过去一年完成了哪些工作，这种时间线也属于递进关系，可以用下面的这些图。▌主次关系图▌总分关系图▌并列关系图上面这些幻灯片，完美使用了「内容逻辑和视觉化表达」制作思路，很多都可以用在我们的 PPT 中，所以，还没有用到的时候，最好就是把它们分门别类地纳入自己的灵感素材库。等需要的时候，就能很快找到，然后套用就行了。总之，多看、多积累、多练习就能做好 PPT 了。上面，只是这套作品内容的一小部分，还有不少有用的东西可以学习，下面说一下。▌专业配色很多时候，你的 PPT 看着很 Low，很可能是配色大众，比如下面这张图表，所有元素都一样，一个使用默认配色，一个使用专业配色，这差距就是一个天上一个地下。除了上面的商务蓝配色，模板还提供了多种专业的配色：▌图表很多人在 PPT 中插入图表时，直接就把 Excel 中的图表复制过来了，丑到爆，来看看这些养眼的图表。▌地图好，就罗列这么多，以上仅是 整套模板 300 张幻灯片中的一小部分，可以说每一张都是精品。知道你等了很久，下面我打算把这套作品分享给你，不过这次想改一下规则，不再采取后台回复信息直接得到这种方式了，为什么呢？因为遇到太多的伸手党，领取后什么表示都没有，有的甚至拿完就取关，说实话，我不太喜欢这样的互动方式。所以，今天改一下，方式也很简单，你如果需要这套 PPT ，可以选取任何一种方式：留言、打赏或者点个好看（点好看需要留言告诉我一下）来和我互动，我看到了之后就会发给你。如果你不想留言、打赏或者点个好看，那看这篇文章就可以，写这篇文章我也是花了不少心思。本文完。推荐阅读：国外最牛逼的一套 PPT 作品分享给你]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由 Python 开发的 7 款世界知名 App]]></title>
    <url>%2F2019%2F01%2F17%2Ftranslating02.html</url>
    <content type="text"><![CDATA[Python 原来这么牛逼。摘要：由 Python 开发的 7 款世界知名 App 。如今 Python 越来越火，大有登顶编程语言榜首的趋势，很多人开始知道或者学习 Python，今天就介绍几款由 Python 开发的世界大牌 App，让你再次认识 它。下面一睹为快，看看有没有你不知道的。▌Instagram这款 App 想必你应该很熟悉，不少人也玩过，前几年还可以随便上，现在不行了。简单介绍一下它，自 2010 年 10 月诞生之日起，就一直稳坐「在线图片及视频分享社交应用软件」世界老大位置。它有多牛逼呢，用两组数据说明一下就知道了。其一，根据 Alexa 世界 500 强网站流量排名，Instragram 位居 美国第 11 位，世界第 17 位，排它后面的是「微博」。其二，日活跃用户达到 3 亿，月活用户达到 10 亿。这个数字你可能没概念，拿国内最近风光无限的「抖音」来对比一下就知道了，根据抖音 1 月公布的官方数据，抖音日活用户 达到 2.5 亿，月活用户有 5 亿。还是比不过 Instagram，不过抖音后劲非常猛，海外版 Tik Tok 已登陆全球 150 个国家，微信都没走出国门，它走出去了。话说回来， Instagram 能这么牛逼，主要是因为它有一个更牛逼的爹：脸书「 Facebook 」。▌RedditReddit 是美国最大的娱乐、社交及新闻网站，由两个维吉尼亚大学的学生在 2005 年创建，最初采用 Common Lisp 语言编写，后面改用 Python 。相比 Instagram，它在国内知名度要低一些，而实际上它比 Ins 要牛逼，是 美国排名第 5 的网站，排它前面的只有：Google、Youtube、Amazon 和 Facebook 这四大巨头。▌UberUber 你应该熟悉，前两年和滴滴打得不可开交，目前拥有 1 亿用户，它使用便捷的 Python 来处理大数据。▌Dropbox如果你经常使用网盘，那应该会比较熟悉 Dropbox ，它也是用 Python 开发的一款顶级 App。提到网盘，就不得不说国内的百度网盘，简单对比一下，在存储空间大小上，百度网盘还是很良心的，免费提提供 2T 存储空间，而 Dropbox 仅提供 2G 免费空间，差了 1000 倍。Dropbox 虽然在容量上输给了百度网盘，但在安全性、协同合作等方面，能甩百度网盘好几条街。▌Pinterest如果你是一个设计师，那么 Pinterest 你一定不陌生，该网站和 App 也是由 Django 搭建的。作为一个图片分享网站，它最大的特点是可以方便地采集和收藏喜欢的图片。比如你喜欢 PPT ，就可以在上面采集 PPT 作品作为灵感储备，类似中国版的「花瓣网」。▌SpotifySpotify 是一个起源于瑞典的音乐流服务公司，也是 当前全球最大的流音乐服务商，国内众多音乐 App 中，能接近它的也只有网易云音乐。▌DisqusDisqus 是一家提供网站留言的公司，超过 75 万个网站使用了它的留言系统功能，它也使用了 Django 的部分功能。最好，用一句话总结一下：这些 App 太酷了，但一个都上不了。你问我有没有办法？有的：∞ 教你科学上网本文完。参考：∞ Top Seven Apps Built With Python∞ 分析了 7 万款 App，全是没想到∞ 分析了 6000 款 App，竟然有这么多佳软没用过]]></content>
      <categories>
        <category>Python分享</category>
      </categories>
      <tags>
        <tag>Python入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[入门 Python 最好的视频]]></title>
    <url>%2F2019%2F01%2F16%2FPython_learning03.html</url>
    <content type="text"><![CDATA[入门 Python 最好的视频。摘要：Coursera 上最火的 Python 课程送给你，带中英字幕。看了标题你可能会质疑，确定是最好的？一图以蔽之，连 Python 之父 「龟叔」都在推荐这门课，你说呢？这门课就是 Coursera 上的最火的 Python 入门系列课程： Python for Everybody Specialization ，堪比机器学习领域中，吴恩达的机器学习课程，真正的零基础入门，全是好评。该系列课一共包含五门课，包括：《Python 入门》《Python 数据结构》《使用 Python 访问网络数据》《Python 与数据库》《Python 处理与可视化》可以说基本是涵盖了 Python 入门的方方面面了。课程是由一位有趣、好玩的密歇根大学教授 CharlesSeverance 主讲的，也叫 Dr. Charles 。为什么要把「有趣、好玩」放在最前面呢，因为这正是这门课最大的特点，也可以说是优势。Dr. Charles 是一位很面善的大叔，随和不故作高深，完全懂我们小白的心思。都说兴趣是最好的老师，但是有这样一位大师来培养你的兴趣，实在不能更好。在这门课你可以学到什么？▌真正地喜欢上 Python很多人在介绍 Python 这门语言的时候，基本都是很枯燥地乏味地这样说：「嗯，这门语言是 20 多年前由一个荷兰人发明的…」而 Dr. Charles 就不一样了，他把 Python 和哈利波特结合起来了，他编了个故事，说想去格兰芬多，结果被安排在了斯莱特林，他很郁闷地问别人为什么，学生告诉他，因为他教 Python （注：Python 另一层含义是蟒蛇）。说完这个笑话，他才开始介绍 Python 历史。是不是很有意思，他从你最熟悉的东西开始，很自然地引导你进入一个新领域。▌计算机系统基础知识在学习编程语言之前，他会先带你了解计算机系统的基本组成和运作原理。是不是很形象生动？▌Python 基础Pyhton 相关的基础知识你都可以学到或者了解到，比如数据结构、循环、字符串、正则表达式等等，这些对于入门来说足够了。我就是在看了他讲解的循环语法，才搞懂 if、while、for 循环这些东西。其他内容就不过多介绍了，感兴趣的话，可以直接看课程目录：▌福利我当时为了学习这门系列课，特地跑到油管下载了全部课程，由于课程是英文字幕，看起来还是略有吃力，没办法英语差，所以费了九牛二虎之力，又找到了中文字幕，前面的视频截图你应该已经看到了。如果你想入门 Pyhton 的话，这么课程实在是再适合不过了。现在呢，我就把这门课程的视频和配套电子书都分享给你，公众号后台回复：Python视频，就可以得到。本文完。]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>Python入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[样机，让你的 PPT 更有逼格]]></title>
    <url>%2F2019%2F01%2F13%2Fweekly_sharing12.html</url>
    <content type="text"><![CDATA[用 PS 和 PPT 轻松制作图片样机。摘要：用 PS 和 PPT 轻松制作图片样机。这是「每周分享」的第 12 期， 也是今年的第 2 期。鉴于最近新加了很多朋友，所以我这里再介绍一下这个栏目，顾名思义，就是会在每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、摄影、Ps 等几个方面。你可以在我公众号界面中的 「不务正业」菜单里集中查看，Python 文章则在另一个「不误正业」菜单中。上一期，给大家分享了一套国外销量最牛逼的 PPT 作品，大伙儿反应很是强烈，光公众号后台回复「ppt模板」的消息就有好几百条，完全超出我的预料，看来大家都想做出一手有逼格的 PPT 啊。话说回来，一套有逼格的 PPT 往往需要精雕细琢每一张幻灯片，然后再组合起来，形成统一的风格。但其实还有一点非常重要，也可以说是锦上添花的技巧，能够 让你 PPT 的逼格再上一层楼，那就是制作一张图片样机。你不知道样机是什么？很可能你都见过。在我们上一期介绍的 GracphicRiver 网站上，很多 PPT 作品都利用了样机进行展示，比如：样机能够直观地展示出你 PPT 的风格，这种 立体的展现形式比平面图片有更强的冲击力，很容易吸引别人注意力，简单做个对比就知道了。这是我上一篇文章的样机封面：倘若不使用样机，而只是像下面这样，将 PPT 简单拼接起来，逼格是不是明显就矮了一截？其实，样机远不止上面一种样式，比如可以像下面这样：还可以像下面这样：怎么样，样机的效果不错吧，其实，样机除了酷炫以外还有实际的用途，比如你年终总结汇报的时候，在其他同事面前秀出来，绝对亮瞎老板和同事的眼；再比如有甲方找你做 PPT，做个这样的 DEMO 页发给过去，这单生意很可能就成了。到这儿，你是不是也想自己弄个样机出来？哈哈，其实制作方法比较简单，可以用 PS 也可以用 PPT 做，下面我就分别介绍一下。在 PS 中做，说难也难，说简单也简单，最简单的方法就是直接在网上找一套不错的样机模板，然后替换成你 PPT 中的图片，几分钟就能搞定，我上面做的那张样机就是用 PS 做的。比如我这里找了一套不错的样机素材，只需用 PS 打开素材文件，然后用图片填充每个智能对象，填充完成导出为图片就可以了，很 easy 对吧。可能你会说，劳资电脑没有 PS 还搞个锤子？没有问题，下面就介绍另外一种方法，就是用 PPT 制作，也比较简单，只需要三步。首先，复制并排列好多个矩形。你可以一个个去复制，但我推荐使用一款叫 iSlide 的 PPT 插件，它的「矩阵布局」功能够帮助你秒绘矩阵，当别人还在吭哧吭哧地复制和对齐时，你样机都做完了。需要注意的是，矩形尺寸比例最好为 16:9，因为现在最常用的的幻灯片尺寸是这个比例，接着，设置三维旋转和立体阴影。矩阵复制好以后，选中全部然后按 Ctrl + G 快捷键合并所有矩形，以便后续统一设置三维旋转和阴影格式，这里的旋转和阴影参数，可以随意设置，不同的参数会产生不同的样机形式，所以你可以随意发挥。最后，填充图片，生成样机。准备好 PPT 图片，然后复制图片，接着点击矩形进行填充，选择剪贴板就行了，就像下面这样。最后，完成制作，导出图片即可。怎么样，样机制作是不是挺简单？哈哈你可能并不想知道是怎么做的，只想知道怎么才能得到，我很懂你对吧。老规矩，上面的 PS 和 PPT 素材为你准备好了，在我的公众号后台回复：ppt样机，就可以得到。不过，据说有打赏、留言和点好看的才能够得到全部素材。本文完。推荐阅读：国外最牛逼的一套 PPT 作品分享给你]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5 行代码入门 Python 爬虫]]></title>
    <url>%2F2019%2F01%2F10%2Fweb_scraping_withpython18.html</url>
    <content type="text"><![CDATA[5 行代码就能写一个 Python 爬虫。摘要：5 行代码就能写一个 Python 爬虫。如果你是比较早关注我的话，会发现我此前的大部分文章都是在写 Python 爬虫，前后大概写了十几个爬虫实战案例，一直在埋头往前写，但却没有回到原点过，没有写过为什么要爬虫、爬虫难不难、怎么入门爬虫这些问题。另外，我觉得关注我的朋友中有不少是刚刚入门 Python 或者想学习 Python 的，为了更加友好一些，所以也有必要说一说这几个问题。基于这两点思考，今天就来谈谈 如何用快速入门爬虫。先说结论：入门爬虫很容易，几行代码就可以，可以说是学习 Python 最简单的途径。以我纯小白、零基础的背景来说，入门爬虫其实很容易，容易在代码编写很简单，简单的爬虫通常几行就能搞定，而不容易在确定爬虫的目标，也就是说为什么要去写爬虫，有没有必要用到爬虫，是不是手动操作几乎无法完成，互联网上有数以百万千万计的网站，到底以哪一个网站作为入门首选，这些问题才是难点。所以在动手写爬虫前，最好花一些时间想一想这清楚这些问题。「Talk is cheap. Show me the code」，下面，就以我写过的一个爬虫为例，说一说如入门 Python 的几个步骤。▌确立目标第一步，确立目标。这里，以我之前写的「爬取国内所有上市公司信息」为例，文章见：∞ 10 行代码爬取全国所有A股/港股/新三板上市公司信息为什么当时想起写这个爬虫呢，是因为这是曾经在工作中想要解决的问题，当时不会爬虫，只能用 Excel 花了数个小时才勉强地把数据爬了下来， 所以在接触到爬虫后，第一个想法就是去实现曾未实现的目标。以这样的方式入门爬虫，好处显而易见，就是有了很明确的动力。很多人学爬虫都是去爬网上教程中的那些网站，网站一样就算了，爬取的方法也一模一样，等于抄一遍，不是说这样无益，但是会容易导致动力不足，因为你没有带着目标去爬，只是为了学爬虫而爬，爬虫虽然是门技术活，但是如果能 建立在兴趣爱好或者工作任务的前提下，学习的动力就会强很多。在确定好爬虫目标后，接着我就在脑中预想了想要得到什么样的结果、如何展示出来、以什么形式展现这些问题。所以，我在爬取网站之前，就预先构想出了想要的一个结果，大致是下面这张图的样子。目标是利用爬下来的数据，尝试从不同维度年份、省份、城市去分析全国的股市信息，然后通过可视化图表呈现出来。抛开数据，可能你会觉得这张图在排版布局、色彩搭配、字体文字等方面还挺好看的。这些呢，就跟爬虫没什么关系了，而跟审美有关，提升审美的一种方式是可以通过做 PPT 来实现：∞ 国外最牛逼的一套 PPT 作品分享给你所以你看，咱们说着说着就从爬虫跳到了 PPT，不得不说我此前发的文章铺垫地很好啊，哈哈。其实，在职场中，你拥有的技能越多越好。▌直接开始确定了目标后，第二步就可以开始写爬虫了，如果你像我一样，之前没有任何编程基础，那我下面说的思路，可能会有用。刚开始动手写爬虫，我只关注最核心的部分，也就是先成功抓到数据，其他的诸如：下载速度、存储方式、代码条理性等先不管，这样的代码简短易懂、容易上手，能够增强信心。所以，我在写第一遍的时候，只用了 5 行代码，就成功抓取了全部所需的信息，当时的感觉就是很爽，觉得爬虫不过如此啊，自信心爆棚。12345import pandas as pdimport csvfor i in range(1,178): # 爬取全部页 tb = pd.read_html('http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s' % (str(i)))[3] tb.to_csv(r'1.csv', mode='a', encoding='utf_8_sig', header=1, index=0)3000+ 上市公司的信息，安安静静地躺在 Excel 中：▌不断完善有了上面的信心后，我开始继续完善代码，因为 5 行代码太单薄，功能也太简单，大致从以下几个方面进行了完善：增加异常处理由于爬取上百页的网页，中途很可能由于各种问题导致爬取失败，所以增加了 try except 、if 等语句，来处理可能出现的异常，让代码更健壮。增加代码灵活性初版代码由于固定了 URL 参数，所以只能爬取固定的内容，但是人的想法是多变的，一会儿想爬这个一会儿可能又需要那个，所以可以通过修改 URL 请求参数，来增加代码灵活性，从而爬取更灵活的数据。修改存储方式初版代码我选择了存储到 Excel 这种最为熟悉简单的方式，人是一种惰性动物，很难离开自己的舒适区。但是为了学习新知识，所以我选择将数据存储到 MySQL 中，以便练习 MySQL 的使用。加快爬取速度初版代码使用了最简单的单进程爬取方式，爬取速度比较慢，考虑到网页数量比较大，所以修改为了多进程的爬取方式。经过以上这几点的完善，代码量从原先的 5 行增加到了下面的几十行：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import requestsimport pandas as pdfrom bs4 import BeautifulSoupfrom lxml import etreeimport timeimport pymysqlfrom sqlalchemy import create_enginefrom urllib.parse import urlencode # 编码 URL 字符串start_time = time.time() #计算程序运行时间def get_one_page(i): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; paras = &#123; 'reportTime': '2017-12-31', #可以改报告日期，比如2018-6-30获得的就是该季度的信息 'pageNum': i #页码 &#125; url = 'http://s.askci.com/stock/a/?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None except RequestException: print('爬取失败')def parse_one_page(html): soup = BeautifulSoup(html,'lxml') content = soup.select('#myTable04')[0] #[0]将返回的list改为bs4类型 tbl = pd.read_html(content.prettify(),header = 0)[0] # prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame tbl.rename(columns = &#123;'序号':'serial_number', '股票代码':'stock_code', '股票简称':'stock_abbre', '公司名称':'company_name', '省份':'province', '城市':'city', '主营业务收入(201712)':'main_bussiness_income', '净利润(201712)':'net_profit', '员工人数':'employees', '上市日期':'listing_date', '招股书':'zhaogushu', '公司财报':'financial_report', '行业分类':'industry_classification', '产品类型':'industry_type', '主营业务':'main_business'&#125;,inplace = True) return tbldef generate_mysql(): conn = pymysql.connect( host='localhost', user='root', password='******', port=3306, charset = 'utf8', db = 'wade') cursor = conn.cursor() sql = 'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))' cursor.execute(sql) conn.close() def write_to_sql(tbl, db = 'wade'): engine = create_engine('mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'.format(db)) try: tbl.to_sql('listed_company2',con = engine,if_exists='append',index=False) # append表示在原有表基础上增加，但该表要有表头 except Exception as e: print(e)def main(page): generate_mysql() for i in range(1,page): html = get_one_page(i) tbl = parse_one_page(html) write_to_sql(tbl) # # 单进程if __name__ == '__main__': main(178) endtime = time.time()-start_time print('程序运行了%.2f秒' %endtime) # 多进程from multiprocessing import Poolif __name__ == '__main__': pool = Pool(4) pool.map(main, [i for i in range(1,178)]) #共有178页 endtime = time.time()-start_time print('程序运行了%.2f秒' %(time.time()-start_time))但是这个过程却觉得很自然，因为每次修改都是针对一个小点，一点点去学，搞懂后添加进来，而如果让我上来就直接写出这几十行的代码，我很可能就放弃了。所以，你可以看到，入门爬虫是有套路的，最重要的是给自己信心。以上，我从一个小点结合一个实例，介绍了入门学习爬虫的方法，希望对你有用。当然还有其他点，之后再说。本文完。推荐阅读：∞ 10 行代码爬取全国所有 A股/港股/新三板上市公司信息∞ 国外最牛逼的一套 PPT 作品分享给你]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[国外最牛逼的一套 PPT 作品分享给你]]></title>
    <url>%2F2019%2F01%2F05%2Fweekly_sharing11.html</url>
    <content type="text"><![CDATA[GraphicRiver 上销量排名第一的 PPT 作品送给你 。摘要：GraphicRiver 上销量排名第一的 PPT 作品送给你。这是「每周分享」的第 11 期， 也是今年的第 1 期，新年来点不一样的，聊聊此前都没有说过的主题：「PPT」。我呢，由于个人兴趣，常混迹于编程圈和 PPT 圈，一段时间观察下来，发现了两点有意思的现象：PPT 做得好的人不多PPT 几乎人人都会，也是职场中一项非常重要的技能，但是 PPT 做得拿得出手的人却不多。通过一个简单的现象就可以看出来，早在去年我就尝鲜了 PPT 2019 版，但不少人仍然还在用着 2010 版以下的版本，虽然使用 PPT 版本的高低和 PPT 做得好不好没有必然关系，但是使用新的版本至少能够证明一点：你愿意去尝试，有尝试才有可能。新版本的 PPT 提供了非常多实用的工具，例如 2016版才有的「变体」功能 。（本文说的都是 PPT，不涉及 Keynote）会写代码 PPT 做得又好的人更少我看了很多编程方面的文字和视频教程，发现很多 PPT 课件都做得很辣眼睛，不少程序员对 PPT 并不以为然，可能是时间精力都用去写代码了，也可能是觉得 PPT 太 easy。不管怎么说，我认为：程序员如果能掌握 PPT 技能，会是一种不错的竞争力。如果你是学生，答辩的时候 show 出惊艳的 PPT ，说不定就能拿个优秀论文；如果你是职场小白，年终总结的时候亮出漂亮的 PPT ，来年升职加薪不是梦。当然，我这里所说的「做出好 PPT」，不是说一定要达到专业 PPT 设计师的那种，只要能够比你周围大多数人做得好那么一点就行。那么问题来了，如果目前自认是 PPT 小白，很想提升 PPT 技能怎么办?根据这几年我入门 PPT 以来的经验，觉得对于刚刚入门 PPT 的人来说，最重要的是先学会：欣赏。至少能够分辨出哪些是好 PPT ，哪些不是，先提升审美再动手去做，多看才会知道自己做的 PPT 不好在哪里，然后针对性地去改进。那么问题又来了，到底哪些算是好 PPT 哪些又不是呢? 我觉得有两点可以参考：多看看国外的 PPT 作品不是说国外的就一定比国内的好，但国外网站上遇到好 PPT 的概率要比国内的高，能节省你筛选的时间。那些免费的、动不动送几十 G 资源包的基本是垃圾这种现象很常见，不只是 PPT 资料，很多公众号都爱搞：关注公众号免费送一百 G 精华电子书和视频，这样的福利，先不说质量到底怎么样，这么多东西，你收藏了之后也基本不会再去看的，所以没有意义。真正好的 PPT 必然是花费了很多心血的，又免费又大量，是不可能的。你可能会觉得：「这些我早都知道了，还用得着你说?」咳咳，上面说的那些都是为下面的干货做铺垫。我刚才说多上国外的 PPT 设计网站看看，这其中就有一个佼佼者叫 「GraphicRiver」，该网站汇集了世界上最优秀设计师的 PPT 作品，水准非常高，基本都是收费的，一套十几到几十刀不等。随意看看上面的一些作品：是不是开始怀疑人生，都是用 PPT 做，差距咋那么大呢。在网站近万套作品中，销量最牛逼的是一套叫「MOTAGUA」的作品。这套幻灯片销量如此之高，必然是有道理的，我们来看看从这套模板中可以学习到哪些东西：▌幻灯片板式幻灯片版式是 PPT 的框架，做 PPT 最重要的就是搭框架，也就是确定每张幻灯片的版式，在这套 PPT 里可以学习到很多版式设计的技巧，比如上下、左右、居中、文字、图片等版式的搭配。▌逻辑关系质量上乘的 PPT ，背后是有很强大的内在逻辑关系支撑的，比如总分、并列、对比、递进等关系。确定了版式后，就可以对幻灯片元素进行逻辑设计，一种不错的设计方式就是使用一些视觉关系管来呈现，模板里面提供了大量的逻辑关系图可以用来参考。▌配色方案配色是 PPT设计中非常重要的一环。很多 PPT 看起来很辣眼睛，往往是配色没搞好，毕竟我们大多数人都没有系统学过色彩理论基础，譬如：同类色、互补色、主色、辅色等等，但没学过不代表搭配不出一套专业的配色，有一种非常简单的方式就是 提取专业 PPT 中的配色。这套作品中提供了 多达 60 种配色方案，什么商务风、简约风、欧美风都可以轻松驾驭，PPT 中点点鼠标就能秒变各种配色方案，非常简单。▌图标、地图素材有时候我们的 PPT 看起来平淡无奇的另一个重要原因就是 缺少可视化元素，全是文字堆砌在那儿。解决的方法也很简单，就是尝试把文字替换为图标、地图等可视化素材，让 PPT 看起来更为丰富多彩。作品中也提供了大量的矢量图标和世界上主要国家的地图，支持任意编辑和放大，可以拿来用在我们自己的 PPT 作品中。总结一下，拿到一套 PPT 作品，正确的使用思路是学会分解，可以选择从上面那几个方面去尝试，多关注自己薄弱的地方，比如版式、逻辑、配色、可视化元素这些地方，然后针对性地模仿学习，这样才能充分利用到 PPT 作品的价值，从而快速提升，千万不要想着一口气就可以把 PPT 做得很牛逼。很多人不会正确使用 PPT 模板，老想着找一套可以完全进行套用的 PPT 模板，这几乎是不现实的，除非私人定制，因为每个人在做 PPT 时的想法是不一样的。有些人找不到一套可以全套用的，就花很多时间找了几套不错的进行拼凑，结果做出来四不像，不忍直视，最后不得不放弃，自己重新来做，陷入死循环。总之，完全套用别人的东西，对于自身 PPT 水平的提高是没有一点益处的，久而久之会发现自己还是原地踏步。知道你等了很久，最后，我把这套珍藏了很久的 PPT 作品送给你，在我的公众号后台回复：ppt模板，就可以得到。如果想得到全部完整版，可以长按下方二维码加入我的知识星球得到，专享全年所有干货，越早加入越划算。本文完。推荐阅读：你的手机截图真丑盘点那些手机上绝对值得安装的 App安卓最强阅读器]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以豌豆荚为例，用 Scrapy 爬取分类多级页面]]></title>
    <url>%2F2019%2F01%2F02%2Fweb_scraping_withpython17.html</url>
    <content type="text"><![CDATA[使用 Scrapy 爬取豌豆荚全网 70000+ App。摘要：使用 Scrapy 爬取豌豆荚全网 70000+ App，并进行探索性分析。写在前面：若对数据抓取部分不感兴趣，可以直接下拉到数据分析部分。1 分析背景之前我们使用了 Scrapy 爬取并分析了酷安网 6000+ App，为什么这篇文章又在讲抓 App 呢?因为我喜欢折腾 App，哈哈。当然，主要是因为下面这几点：第一、之前抓取的网页很简单在抓取酷安网时，我们使用 for 循环，遍历了几百页就完成了所有内容的抓取，非常简单，但现实往往不会这么 easy，有时我们要抓的内容会比较庞大，比如抓取整个网站的数据，为了增强爬虫技能，所以本文选择了「豌豆荚」这个网站。目标是： 爬取该网站所有分类下的 App 信息并下载 App 图标，数量在 70,000 左右，比酷安升了一个数量级。第二、再次练习使用强大的 Scrapy 框架之前只是初步地使用了 Scrapy 进行抓取，还没有充分领会到 Scrapy 有多么牛逼，所以本文尝试深入使用 Scrapy，增加随机 UserAgent、代理 IP 和图片下载等设置。第三、对比一下酷安和豌豆荚两个网站相信很多人都在使用豌豆荚下载 App，我则使用酷安较多，所以也想比较一下这两个网站的 App 特点。话不多说，下面开始抓取流程。▌分析目标首先，我们先来了解一下要抓取的豌豆荚网页是什么样的，可以看到该网站上的 App 分成了很多类，包括：「应用播放」、「系统工具」等，一共有 14 个大类别，每个大类下又细分了多个小类，例如，影音播放下包括：「视频」、「直播」等。点击「视频」进入第二级子类页面，可以看到每款 App 的部分信息，包括：图标、名称、安装数量、体积、评论等。在之前的一篇文章中（见下方链接），我们分析了这个页面：采用 AJAX 加载，GET 请求，参数很容易构造，但是具体页数不确定，最后分别使用了 For 和 While 循环抓取了所有页数的数据。∞ Python For 和 While 循环爬取不确定页数的网页接着，我们可以再进入第三级页面，也就是每款 App 的详情页，可以看到多了下载数、好评率、评论数这几样参数，抓取思路和第二级页面大同小异，同时为了减小网站压力，所以 App 详情页就不抓取了。所以，这是一个分类多级页面的抓取问题，依次抓取每一个大类下的全部子类数据。学会了这种抓取思路，很多网站我们都可以去抓，比如很多人爱爬的「豆瓣电影」也是这样的结构。▌分析内容数据抓取完成后，本文主要是对分类型数据的进行简单的探索性分析，包括这么几个方面：下载量最多 / 最少的 App 总排名下载量最多 / 最少的 App 分类 / 子分类排名App 下载量区间分布App 名称重名的有多少和酷安 App 进行对比▌分析工具PythonScrapyMongoDBPyechartsMatplotlib2 数据抓取▌网站分析我们刚才已经初步对网站进行了分析，大致思路可以分为两步，首先是提取所有子类的 URL 链接，然后分别抓取每个 URL 下的 App 信息就行了。可以看到，子类的 URL 是由两个数字构成，前面的数字表示分类编号，后面的数字表示子分类编号，得到了这两个编号，就可以抓取该分类下的所有 App 信息，那么怎么获取这两个数值代码呢?回到分类页面，定位查看信息，可以看到分类信息都包裹在每个 li 节点中，子分类 URL 则又在子节点 a 的 href 属性中，大分类一共有 14 个，子分类一共有 88 个。到这儿，思路就很清晰了，我们可以用 CSS 提取出全部子分类的 URL，然后分别抓取所需信息即可。另外还需注意一点，该网站的 首页信息是静态加载的，从第 2 页开始是采用了 Ajax 动态加载，URL 不同，需要分别进行解析提取。▌Scrapy抓取我们要爬取两部分内容，一是 APP 的数据信息，包括前面所说的：名称、安装数量、体积、评论等，二是下载每款 App 的图标，分文件夹进行存放。由于该网站有一定的反爬措施，所以我们需要添加随机 UA 和代理 IP，关于这两个知识点，我此前单独写了两篇文章进行铺垫，传送门：∞ Scrapy 中设置随机 User-Agent 的方法汇总∞ Python 爬虫的代理 IP 设置方法汇总这里随机 UA 使用 scrapy-fake-useragent 库，一行代码就能搞定，代理 IP 直接上阿布云付费代理，几块钱搞定简单省事。下面，就直接上代码了：items.py12345678910import scrapyclass WandoujiaItem(scrapy.Item): cate_name = scrapy.Field() #分类名 child_cate_name = scrapy.Field() #分类编号 app_name = scrapy.Field() # 子分类名 install = scrapy.Field() # 子分类编号 volume = scrapy.Field() # 体积 comment = scrapy.Field() # 评论 icon_url = scrapy.Field() # 图标urlmiddles.py中间件主要用于设置代理 IP。1234567891011import base64proxyServer = "http://http-dyn.abuyun.com:9020"proxyUser = "你的信息"proxyPass = "你的信息"proxyAuth = "Basic " + base64.urlsafe_b64encode(bytes((proxyUser + ":" + proxyPass), "ascii")).decode("utf8")class AbuyunProxyMiddleware(object): def process_request(self, request, spider): request.meta["proxy"] = proxyServer request.headers["Proxy-Authorization"] = proxyAuth logging.debug('Using Proxy:%s'%proxyServer)pipelines.py该文件用于存储数据到 MongoDB 和下载图标到分类文件夹中。存储到 MongoDB：12345678910111213141516171819202122232425MongoDB 存储class MongoPipeline(object): def __init__(self,mongo_url,mongo_db): self.mongo_url = mongo_url self.mongo_db = mongo_db @classmethod def from_crawler(cls,crawler): return cls( mongo_url = crawler.settings.get('MONGO_URL'), mongo_db = crawler.settings.get('MONGO_DB') ) def open_spider(self,spider): self.client = pymongo.MongoClient(self.mongo_url) self.db = self.client[self.mongo_db] def process_item(self,item,spider): name = item.__class__.__name__ # self.db[name].insert(dict(item)) self.db[name].update_one(item, &#123;'$set': item&#125;, upsert=True) return item def close_spider(self,spider): self.client.close()按文件夹下载图标：1234567891011121314151617181920# 分文件夹下载class ImagedownloadPipeline(ImagesPipeline): def get_media_requests(self,item,info): if item['icon_url']: yield scrapy.Request(item['icon_url'],meta=&#123;'item':item&#125;) def file_path(self, request, response=None, info=None): name = request.meta['item']['app_name'] cate_name = request.meta['item']['cate_name'] child_cate_name = request.meta['item']['child_cate_name'] path1 = r'/wandoujia/%s/%s' %(cate_name,child_cate_name) path = r'&#123;&#125;\&#123;&#125;.&#123;&#125;'.format(path1, name, 'jpg') return path def item_completed(self,results,item,info): image_path = [x['path'] for ok,x in results if ok] if not image_path: raise DropItem('Item contains no images') return itemsettings.py12345678910111213141516171819202122232425BOT_NAME = 'wandoujia'SPIDER_MODULES = ['wandoujia.spiders']NEWSPIDER_MODULE = 'wandoujia.spiders'MONGO_URL = 'localhost'MONGO_DB = 'wandoujia'# 是否遵循机器人规则ROBOTSTXT_OBEY = False# 下载设置延迟 由于买的阿布云一秒只能请求5次，所以每个请求设置了 0.2s延迟DOWNLOAD_DELAY = 0.2DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 100, # 随机UA 'wandoujia.middlewares.AbuyunProxyMiddleware': 200 # 阿布云代理 ） ITEM_PIPELINES = &#123; 'wandoujia.pipelines.MongoPipeline': 300, 'wandoujia.pipelines.ImagedownloadPipeline': 400,&#125; # URL不去重DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'wandou.py主程序这里列出关键的部分：1234567891011121314def __init__(self): self.cate_url = 'https://www.wandoujia.com/category/app' # 子分类首页url self.url = 'https://www.wandoujia.com/category/' # 子分类 ajax请求页url self.ajax_url = 'https://www.wandoujia.com/wdjweb/api/category/more?' # 实例化分类标签 self.wandou_category = Get_category()def start_requests(self): yield scrapy.Request(self.cate_url,callback=self.get_category) def get_category(self,response): cate_content = self.wandou_category.parse_category(response) # ...这里，首先定义几个 URL，包括：分类页面、子分类首页、子分类 AJAX 页，也就是第 2 页开始的 URL，然后又定义了一个类 Get_category() 专门用于提取全部的子分类 URL，稍后我们将展开该类的代码。程序从 start_requests 开始运行，解析首页获得响应，调用 get_category() 方法，然后使用 Get_category() 类中的 parse_category() 方法提取出所有 URL，具体代码如下：1234567891011121314151617181920212223242526272829303132class Get_category(): def parse_category(self, response): category = response.css('.parent-cate') data = [&#123; 'cate_name': item.css('.cate-link::text').extract_first(), 'cate_code': self.get_category_code(item), 'child_cate_codes': self.get_child_category(item), &#125; for item in category] return data # 获取所有主分类标签数值代码 def get_category_code(self, item): cate_url = item.css('.cate-link::attr("href")').extract_first() pattern = re.compile(r'.*/(\d+)') # 提取主类标签代码 cate_code = re.search(pattern, cate_url) return cate_code.group(1) # 获取所有子分类名称和编码 def get_child_category(self, item): child_cate = item.css('.child-cate a') child_cate_url = [&#123; 'child_cate_name': child.css('::text').extract_first(), 'child_cate_code': self.get_child_category_code(child) &#125; for child in child_cate] return child_cate_url # 正则提取子分类编码 def get_child_category_code(self, child): child_cate_url = child.css('::attr("href")').extract_first() pattern = re.compile(r'.*_(\d+)') # 提取小类标签编号 child_cate_code = re.search(pattern, child_cate_url) return child_cate_code.group(1)这里，除了分类名称 cate_name 可以很方便地直接提取出来，分类编码和子分类的子分类的名称和编码，我们使用了 get_category_code() 等三个方法进行提取。提取方法使用了 CSS 和正则表达式，比较简单。最终提取的分类名称和编码结果如下，利用这些编码，我们就可以构造 URL 请求开始提取每个子分类下的 App 信息了。1234567891011&#123;'cate_name': '影音播放', 'cate_code': '5029', 'child_cate_codes': [ &#123;'child_cate_name': '视频', 'child_cate_code': '716'&#125;, &#123;'child_cate_name': '直播', 'child_cate_code': '1006'&#125;, ... ]&#125;, &#123;'cate_name': '系统工具', 'cate_code': '5018', 'child_cate_codes': [ &#123;'child_cate_name': 'WiFi', 'child_cate_code': '895'&#125;, &#123;'child_cate_name': '浏览器', 'child_cate_code': '599'&#125;, ... ]&#125;, ...接着前面的 get_category() 继续往下写，提取 App 的信息：123456789101112131415161718192021222324def get_category(self,response): cate_content = self.wandou_category.parse_category(response) # ... for item in cate_content: child_cate = item['child_cate_codes'] for cate in child_cate: cate_code = item['cate_code'] cate_name = item['cate_name'] child_cate_code = cate['child_cate_code'] child_cate_name = cate['child_cate_name'] page = 1 # 设置爬取起始页数 if page == 1: # 构造首页url category_url = '&#123;&#125;&#123;&#125;_&#123;&#125;' .format(self.url, cate_code, child_cate_code) else: params = &#123; 'catId': cate_code, # 类别 'subCatId': child_cate_code, # 子类别 'page': page, &#125; category_url = self.ajax_url + urlencode(params) dict = &#123;'page':page,'cate_name':cate_name,'cate_code':cate_code,'child_cate_name':child_cate_name,'child_cate_code':child_cate_code&#125; yield scrapy.Request(category_url,callback=self.parse,meta=dict)这里，依次提取出全部的分类名称和编码，用于构造请求的 URL。由于首页的 URL 和第 2 页开始的 URL 形式不同，所以使用了 if 语句分别进行构造。接下来，请求该 URL 然后调用 self.parse() 方法进行解析，这里使用了 meta 参数用于传递相关参数。123456789101112131415161718192021222324252627282930313233343536373839def parse(self, response): if len(response.body) &gt;= 100: # 判断该页是否爬完，数值定为100是因为无内容时长度是87 page = response.meta['page'] cate_name = response.meta['cate_name'] cate_code = response.meta['cate_code'] child_cate_name = response.meta['child_cate_name'] child_cate_code = response.meta['child_cate_code'] if page == 1: contents = response else: jsonresponse = json.loads(response.body_as_unicode()) contents = jsonresponse['data']['content'] # response 是json,json内容是html，html 为文本不能直接使用.css 提取，要先转换 contents = scrapy.Selector(text=contents, type="html") contents = contents.css('.card') for content in contents: # num += 1 item = WandoujiaItem() item['cate_name'] = cate_name item['child_cate_name'] = child_cate_name item['app_name'] = self.clean_name(content.css('.name::text').extract_first()) item['install'] = content.css('.install-count::text').extract_first() item['volume'] = content.css('.meta span:last-child::text').extract_first() item['comment'] = content.css('.comment::text').extract_first().strip() item['icon_url'] = self.get_icon_url(content.css('.icon-wrap a img'),page) yield item # 递归爬下一页 page += 1 params = &#123; 'catId': cate_code, # 大类别 'subCatId': child_cate_code, # 小类别 'page': page, &#125; ajax_url = self.ajax_url + urlencode(params) dict = &#123;'page':page,'cate_name':cate_name,'cate_code':cate_code,'child_cate_name':child_cate_name,'child_cate_code':child_cate_code&#125; yield scrapy.Request(ajax_url,callback=self.parse,meta=dict)最后，parse() 方法用来解析提取最终我们需要的 App 名称、安装量等信息，解析完成一页后，page 进行递增，然后重复调用 parse() 方法循环解析，直到解析完全部分类的最后一页。最终，几个小时后，我们就可以完成全部 App 信息的抓取，我这里得到 73,755 条信息和 72,150 个图标，两个数值不一样是因为有些 App 只有信息没有图标。图标下载：下面将对提取的信息，进行的数据分析。3 数据分析▌总体情况首先来看一下 App 的安装量情况，毕竟 70000 多款 App，自然很感兴趣 哪些 App 使用地最多，哪些又使用地最少。代码实现如下：123456789101112131415161718192021plt.style.use('ggplot')colors = '#6D6D6D' #字体颜色colorline = '#63AB47' #红色CC2824 #豌豆荚绿fontsize_title = 20fontsize_text = 10# 下载量总排名def analysis_maxmin(data): data_max = (data[:10]).sort_values(by='install_count') data_max['install_count'] = (data_max['install_count'] / 100000000).round(1) data_max.plot.barh(x='app_name',y='install_count',color=colorline) for y, x in enumerate(list((data_max['install_count']))): plt.text(x + 0.1, y - 0.08, '%s' % round(x, 1), ha='center', color=colors) plt.title('安装量最多的 10 款 App ?',color=colors) plt.xlabel('下载量(亿次)') plt.ylabel('App') plt.tight_layout() # plt.savefig('安装量最多的App.png',dpi=200) plt.show()看了上图，有两个「没想到」：排名第一的居然是一款手机管理软件对豌豆荚网上的这个第一名感到意外，一是，好奇大家都那么爱手机清理或者怕中毒么?毕竟，我自己的手机都「裸奔」了好些年；二是，第一名居然不是鹅厂的其他产品，比入「微信」或者「QQ」。榜单放眼望去，以为会出现的没有出现，没有想到的却出现了前十名中，居然出现了书旗小说、印客这些比较少听过的名字，而国民 App 微信、支付宝等，甚至都没有出现在这个榜单中。带着疑问和好奇，分别找到了「腾讯手机管家」和「微信」两款 App 的主页：腾讯手机管家下载和安装量：微信下载和安装量：这是什么情况?腾讯管家 3 亿多的下载量等同于安装量，而微信 20 多亿的下载量，只有区区一千多万的安装量，两组数据对比，大致反映了两个问题：要么是腾讯管家的下载量实际并没有那么多要么是微信的下载量写少了不管是哪个问题，都反映了一个问题：该网站做得不够走心啊。为了证明这个观点，将前十名的安装量和下载量都作了对比，发现很多 App 的安装量都和下载量是一样的，也就是说：这些 App 的实际下载量并没有那么多，而如果这样的话，那么这份榜单就有很大水分了。难道，辛辛苦苦爬了那么久，就得到这样的结果?不死心，接着再看看安装量最少的 App 是什么情况，这里找出了其中最少的 10 款：扫了一眼，更加没想到了：「QQ 音乐」竟然是倒数第一，竟然只有 3 次安装量！确定这和刚刚上市、市值千亿的 QQ 音乐是同一款产品?再次核实了一下：没有看错，是写着 3人安装！这是已经不走心到什么程度了? 这个安装量，鹅厂还能「用心做好音乐」?说实话，到这儿已经不想再往下分析下去了，担心爬扒出更多没想到的东西，不过辛苦爬了这么久，还是再往下看看吧。看了首尾，我们再看看整体，了解一下全部 App 的安装数量分布，这里去除了有很大水分的前十名 App。很惊讶地发现，竟然有 多达 67,195 款，占总数的 94% 的 App 的安装量不足 1万！如果这个网站的所有数据都是真的话，那么上面排名第一的手机管家，它 一款就差不多抵得上这 6 万多款 App 的安装量了！对于多数 App 开发者，只能说：现实很残酷，辛苦开发出来的 App，用户不超过 1万人的可能性高达近 95% 。代码实现如下：123456789101112131415161718def analysis_distribution(data): data = data.loc[10:,:] data['install_count'] = data['install_count'].apply(lambda x:x/10000) bins = [0,1,10,100,1000,10000] group_names = ['1万以下','1-10万','10-100万','100-1000万','1000万-1亿'] cats = pd.cut(data['install_count'],bins,labels=group_names) cats = pd.value_counts(cats) bar = Bar('App 下载数量分布','高达 94% 的 App 下载量低于1万') bar.use_theme('macarons') bar.add( 'App 数量', list(cats.index), list(cats.values), is_label_show = True, xaxis_interval = 0, is_splitline_show = 0, ) bar.render(path='App下载数量分布.png',pixel_ration=1)▌分类情况下面，我们来看看各分类下 App 情况，不再看安装量，而看数量，以排出干扰。可以看到 14 个大分类中，每个分类的 App 数量差距都不大，数量最多的「生活休闲」是「摄影图像」的两倍多一点。接着，我们进一步看看 88 个子分类的 App 数量情况，筛选出数量最多和最少的 10 个子类：可以发现两点有意思的现象：「收音机」类别 App 数量最多，达到 1,300 多款这个很意外，当下收音机完全可以说是个老古董了，居然还有那么人去开发。App 子类数量差距较大最多的「收音机」是最少的「动态壁纸」近 20 倍，如果我是一个 App 开发者，那我更愿意去尝试开发些小众类的 App，竞争小一点，比如：「背单词」、「小儿百科」这些。看完了总体和分类情况，突然想到一个问题：这么多 App，有没有重名的呢?惊奇地发现，叫「一键锁屏」的 App 多达 40 款，这个功能 App 很难再想出别的名字了么? 现在很多手机都支持触控锁屏了，比一键锁屏操作更加方便。接下来，我们简单对比下豌豆荚和酷安两个网站的 App 情况。▌对比酷安二者最直观的一个区别是在 App 数量上，豌豆荚拥有绝对的优势，达到了酷安的十倍之多，那么我们自然感兴趣：豌豆荚是否包括了酷安上所有的 App ?如果是，「你有的我都有，你没有的我也有」，那么酷安就没什么优势了。统计之后，发现豌豆荚 仅包括了 3,018 款，也就是一半左右，剩下的另一半则没有包括。这里面固然存在两个平台上 App 名称不一致的现象，但更有理由相信 酷安很多小众的精品 App 是独有的，豌豆荚并没有。代码实现如下：123456789101112131415161718192021include = data3.shape[0]notinclude = data2.shape[0] - data3.shape[0]sizes= [include,notinclude]labels = [u'包含',u'不包含']explode = [0,0.05]plt.pie( sizes, autopct = '%.1f%%', labels = labels, colors = [colorline,'#7FC161'], # 豌豆荚绿 shadow = False, startangle = 90, explode = explode, textprops = &#123;'fontsize':14,'color':colors&#125;)plt.title('豌豆荚仅包括酷安上一半的 App 数量',color=colorline,fontsize=16)plt.axis('equal')plt.axis('off')plt.tight_layout()plt.savefig('包含不保包含对比.png',dpi=200)plt.show()接下来，我们看看所包含的 App 当中，在两个平台上的下载量是怎么样的：可以看到，两个平台上 App 下载数量差距还是很明显。最后，我面再看看豌豆荚上没有包括哪些APP：可以看到很多神器都没有包括，比如：RE、绿色守护、一个木函等等。豌豆荚和酷安的对比就到这里，如果用一句话来总结，我可能会说：豌豆荚太牛逼了， App 数量是酷安的十倍，所以我选酷安。以上，就是利用 Scrapy 爬取分类多级页面的抓取和分析的一次实战。感兴趣的话可以找类似的网站练练手，如需本文的完整代码，可以加入我的知识星球：「第2脑袋」获得，里面有很多干货，可以扫描下方二维码预览下，觉得合适就入圈。本文完。推荐阅读：∞ Python For 和 While 循环爬取不确定页数的网页∞ Python 爬虫的代理 IP 设置方法汇总∞ Python爬虫的随机 User-Agent 设置方法汇总∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软∞ pyspider 爬取并分析虎嗅网 5 万篇文章]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原创精华 | 2018 年文章汇总]]></title>
    <url>%2F2018%2F12%2F31%2Flife05.html</url>
    <content type="text"><![CDATA[2018 下半年建立博客至今，写了 38 篇文章。最后一天，对这半年写过的文章进行一下汇总，便于你快速找到想看的文章，文章总体主要包括 5 个方面：Python 基础、Python 爬虫、数据分析、每周分享干货和生活随笔。▌Python 基础∞ 入门必看|大佬们推荐的 Python 书单汇总∞ WordCloud 中英文词云图绘制方法汇总∞ Python 日期型数据处理∞ Python 折线图绘制技巧∞ 从函数 def 到类 Class∞ 从 Class 类到 Scrapy▌Python 爬虫∞ 爬虫入门第一课：多种方法爬取猫眼 TOP100 电影∞ 10 行代码爬取全国所有 A 股/港股/新三板上市公司信息∞ 20 秒纵览中国大学十年排行榜变迁∞ 福利 | 单页图片下载，以网易「数独」为例∞ 福利 | 分析 Ajax 实现多页图片下载，以澎湃网为例∞ 中国 66 家环保股上市公司市值 TOP20 强∞ Selenium 自动爬取东方财富网股票财务报表∞ 50 行代码爬取东方财富网百万行财务报表数据∞ Selenium + Ajax 爬取IT 桔子网∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）∞ Python 模拟登录方法汇总∞ 如何让 MongoDB 避免存储重复数据∞ Scrapy 中设置随机 User-Agent 的方法汇总∞ Python 爬虫的代理 IP 设置方法汇总∞ For 和 While 循环爬取不确定页数的网页▌数据分析∞ pyspider 爬取并分析虎嗅网 5 万篇文章∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（分析篇）▌每周分享∞ 用 GitHub + Hexo 搭建你的个人博客：搭建篇∞ 用 GitHub + Hexo 搭建你的个人博客：美化篇∞ 5 分钟内又快又好搞定公众号排版∞ 苹果微软都在使用的一套文案排版∞ 提升审美的带壳截图 App∞ 不用翻墙，轻松看世界新闻∞ 盘点那些手机上绝对值得安装的 App∞ 盘点那些手机上绝对值得安装的 App (2)∞ 安卓最强阅读器∞ 一掏出手机，就暴露了程序猿身份∞ 关于 PDF 阅读处理软件，你需要的都在这里了▌生活随笔∞ 年终总结 | 巨大转变的 2018 年∞ 27 岁，强烈的中年危机感∞ 上了年纪的工程师有哪些出路∞ 我的白血病妻子文章罗列完了，但并没有完，还有两点很重要的需要说明一下：第一、里面的很多文章，并不是孤立的，而是有一定的关联顺序。我在初学 Python 的时候，看过很多公众号的文章精华汇总，数量是非常多，猛地一看，觉得像挖到了宝，心想看这些就够了。但慢慢发现这样的文章汇总其实是一锅大杂烩，因为每篇文章的知识点都很分散，需要自己一点点地去将孤立的知识点串联起来，非常地花时间，学习效率很低。而学习是一个循序渐进的过程，由浅入深，层层递进是比较好的入门方式，所以个人觉得文章不在于多，而在于精，看逻辑清晰、承上启下的文章能够更容易地吸收，更快地入门。第二、上面不少文章中的代码或者资源，只提供了一部分，完整地放在了我的「知识星球」。不是我耍什么把戏，坦诚地说，就是想赚点钱。写这些原创文章，花了我非常多的时间，很多时候是在医院里完成的，我很乐于分享，但同时也希望能够得到些正向激励，这样在新的一年才会有更大动力去写更多、更值得一看的文章。所以，欢迎扫描二维码预览，觉得不错可以加入，早加早划算。99 块，也不多，但能得到接下来一年我文章中所有的精华干货。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[年终总结 | 巨大转变的 2018 年]]></title>
    <url>%2F2018%2F12%2F31%2Flife04.html</url>
    <content type="text"><![CDATA[记录我的 2018 年。一晃就到了 2018 年的最后一天，今天就东拉西扯，回顾下自己的这一年，若这篇文章，哪怕是某一句话，能对你有一点点用，我觉得算没白写。前几年从没想着年尾的时候做个年终总结，所以一年年的时间稀里糊涂地就溜完了，目前还能够记得比较清楚的是 2014 年的时候做过一次总结，以 PPT 视频的形式，名字叫做「我的 1314」，那会儿不过 22、23 岁，我想关注我公众号的不少朋友差不多也是这个年龄，视频里有一点我觉得可能会对你有用，就是 去计划一个「毕业旅行」。传送门：https://v.qq.com/x/page/w08212s3lch.html岁月不饶人，一转眼，刚刚过了 27 岁，我想这个时候如果再不主动去抓住时间，那么它会溜得越来越快，也就意味着会老得更快，竞争力也就越来越弱。下面就来说说这一年的自己的经历和收获，最后再设定几个来年的小目标，有总结有计划，才能做时间的主人。2018 年对我来说，可以用一个词来形容，就是「巨大转变」，主要体现在这么几方面：▌工作一年，就辞职了2017 年夏天毕业后开始工作，到 2018 年 6 月辞职，刚好一年，这里面有客观家庭原因，另外一方面也是想来个彻底转变，顺从自己的内心，果断去放弃不喜欢的本行工作，投身自己的爱好并尝试转行。不过这里要提醒你，我是特殊情况，最好不要辞职自学转行，风险太大。▌辗转东莞、深圳、广州和北京四地这一年，辗转了东莞、深圳、广州和北京多个地方，每个地方都呆了几个月，这也可能是觉得今年过得特别快的一个原因。第一份工作是在东莞的一家上市公司，主要的工作就是对着 Excel 算这算那，觉得自己的专业一点没用到，所以半年后就辞职了。大年初四，便去深圳开始第二份新工作，做了自己的本专业方面工作，几个月后，最终确定对自己的专业不感兴趣，决定放弃转行，这个过程中偶然接触到了 Python，开始产生了兴趣。▌自学 Python7 月份开始利用空闲时间，零基础自学 Python，强迫自己在博客和公众号上输出学习总结，现在看来，选择是正确的，半年下来，在以下三方面取得点小小的个人进步：更新了公众号2014 年夏天，在旅行的途中开通了公众号，却没坚持写下来，一断更就断到了今年，好在，这次坚持了下来。统计了下，今年写了 38 篇文章，37 篇原创，平均一个月 6 篇左右，粉丝数上升不多，没有太刻意去采取措施吸粉，相比粉丝数，更加注重的是文章阅读率，达到了 14% 还算不错。建立了个人博客微信公众号，归根结底还是比较大众的一个东西，一直都想拥有一个独一无二的个人博客，在上面记点东西，写个几十年。有了强烈的想法后，就会发现浑身有用不完的能量。7 月份，是最忙的一阵子，晚上在医院守一晚夜，白天回到出租房里开始学习搭建现在的这个博客，前后花了半个月才弄好。到今天，博客底部的统计显示，一共写了 11 万字，从最初的 0 人，达到了今天的 6,000 人访问，17,000 次浏览量，中间一阵子出了点故障没有记录到，真实值可能在两万次左右，这个数值，跟别人比显得很少，跟半年前的自己比，就是上万倍的提升。开通了知识星球早在 2017 年 2 月我就加入了几个知识星球，那时候还叫「小密圈」，但从没想过自己去开一个，这一拖就是快两年。终于，一个月前，我开通了自己的付费知识星球，开之前犹豫了很久，要不要开？开付费的还是免费的？很多人说，个人影响力还不大的时候就先不要开付费星球。我觉得各有利弊，知识星球和公众号可以很好地互补，也能逼自己做更多的持续输出，虽然目前加入的球友不多，但好歹是上路了，在这里 感谢最早加入和支持我的几位朋友。▌看了二十本书我以前不爱看书，几乎任何书都不喜欢看的那种，印象中没有完整地看过一本书，不是对书没兴趣，是觉得没有外界的推动或者逼迫去让自己要养成阅读这个习惯，归根结底就是太懒。好在今年得到了很大的改善，下半年零基础开始学习编程的时候，发现自己一无所知，甚至比不了一个科班的大一学生，无数个时刻想锤自己脑袋，前些年怎么不多装些东西。于是开始疯狂看书，先看电子书，不错的就把纸质版买到手，不知不觉就买了许多书，一些算是完整看了，一些只是快速翻了两遍，有的还没来得及看。看书绝对是任何阶段都应该保有的一个爱好。▌跑了上百公里步跑步这个习惯最早是 10 多年之前上初中的时候养成的，那时候感觉浑身有用不完的力气，后来却因为种种原因中断了，这一停，就长达 10 年之久，没成想那几年就是自己人生精力的最顶峰。上了高中，觉得课业太重，没时间；到了大学，看到少有人跑，就算了；出来工作，上班已然很累，就不要折腾自己了。下半年，由于不得不早睡早起，所以渐渐戒掉了熬夜的习惯，作息开始规律起来：晚十点睡，早六点起，跑 2-3 公里步。大致经历了：从「要人命」到「跑不停」的阶段，以前从不相信别人口中「跑步是会上瘾」这种说法的，总以为他们是在秀优越，之所以有这样的感觉，我发现往往是因为自己只坚持了两三天，而这 头两三天是开始习惯跑步最痛苦的阶段，我已经记不清有多少次在这个阶段放弃了。而事实上，第一次跑会难受一个礼拜，第二次跑会难受三天，到了第三第四次，隔天再跑你会惊奇地发现「我竟然可以连续跑了」。以上，就是 2018 年的一些回顾，下面简单说说 2019 年设定的几个小目标。▌不要停，向前看这几个月，我几乎是没有停下脚步，一直逼自己在学，但还有很多东西等着去学，初步确定明年要主攻：机器学习、数据挖掘、数据结构与算法、Linux 系统、数据库等方面的知识，然后继续总结输出。▌100 篇原创文章参照这小半年写了近 40 篇原创文章来算，我觉得明年实现这个目标，不算好高骛远。▌跑步 600 公里根据这几个月跑步的距离和频次，定了这个目标，拆开来算，其实不多，一年跑 300 天，每天跑 2公里就行，也就是 5 圈操场跑道。编程界大佬廖雪峰老师，知乎上的个人标签写着「业余马拉松选手」，我想多跑跑步总没坏处，当程序猿没个好身体搞不起的。好，东拉西扯了一番，也欢迎你留言说说自己，当立个 Flag，另外说不定后期也会有福利。本文完。7 月份开始利用空闲时间，零基础自学 Python，强迫自己输出学习总结，现在看来，选择是正确的，半年下来，在以下三方面取得点小小的个人进步：]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你的手机截图真丑]]></title>
    <url>%2F2018%2F12%2F29%2Fweekly_sharing10.html</url>
    <content type="text"><![CDATA[介绍两款炒鸡好用的手机截图 App，提升你的审美。摘要：介绍两款炒鸡好用的手机截图 App，提升你的审美。这是「每周分享」的第 10 期，也是 2018 年的最后一期，最后一期咱们轻松一点，来介绍两款很实用、堪称神器的手机带壳截图 App。如果你足够细心的话，会发现我此前的很多文章中，都用到手机带壳截图，也就是在图片外面套了个手机壳，之所以想套个壳，是觉得直接放截图太丑了，尤其是当看到朋友圈中那些摆满九宫格的手机截图，我就很受不了，普通的截图图片用了之后逼格立马就上来了。这里，我找了三个场景下的截图：聊天界面、朋友圈、公众号文章，来对比一下无手机壳和带手机壳的截图效果：无手机壳：带手机壳：所以当你发朋友圈或者给别人发截图时，想要有逼格一点，可以尝试使用带壳截图。那么，问题就来了，怎么给手机带壳截图？咳咳，下面就到了神器亮相的时候了，介绍一款由「少数派」开发的非常不错的带壳截图 App，页面长这个样子：这款 App 支持常见的 11 款手机品牌，共包含 47 个机型，比如 HTC、华为、Sony 等，只要点击某款机型然后插入图片即可完成带壳截图，小学生都会的操作，比如使用 Nexus 可以得到这样的效果：手机壳很好看对吧，500px 上搜索「Sean Archer」还有更多，只能帮你到这儿了，注意身体。接着说，你会发现，里面可能没有你使用的 iPhone、OPPO、VIVO 等品牌或者有品牌但没有相应的机型，对的，我也是在换了手机后发现没有适应的机型，套别的机型会有什么问题呢？问题大了，由于手机尺寸大小不一样，所以手机截图会变形，这样就不好看。于是，我又开始物色有没有更好、支持更多机型的 App，还真给我让我找到了，就是下面将要介绍的这款终极截图 App ，名字叫「带壳截图 PRO」，页面做得很清爽：我大致数了下，这款 App 支持 21 款手机，共计上百款机型，可以说你使用的手机基本上都能够在这里找到模型：这里，随便挑了几款机型试试看截图效果，今天很冷，就放几张冬天的高清图吧：再比如：还比如：除了上述常规截图玩法，你还可以做成手持效果：还可以来个双面特写：逼格还不够高，就再来个「千手观音」：以上就是本期的分享，如果对里面的 App 感兴趣的话可以去找来试试。还是老规矩，为了更方便你，我这里准备已下载好了，公众号后台回复：「App截图」就可以得到。 另外若喜欢里面的几张高清图片，可以扫描下方二维码加入我的知识星球得到，里面有很多干货，越早加入越划算。本文完。推荐阅读：盘点那些手机上绝对值得安装的 AppScrapy 爬取并分析酷安 6000 款 App，找到良心佳软安卓最强阅读器]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python For 和 While 循环爬取不确定页数的网页]]></title>
    <url>%2F2018%2F12%2F26%2Fweb_scraping_withpython16.html</url>
    <content type="text"><![CDATA[Requests 和 Scrapy 中分别用 For 循环和 While 循环爬取不确定页数的网页。摘要：Requests 和 Scrapy 中分别用 For 循环和 While 循环爬取不确定页数的网页。我们通常遇到的网站页数展现形式有这么几种：第一种是直观地显示所有页数，比如此前爬过的酷安、东方财富网，文章见：∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软∞ 50 行代码爬取东方财富网百万行财务报表数据第二种是不直观显示网页总页数，需要在后台才可以查看到，比如之前爬过的虎嗅网，文章见：∞ pyspider 爬取并分析虎嗅网 5 万篇文章第三种是今天要说的，不知道具体有多少页的网页，比如豌豆荚：对于，前两种形式的网页，爬取方法非常简单，使用 For 循环从首页爬到尾页就行了，第三种形式则不适用，因为不知道尾页的页数，所以循环到哪一页结束无法判断。那如何解决呢？有两种方法。第一种方式 使用 For 循环配合 break 语句，尾页的页数设置一个较大的参数，足够循环爬完所有页面，爬取完成时，break 跳出循环，结束爬取。第二种方法 使用 While 循环，可以结合 break 语句，也可以设起始循环判断条件为 True，从头开始循环爬取直到爬完最后一页，然后更改判断条件为 False 跳出循环，结束爬取。实际案例下面，我们以 豌豆荚 网站中「视频」类别下的 App 信息为例，使用上面两种方法抓取该分类下的所有 App 信息，包括 App 名称、评论、安装数量和体积。首先，简要分析下网站，可以看到页面是通过 Ajax 加载的，GET 请求附带一些参数，可以使用 params 参数构造 URL 请求，但不知道一共有多少页，为了确保下载完所有页，设置较大的页数，比如 100页 甚至 1000 页都行。下面我们尝试使用 For 和 While 循环爬取 。Requests▌For 循环主要代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Get_page(): def __init__(self): # ajax 请求url self.ajax_url = 'https://www.wandoujia.com/wdjweb/api/category/more' def get_page(self,page,cate_code,child_cate_code): params = &#123; 'catId': cate_code, 'subCatId': child_cate_code, 'page': page, &#125; response = requests.get(self.ajax_url, headers=headers, params=params) content = response.json()['data']['content'] #提取json中的html页面数据 return content def parse_page(self, content): # 解析网页内容 contents = pq(content)('.card').items() data = [] for content in contents: data1 = &#123; 'app_name': content('.name').text(), 'install': content('.install-count').text(), 'volume': content('.meta span:last-child').text(), 'comment': content('.comment').text(), &#125; data.append(data1) if data: # 写入MongoDB self.write_to_mongodb(data) if __name__ == '__main__': # 实例化数据提取类 wandou_page = Get_page() cate_code = 5029 # 影音播放大类别编号 child_cate_code = 716 # 视频小类别编号 for page in range(2, 100): print('*' * 50) print('正在爬取：第 %s 页' % page) content = wandou_page.get_page(page,cate_code,child_cate_code) # 添加循环判断，如果content 为空表示此页已经下载完成了,break 跳出循环 if not content == '': wandou_page.parse_page(content) sleep = np.random.randint(3,6) time.sleep(sleep) else: print('该类别已下载完最后一页') break这里，首先创建了一个 Get_page 类，get_page 方法用于获取 Response 返回的 json 数据，通过 json.cn 网站解析 json 解析后发现需要提取的内容是一段包裹在 data 字段下 content 键中的 html 文本，可以使用 parse_page 方法中的 pyquery 函数进行解析，最后提取出 App 名称、评论、安装数量和体积四项信息，完成抓取。在主函数中，使用了 if 函数进行条件判断，若 content 不为空，表示该页有内容，则循环爬下去，若为空则表示此页面已完成了爬取，执行 else 分支下的 break 语句结束循环，完成爬取。爬取结果如下，可以看到该分类下一共完成了全部 41 页的信息抓取。▌While 循环While 循环和 For 循环思路大致相同，不过有两种写法，一种仍然是结合 break 语句，一种则是更改判断条件。总体代码不变，只需修改 For 循环部分：12345678910111213page = 2 # 设置爬取起始页数while True: print('*' * 50) print('正在爬取：第 %s 页' %page) content = wandou_page.get_page(page,cate_code,child_cate_code) if not content == '': wandou_page.parse_page(content) page += 1 sleep = np.random.randint(3,6) time.sleep(sleep) else: print('该类别已下载完最后一页') break或者：1234567page = 2 # 设置爬取起始页数page_last = False # while 循环初始条件while not page_last: #... else: # break page_last = True # 更改page_last 为 True 跳出循环结果如下，可以看到和 For 循环的结果是一样的。我们可以再测试一下其他类别下的网页，比如选择「K歌」类别，编码为：718，然后只需要对应修改主函数中的child_cate_code 即可，再次运行程序，可以看到该类别下一共爬取了 32 页。由于 Scrapy 中的写法和 Requests 稍有不同，所以接下来，我们在 Scrapy 中再次实现两种循环的爬取方式 。Scrapy▌For 循环Scrapy 中使用 For 循环递归爬取的思路非常简单，即先批量生成所有请求的 URL，包括最后无效的 URL，后续在 parse 方法中添加 if 判断过滤无效请求，然后爬取所有页面。由于 Scrapy 依赖于Twisted框架，采用的是异步请求处理方式，也就是说 Scrapy 边发送请求边解析内容，所以这会发送很多无用请求。1234567def start_requests(self): pages=[] for i in range(1,10): url='http://www.example.com/?page=%s'%i page = scrapy.Request(url,callback==self.pare) pages.append(page) return pages下面，我们选取豌豆荚「新闻阅读」分类下的「电子书」类 App 页面信息，使用 For 循环尝试爬取，主要代码如下：12345678910111213141516171819202122232425262728293031def start_requests(self): cate_code = 5019 # 新闻阅读 child_cate_code = 940 # 电子书 print('*' * 50) pages = [] for page in range(2,50): print('正在爬取：第 %s 页 ' %page) params = &#123; 'catId': cate_code, 'subCatId': child_cate_code, 'page': page, &#125; category_url = self.ajax_url + urlencode(params) pa = yield scrapy.Request(category_url,callback=self.parse) pages.append(pa) return pagesdef parse(self, response): if len(response.body) &gt;= 100: # 判断该页是否爬完，数值定为100是因为response无内容时的长度是87 jsonresponse = json.loads(response.body_as_unicode()) contents = jsonresponse['data']['content'] # response 是json,json内容是html，html 为文本不能直接使用.css 提取，要先转换 contents = scrapy.Selector(text=contents, type="html") contents = contents.css('.card') for content in contents: item = WandoujiaItem() item['app_name'] = content.css('.name::text').extract_first() item['install'] = content.css('.install-count::text').extract_first() item['volume'] = content.css('.meta span:last-child::text').extract_first() item['comment'] = content.css('.comment::text').extract_first().strip() yield item上面代码很好理解，简要说明几点：第一、判断当前页是否爬取完成的判断条件改为了 response.body 的长度大于 100。因为请求已爬取完成的页面，返回的 Response 结果是不为空的，而是有长度的 json 内容（长度为 87），其中 content 键值内容才为空，所以这里判断条件选择比 87 大的数值即可，比如 100，即大于 100 的表示此页有内容，小于 100 表示此页已爬取完成。1&#123;"state":&#123;"code":2000000,"msg":"Ok","tips":""&#125;,"data":&#123;"currPage":-1,"content":""&#125;&#125;第二、当需要从文本中解析内容时，不能直接解析，需要先转换。通常情况下，我们在解析内容时是直接对返回的 response 进行解析，比如使用 response.css() 方法，但此处，我们的解析对象不是 response，而是 response 返回的 json 内容中的 html 文本，文本是不能直接使用 .css() 方法解析的，所以在对 html 进行解析之前，需要添加下面一行代码转换后才能解析。1contents = scrapy.Selector(text=contents, type="html")结果如下，可以看到发送了全部 48 个请求，实际上该分类只有 22 页内容，即多发送了无用的 26 个请求。▌While 循环接下来，我们使用 While 循环再次尝试抓取，代码省略了和 For 循环中相同的部分：123456789101112131415161718192021222324def start_requests(self): page = 2 # 设置爬取起始页数 dict = &#123;'page':page,'cate_code':cate_code,'child_cate_code':child_cate_code&#125; # meta传递参数 yield scrapy.Request(category_url,callback=self.parse,meta=dict)def parse(self, response): if len(response.body) &gt;= 100: # 判断该页是否爬完，数值定为100是因为无内容时长度是87 page = response.meta['page'] cate_code = response.meta['cate_code'] child_cate_code = response.meta['child_cate_code'] #... for content in contents: yield item # while循环构造url递归爬下一页 page += 1 params = &#123; 'catId': cate_code, 'subCatId': child_cate_code, 'page': page, &#125; ajax_url = self.ajax_url + urlencode(params) dict = &#123;'page':page,'cate_code':cate_code,'child_cate_code':child_cate_code&#125; yield scrapy.Request(ajax_url,callback=self.parse,meta=dict)这里，简要说明几点：第一、While 循环的思路是先从头开始爬取，使用 parse() 方法进行解析，然后递增页数构造下一页的 URL 请求，再循环解析，直到爬取完最后一页即可，这样 不会像 For 循环那样发送无用的请求。第二、parse() 方法构造下一页请求时需要利用 start_requests() 方法中的参数，可以 使用 meta 方法来传递参数。运行结果如下，可以看到请求数量刚好是 22 个，也就完成了所有页面的 App 信息爬取。以上，就是本文的所有内容，小结一下：在爬取不确定页数的网页时，可以采取 For 循环和 While 循环两种思路，方法大致相同。在 Requests 和 Scrapy 中使用 For 循环和 While 循环的方法稍有不同，因此本文以豌豆荚网站为例，详细介绍了循环构造方法。之所以写本文内容和之前的几篇文章（设置随机 UA、代理 IP），是为了下一篇文章「分析豌豆荚全网 70000+ App 信息」做铺垫，敬请期待。完整案例代码如需本文完整的案例代码，可以扫描下方图片二维码加入我的知识星球：「第2脑袋」，里面有很多干货，期待你的到来。本文完。推荐阅读：∞ Python 爬虫的代理 IP 设置方法汇总∞ Python爬虫的随机 User-Agent 设置方法汇总∞ 爬虫断了？一招搞定 MongoDB 重复数据∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软∞ 50 行代码爬取东方财富网百万行财务报表数据∞ pyspider 爬取并分析虎嗅网 5 万篇文章]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 爬虫的代理 IP 设置方法汇总]]></title>
    <url>%2F2018%2F12%2F25%2Fweb_scraping_withpython15.html</url>
    <content type="text"><![CDATA[Requests 和 Scrapy 中的代理 IP 设置。摘要：对于采取了比较强的反爬措施网站来说，要想顺利爬取网站数据，设置随机 User-Agent 和代理 IP 是非常有效的两个方法，继上一篇文章介绍了随机 UserAgent 的设置方法之后，本文接着介绍如何在 Requests 和 Scrapy 中设置代理 IP。上一篇文章见：∞ Scrapy 中设置随机 User-Agent 的方法汇总本文的目标测试网页选择下面这个 URL，请求该网页可以返回当前的 IP 地址：∞ http://icanhazip.com下面，我们就先来说说 Requests 中如何设置代理 IP。Requests▌不使用代理首先，先来看一下不使用代理 IP 的情况：123456789import requestsurl = 'http://icanhazip.com'try: response = requests.get(url) #不使用代理 print(response.status_code) if response.status_code == 200: print(response.text)except requests.ConnectionError as e: print(e.args)运行上面的程序，会返回我们电脑本机的 IP，可以通过百度查询 IP 地址对比一下就知道了。1234200124.238.223.xxx # 后三位隐去了[Finished in 0.8s]▌使用代理然后，我们测试一下使用代理后的情况。常见的代理包括 HTTP 代理和 SOCKS5 代理，前者可以找一些免费代理 IP 进行测试，由于我电脑上使用的是 Shadowsocks，所以就介绍一下 SOCKS5 代理的设置。首先，电脑上要安装有 Shadowsocks ，如果你还没听过或者使用过这个神器，可以参考下我之前写的篇文章：∞ 如何正确地科学上网启动该软件后默认会在 1080 端口下创建 SOCKS5 代理服务，代理为：127.0.0.1:1080，然后我们在 Requests 中使用该代理，方法很简单只需要添加一项 proxies 参数即可：1234567891011121314proxies = [ &#123;'http':'socks5://127.0.0.1:1080'&#125;, &#123;'https':'socks5://127.0.0.1:1080'&#125;]proxies = random.choice(proxies)print(proxies)url = 'http://icanhazip.com'try: response = requests.get(url,proxies=proxies) #使用代理 print(response.status_code) if response.status_code == 200: print(response.text)except requests.ConnectionError as e: print(e.args)这里，proxies 参数是字典类型，键名&#39;http&#39; 表示协议类型，键值 &#39;socks5://127.0.0.1:1080&#39;表示代理，这里添加了 http 和 https 两个代理，这样写是因为有些网页采用 http 协议，有的则是采用 https 协议，为了在这两类网页上都能顺利使用代理，所以一般都同时写上，当然，如果确定了某网页的请求类型，可以只写一种，比如这里我们请求的 url 使用的是 http 协议，那么使用 http 代理就可以，random 函数用来随机选择一个代理，我们来看一下结果：123&#123;'http': 'socks5://127.0.0.1:1080'&#125;20045.78.42.xxx #xxx表示隐去了部分信息可以看到，这里随机选择了 http 协议的代理后，返回的 IP 就是我真实的 IP 代理地址，成功代理后就可以爬一些墙外的网页了。延伸一下，假如随机选择的是 https 代理，那么返回的 IP 结果还一样么？我们尝试重复运行一下上面的程序：123&#123;'https': 'socks5://127.0.0.1:1080'&#125;200124.238.223.xxx可以看到这次使用了 https 代理，返回的 IP 却是本机的真实 IP，也就是说代理没有起作用。进一步地，我们将 url 改为 https 协议 &#39;https://icanhazip.com&#39;，然后再尝试分别用 http 和 https 代理请求，查看一下结果：123456789#http 请求&#123;'http': 'socks5://127.0.0.1:1080'&#125;200124.238.223.xxx#https 请求&#123;'https': 'socks5://127.0.0.1:1080'&#125;20045.78.42.xxx可以看到，两种请求的结果和之前的刚好相反了，由于 url 采用了 https 协议，则起作用的是 https 代理，而 http 代理则不起作用了，所以显示的是本机 IP。因此，可以得到这样的一个结论：HTTP 代理，只代理 HTTP 网站，对于 HTTPS 的网站不起作用，也就是说，用的是本机 IP。HTTPS 代理则同理。▌使用付费代理上面，我们只使用了一个代理，而在爬虫中往往需要使用多个代理，那有如何构造呢，这里主要两种方法，一种是使用免费的多个 IP，一种是使用付费的 IP 代理，免费的 IP 往往效果不好，那么可以搭建 IP 代理池，但对新手来说搞一个 IP 代理池成本太高，如果只是个人平时玩玩爬虫，完全可以考虑付费 IP，几块钱买个几小时动态 IP，多数情况下都足够爬一个网站了。这里推荐一个付费代理「阿布云代理」，最近使用了一下，效果非常不错，5 块钱买了 5个小时，爬完了一个网站，所以没有必要为了省 5 块钱，而费劲地去搞 IP 代理池。首次使用的话，可以选择购买一个小时的动态版试用下，点击生成隧道代理信息作为凭证加入到代码中。将信息复制到官方提供的 Requests 代码中，运行来查看一下代理 IP 的效果：1234567891011121314151617181920212223242526272829import requests# 待测试目标网页targetUrl = "http://icanhazip.com"def get_proxies(): # 代理服务器 proxyHost = "http-dyn.abuyun.com" proxyPort = "9020" # 代理隧道验证信息 proxyUser = "HS77K12Q77V4G9MD" proxyPass = "4131FFDFCE27F104" proxyMeta = "http://%(user)s:%(pass)s@%(host)s:%(port)s" % &#123; "host" : proxyHost, "port" : proxyPort, "user" : proxyUser, "pass" : proxyPass, &#125; proxies = &#123; "http" : proxyMeta, "https" : proxyMeta, &#125; for i in range(1,6): resp = requests.get(targetUrl, proxies=proxies) # print(resp.status_code) print('第%s次请求的IP为：%s'%(i,resp.text)) get_proxies()可以看到每次请求都会使用不同的 IP，是不是很简单？比搞 IP 代理池省事多了。以上，介绍了 Requests 中设置代理 IP 的方法，下面我们接着介绍在 Scrapy 中如何设置。Scrapy▌middlewares.py 中设置这种方法需要先在 middlewares.py 中设置代理 IP 中间件：12345678910111213import randomclass ProxyMiddleware(object): def __init__(self, ip): self.ip = ip @classmethod def from_crawler(cls, crawler): return cls(ip=crawler.settings.get('PROXIES')) def process_request(self, request, spider): ip = random.choice(self.ip) request.meta['proxy'] = ip logging.debug('Using Proxy:%s'%ip)接着，需要在 settings.py 添加几个在西刺上找的代理 IP，格式如下：12345PROXIES = [ 'https://127.0.0.1:8112', 'https://119.101.112.176:9999', 'https://119.101.115.53:9999', 'https://119.101.117.226:9999']然后，我们仍然以 “http://icanhazip.com&quot; 为目标网页，运行 Scrapy 项目重复请求 5 次，查看一下每次返回的 IP 情况：123456789def start_requests(self): items = [] for i in range(1,6): item = yield scrapy.Request(self.cate_url,callback=self.get_category) items.append(item) return items def get_category(self, response): print(response.text)结果如下：可以看到部分 IP 成功请求得到了相应，部分 IP 则无效请求失败，因为这几个 IP 是免费的 IP，所有失效很正常。▌使用付费代理接下来我们使用阿布云付费代理，继续尝试一下，在 middlewares.py 中添加下面的代码：12345678910111213141516""" 阿布云ip代理配置，包括账号密码 """# 阿布云scrapy 写法import base64proxyServer = "http://http-dyn.abuyun.com:9020"proxyUser = "HS77K12Q77V4G9MD" # 购买后点击生成获得proxyPass = "4131FFDFCE27F104" # 购买后点击生成获得# for Python3proxyAuth = "Basic " + base64.urlsafe_b64encode(bytes((proxyUser + ":" + proxyPass), "ascii")).decode("utf8")class AbuyunProxyMiddleware(object): """ 阿布云ip代理配置 """ def process_request(self, request, spider): request.meta["proxy"] = proxyServer request.headers["Proxy-Authorization"] = proxyAuth logging.debug('Using Proxy:%s'%proxyServer)由于，在阿布云购买的是最基础的代理，即每秒 5 个请求，因为 Scrapy 默认的并发数是 16 个，所以需要对 Scrapy 请求数量进行一下限制，可以设置每个请求的延迟时间为 0.2s ，这样一秒就刚好请求 5 个，最后启用上面的代理中间件类即可：12345678910""" 启用限速设置 """AUTOTHROTTLE_ENABLED = TrueDOWNLOAD_DELAY = 0.2 # 每次请求间隔时间DOWNLOADER_MIDDLEWARES = &#123; #'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, #启用随机UA #'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 100, #启用随机UA 'wandoujia.middlewares.AbuyunProxyMiddleware': 200, # 启用阿布云代理 #value值越小优先级越高&#125;然后同样地请求 5 次，查看每次请求返回的 IP ：可以看到，每个 IP 都顺利请求成功了，所以说付费地效果还是好。▌使用 scrapy-proxies 库代理除了上述两种方法，我们还可以使用 GitHub 上的一个 IP 代理库：scrapy-proxies，库的使用方法很简单， 三个步骤就可以开启代理 IP。首先，运行下面命令安装好这个库：1pip install scrapy_proxies然后，在 Scrapy 项目中的 settings.py 文件中，添加下面一段代码：12345678910RETRY_TIMES = 3 # 自定义请求失败重试次数#重试的包含的错误请求代码RETRY_HTTP_CODES = [500, 503, 504, 400, 403, 404, 408]DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90, 'scrapy_proxies.RandomProxy': 100, 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,&#125;PROXY_LIST = r'/proxies.txt' # proxy 代理文件存放位置，此处为程序所在磁盘根目录下PROXY_MODE = 0 # 每次请求都使用不同的代理最后，需要提供多个代理 IP，我们在西刺上随便找几个 IP，然后存放在 PROXY_LIST 指定的 txt 文件中即可，格式如下：123https://119.101.112.176:9999https://119.101.115.53:9999https://119.101.117.53:999然后重复之前的操作，查看代理 IP 的设置效果。我在使用该库的过程中，发现有一些问题，不知道是配置不对还是怎么回事，效果不是太好，所以推荐使用前两种方法。好，以上就是在 Requests 和 Scrapy 中使用代理 IP 的方法总结，如果爬虫项目不大、追求稳定且不差钱的话，建议直接上付费代理。如需完整实例代码，可以扫描下方图片二维码加入我的知识星球：「第2脑袋」，里面有很多干货，期待你的到来。本文完。推荐阅读：∞ Scrapy 中设置随机 User-Agent 的方法汇总∞ 爬虫断了？一招搞定 MongoDB 重复数据∞ 从函数 def 到类 Class∞ 从 类 Class 到 Scrapy参考：∞ HTTP 代理和 HTTPS 代理的区别∞ HTTP、HTTPS代理分析及原理∞ Shadowsocks 代理方式]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[入门必看|大佬们推荐的 Python 书单汇总]]></title>
    <url>%2F2018%2F12%2F22%2Fweekly_sharing9.html</url>
    <content type="text"><![CDATA[介绍 Python 大佬推荐的学习书，以及我入门 Python 的学习路径。摘要：此文主要针对想入门 Python 但不知道看什么书好和有选择纠结症的童鞋，大佬们可绕道。写在前面：本文有很多超链接，建议点击底部「阅读原文」进行跳转查看。这是「每周分享」的第 9 期，前 8 期基本都在分享一些我珍藏推荐的好软件、App，（前期文章汇总见底部推荐阅读处）数量不算少，够诸位消化一阵了。转眼也到了年终，这一期换个话题，围绕这几个问题：「学习 Python 该看哪些书？不同的书该怎么看？按照什么样的顺序看？」，来聊一聊如何入门 Python，为了更有说服性一些，这里我把入门时看过的一些大佬推荐的书单进行了汇总，最后结合我的学习路径谈谈怎么读书。半年前，Python 对我来说就是谜一样的东西，根本不知道如何下手、从何处下手，整天像无头苍蝇一样到处找资源，个把月过去了还没找到 Python 大门在哪儿，主要是花了很多的时间在纠结「该学习 Python 还是 R、学习 Python3 还是 Python 2 、看什么入门书最合适？」这些问题。知乎、豆瓣、CSDN、各大佬的公众号搜罗逛了一圈下来，只明确了前两个问题，就是要学习 Python，而且是 Python3，但对于看什么书，陷入了纠结迟迟下不了手。现在看来，这应该是属于必经的过程，当涉足一个陌生的学习领域，对什么都不了解，即使别人给的建议再对，也会掂量犹豫几下。慢慢地，我开始进行总结，把一些大佬推荐的入门书籍文章进行汇总对比，然后就发现有些书是都在推荐的，于是决定重点就看这些书，这样才算慢慢摸到 Python 的大门。话不多说，下面就分享 5 位大佬推荐的书单，除了入门书，还包括数据分析、数据挖掘、机器学习等方面，可以说是非常全面。▌刘志军 (Python 之禅 作者)刘志军是位不折不扣的 Python 大佬，他博客中的 Python 文章最早可以追溯到 2013 年。▌leoxin (菜鸟学 Python 作者)辛哥爬取分析了豆瓣 Python 相关的 1000 多本书籍，从各个角度找到了最受欢迎的书目，然后给出了自己的推荐。▌刘顺祥 (数据分析 1480 作者)刘顺祥大佬的公众号干货很多，入门时学习到很多。▌秦路 (七周成为数据分析师课程作者)秦路大佬在天善智能开设的《七周成为数据分析师》课程非常全面，他的推荐非常值得参考。▌王大伟 (Python爱好者作者)王大伟大佬写的文章非常有趣，我看了他的几篇关于类（Class） 的文章后才彻底搞懂类是怎么回事。以上就是 5 位大佬的推荐，想必你心里大概有个谱了，下面再说说我看过的一些书，然后分享一下我的入门路径。▌我都看了哪些书你可能注意到了，以上推荐了少说也有好几十本书，范围还是有点大，就算都是值得看的书，也没么多时间精力都去看，所以上面只是入门 Python 的第一个步骤，即筛选书的范围，还有更为重要的两个步骤。第一，首先要确先定你学你 Python 的目的。也就是你想学了去干嘛，是做爬虫、数据分析挖掘、机器学习、web 开发还是什么其他的，虽说不同的方向都需要有 Python 基础，但对 Python 的基础也是有所侧重，只有确定一个方向才可以进一步筛选书和书中章节的范围。第二，确定了书的范围后，要琢磨好怎么去看每一本书、以什么样的顺序去看书。不然，同时看好几本书，每一本都从头开始看，坚持不了几天就会放弃。下面以我入门的过程来具体说一下。由于我此前是零编程基础，helloworld 都不会打的那种，首先在知乎上看了几个 Python 入门的回答后，觉得用 Python 做数据分析这个方向不错，加上我此前学 Excel 时就对数据分析比较感兴趣，所以就确定了这个方向，但很快就发现行不通，因为我连基本的 Python 操作都不会，处处卡壳，时间都花在抠一个个的小问题上去了，折腾到最后也没太大兴趣去分析了，而且数据分析本身是有一套理论方法的，我更不会。然后我就想如果同时学 Python 操作和分析方法，比较耗费精力，所以就放弃直接学数据分析这个想法。然后我选了另外一条路，就是爬虫，因为基础的爬虫比数据分析简单，学习曲线不陡，而且爬虫比较有意思，写出来别人也更愿意看，进一步了解到初步的爬虫学习主要学几个爬虫类库、网页解析提取库、框架这几块就行了，这样一下就缩小了书的选择范围和内容范围。至此，我就选择了「Python 基础——爬虫——数据分析」这样一条路线。首先，我选择了《深入浅出 Python 》这本书作为入门的第一本书，这本书浅显易懂，注释详尽，对新手很友好。接着，我又大致过了一遍《Python 编程从入门到实践》，前面几章写得非常实用，这样对 Python 就有了一个大致了解。接着，便开始上手爬虫，但爬虫类的书非常少，起先只找到两本，一本是国外的《Python 网络数据采集》，书不厚，看了后大致了解了：爬虫是怎么一回事、爬虫能做什么、要会哪些东西等这几个问题，另一本是韦玮老师的《精通 Python 网络爬虫》，这本书当时觉得还不错，有很多实操案例，但是理论部分欠缺一些。后来偶然搜到了崔庆才大佬的爬虫文章，很赞果断就买了他刚出的《Python3 网络爬虫实战》这本书，由此算是找到了爬虫方向。之后通过爬虫把数据爬下来后就开始尝试一些简单的分析，但发现很多操作根本不熟练，于是采取了两种方法去学习，首先是谷歌解决实际问题，然后闲的时候翻看了《利用 Python 进行数据分析》、《流畅的 python》、《 Python Cookbook》这几本书，算是系统地巩固了一下相关知识。就这样，几个月下来，练习了 10 个左右的爬虫，自认为算是入门了 Python 爬虫和数据分析。以上就是本期的推荐，如果对里面的书感兴趣的话可以去找来看看，老规矩，为了更方便你，我这里准备好了部分电子书，公众号回复「Python书」就可以得到。 下载下来结合「静读天下」 App 看，会让手机阅读学习变得更简单。本文完。推荐阅读：盘点那些手机上绝对值得安装的 AppScrapy 爬取并分析酷安 6000 款 App，找到良心佳软安卓最强阅读器PDF 阅读处理软件，你需要的都在这里了]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 中设置随机 User-Agent 的方法汇总]]></title>
    <url>%2F2018%2F12%2F21%2Fweb_scraping_withpython14.html</url>
    <content type="text"><![CDATA[一行代码搞定 Scrapy 中的随机 UA 设置。摘要：爬虫过程中的反爬措施非常重要，其中设置随机 User-Agent 是一项重要的反爬措施，Scrapy 中设置随机 UA 的方式有很多种，有的复杂有的简单，本文就对这些方法进行汇总，提供一种只需要一行代码的设置方式。最近使用 Scrapy 爬一个网站，遇到了网站反爬的情况，于是开始搜索一些反爬措施，了解到设置随机 UA 来伪装请求头是一种常用的方式，这能够做到一定程度上避免网站直接识别出你是一个爬虫从而封掉你。设置随机 UA 的方法有挺多种，有的需要好多行代码，有的却只需要一行代码就搞定了，接下来就来介绍下。▌常规设置 UA首先，说一下常规情况不使用 Scrapy 时的用法，比较方便的方法是利用 fake_useragent包，这个包内置大量的 UA 可以随机替换，这比自己去搜集罗列要方便很多，下面来看一下如何操作。首先，安装好fake_useragent包，一行代码搞定：1pip install fake-useragent然后，就可以测试了：1234from fake_useragent import UserAgentua = UserAgent()for i in range(10): print(ua.random)这里，使用了 ua.random 方法，可以随机生成各种浏览器的 UA，见下图：如果只想要某一个浏览器的，比如 Chrome ，那可以改成 ua.chrome，再次生成随机 UA 查看一下：以上就是常规设置随机 UA 的一种方法，非常方便。下面，我们来介绍在 Scrapy 中设置随机 UA 的几种方法。先新建一个 Project，命名为 wanojia，测试的网站选择为：http://httpbin.org/get。首先，我们来看一下，如果不添加 UA 会得到什么结果，可以看到显示了scrapy，这样就暴露了我们的爬虫，很容易被封。下面，我们添加上 UA 。▌直接设置 UA第一种方法是和上面程序一样，直接在主程序中设置 UA，然后运行程序，通过下面这句命令可以输出该网站的 UA，见上图箭头处所示，每次请求都会随机生成 UA，这种方法比较简单，但是每个 requests 下的请求都需要设置，不是很方便，既然使用了 Scrapy，它提供了专门设置 UA 的地方，所以接下来我们看一下如何单独设置 UA。1response.request.headers['User-Agent']▌手动添加 UA第二种方法，是在 settings.py 文件中手动添加一些 UA，然后通过 random.choise 方法随机调用，即可生成 UA，这种方便比较麻烦的就是需要自己去找 UA，而且增加了代码行数量。▌middlewares.py 中设置 UA第三种方法，是使用 fake-useragent 包，在 middlewares.py 中间件中改写 process_request() 方法，添加以下几行代码即可。12345from fake_useragent import UserAgentclass RandomUserAgent(object): def process_request(self, request, spider): ua = UserAgent() request.headers['User-Agent'] = ua.random然后，我们回到 settings.py 文件中调用自定义的 UserAgent，注意这里要先关闭默认的 UA 设置方法才行。1234DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'wandoujia.middlewares.RandomUserAgent': 543,&#125;可以看到，我们成功得到了随机 UA。▌一行代码设置 UA可以看到，上面几种方法其实都不太方便，代码量也比较多，有没有更简单的设置方法呢？有的，只需要一行代码就搞定，利用一款名为 scrapy-fake-useragent 的包。先贴一下该包的官方网址：https://pypi.org/project/scrapy-fake-useragent/，使用方法非常简单，安装好然后使用就行了。执行下面的命令进行安装，然后在 settings.py 中启用随机 UA 设置命令就可以了，非常简单省事。1pip install scrapy-fake-useragent1234DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, # 关闭默认方法 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400, # 开启&#125;我们输出一下 UA 和网页 Response，可以看到成功输出了结果。以上就是 Scrapy 中设置随机 UA 的几种方法，推荐最后一种方法，即安装 scrapy-fake-useragent 库，然后在 settings 中添加下面这一行代码即可：1'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,另外，反爬措施除了设置随机 UA 以外，还有一种非常重要的措施是设置随机 IP，我们后续再进行介绍。本文完。推荐阅读：爬虫断了？一招搞定 MongoDB 重复数据从函数 def 到类 Class从 类 Class 到 Scrapy]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫断了？一招搞定 MongoDB 重复数据]]></title>
    <url>%2F2018%2F12%2F19%2Fweb_scraping_withpython13.html</url>
    <content type="text"><![CDATA[MongoDB 避免插入重复数据。摘要：尽量使用 update_one() 方法而不是 insert_one() 插入数据。相信你一定有过这样的经历：大晚上好不容易写好一个爬虫，添加了种种可能出现的异常处理，测试了很多遍都没有问题，点击了 RUN 开始正式运行 ，然后美滋滋地准备钻被窝睡觉，睡前还特意检查了下确认没有问题，合上眼后期待着第二天起来，数据都乖乖地躺在 MongoDB 中。第二天早上一睁眼就满心欢喜地冲到电脑前，结果发现爬虫半夜断了，你气得想要砸电脑，然后你看了一下 MongoDB 中爬了一半的数据，在想是删掉重新爬，还是保留下来接着爬。到这儿问题就来了，删掉太可惜，接着爬很可能会爬到重复数据，虽然后期可以去重，但你有强迫症，就是不想爬到重复数据，怎么办呢？这就遇到了「爬虫断点续传」问题，关于这个问题的解决方法有很多种，不过本文主要介绍数据存储到 MongoDB 时如何做到只插入新数据，而重复数据自动过滤不插入。先来个简单例子，比如现在有两个 list ，data2 中的第一条数据和 data 列表中的第一条数据是重复的，我们想将这两个 list 依次插入 MnogoDB 中去， 通常我们会使用 insert_one() 或者 insert_many() 方法插入，这里我们使用 insert_one() 插入，看一下效果。123456789101112131415161718data = [&#123;'index':'A','name':'James','rank':'1' &#125;,&#123;'index':'B','name':'Wade','rank':'2' &#125;,&#123;'index':'C','name':'Paul','rank':'3' &#125;,]data2 = [&#123;'index':'A','name':'James','rank':'1' &#125;,&#123;'index':'D','name':'Anthony','rank':'4' &#125;,]import pymongoclient = pymongo.MongoClient('localhost',27017)db = client.Doubanmongo_collection = db.doubanfor i in data: mongo_collection.insert_one(i)插入第一个 list ：插入第二个 list ：你会发现，重复的数据 A 被插入进去了，那么怎么只插入 D，而不插入 A 呢，这里就要用到 update_one() 方法了，改写一下插入方法：12for i in data2: mongo_collection.update_one(i,&#123;'$set':i&#125;,upsert=True)这里用到了 $set 运算符，该运算符作用是将字段的值替换为指定的值，upsert 为 True 表示插入。这里也可以用 update() 方法，但是这个方法比较老了，不建议使用。另外尝试使用 update_many() 方法发现不能更新多个相同的值。12for i in data2: mongo_collection.update(i, i, upsert=True)下面举一个豆瓣电影 TOP250 的实例，假设我们先获取 10 个电影的信息，然后再获取前 20 个电影，分别用 insert_one() 和 update_one() 方法对比一下结果。insert_one() 方法会重复爬取 前 10 个电影的数据：update_one() 方法则只会插入新的 10 个电影的数据：这就很好了对吧，所以当我们去爬那些需要分页的网站，最好在爬取之前使用 update_one() 方法，这样就算爬虫中断了，也不用担心会爬取重复数据。​​代码实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsimport jsonimport csvimport pandas as pdfrom urllib.parse import urlencodeimport pymongoclient = pymongo.MongoClient('localhost', 27017)db = client.Doubanmongo_collection = db.doubanclass Douban(object): def __init__(self): self.url = 'https://api.douban.com/v2/movie/top250?' def get_content(self, start_page): params = &#123; 'start': start_page, 'count': 10 &#125; response = requests.get(self.url, params=params).json() movies = response['subjects'] data = [&#123; 'rating': item['rating']['average'], 'genres':item['genres'], 'name':item['title'], 'actor':self.get_actor(item['casts']), 'original_title':item['original_title'], 'year':item['year'], &#125; for item in movies] self.write_to_mongodb(data) def get_actor(self, actors): actor = [i['name'] for i in actors] return actor def write_to_mongodb(self, data): for item in data: if mongo_collection.update_one(item, &#123;'$set': item&#125;, upsert=True): # if mongo_collection.insert_one(item): print('存储成功') else: print('存储失败') def get_douban(self, total_movie): # 每页10条，start_page循环1次 for start_page in range(0, total_movie, 10): self.get_content(start_page)if __name__ == '__main__': douban = Douban() douban.get_douban(10)本文完。推荐阅读：从函数 def 到类 Class从 类 Class 到 Scrapy]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[盘点那些手机上绝对值得安装的 App（2）]]></title>
    <url>%2F2018%2F12%2F15%2Fweekly_sharing8.html</url>
    <content type="text"><![CDATA[这些良心 App 值得安装。这是「每周分享」的第 8 期，这一期的主题接着上一期：「介绍一些值得安装的手机 App 」。上一期介绍了前 3 排 共12 款 App，文章参见：盘点那些手机上绝对值得安装的 App这一期介绍下面图中的后 12 款。1. QQ 轻聊版QQ 现在大家用地越来越少了，平常主要用来接收一些群消息，但还是属于必安的 App。不过普通的 QQ 版本做得让人很不爽，各种乱七八糟的订阅号会不停地弹出消息，底部提供的选项卡也都没什么用，一个新闻选项卡充斥着无聊的新闻，如果稍微没点自制力，就会「中毒」，从此一发而不可收拾，另一个动态选项卡，给你一堆诸如空间动态、兴趣部落、微视、直播这些功能，我不知道还有多少人常使用这些，至少我是觉得很鸡肋，完全没有必要出现在一个聊天通讯软件中。其实，我们需要的是一个功能纯粹简单的 QQ ，能收发消息就行，其他统统不要。你可能会觉得我在说笑，你还别说，真的有这样的 QQ，下面就推荐这款我用了几年的「QQ 轻聊版」。顾名思义，就是很轻便的 QQ，没有乱七八糟的功能，专注于沟通聊天，上图直接对比两款 QQ 版本的界面就知道了。2. share 微博客户端很久没有刷微博的习惯了，为了这个 App 特地翻看了一下以前发的些微博，都是 2014 年的事了，那会儿还想当摄影师。如果你爱刷微博，那不妨试用下这款，和官方的微博版本相比这款第三方客户端轻便很多，颜值也高，用了它，能抵消一些微博海量信息带来的浮躁。3. 酷安这个 App 就不多说了，搞机爱好者的天堂，众多精品 App 都出自这个社区。4. 纯纯写作自从习惯使用 Markdown 以后， 电脑上的 Word 很少再打开，一直也希望在手机端也能够使用 Markdown ，找了很多 App 后，发现这款「纯纯写作」非常不错，除了 完美支持 Markdown 以外，还能够快速实现电脑和手机端的同步，如果你喜欢写作，那么用它会非常方便。5. 快图浏览如果要给相册类 App 排个名的话，「快图浏览」说第二，那没有 App 敢称自己是第一了。 只有 3M 大小的它，能够瞬间发现和加载上万张图片，如果你是拍照狂魔，用它打开再多的照片也能秒开，另外还拥有隐藏私密照片、自动备份百度网盘等功能，非常的实用。6. 静读天下「静读天下」这款 App 可以说是 离线文档阅读类 App 中最牛逼的了，如果你爱手机阅读电子书，那么一定不要错过它，我之前专门写过一篇文章来介绍它的一些实用功能：安卓最强阅读器如果想使用全部功能，需要付费购买专业版，也不贵，我这里直接是专业版，你懂的，如有条件尽量支持正版。7. WPS 阅读器我们平常在手机中接触到的很多文件都是 PDF 形式，除了阅读以外，有时还会有这些需求：转换为 Word、提取 PDF、合并 PDF、转为图片、拍照扫描等，那么，这款 「WPS」能够完美实现这些功能。如果你想了解更多 PDF 处理的软件，可以看我之前写的这篇文章：PDF 阅读处理软件，你需要的都在这里了8. 搜书大师如果你还在百度搜索下载电子书的话，那么已经非常 OUT 了，这款专门搜索下载电子书的 App 拥有海量的电子书资源，资源直接是百度资源，保存下载就能打开看了。9. 海贝音乐音乐类 App 网易云已经做得非常好了，但这里提这款「海贝音乐」是因为发现它有一个强大的功能是：能够自动识别不同文件夹下的音频文件然后播放，这有什么用呢，提示一下，如果你买了些网课，是分不同文件夹存在百度网盘里的，那么用它就对了。10. 绿色守护这款 App 是 这期介绍的唯一一款系统类软件，它的作用就是能够强制休眠 App，如果你发现的手机一天耗电很快，那很可能是因为太多垃圾 App 在后台自启动了，使用它能够强制休眠这些 App ，增强你手机的的续航能力。11. Poweramp如果你爱听歌，且对音质很讲究，那么你很可能会喜欢上这款「Poweramp」，使用它的 自定义均衡器设置，你完全可以调出各种音质。12. 咳咳这是最后一款 App，听名字你就应该很熟悉了，可以看到外面丰富多彩的世界。好，以上就是这一期介绍的 12 款佳软，如需可以在公众号后台回复「佳软2」得到部分，如果想获得全部，可以长按下方图片二维码加入我的知识星球：「第2脑袋」，里面有很多干货，期待你的到来。欢迎点赞、评论和分享，利他最终一定利己.推荐阅读：盘点那些手机上绝对值得安装的 AppScrapy 爬取并分析酷安 6000 款 App，找到良心佳软安卓最强阅读器PDF 阅读处理软件，你需要的都在这里了]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 Class 类到 Scrapy]]></title>
    <url>%2F2018%2F12%2F14%2Fweb_scraping_withpython12.html</url>
    <content type="text"><![CDATA[对于 Python 初学者来说，习惯使用函数写代码后，开始学 Scrapy 会感到很复杂，不知如何下手写代码，本文通过实际案例，对比普通函数（类）和 Scrapy 中代码的写法，助你快速入门 Scrapy。摘要：通过实际爬虫案例，分别用普通函数（类）和 Scrapy 进行实现，通过代码，助你快速入门 Scrapy。上一篇文章，我们通过 3 个实际爬虫案例，分别用函数（def）和 类（Class） 两种方法进行了实现，相信能够帮助你加深对类（Class）概念和用法的理解。在该文的第 3 个例子中，我们从类的写法延伸到了 Pyspider 中类代码的写法，本文进一步补充，通过实际爬虫案例分别用普通类的写法和 Scrapy 中类代码的写法进行实现。从函数 def 到类 ClassScrapy 爬虫框架非常强大，但是初学起来会觉得有点复杂，因为完整的一段代码需要拆分放在不同的模块下，比如写一个爬虫，原先我们只需要用函数或者类从头写到尾即可，一目了然，但是在 Scrapy 中则不同，我们首先要在 items.py 中定义爬取的字段内容，在主程序模块中编写爬虫主程序，在 pipeline.py模块中实现数据处理、存储，在 middlewares.py 模块中定义代理 IP、UA 等。总之代码的写法会发生一些变化，我在没适应用 Scrapy 之前，习惯在 Sublime 中完整地用函数实现一遍，然后再迁移到 Scrapy 框架中，虽然慢，但是写多几次后就适应了Scrapy 的写法，这比一上来就直接在 Scrapy 中写过渡地要顺利一些。好，下面我们就以之前一篇爬取酷安 App 的文章为例进行说明，这篇文章用了 Scrapy 来实现，下面再用普通的函数写法实现一遍，并对关键的地方进行一下对比。Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软▌爬取思路分析在上面这篇文章里，我面已经对 目标网站 进行了分析，这里简单回顾一下，便于把握后续的抓取思路。首先，网页请求是 GET 形式，URL 只有一个页数递增参数，构造翻页非常简单。每页显示了 10 条 App 信息，通过点击尾页，发现一共有 610 页，也就是说一共有 6100 款左右的 App 。接下来，我们需要进入每一个 App 的主页，抓取 App 相关字段信息，确定了 8 个关键字段，分别是：App 名称、下载量、评分、评分人数、评论数、关注人数、体积、App 分类标签。然后，打开网页后台，利用正则表达式、CSS分别提取每个字段的信息即可。如果你还不熟悉正则、CSS、Xpath 这几种网页内容提取方法，可以参考我早先总结的这篇文章：四种方法爬取猫眼 TOP100 电影通过上述分析，就可以确定爬取思路了：首先可以通过两种方式构造分页循环，一种是利用 for 循环直接构造 610 页 URL 链接，另外一种是获取下一页的节点，不断递增直到最后一页。第一种方式简单但只适合总页数确定的形式，第二种方式稍微复杂一点，但不管知不知道总页数都可以循环。接着，每页抓取 10 款 App URL，进入 App 详情页后，利用 CSS 语法抓取每个 App 的 8 个字段信息，最后保存到 MongoDB中，结果形式如下：下面我们就来实操对比一下。▌获取网页 Response首先，遍历每页的 URL 请求获得响应 Response，提取每款 App 主页的 URL 请求，以便下一步解析提取字段内容。def 写法：两次 for 循环，提取所有的 URL 链接，供下一步解析内容。12345678910111213141516headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125;def get_page(page): url = 'https://www.coolapk.com/apk?p=%s' %page response = requests.get(url,headers=headers).text content = pq(response)('.app_left_list&gt;a').items() urls = [] for item in content: url = urljoin('https://www.coolapk.com',item.attr('href')) urls.append(url) return urlsif __name__ == '__main__': for page in range(1, 610): get_page(page)Scrapy 写法：12345678910111213141516171819202122class KuspiderSpider(scrapy.Spider): name = 'kuspider' allowed_domains = ['www.coolapk.com'] start_urls = ['https://www.coolapk.com/apk/'] headers = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; def start_requests(self): pages = [] for page in range(1,610): url = 'https://www.coolapk.com/apk/?page=%s' % page page = scrapy.Request( url, callback=self.parse, headers=self.headers) pages.append(page) return pages def parse(self, response): contents = response.css('.app_left_list&gt;a') for content in contents: url = content.css('::attr("href")').extract_first() url = response.urljoin(url) yield scrapy.Request(url, callback=self.parse_url)这里有几点不同的地方，简单进行说明：循环构造方式不同普通函数用两个 for 循环就可以，Scrapy 中是构造最外层的循环，实现方法是先构造一个空列表，存放 page，URL 构造好之后通过 scrapy.Request () 方法进行请求，获得响应 response ，传递给 callback 参数指定的 parse() 方法，再进一步进行第二个 for 循环。内容提取形式不同以 CSS 语法提取为例，普通函数和 Scrapy 中内容提取的方法稍有不同， 下面以提取提取单个节点文本、提取属性、提取多个节点，这三种最为常见的提取形式为例，将普通函数和 Scrapy 的写法进行对比：12345678910#提取单个节点文本name = item('.list_app_title').text()name = item.css('.detail_app_title::text').extract_first()#提取属性url = item('.app_left_list&gt;a').attr('href')url = item.css('::attr("href")').extract_first()#提取多个节点content = pq(response)('.app_left_list&gt;a').items() contents = response.css('.app_left_list&gt;a')这里顺便再说一下 Scrapy 遍历分页的第二种方式。如果不通过构造 for 循环的方式遍历，可以先请求第一页获得 response 进行解析，然后再获取下一页 url 重复调用解析方法，直到解析完最后一页为止，这种方法 start_requests 构造就很简单，直接传递 url 到下一个 parse() 方法即可。12345678910111213141516def start_requests(self): url = 'https://www.coolapk.com/apk/?page=1' yield scrapy.Request( url, callback=self.parse, headers=self.headers) pages.append(page) return pagesdef parse(self, response): contents = response.css('.app_left_list&gt;a') for content in contents: url = content.css('::attr("href")').extract_first() url = response.urljoin(url) yield scrapy.Request(url, callback=self.parse_url) next_page = response.css('.pagination li:nth-child(8) a::attr("href")').extract_first() url = response.urljoin(next_page) yield scrapy.Request(url,callback=self.parse )▌解析网页提取字段接下来，我们就要提取App 名称、下载量、评分这些字段信息了。def 写法：1234567891011121314151617181920212223242526def parse_content(urls): lst = [] for url in urls: response = requests.get(url,headers=headers).text doc = pq(response) # pyquery解析 name = doc('.detail_app_title').remove('span').text() # 若不想要子节点文本则先去除掉 results = get_comment(doc) tags = get_tags(doc) score = doc('.rank_num').text() # 评论数 num_score = doc('.apk_rank_p1').text() num_score = re.search('共(.*?)个评分',num_score).group(1) data =&#123; 'name':name, 'volume':results[0], 'download':results[1], 'follow':results[2], 'comment':results[3], 'tags':str(tags), 'score':score, 'num_score':num_score &#125; lst.append(data) data = pd.DataFrame(lst) return data这里，值得注意一点：pyquery 提取文本的时候，默认会提取节点内所有的文本内容，如果你只想要其中某个节点的，那么最好先删除掉不需要的节点，再提取文本。比如这里，我们在提取 app 名称的时候，如果直接用：1name = doc('.detail_app_title')text()提取出来的则是「酷安 8.8.3」，如果只想要「酷安」，不想要下面的版本信息：8.8.3，需要删除子节点 span 后再提取：1name = doc('.detail_app_title').remove('span').text()Scrapy 写法：获取字段信息，我们需要现在 settings.py 中设置，然后才能提取。12345678910class KuanItem(scrapy.Item):# define the fields for your item here like:name = scrapy.Field()volume = scrapy.Field()download = scrapy.Field()follow = scrapy.Field()comment = scrapy.Field()tags = scrapy.Field()score = scrapy.Field()num_score = scrapy.Field()回到主程序中，通过 item = Kuan2Item() 来调用上面定义的字段信息。1234567891011121314151617181920def parse(self, response): contents = response.css('.app_left_list&gt;a') for content in contents: url = content.css('::attr("href")').extract_first() url = response.urljoin(url) yield scrapy.Request(url, callback=self.parse_url)def parse_url(self, response): item = Kuan2Item() item['name'] = response.css('.detail_app_title::text').extract_first() results = self.get_comment(response) item['volume'] = results[0] item['download'] = results[1] item['follow'] = results[2] item['comment'] = results[3] item['tags'] = self.get_tags(response) item['score'] = response.css('.rank_num::text').extract_first() num_score = response.css('.apk_rank_p1::text').extract_first() item['num_score'] = re.search('共(.*?)个评分', num_score).group(1) yield item▌存储到 MongoDB提取完信息以后，我们便可以选择将数据存储到 MongoDB 中。通过上面的方法，我们提取出了字段内容 data，然后转换为了 DataFrame，DataFrame 存储到 MongoDB 非常简单，几行代码就能搞定。def 写法：1234567client = pymongo.MongoClient('localhost',27017)db = client.KuAnmongo_collection = db.kuandef save_file(data): content = json.loads(data.T.to_json()).values() if mongo_collection.insert_many(content): print('存储到 mongondb 成功')这里用了 inset_many () 方法来插入数据，但其实不太建议，因为一旦出现爬虫中断，我们再接着爬的时候，它会插入重复数据，虽然我们可以再后续处理时去除重复数据，但有更好的方法，那就是用 update_one() 方法，该方法能够保证直插入新数据，重复数据不插入，下面我们在 Scrapy 中使用：Scrapy 写法：123456789101112131415161718192021222324class MongoPipeline(object): def __init__(self,mongo_url,mongo_db): self.mongo_url = mongo_url self.mongo_db = mongo_db @classmethod def from_crawler(cls,crawler): return cls( mongo_url = crawler.settings.get('MONGO_URL'), mongo_db = crawler.settings.get('MONGO_DB') ) def open_spider(self,spider): self.client = pymongo.MongoClient(self.mongo_url) self.db = self.client[self.mongo_db] def process_item(self,item,spider): name = item.__class__.__name__ # update_one 方法可以不插入重复内容 self.db[name].update_one(item, &#123;'$set': item&#125;, upsert=True) return item def close_spider(self,spider): self.client.close()简单说明几点：from crawler() 是一个类方法，用 ＠class method 标识，这个方法的作用主要是用来获取我们在 settings.py 中设置的这几项参数：12345MONGO_URL = 'localhost'MONGO_DB = 'KuAn'ITEM_PIPELINES = &#123; 'kuan.pipelines.MongoPipeline': 300,&#125;open_spider() 方法主要进行一些初始化操作 ，在 Spider 开启时，这个方法就会被调用 。process_item() 方法是最重要的方法，实现插入数据到 MongoDB 中。Scrapy 字段提取后，通过 yield 返回的是生成器，内容是单个字典信息，此时，我们可以下面这句代码，实现只插入新数据，忽略重复数据。1self.db[name].update_one(item, &#123;'$set': item&#125;, upsert=True)以上，我们从获取网页 Response、解析内容、MongoDB 存储三个方面，对比了普通函数和 Scrapy 代码的写法，这三部分内容是多数爬虫的主要部分。当然，还有其他的内容比如：下载图片、反爬措施等，我们留在后续的 Scrapy 文章中继续介绍。如需完整代码，可以加入我的知识星球「第2脑袋」获取，里面有很多干货，期待你的到来。本文完。推荐阅读：从函数 def 到类 ClassScrapy 爬取并分析酷安 6000 款 App，找到良心佳软]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[盘点那些手机上绝对值得安装的 App]]></title>
    <url>%2F2018%2F12%2F08%2Fweekly_sharing7.html</url>
    <content type="text"><![CDATA[这些良心 App 值得安装。这是「每周分享」的第 7 期，这一期的主题是「介绍手机上一些值得安装的 App 」。最近写了一篇爬酷安 App 的文章：Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软文章里面总结了一些非常良心好用的 App，堪称「神器」，我从里面优中选优，挑选了 24 款佳软（见下面这张图），好东西就要拿出来分享，所以今天就和你来介绍一下它们各自的功能特点，希望里面刚好有你需要的。这一期先介绍一半，也就是前 3 排 共12 款 App，剩下的留到下一期，可能有些你正在用或者用过，或者你有用到更好的 App，不妨留言告诉我。1. 存储空间清理先来第 1 款系统空间清理软件，这款 App 做得很精致，文件扫描和分析功能很强大，能够快速帮你到手机里的垃圾文件、大型文件，然后轻松有效释放手机空间，3.5 元就可以升级解锁所有功能，良心作者。2. 百度网盘第 2 款属于重磅 App，不用过多介绍。以前的网盘还算良心，现在，呵呵。不是会员的话，下载速度低地可怜，这就催生一波不可描述版本，比如这款 SVIP 版，下载速度杠杠地，100K/s 和 3M/s 的差距。有人可能会说不安全会限速封号，反正我用了几个月至今木有问题。3. 山寨云第 3 款 App 也是和百度网盘有关，看来树大招风啊，尤其是歪的树，它的名字够爽快直白「就山寨你」，但功能可一点不含糊，最犀利的就是下载速度快，轻松达到 4M/s。另外一个牛逼的功能就是提供了多个网盘搜索源，可以直接搜索任何网盘资源，太良心了有木有。4. 强力监测这一款超高颜值的 App 主要用来监测你的手机运行状况，可视化查看手机温度、CPU占用、内存空间、电量等信息，一目了然。5. ES 文件浏览器接下来这款「ES 文件浏览器」是手机文件管理浏览类 App，功能非常强大，随着手机使用越久，内存空间就被各种文档、垃圾文件占满了，用它能快速清理手机空间。另外它能将手机内的文档进行归档，比如图片、音乐、视频、文件。这就很方便，比如我最近看了「霸王别姬」后，被张国荣圈粉，于是下了很多他的音乐，很方便地就能在这个 App 中查看到。6. Root Explorer这款 Root Explorer 我们在之前的文章分析中就曾多次提及，最牛逼的文件管理类 App 之一，能够卸载手机内置的 App，好东西不嫌多，强烈推荐，可以配合 ES 文件浏览器使用。7. 一个木函这款叫做 「一个木函」名字有点怪，但堪称「真神器」，不到 3M 大小的它拥有几十种实用强大的黑科技功能。随便介绍两个功能，「电量伪装」有什么用呢，比如有时你想找借口停止聊天或者挂人电话，就可以「手机没电」为由，修改手机电量，然后发个图给他看，表明没有「忽悠」他。「网速测试」功能很简单了，轻轻一点就能测出当前手机的网速。其他诸如：查快递、查 WIFI 密码、制作微信表情包、带壳截图这些功能都太实用了有木有。8. MX 播放器下面这款「MX 播放器」可以说是最好用的视频播放器了，堪称手机中里的 potplayer，简洁地不能再简洁了，各种操作都可以通过手势来完成。觉得最好用的一项功能是结合前面的「ES 文件浏览器」然后加速播放百度网盘里的视频，也许你可能不太明白什么意思，简单来说就是百度网盘看视频不支持加速播放，很蛋疼对吧，但这款播放器就弥补了这个缺点，它最高支持 4 倍播放速度，原本 1 个小时的视频，用它只需要 15 分钟就能看完，时间就是金钱啊。9. Via下面要强烈介绍的是两款浏览器 App，第一款 Via 堪称众多浏览器中的佼佼者，只有 500K 大小的它，简洁清爽，功能却一点不含糊，甚至比很多体积大得多的浏览器功能都要强大。介绍下它的特点，就是提供了很多插件，插件就是外挂啊，比如知乎去网页限制、各大平台音乐下载、抖音下载等，以知乎这个插件来解释下什么意思，我们知道一般浏览器查看知乎内容时，如果想看全文，它会强制你下载知乎 App 才能看，很麻烦对不对，但是用这个浏览器，你就不用下载 App 了，能够直接看全文内容，很爽对吧。其他平台的 App 都类似，也就是说 用了这个浏览器，能让你少安装很多 App。10. X 浏览器第 2 款浏览器「X 浏览器」，是真牛 X，同样的走简洁风，提供的功能很多包括：电脑桌面模式、护眼模式、无痕模式等，说说「无痕模式」，它可以隐藏你的网页浏览痕迹，假如你想上点不可描述的网站，也不会怕别人翻手机看到。再说一个「拦截广告」功能，它可以智能拦截一些广告，比如百度搜索「python 书」，普通模式就会弹出很多广告，而使用了拦截广告功能，就看不到那些烦人的广告了，很爽对吧。11. 幸运破解器接下来推荐一款很有意思的 App「幸运破解器」，名字起得很贴切，它可以对一些 App 进行破解，比如去除广告、破解付费内容等，很强大，你可以去一个个去尝试想要破解的 App，运气好的话就能成功破解。12. TickTick最后一款 「TickTick」 是时间任务管理类 App，中文版叫「滴答清单」，很好用，提供的功能非常多。如果你想成为一个时间管理高手，那这款 App 能帮到你，但是它的很多功能都是付费的，价格还不菲。如果你细心的话，可以看到图中这款我用的 App 截图箭头所指的地方，还剩 17000 多天的试用期，折算就是差不多 50 年，嘿嘿。以上就是这一期介绍的 12 款佳软，如需可以在公众号后台回复「佳软」得到部分，如果想获得全部，可以长按下方图片二维码加入我的知识星球：「第2脑袋」，里面有很多干货，期待你的到来。欢迎点赞、评论和分享，利他最终一定利己.]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从函数 def 到类 Class]]></title>
    <url>%2F2018%2F12%2F07%2Fweb_scraping_withpython11.html</url>
    <content type="text"><![CDATA[实例对比，快速上手 Python 类（Class）和 Pyspider 代码的写法。摘要：初学 Python 过程中，我们可能习惯了使用函数（def），在开始学习类（Class）的用法时，可能会觉得它的写法别扭，类的代码写法也不像函数那么简单直接，也会产生「有了函数为什么还需要类」的疑问。然而面向对象编程是 Python 最重要的思想，类（Class）又是面向对象最重要的概念之一，所以要想精通 Python ，则必须得会使用类（Class）来编写代码，而且 Pyspider 和 Scrapy 两大框架都使用了类的写法，基于此，本文将介绍如何从函数的写法顺利过渡到类的编写习惯。关于类（Class）的教程，网上主要有两类，一类是廖雪峰大佬的，另一类是不加说明默认你已经会这种写法，而直接使用的。廖雪峰的教程非常棒，但是更适合入门 Python 有一段时间或者看过一些更基础的教程之后再回过头来看，否则可能会觉得他的教程理论性重于实用性。我第一次看了他的教程中关于类的相关知识后，觉得理解了，但一尝试自己写时就不太会了。第二类教程，网上有很多案例，这类教程存在的问题就是，你能看懂意思，但还是不太会运用到自己的案例中。总结一下这两类教程对新手不友好的地方就是 没有同时给出两种写法的实例，这就没办法对比，而「对比学习」是一种学习新知识非常快的途径，简单来说就是学新知识的时候，先从我们已经掌握的知识出发，和新知识进行对比，快速找到共同点和不同点，共同点我们能很快掌握，针对不同点通过对比去感悟领会，从而快速学会新知识。接下来，就举几个同时使用了函数写法和类的写法的案例，希望能够帮助你快速完成从函数到类的编程思想的过渡转换。▌爬取豆瓣电影 TOP250第一个案例：爬取豆瓣电影 TOP250，我们的目标是通过调用豆瓣 API 接口，获取电影名称、评分、演员等信息，然后存储到 CSV 文件中，部分代码如下：12345678910111213141516171819202122232425262728293031323334def get_content(start_page): url = 'https://api.douban.com/v2/movie/top250?' params = &#123; 'start':start_page, 'count':50 &#125; response = requests.get(url,params=params).json() movies = response['subjects'] data = [&#123; 'rating':item['rating']['average'], 'genres':item['genres'], 'name':item['title'], 'actor':get_actor(item['casts']), 'original_title':item['original_title'], 'year':item['year'], &#125; for item in movies] write_to_file(data)def get_actor(actors): actor = [i['name'] for i in actors] return actordef write_to_file(data): with open('douban_def.csv','a',encoding='utf_8_sig',newline='') as f: w = csv.writer(f) for item in data: w.writerow(item.values())def get_douban(total_movie): # 每页50条，start_page循环5次 for start_page in range(0,total_movie,50): get_content(start_page)if __name__ == '__main__': get_douban(250)打开 CSV 文件查看输出的结果：以上，我们通过四个函数就完成了数据的爬取和存储，逻辑很清晰，下面我们使用类的写法实现同样的功能，部分代码如下：12345678910111213141516171819202122232425262728293031323334353637class Douban(object): def __init__(self): self.url = 'https://api.douban.com/v2/movie/top250?' def get_content(self,start_page): params = &#123; 'start':start_page, 'count':50 &#125; response = requests.get(self.url,params=params).json() movies = response['subjects'] data = [&#123; 'rating':item['rating']['average'], 'genres':item['genres'], 'name':item['title'], 'actor':self.get_actor(item['casts']), 'original_title':item['original_title'], 'year':item['year'], &#125; for item in movies] self.write_to_file(data) def get_actor(self,actors): actor = [i['name'] for i in actors] return actor def write_to_file(self,data): with open('douban_class.csv','a',encoding='utf_8_sig',newline='') as f: w = csv.writer(f) for item in data: w.writerow(item.values()) def get_douban(self,total_movie): # 每页50条，start_page循环5次 for start_page in range(0,total_movie,50): self.get_content(start_page)if __name__ == '__main__': douban = Douban() douban.get_douban(250)可以看到，上面的案例中，类的写法和函数的写法大部分都是一样的，仅少数部分存在差异。主要有这么几点差异：增加了一个 __init__函数。这是一个特殊的函数，它的作用主要是事先把一些重要的属性填写进来，它的特点是第一个参数永远是self，表示创建的实例本身，这里的实例就是最下面的 douban（实例通过类名+() 创建）。类中的函数和普通的函数相比，只有一点不同。类中的函数（也称为方法）的第一个参数永远是实例变量self，并且调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别。在执行类的时候需要先实例化这里我们定义了一个类，类名是 Douban（首字母要大写），在运行类的时候，需要先实例化，这里实例化为 douban，然后调用 get_douban（）方法完成数据的爬取和存储。下面，我们再来看看第二个例子。▌模拟登陆 IT 桔子我们使用 Selenium 模拟登陆 IT 桔子网并输出网页源码，使用函数的部分代码如下：1234567891011121314151617def login(): browser = webdriver.Chrome() browser.get('https://www.itjuzi.com/user/login') account = browser.find_element(By.ID, "create_account_email") password = browser.find_element(By.ID, "create_account_password") account.send_keys('irw27812@awsoo.com') # 输入账号和密码 password.send_keys('test2018') # 输入账号密码 submit = browser.find_element(By.ID,"login_btn") submit.click() # 点击登录按钮 get_content()def get_content(): browser.get('http://radar.itjuzi.com/investevent') print(browser.page_source) # 输出网页源码if __name__ == '__main__': login()以上代码实现了自动输入账号密码然后进入 IT 桔子网，下面我们再来看看类的写法：1234567891011121314151617181920212223242526class Spider(object): def __init__(self,account,password): self.login_url = 'https://www.itjuzi.com/user/login' self.get_url = 'http://radar.itjuzi.com/investevent' self.account = account self.password = password def login(self): browser.get(self.login_url) account = browser.find_element(By.ID, "create_account_email") password = browser.find_element(By.ID, "create_account_password") account.send_keys(self.account) # 输入账号和密码 password.send_keys(self.password) submit = browser.find_element(By.ID,"login_btn") submit.click() # 点击登录按钮 self.get_content() # 调用下面的方法 def get_content(self): browser.get(self.get_url) print(browser.page_source) # 输出网页源码if __name__ == '__main__': spider = Spider(account='irw27812@awsoo.com',password='test2018') # 当有其他账号时，在这里更改即可,很方便 # spider = Spider('fru68354@nbzmr.com','test2018') spider.login()这里，我们将一些固定的参数，比如 URL、Headers 都放在 __init__ 方法中，需要的时候在各个函数中进行调用，这样的写法逻辑更加清晰。下面，我们再看看第三个例子爬取虎嗅文章，从普通类的写法过渡到 pyspider 框架中类的写法，这样有助于快速上手 pyspider 框架。▌爬取虎嗅文章我们目标是通过分析 AJAX 请求，遍历爬取虎嗅网的文章信息，先来看看普通类的写法，部分代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950client = pymongo.MongoClient('localhost',27017)db = client.Huxiumongo_collection = db.huxiu_newsclass Huxiu(object): def __init__(self): self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', 'X-Requested-With': 'XMLHttpRequest' &#125; self.url = 'https://www.huxiu.com/v2_action/article_list' def get_content(self,page): data = &#123; 'page': page, &#125; response = requests.post(self.url,data=data,headers=self.headers) self.parse_content(response) def parse_content(self,response): content = response.json()['data'] doc = pq(content) lis = doc('.mod-art').items() data =[&#123; 'title': item('.msubstr-row2').text(), 'url':'https://www.huxiu.com'+ str(item('.msubstr-row2').attr('href')), 'name': item('.author-name').text(), 'write_time':item('.time').text(), 'comment':item('.icon-cmt+ em').text(), 'favorites':item('.icon-fvr+ em').text(), 'abstract':item('.mob-sub').text() &#125; for item in lis] self.save_to_file(data) # 存储到 mongodb def save_to_file(self,data): df = pd.DataFrame(data) content = json.loads(df.T.to_json()).values() if mongo_collection.insert_many(content): print('存储到 mongondb 成功') else: print('存储失败') def get_huxiu(self,start_page,end_page): for page in range(start_page,end_page) : print('正在爬取第 %s 页' % page) self.get_content(page)if __name__ == '__main__': huxiu = Huxiu() huxiu.get_huxiu(1,2000)然后再看看在 Pyspider 中的写法：123456789101112131415161718192021222324252627282930313233343536373839404142class Handler(BaseHandler): crawl_config:&#123; "headers":&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', 'X-Requested-With': 'XMLHttpRequest' &#125; &#125; # 修改taskid，避免只下载一个post请求 def get_taskid(self,task): return md5string(task['url']+json.dumps(task['fetch'].get('data',''))) def on_start(self): for page in range(1,2000): print('正在爬取第 %s 页' % page) self.crawl('https://www.huxiu.com/v2_action/article_list',method='POST',data=&#123;'page':page&#125;, callback=self.index_page) def index_page(self, response): content = response.json['data'] # 注意，在上面的class写法中，json后面需要添加()，pyspider中则不用 doc = pq(content) lis = doc('.mod-art').items() data = [&#123; 'title': item('.msubstr-row2').text(), 'url':'https://www.huxiu.com'+ str(item('.msubstr-row2').attr('href')), 'name': item('.author-name').text(), 'write_time':item('.time').text(), 'comment':item('.icon-cmt+ em').text(), 'favorites':item('.icon-fvr+ em').text(), 'abstract':item('.mob-sub').text() &#125; for item in lis ] return data def on_result(self,result): if result: self.save_to_mongo(result) def save_to_mongo(self,result): df = pd.DataFrame(result) content = json.loads(df.T.to_json()).values() if mongo_collection.insert_many(content): print('存储到 mongondb 成功')可以看到，pyspider 中主体部分和普通类的写法差不多，不同的地方在于 pyspider 中有一些固定的语法，这可以通过参考 pyspider 教程快速掌握。通过以上三个例子的对比，我们可以感受到函数（def）、 类（Class）和 pyspider 三种代码写法的异同点，采取这样对比式的学习能够快速掌握新的知识。如需完整代码，可以加入我的知识星球「第2脑袋」获取，里面有很多干货，期待你的到来。本文完。推荐阅读：Selenium 爬取 IT 桔子创业公司信息pyspider 爬取并分析虎嗅网 5 万篇文章]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活随笔：27 岁，强烈的中年危机感]]></title>
    <url>%2F2018%2F12%2F03%2Flife03.html</url>
    <content type="text"><![CDATA[加油吧，骚年。今天，刚刚好 27 岁。今年之前，一直都还觉得自己很年轻，恍恍惚惚地觉得才刚刚跨出校门不久，加上时常被人问起「你是 90 后吧」心中还不免一喜，又被提醒了一次「在别人眼中，自己还很年轻」。今年之前，每年的生日，都是在 20 岁的基础上往上加 1 岁，21 岁的时候觉得仅仅是刚刚过了 20 岁，还小；22 岁的时候想着昨天还只是 21 岁，还早。如此往复，渐渐就有了一种错觉，以至于 26 岁的时候觉得自己好像还只是 22、23 岁。到现在，也仅仅只毕业了一年半而已，一年半之前，却还时常厌倦 7 年的校园生活，迫不及待地想要进入社会、进入职场。17 岁的时候，想快点 20 岁，27 岁的时候，却害怕 30 岁。▌以前的日子太顺风顺水了到了今年，到了 27 岁，才慢慢意识到，原来之所以以前会有「自己很年轻」的错觉是因为 日子过得太顺风顺水了，因为 对自己太随便了，因为 对自己一点都不狠。7 年的校园生活，搬过两次校区、做过家教、进入了卓越工程师班、保了研、穷游了 70 天、加入过校学生会、给老板打工做过项目。这一年半却不知不觉辗转了 4 个地方，成都、东莞、深圳和北京，换了两份工作，经历了 职场新人到无业游民。▌以前对自己太随便了2010 年高中毕业，填报高考志愿时仅仅因为不知道对什么感兴趣，因为周边人说「不要学计算机、软件，是烂大街的专业」，所以就随便在志愿上填报了「水利工程」专业，结果一上大一就后悔了。到大一下可以转专业的时候，心想转到其他学院，意味着要补上落了一年的课，大二就要比别人辛苦两倍，还是算了，不要那么辛苦，好好摆正心态，此前对本专业不感兴趣肯定是自己不够努力，于是后面进入了卓越工程师班。大三想跨专业考研，后面有了保研机会，心想能保研干嘛还考研，研究生本硕同专业肯定比跨专业轻松，于是就保了研，后面的半年无心学术，做家教攒够了钱，毕业出国穷游了 70 天。研一，导师让学 Fortran 编程，当时觉得编程太枯燥没什么用，跟老板扭了一阵后，改学使用现成的商业模型，直到毕业，才发现就只会用几个软件模型。而这一年半的收获，我觉得比前 7 年都大。前一年的两份工作中，有了大量使用 Excel 的机会，就此慢慢产生了兴趣，开始学函数、数据透视表、VBA，兴趣愈发浓烈，便又接触使用了 POWER BI，尝试做了酷炫的可视化报表，后期利用简单的数据抓取功能实现了一个股票数据的爬虫，很是兴奋，再往后很快就发现它的功能不够强大，通过搜索知道了 R 语言可以做爬虫，便上手开始学习，这才是第一次主动接触编程，之后发现 Python 比 R 更强大，便又开始学 Python，直到现在。▌ 以前对自己一点都不狠回想大学期间，上课基本上都是在玩手机，不喜欢的课甚至干脆不去，没课的时候都去兼职，总之就是怎么舒服、爽快怎么来。对未来做什么工作没有过规划，甚至连校招找工作时都没有。那时候总以为 时间是用不完的，船到桥头自然会直的。这半年，辞了职后下定决心放手一搏，准备跨行。于是便从零开始学编程、购买书和网课、加入付费社群，接触到了一群 95 后。开始学爬虫、建博客、更新公众号，直到现在。「只要出发了，一切都不算晚。」本文完。]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（分析篇）]]></title>
    <url>%2F2018%2F12%2F02%2Fdata_analysis%26mining02.html</url>
    <content type="text"><![CDATA[发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。摘要： 如今移动互联网越来越发达，我们每个人的手机上至少都安装了好几十款 App，随着各式各样的 App 层出不穷，也就产生了优劣之分，而我们肯定愿意去使用那些良心佳软，而如何去发现这些 App 呢，本文使用 Scrapy 框架爬取了著名应用下载市场「酷安网」上的 6000 余款 App，通过分析，发现了各个类别领域下的佼佼者，这些 App 堪称真正的良心心之作，使用它们将会给你带来全新的手机使用体验。上一篇文章我们完成了 App 数据的抓取工作，这一篇文章我们将对这些数据进行探索性分析。1. 总体情况我们主要从总体和分类两个维度对 App 下载量、评分、体积等指标进行分析。1.1. 下载量排名首先来看一下 App 的下载量情况，很多时候我们下载一个 App ，下载量是一个非常重要的参考指标，由于绝大多数 App 的下载量都相对较少，直方图无法看出趋势，所以我们择将数据进行分段，离散化为柱状图，绘图工具采用的是 Pyecharts。可以看到多达 5517 款（占总数 84%）App 的下载量不到 10 万， 而下载量超过 500 万的仅有 20 款，开发一个要想盈利的 App ，用户下载量尤为重要，从这一点来看，大部分 App 的处境都比较尴尬，至少是在酷安平台上。代码实现如下：12345678910111213141516from pyecharts import Bar# 下载量分布bins = [0,10,100,500,10000]group_names = ['&lt;=10万','10-100万','100-500万','&gt;500万']cats = pd.cut(df['download'],bins,labels=group_names) # 用 pd.cut() 方法进行分段cats = pd.value_counts(cats)bar = Bar('App 下载数量区间分布','绝大部分 App 下载量低于 10 万')# bar.use_theme('macarons')bar.add( 'App 数量 (个)', list(cats.index), list(cats.values), is_label_show = True, is_splitline_show = False,)bar.render(path='download_interval.png',pixel_ration=1)接下来，我们看看 下载量最多的 20 款 App 是哪些：可以看到，这里「酷安」App 以 5000 万+ 次的下载量遥遥领先，是第二名微信 2700 万下载量的近两倍，这么巨大的优势也很容易理解，毕竟是自家的 App，如果你手机上没有「酷安」，说明你还不算是一个真正的「搞机爱好者」，从图中我们还可以看出以下几点信息：TOP 20 款 App 中，很多都是 装机必备，算是比较大众型的 App。右侧 App 评分图中可以看到仅有 5 款 App 评分超过了 4 分（5 分制），绝大多数的评分都不到 3 分，甚至到不到 2 分，到底是因为这些 App 开发者做不出好 App 还是根本不想做出来？相较于其他 App，RE 管理器、绿色守护 这几款非常突出，其中 RE 管理器在如此高的下载量下，仍然能够得到 4.8 分（最高分）并且体积只有几 M，实属难得，什么是「良心 App」，这类就是。作为对比，我们再看一下 下载量最少的 20 个 App。可以看到，与上面的那些下载量多的 App 相比，这些就相形见绌了，下载量最少的 「广州限行通」更是只有 63 次下载。这也不奇怪，可能是 App 没有宣传、也可能是刚开发出来，这么少的下载量评分还不错，也还能继续更新，为这些开发者点赞。其实，这类 App 不算囧，真正囧的应该是那些 下载量很多、评分却低到不能再低 的 App，给人的感觉是：「我就这么烂，爱咋咋地有本事别用」。1.2. 评分排名接下来，我们看看 App 的总体得分情况。这里，将得分分为了以下 4 个区间段，并且为不同分数定义了相应的等级。可以发现这么几点有意思的现象：3 分以下的软件非常少，只占不到 10%，而之前下载量最多的 20 款 APP 中，微信、QQ、淘宝、支付宝等大多数软件的得分都不到 3 分，这就有点尴尬了。中品也就是中等得分的 App 数量最多。4 分以上的 高分 APP 数量占了近一半（46%），可能是这些 App 的确还不错，也可能是由于评分数量过少，为了优中选优，后续有必要设置一定筛选门槛。接下来，我们看看评分最高的 20 款 App 有哪些，很多时候我们下载 App 都是跟着「哪个评分高，下载哪个」这种感觉走。可以看到，评分最高的 20 个 App，它们都得到了 4.8 分 ，包括：RE 管理器（再次出现）、Pure 轻雨图标包等，还有一些不太常见，可能这些都是不错的 App，不过我们还需要结合看一下下载量，它们的下载量都在 1 万以上，有了一定的下载量，评分才算比较可靠，我们就能放心的下载下来体验一下了。经过上面的总体分析，我们大致发现了一些不错的 App ，但还不够，所以接下来将进行细分并设置一定筛选条件。2. 分类情况按照 App 功能和日常使用场景，将 App 分为以下 9 大类别，然后 从每个类别中筛选出 20 款最棒的 App。为了尽可能找出最好的 App，这里不妨设置 3 个条件：评分不低于 4 分下载量不低于 1 万设置一个总分评价指标（总分 = 下载量 * 评分），再标准化为满分 1000 分，作为 App 的排名参照指标。经过评选之后，我们依次得到了各个类别下分数最高的 20 款 App，这些 App 大部分的确是良心软件。2.1. 系统工具系统工具包括了：输入法、文件管理 、系统清理、桌面、插件、锁屏等。可以看到，第一名是大名鼎鼎的老牌文件管理器「RE 管理器」，仅有 5 M 大小的它除了具备普通文件管理器的各项功能以外，最大的特点是能够卸载手机自带的 App，不过需要 Root。「ES 文件浏览器」的文件分析器功能非常强大，能够有效清理臃肿的手机空间。「一个木函」这款 App 就比较牛逼了，正如它的软件介绍「拥有很多，不如有我」所说，打开它你能发现它提供了好几十项实用功能，比如：翻译、以图搜图、快递查询、制作表情包等等。再往下的「Super SU」、「存储空间清理」、「镧」、「MT 管理器」、「My Android Tools」都力荐，总之，这份榜单上的 App 可以说都值得进入你的手机 App 使用名单。2.2. 社交聊天社交聊天类中， 「Share 微博客户端」位居第一，作为一款第三方客户端 App，它自然有比官方版本好的地方，比如相比正版 70M 的体积，它只有其十分之一大小，也几乎没有广告，还有额外强大的诸多功能，如果你爱刷微博，那么不妨尝试下这款「Share」。「即刻」这款 App 也相当不错，再往下还能看到前阵子很火的「子弹短信」，宣称将要取代微信，看来短期内应该是做不到了。你可能会发现，这份社交榜单上没有出现「知乎」、「豆瓣」、「简书」这类常见的 App，是因为它们的评分都比较低，分别只有 2.9分、3.5分和 2.9 分，自然进入不了这份名单，如果你一定想用它们，推荐去使用它们的第三方客户端或者历史版本。2.3. 资讯阅读可以看到，在资讯阅读类中，「静读天下」牢牢占据了第一名，我之前专门写过一篇文章介绍它：安卓最强阅读器。同类别中的「多看阅读」、「追书神器」、「微信读书」也都进入了榜单。另外，如果你经常为不知道去哪里下载电子书而头疼，那不妨试一下「搜书大师」、「老子搜书」。2.4. 影音娱乐接下来是影音娱乐版块，网易家的「网易云音乐」毫无压力地占据头名，难得的大厂精品。如果你爱玩游戏，那么 「Adobe AIR」应该尝试一下。如果你很文艺，那么应该会喜欢「VUE」这款短视频拍摄 App，创作好以后发到朋友圈绝对能装逼。最后一位的「海贝音乐」很赞，最近发现它有一个强大的功能是结合百度网盘使用，它能够自动识别音频文件然后播放。2.5. 通讯网络下面到了通讯网络类别，这个类别主要包括：浏览器、通讯录、通知、邮箱等小类。浏览器，我们每个人手机上都有，用的也五花八门，有些人就用手机自带的浏览器，有些人用 Chrome、火狐这类大牌浏览器。不过你会发现榜单上的前三位你可能听都没听过，但是它们真的很牛逼，用「极简高效、清爽极速」来形容再适合不过，其中 「Via 」和 「X 浏览器」 体积不到 1M ，真正的「麻雀虽小、五脏俱全」，强烈推荐。2.6. 摄影图片拍照修图也是我们常用的功能。也许你有自己的图片管理软件，但是这里要强烈推荐第一名「快图浏览」这款 App，只有 3M 大小的它，能够瞬间发现和加载上万张图片，如果你是拍照狂魔，用它打开再多的照片也能秒开，另外还拥有隐藏私密照片、自动备份百度网盘等功能。它是我使用时间最久的 App 之一。2.7. 文档写作我们时常需要在手机上写作、做备忘录，那么自然需要好的文档写作类 App。「印象笔记」就不用多说了，我觉得最好用的学习总结类 App，免费版一般也够用，但是推荐订阅会员，遇到双十一、周年庆这种日子，会有 6折优惠，一年不到 100 块还是很划算了。如果你喜欢使用 Markdown 写作，那么「纯纯写作」这款精巧的 App 应该会很适合你。体积不到 3M 却拥有云备份、生成长图、中英文自动空格等数十项功能，即使这样，仍然保持了蕴繁于简的设计风格，这大概就是两三个月之内，下载量就从两三万飙升了十倍的原因，而这款 App 的背后是一位 牺牲了几年的业余时间不断开发和更新的大佬，值得敬佩。2.8. 出行交通购物这个类别中，排名第一的居然是 12306，一提起它，就会想起那一张张奇葩的验证码。不过这里的 App 不是官网的 ，而是第三方开发的。最牛逼的功能应该就是「抢票了」，如果你还在靠发朋友圈来抢票的话，那不妨试一下它。2.9. Xposed 插件最后一个类别是 Xposed，很多人应该不太熟悉，但是一提微信上的抢红包、防撤回功能，应该很多人就知道了。这些牛逼又不同寻常的功能就用到了 Xposed 框架里的各种模块功能。这个框架由国外著名的 XDA 手机论坛，你经常听到的一些所谓由 XDA 大神破解的软件，就是来自这个论坛。简单地说就是，安装了 Xposed 这个框架之后，就可以在里面安装一些好玩有趣的插件，有了这些插件，你的手机就能实现更多更大的功能。比如：能够去除广告、破解 App 付费功能、杀死耗电的自启动进程、虚拟手机定位等功能。不过使用这个框架和这些插件需要刷机、ROOT，门槛有点高。3. 小结本文使用 Scrapy 框架爬取分析了酷安网的 6000 款 App，初学 Scrapy 可能会觉得程序写起来比较散乱，所以可以尝试先使用普通的函数方法，把程序完整地写在一起，再分块拆分到 Scrapy 项目中，这样也有助于从单一程序到框架写法的思维转变，之后会写单独写一篇文章。由于网页版的 App 数量比 App 中的少，所以还有很多好用的 App 没有包括进来，比如 Chrome 、MX player、Snapseed 等，建议使用酷安 App，那里有更多好玩的东西。通过这两篇文章，我们完成一个项目从抓取到分析的过程，文中涉及了很多精品佳软，如有兴趣可以去尝试下载体验一下，为了更方便你，我这里也收集好了 24 款精品 App。4. 资源获取如需完整代码和上图中的 App 可以加入我的「知识星球：第2脑袋」获取，期待你的到来。本文完。推荐阅读：pyspider 爬取并分析虎嗅网 5 万篇文章]]></content>
      <categories>
        <category>Python数据分析</category>
      </categories>
      <tags>
        <tag>Python数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）]]></title>
    <url>%2F2018%2F11%2F29%2Fweb_scraping_withpython10.html</url>
    <content type="text"><![CDATA[发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。摘要： 如今移动互联网越来越发达，我们每个人的手机上至少都安装了好几十款 App，随着各式各样的 App 层出不穷，也就产生了优劣之分，而我们肯定愿意去使用那些良心佳软，而如何去发现这些 App 呢，本文使用 Scrapy 框架爬取了著名应用下载市场「酷安网」上的 6000 余款 App，通过分析，发现了各个类别领域下的佼佼者，这些 App 堪称真正的良心心之作，使用它们将会给你带来全新的手机使用体验。1. 分析背景1.1. 为什么选择酷安如果说 GitHub 是程序员的天堂，那么 酷安 则是手机 App 爱好者们（别称「搞机」爱好者）的天堂，相比于那些传统的手机应用下载市场，酷安有三点特别之处：第一、可以搜索下载到各种 神器、佳软，其他应用下载市场几乎很难找得到。比如之前的文章中说过的终端桌面「Aris」、安卓最强阅读器「静读天下」、RSS 阅读器 「Feedme」 等。第二、可以找到很多 App 的破解版。我们提倡「为好东西付费」，但是有些 App 很蛋疼，比如「百度网盘」，在这里面就可以找到很多 App 的破解版。第三、可以找到 App 的历史版本。很多人喜欢用最新版本的 App，一有更新就马上升级，但是现在很多 App 越来越功利、越更新越臃肿、广告满天飞，倒不如回归本源，使用体积小巧、功能精简、无广告的早期版本。作为一名 App 爱好者，我在酷安上发现了很多不错的 App，越用越感觉自己知道的仅仅是冰山一角，便想扒一扒这个网站上到底有多少好东西，手动一个个去找肯定是不现实了，自然想到最好的方法——用爬虫来解决，为了实现此目的，最近就学习了一下 Scrapy 爬虫框架，爬取了该网 6000 款左右的 App，通过分析，找到了不同领域下的精品 App，下面我们就来一探究竟。1.2. 分析内容总体分析 6000 款 App 的评分、下载量、体积等指标。根据日常使用功能场景，将 App 划分为：系统工具、资讯阅读、社交娱乐等 10 大类别，筛选出每个类别下的精品 App。1.3. 分析工具PythonScrapyMongoDBPyechartsMatplotlib2. 数据抓取由于酷安手机端 App 设置了反扒措施，使用 Charles 尝试后发现无法抓包， 暂退而求其次，使用 Scrapy 抓取网页端的 App 信息。抓取时期截止到 2018 年 11 月 23日，共计 6086 款 App，共抓取 了 8 个字段信息：App 名称、下载量、评分、评分人数、评论数、关注人数、体积、App 分类标签。2.1. 目标网站分析这是我们要抓取的 目标网页，点击翻页可以发现两点有用的信息：每页显示了 10 条 App 信息，一共有610页，也就是 6100 个左右的 App 。网页请求是 GET 形式，URL 只有一个页数递增参数，构造翻页非常简单。接下来，我们来看看选择抓取哪些信息，可以看到，主页面内显示了 App 名称、下载量、评分等信息，我们再点击 App 图标进入详情页，可以看到提供了更齐全的信息，包括：分类标签、评分人数、关注人数等。由于，我们后续需要对 App 进行分类筛选，故分类标签很有用，所以这里我们选择进入每个 App 主页抓取所需信息指标。通过上述分析，我们就可以确定抓取流程了，首先遍历主页面 ，抓取 10 个 App 的详情页 URL，然后详情页再抓取每个 App 的指标，如此遍历下来，我们需要抓取 6000 个左右网页内容，抓取工作量不算小，所以，我们接下来尝试使用 Scrapy 框架进行抓取。2.2. Scrapy 框架介绍介绍 Scrapy 框架之前，我们先回忆一下 Pyspider 框架，之前一篇爬取分析 虎嗅网 的文章，我们使用了它，它是由国内大神编写的一个爬虫利器， Github Star 超过 10K，但是它的整体功能还是相对单薄一些，还有比它更强大的框架么？有的，就是这里要说的 Scrapy 框架，Github Star 超过 30K，是 Python 爬虫界使用最广泛的爬虫框架，玩爬虫这个框架必须得会。网上关于 Scrapy 的官方文档和教程很多，这里罗列几个。Scrapy 中文文档崔庆才的 Scrapy 专栏Scrapy 爬拉勾Scrapy 爬豆瓣电影Scrapy 框架相对于 Pyspider 相对要复杂一些，有不同的处理模块，项目文件也由好几个程序组成，不同的爬虫模块需要放在不同的程序中去，所以刚开始入门会觉得程序七零八散，容易把人搞晕，建议采取以下思路快速入门 Scrapy：首先，快速过一下上面的参考教程，了解 Scrapy 的爬虫逻辑和各程序的用途与配合。接着，看上面两个实操案例，熟悉在 Scrapy 中怎么写爬虫。最后，找个自己感兴趣的网站作为爬虫项目，遇到不懂的就看教程或者 Google。这样的学习路径是比较快速而有效的，比一直抠教程不动手要好很多。下面，我们就以酷安网为例，用 Scrapy 来爬取一下。2.3. 抓取数据首先要安装好 Scrapy 框架，如果是 Windwos 系统，且已经安装了 Anaconda，那么安装 Scrapy 框架就非常简单，只需打开 Anaconda Prompt 命令窗口，输入下面一句命令即可，会自动帮我们安装好 Scrapy 所有需要安装和依赖的库。1conda pip scrapy2.3.1. 创建项目接着，我们需要创建一个爬虫项目，所以我们先从根目录切换到需要放置项目的工作路径，比如我这里设置的存放路径为：E:\my_Python\training\kuan，接着继续输入下面一行代码即可创建 kuan 爬虫项目：12345# 切换工作路径e:cd E:\my_Python\training\kuan# 生成项目scrapy startproject kuspider执行上面的命令后，就会生成一个名为 kuan 的 scrapy 爬虫项目，包含以下几个文件：123456789scrapy. cfg # Scrapy 部署时的配置文件kuan # 项目的模块，需要从这里引入_init__.pyitems.py # 定义爬取的数据结构middlewares.py # Middlewares 中间件pipelines.py # 数据管道文件，可用于后续存储settings.py # 配置文件spiders # 爬取主程序文件夹_init_.py下面，我们需要再 spiders 文件夹中创建一个爬取主程序：kuan.py，接着运行下面两行命令即可：12cd kuan # 进入刚才生成的 kuan 项目文件夹scrapy genspider kuan www.coolapk.com # 生成爬虫主程序文件 kuan.py2.3.2. 声明 item项目文件创建好以后，我们就可以开始写爬虫程序了。首先，需要在 items.py 文件中，预先定义好要爬取的字段信息名称，如下所示：12345678910class KuanItem(scrapy.Item):# define the fields for your item here like:name = scrapy.Field()volume = scrapy.Field()download = scrapy.Field()follow = scrapy.Field()comment = scrapy.Field()tags = scrapy.Field()score = scrapy.Field()num_score = scrapy.Field()这里的字段信息就是我们前面在网页中定位的 8 个字段信息，包括：name 表示 App 名称、volume 表示体积、download 表示 下载数量。在这里定义好之后，我们在后续的爬取主程序中会利用到这些字段信息。2.3.3. 爬取主程序创建好 kuan 项目后，Scrapy 框架会自动生成爬取的部分代码，我们接下来就需要在 parse 方法中增加网页抓取的字段解析内容。1234567class KuspiderSpider(scrapy.Spider): name = 'kuan' allowed_domains = ['www.coolapk.com'] start_urls = ['http://www.coolapk.com/'] def parse(self, response): pass打开主页 Dev Tools，找到每项抓取指标的节点位置，然后可以采用 CSS、Xpath、正则等方法进行提取解析，这些方法 Scrapy 都支持，可随意选择，这里我们选用 CSS 语法来定位节点，不过需要注意的是，Scrapy 的 CSS 语法和之前我们利用 pyquery 使用的 CSS 语法稍有不同，举几个例子，对比说明一下。首先，我们定位到第一个 APP 的主页 URL 节点，可以看到 URL 节点位于 class 属性为 app_left_list 的 div 节点下的 a 节点中，其 href 属性就是我们需要的 URL 信息，这里是相对地址，拼接后就是完整的 URL ：www.coolapk.com/apk/com.coolapk.market。接着我们进入酷安详情页，选择 App 名称并进行定位，可以看到 App 名称节点位于 class 属性为 .detail_app_title 的 p 节点的文本中。定位到这两个节点之后，我们就可以使用 CSS 提取字段信息了，这里对比一下常规写法和 Scrapy 中的写法：123456# 常规写法url = item('.app_left_list&gt;a').attr('href')name = item('.list_app_title').text()# Scrapy 写法url = item.css('::attr("href")').extract_first()name = item.css('.detail_app_title::text').extract_first()可以看到，要获取 href 或者 text 属性，需要用 :: 表示，比如获取 text，则用 ::text。extract_first() 表示提取第一个元素，如果有多个元素，则用 extract() 。接着，我们就可以参照写出 8 个字段信息的解析代码。首先，我们需要在主页提取 App 的 URL 列表，然后再进入每个 App 的详情页进一步提取 8 个字段信息。123456def parse(self, response): contents = response.css('.app_left_list&gt;a') for content in contents: url = content.css('::attr("href")').extract_first() url = response.urljoin(url) # 拼接相对 url 为绝对 url yield scrapy.Request(url,callback=self.parse_url)这里，利用 response.urljoin() 方法将提取出的相对 URL 拼接为完整的 URL，然后利用 scrapy.Request() 方法构造每个 App 详情页的请求，这里我们传递两个参数：url 和 callback，url 为详情页 URL，callback 是回调函数，它将主页 URL 请求返回的响应 response 传给专门用来解析字段内容的 parse_url() 方法，如下所示：12345678910111213141516171819202122232425def parse_url(self,response): item = KuanItem() item['name'] = response.css('.detail_app_title::text').extract_first() results = self.get_comment(response) item['volume'] = results[0] item['download'] = results[1] item['follow'] = results[2] item['comment'] = results[3] item['tags'] = self.get_tags(response) item['score'] = response.css('.rank_num::text').extract_first() num_score = response.css('.apk_rank_p1::text').extract_first() item['num_score'] = re.search('共(.*?)个评分',num_score).group(1) yield item def get_comment(self,response): messages = response.css('.apk_topba_message::text').extract_first() result = re.findall(r'\s+(.*?)\s+/\s+(.*?)下载\s+/\s+(.*?)人关注\s+/\s+(.*?)个评论.*?',messages) # \s+ 表示匹配任意空白字符一次以上 if result: # 不为空 results = list(result[0]) # 提取出list 中第一个元素 return resultsdef get_tags(self,response): data = response.css('.apk_left_span2') tags = [item.css('::text').extract_first() for item in data] return tags这里，单独定义了 get_comment() 和 get_tags() 两个方法.get_comment() 方法通过正则匹配提取 volume、download、follow、comment 四个字段信息，正则匹配结果如下：12345678910111213result = re.findall(r'\s+(.*?)\s+/\s+(.*?)下载\s+/\s+(.*?)人关注\s+/\s+(.*?)个评论.*?',messages)print(result) # 输出第一页的结果信息# 结果如下：[('21.74M', '5218万', '2.4万', '5.4万')][('75.53M', '2768万', '2.3万', '3.0万')][('46.21M', '1686万', '2.3万', '3.4万')][('54.77M', '1603万', '3.8万', '4.9万')][('3.32M', '1530万', '1.5万', '3343')][('75.07M', '1127万', '1.6万', '2.2万')][('92.70M', '1108万', '9167', '1.3万')][('68.94M', '1072万', '5718', '9869')][('61.45M', '935万', '1.1万', '1.6万')][('23.96M', '925万', '4157', '1956')]然后利用 result[0]、result[1] 等分别提取出四项信息，以 volume 为例，输出第一页的提取结果：123456789101112item['volume'] = results[0]print(item['volume'])21.74M75.53M46.21M54.77M3.32M75.07M92.70M68.94M61.45M23.96M这样一来，第一页 10 款 App 的所有字段信息都被成功提取出来，然后返回到 yied item 生成器中，我们输出一下它的内容：12345[&#123;'name': '酷安', 'volume': '21.74M', 'download': '5218万', 'follow': '2.4万', 'comment': '5.4万', 'tags': "['酷市场', '酷安', '市场', 'coolapk', '装机必备']", 'score': '4.4', 'num_score': '1.4万'&#125;, &#123;'name': '微信', 'volume': '75.53M', 'download': '2768万', 'follow': '2.3万', 'comment': '3.0万', 'tags': "['微信', 'qq', '腾讯', 'tencent', '即时聊天', '装机必备']",'score': '2.3', 'num_score': '1.1万'&#125;,...]2.3.4. 分页爬取以上，我们爬取了第一页内容，接下去需要遍历爬取全部 610 页的内容，这里有两种思路：第一种是提取翻页的节点信息，然后构造出下一页的请求，然后重复调用 parse 方法进行解析，如此循环往复，直到解析完最后一页。第二种是先直接构造出 610 页的 URL 地址，然后批量调用 parse 方法进行解析。这里，我们分别写出两种方法的解析代码。第一种方法很简单，直接接着 parse 方法继续添加以下几行代码即可：12345678def parse(self, response): contents = response.css('.app_left_list&gt;a') for content in contents: ... next_page = response.css('.pagination li:nth-child(8) a::attr(href)').extract_first() url = response.urljoin(next_page) yield scrapy.Request(url,callback=self.parse )第二种方法，我们在最开头的 parse() 方法前，定义一个 start_requests() 方法，用来批量生成 610 页的 URL，然后通过 scrapy.Request() 方法中的 callback 参数，传递给下面的 parse() 方法进行解析。1234567def start_requests(self): pages = [] for page in range(1,610): # 一共有610页 url = 'https://www.coolapk.com/apk/?page=%s'%page page = scrapy.Request(url,callback=self.parse) pages.append(page) return pages以上就是全部页面的爬取思路，爬取成功后，我们需要存储下来。这里，我面选择存储到 MongoDB 中，不得不说，相比 MySQL，MongoDB 要方便省事很多。2.3.5. 存储结果我们在 pipelines.py 程序中，定义数据存储方法，MongoDB 的一些参数，比如地址和数据库名称，需单独存放在 settings.py 设置文件中去，然后在 pipelines 程序中进行调用即可。1234567891011121314151617181920import pymongoclass MongoPipeline(object): def __init__(self,mongo_url,mongo_db): self.mongo_url = mongo_url self.mongo_db = mongo_db @classmethod def from_crawler(cls,crawler): return cls( mongo_url = crawler.settings.get('MONGO_URL'), mongo_db = crawler.settings.get('MONGO_DB') ) def open_spider(self,spider): self.client = pymongo.MongoClient(self.mongo_url) self.db = self.client[self.mongo_db] def process_item(self,item,spider): name = item.__class__.__name__ self.db[name].insert(dict(item)) return item def close_spider(self,spider): self.client.close()首先，我们定义一个 MongoPipeline(）存储类，里面定义了几个方法，简单进行一下说明：from crawler() 是一个类方法，用 ＠class method 标识，这个方法的作用主要是用来获取我们在 settings.py 中设置的这几项参数：12345MONGO_URL = 'localhost'MONGO_DB = 'KuAn'ITEM_PIPELINES = &#123; 'kuan.pipelines.MongoPipeline': 300,&#125;open_spider() 方法主要进行一些初始化操作 ，在 Spider 开启时，这个方法就会被调用 。process_item() 方法是最重要的方法，实现插入数据到 MongoDB 中。完成上述代码以后，输入下面一行命令就可以开始整个爬虫的抓取和存储过程了，单机跑的话，6000 个网页需要不少时间才能完成，保持耐心。1scrapy crawl kuan这里，还有两点补充：第一，为了减轻网站压力，我们最好在每个请求之间设置几秒延时，可以在 KuspiderSpider() 方法开头出，加入以下几行代码：1234custom_settings = &#123; "DOWNLOAD_DELAY": 3, # 延迟3s,默认是0，即不延迟 "CONCURRENT_REQUESTS_PER_DOMAIN": 8 # 每秒默认并发8次，可适当降低 &#125;第二，为了更好监控爬虫程序运行，有必要 设置输出日志文件，可以通过 Python 自带的 logging 包实现：12345import logginglogging.basicConfig(filename='kuan.log',filemode='w',level=logging.WARNING,format='%(asctime)s %(message)s',datefmt='%Y/%m/%d %I:%M:%S %p')logging.warning("warn message")logging.error("error message")这里的 level 参数表示警告级别，严重程度从低到高分别是：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，如果想日志文件不要记录太多内容，可以设置高一点的级别，这里设置为 WARNING，意味着只有 WARNING 级别以上的信息才会输出到日志中去。添加 datefmt 参数是为了在每条日志前面加具体的时间，这点很有用处。以上，我们就完成了整个数据的抓取，有了数据我们就可以着手进行分析，不过这之前还需简单地对数据做一下清洗和处理。3. 数据清洗处理首先，我们从 MongoDB 中读取数据并转化为 DataFrame，然后查看一下数据的基本情况。12345678910def parse_kuan(): client = pymongo.MongoClient(host='localhost', port=27017) db = client['KuAn'] collection = db['KuAnItem'] # 将数据库数据转为DataFrame data = pd.DataFrame(list(collection.find())) print(data.head()) print(df.shape) print(df.info()) print(df.describe())从 data.head() 输出的前 5 行数据中可以看到，除了 score 列是 float 格式以外，其他列都是 object 文本类型。comment、download、follow、num_score 这 5 列数据中部分行带有「万」字后缀，需要将字符去掉再转换为数值型；volume 体积列，则分别带有「M」和「K」后缀，为了统一大小，则需将「K」除以 1024，转换为 「M」体积。整个数据一共有 6086 行 x 8 列，每列均没有缺失值。df.describe() 方法对 score 列做了基本统计，可以看到，所有 App 的平均得分是 3.9 分（5 分制），最低得分 1.6 分，最高得分 4.8 分。下面，我们将以上几列文本型数据转换为数值型数据，代码实现如下：1234567891011121314151617181920212223242526272829303132def data_processing(df):#处理'comment','download','follow','num_score','volume' 5列数据，将单位万转换为单位1，再转换为数值型 str = '_ori' cols = ['comment','download','follow','num_score','volume'] for col in cols: colori = col+str df[colori] = df[col] # 复制保留原始列 if not (col == 'volume'): df[col] = clean_symbol(df,col)# 处理原始列生成新列 else: df[col] = clean_symbol2(df,col)# 处理原始列生成新列 # 将download单独转换为万单位 df['download'] = df['download'].apply(lambda x:x/10000) # 批量转为数值型 df = df.apply(pd.to_numeric,errors='ignore') def clean_symbol(df,col): # 将字符“万”替换为空 con = df[col].str.contains('万$') df.loc[con,col] = pd.to_numeric(df.loc[con,col].str.replace('万','')) * 10000 df[col] = pd.to_numeric(df[col]) return df[col]def clean_symbol2(df,col): # 字符M替换为空 df[col] = df[col].str.replace('M$','') # 体积为K的除以 1024 转换为M con = df[col].str.contains('K$') df.loc[con,col] = pd.to_numeric(df.loc[con,col].str.replace('K$',''))/1024 df[col] = pd.to_numeric(df[col]) return df[col]以上，就完成了几列文本型数据的转换，我们再来查看一下基本情况：commentdownloadfollownum_scorescorevolumecount608660866086608660866086mean255.513.7729.3133.13.917.7std1437.3981893.7595.40.620.6min00011.6025%160.2655.23.73.550%380.818017410.875%1194.5573.8684.325.3max53000519038000170004.8294.2从中可以看出以下几点信息：download 列为 App 下载数量，下载量最多的 App 有 5190 万次，最少的为 0 (很少很少)，平均下载次数为 14 万次；volume 列为 App 体积，体积最大的 App 达到近 300M，体积最小的几乎为 0，平均体积在 18M 左右。comment 列为 App 评分，评分数最多的达到了 5 万多条，平均有 200 多条。以上，就完成了基本的数据清洗处理过程，下面一篇文章我们将对数据进行探索性分析。4. 资源获取如需完整代码可以搜索加入我的「知识星球：第2脑袋」获取，期待你的到来。本文完。推荐阅读：pyspider 爬取并分析虎嗅网 5 万篇文章]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有了它，你手机上的新闻资讯类 App 都可以卸了]]></title>
    <url>%2F2018%2F11%2F24%2Fweekly_sharing6.html</url>
    <content type="text"><![CDATA[InoReader + Feedme 订阅一切，大概是不用翻墙就能看世界各地新闻最简单的方式。这是「每周分享」的第 6 期。这一期的主题是「改善手机阅读新闻资讯的体验」。我们多数人的手机上会装不只一个新闻和资讯方面的 App，因为一个 App 无法满足我们广泛的资讯需求。比如有时想无脑娱乐下，那就想上「今日头条」；有时想看点深度文章，「虎嗅」和「澎湃」就比较适合；有时文艺病犯了，打开「简书」、「豆瓣」就没错；如果你是程序员的话还会常逛「GitHub」、「Medium」这些。这样一来，手机上就不知不觉安装了大量的 App，我也经历了这样的一个时期。但现在我只留下了上面这一个可以「匹敌上百款资讯软件」的 App。下面说一说，为什么我会卸载掉那么多的 App 而只保留一个。在使用 App 的过程中，我发现虽然可以靠下载很多的 App 来获取多样的信息，但随之而来的是各种不便，比如有这么几点：占用了大量内存空间现在一个 App 动辄几十上百 M，十几个 App 装下来，几百 M 甚至上 G 的空间就被馋蚀了，你可能觉得这点不算什么，毕竟，现在的手机基本都有 64G、128G甚至更多多的存储空间。但这仅仅只是开始 ，随着使用时长增加，软件升级、文章图片下载、内容缓存带来的空间占用才是大头。最直观的就是微信了，微信安装包才几十 M，但用久了它会占用几个 G 的空间。增加了 App 切换时间成本App 一多，就增加了寻找和切换的时间成本，可能你会说：我用的 Nova 桌面对 App 做了精细的文件夹分类管理，但我仍然觉得那不还足够方便。提供了很多实际不需要的功能很多情况下，我们选择使用一个 App 是因为它里面的一项或者两三项功能，其他大部分功能是不需要的。比如拿豆瓣举例，我只喜欢里面的「图书」功能，其他的电影、广播、小组这些功能并不需要；再比如我喜欢看「腾讯新闻」的 NBA，其他的板块的内容并不感兴趣，但你不得不先下载这个 App，然后点击多次才能进入 NBA 板块页面。所以常常仅为了一个功能，我们就要去下载个 App。你可能会说：「为什么要下载 App ，使用网页版不好么？」 ，我想说在手机上使用 Chrome 浏览器网页版的「书签」功能，仍然是非常不方便的，何况很多人并不太会使用「书签」功能。带来了大量诱惑信息我们处于一个信息爆炸的时代，手机碎片化阅读变得越来越普遍。有时候，只想花 10 分钟简单纯粹地看下热点信息或者感兴趣的话题内容，但是一打开 App 就会出现太多具有诱惑力的信息，忍不住要点进去看，看完一篇又一篇，一看就是半个钟。还有一个普遍的现象是文章底部评论板块的沦陷，不知道什么时候开始，很多 App 的评论区变成了脑残、杠精和键盘侠的战场，比如 NBA 板块。虽然我们可能并不会参与到这些评论中去，但就是忍不住想去看看那些逗比说的话，久而久之就形成了「不看文章，直奔评论区」的焦虑心态，而这显然不是我们的初衷。设置了烦人的广告这一点应该是最让人不爽的，很多 App 会在启动界面设置好几秒的广告，如果不想看那么就得去购买高级付费版。有的 App 甚至还会在文章中插入很多广告，干扰我们的阅读。保存收藏起来比较麻烦当我们在 App 中看到一篇不错的文章，想要保存下来时就比较麻烦了。通常只能是通过分享保存到印象笔记、微信公众号中。而这种无法分类存档、只是堆积的文章收藏形式，其实没什么用处，以为收藏了自己回头会看，其实再也不会看。所以这种形式的存储，作用仅限于抚慰对知识获取的焦虑，过不多久就会忘掉，久而久之，仍然会有一种：读了那么多文章，脑中依然空无一物的失落。不知从什么时候开始，想要获得一种「读书看报」式的简单体验，已经变成了一种奢求。那么有没有什么好的方法能够解决以上痛点？当然是有的，而且还是最好的方法，那就是：「RSS 订阅」。RSS 是英文 Really Simple Syndication 的简称，也就是「简易信息聚合」。它可以让我们根据自己的阅读喜好，选择感兴趣的网站、博客、栏目中的资讯信息内容，然后将这些聚合汇总供我们集中阅读。这项技术其实早在 1995 年就出现了，经过前些年的辉煌期后，近些年反而变得小众了，但事实上它才是最棒的阅读方式。为什么这么说呢，因为它与上面那些 APP 之间有一个根本的区别。普通的新闻 APP 为了尽可能地足所有人需求和口味，恨不得把所有的内容都装进来，而你只能被动地去接受。但是，使用 「RSS 订阅」则由你做主，你可以自主选择你想要的信息内容，然后借助 RSS 阅读器去集中查看就可以了。所以，这意味着：你可以只要腾讯新闻 APP 的 NBA 板块内容，而不要娱乐、军事等内容。你可以把虎嗅、澎湃等很多个 App 或者网页的内容都装在一起，最终在在一个 RSS 阅读器中查看 。你甚至无需「不可描述工具」，就能看到墙外面的新闻！用一句话总结就是：如果你喜欢使用 RSS 阅读，那么你将会拥有一片新大陆。这大概是不用翻墙就能看世界各地新闻最简单的方式了。到这儿你可能有点激动，摩拳擦掌，跃跃欲试了。先介绍一下怎么使用 RSS 订阅。阅读最重要的就是找到 RSS 订阅来源，其实很好找，很多网站、博客都提供。它的形式通常是一个由 XML 文档构成的URL，只要将这个 URL 复制到一个 RSS 阅读器中进行搜索，搜索到之后订阅即可，以后这个网页只要有更新，那么就会出现在你的 RSS 阅读器中，无需再去打开这个网页查看内容了，非常地方便。这里，比如以我的博客为例，简单说明下：可以看到我的博客主页：https://www.makcyun.top/ 提供了 RSS 订阅的标志，点击打开该网页会出现一个 XML 文档，不用管文档的具体内容，只需要复制网页的 URL：https://www.makcyun.top/atom.xml 到 RSS 阅读器中，就可以订阅博客内容了，从此当博客有更新，就能及时查看最新内容。看到这儿，你可能会有两个感兴趣的地方：有没有集中的资讯 RSS 订阅源？？用什么 App 看这些信息？下面就是这期的干货了。推荐两个非常棒的 RSS 来源，首先是 RSSHub 。这个 GitHub 库提供了多种类型的订阅源，多达上百个网站，不同网站下又可以订阅不同板块的内容如果上面找不到你喜欢的网站，你还可以自己动手制作 RSS 订阅源，可以说是「万网皆可订阅了」。第二个是 RSS - IT 人 ，这个库提供了了一些 IT 大佬的博客，比如阮一峰 （GitHub 上排名第一的中国人）。有了这两个库，订阅源已经不是问题了，剩下的问题在于：用什么 App 去阅读？市面上的 RSS 阅读器多如牛毛，要想挑选出一个功能强大、阅读体验好、颜值高又的 App 很不容易。比较知名的 InoReader、Feedly、NewsBlur 等等。但用了之后，发现它们多多少少都有些不如意的地方，比如订阅不方便、界面不美观等。这里推荐一种更好的阅读方式：在电脑上使用 InoReader ，在手机上使用一款小而精的佳软：Feedme。以阮一峰老师的博客为例，先对比一下普通网页版、InoReader 和 Feddme 版本的排版，可以看到 Feedme 要好看很多。其次，它的订阅功能很强大，支持中文关键字搜索。这样，当我们没有一个明确的 RSS 订阅源时，可以通过关键字搜索来订阅感兴趣的资讯。以上就是在 App 中使用 RSS 阅读的正确打开方式了。但这仍然不足以体现 RSS 订阅功能的强大，更牛逼的地方是在电脑上使用。在电脑上打开 InoReader ，可以看到它为我们打造了一套个人专属、功能丰富的资讯信息面板。在这个面板中，可以实现这么几个功能：查看过往阅读历史很多时候，我们看过的新闻看过就看过了，但是在这上面你可以查到你的文章阅读记录。利用文件夹和标签功能对收藏的文章进行分类这个功能类似于印象笔记，但是比印象笔记的剪藏功能强大多了。文章分门别类之后，我们再回头进行总结和记录就很方便。将文章保存为 PDF 永久保存专注阅读在这里面阅读几乎没有广告，提供了更好的阅读体验。好，以上就是这期的内容。文中所说的阅读 App，可以在公众号后台回复「RSS」获得。本文完。欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上了年纪的工程师有哪些出路]]></title>
    <url>%2F2018%2F11%2F20%2Flife02.html</url>
    <content type="text"><![CDATA[吴军《谷歌方法论》专栏课程记录。一直很喜欢吴军老师在得到开的「谷歌方法论」专栏课程，收获非常多。惊叹于吴老师不仅对 IT 领域有着非常深厚和独到的见解，而且在其他众多领域也有着丰富的知识储备。他能够把晦涩、深奥的专业知识以通俗易懂的方式和道理讲述出来，仅从这方面来说，他无疑是一个真正的大师与专家。除此之外，针对读者留言提出的一些关于工作瓶颈、职场困惑、人生抉择等方面的具体问题，他也会给出非常值得借鉴和参考的建议。不久前，他在一封来信中，针对一位读者提出的「大龄工程师有哪些出路」这个问题给了几点建议，对于我这个还不算是 IT 工程师就已经是大龄的人来说，深有感触并且得到了启发。如果你目前或者不远的将来也担心面临这样的处境，不妨参考一下他说的这几点建议。下面，我将分享与你。问题大意是说：国外的一线大龄（近 50 岁）工程师，能够很轻松地选择跳槽，而对于国内的一线工程师（年近 40 岁）来说，竞争力就在慢慢下降也基本不会再换工作，而且很多公司基本不要超过 35 岁的工程师。是什么原因导致了这种巨大差距，对于大龄工程师又有些什么建议？原文：这个问题很有代表性。我们在讨论这个问题之前，首先要明确两个概念。第一个是关于一线工程师的定义。一般人会理解为做具体工作的工程师，而且比较倾向于看成是甶领导分配任务，自己独立完成任务的人。但这些人其实只是我所说的第五级的工程师。一线工程师可以是四级、三级，甚至更高。如果一个人到了四十岁，还是五级工程师，就怨不得别人不要他了，因为这说明他的学习能力太差，没有发展潜力。事实上，中国还真是很缺四级的工程师，在 Google 和微软这样的公司，只要是所谓的高级工程师（勉强达到四级的要求），目前中国各大企业都抢着要，开出的薪水可比一个总监高得多，这些人在 BAT 这样的企业里，我还没见到谁的收入在百万以下。Google 中国在历史上最好的工程师（没有之一）当属郄（qiè）小虎，今天 20 多岁的年轻人写代码还真写不过他。用腾讯原来主管投资的副总裁彭志坚的话讲，他是中国互联网企业从 Google 全世界华裔工程师中最想挖的三个工程师之一。今天全中国能做高管的人多如牛毛，但郄小虎这样的人，一个巴掌就数过来了，因此后来他想去哪家公司就去哪家，而且只接受 CTO 的头衔。就这样，排着队想挖他的大公司至少有两位数。第二个要明确的概念是，什么叫做竞争力下降。如果拼熬夜，40 岁的人确实拼不过 20 岁的，更何况 40 岁的人还有一大堆家庭负担，工作的时间和强度远不如 20 岁的人。但是，今天 40 岁的人其实智力远没有衰退，如果 40 岁的人能够解决一些 20 岁的人解决不了的问题，那么就不存在所谓的竞争力不足的问题了。如果 40 岁的人做不到这一点，除了知识没有及时更新，变得老化之外，很重要的原因是 心态上不愿意像 20 岁的人那样踏踏实实做具体工作。今天一些年轻人愿意死磕一个星期，找到计算机程序里的 bug ，这样他们可以证明自己的水平。但是 40 岁的人常常不愿意再这样工作，当然竞争力就会下降了。相比之下，美国和德国很多 40 岁的工程师，依然在像 20 岁那样工作，当然就没有被淘汰的问题。而一旦有一些 40 岁的人开始以慢节奏工作时，作为企业的主管，出于对招聘安全的考虑，干脆所有年纪大一点的人都不招了。对于年纪大一点的工程师，他给出了三点简单的建议：1. 自省，看看自己的本事是否随着年龄的增长在增长。如果是，其实不用担心。就像郄小虎，永远不用担心没人要他一样。2.如果发现自己的本事和 20 岁的时候没有变化，那也怪不得别人，因为现在的果，源于过去的因。如果自己现在还在单位里，没有换工作的打算，赶快甩掉身上的懒肉，补上这十多年来应该具有的进步；如果自己正在换工作，也没有关系，总结一下自己过去完成的不超过三件最拿得出手的工作，好好包装一下自己。虽然中国的企业不喜欢要年纪大的人，但是它们也不愿意培养新人，通常希望来了人，就能马上上手干活。这就给有工作经验的人提供了机会，虽然包装出来的本事未必是真本事。为什么不要写三个以上的成就呢？坦率地来讲，今天绝大部分人没有那么多亮眼的成就，能有三个就很不错了，如果列举多了，一定是凑数，把平庸的工作也列举出来了。即使是郄小虎，在我印象中，他最拿得出手的成果有两样，首先是把Google 整个广告收入提高了 10% 左右，这可是不得了的贡献，他也因此获得了 Google 的最高奖——创始人奖。其次，他成功地将 Google 花了 30 多亿美元收购的双击公司 (Doubleclick) 的广告系统换成了 Google 的，完成了 Google 对双击公司在工程和产品上的整合，这也是一件了不得的事情。当然，他还做了很多其他的工作，肯定水平不差，但是讲不讲都不重要了。3.如果工作中使用的工程工具已经老化，或者你所从事的工作是没有前途的，趁早换新的，不要等到自己手上没有武器时再发愁。以上是他给出的几点建议，希望对你也能有所启发。本文完。欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这个 GitHub 库能拯救你的文章排版]]></title>
    <url>%2F2018%2F11%2F16%2Fweekly_sharing5.html</url>
    <content type="text"><![CDATA[Apple、Microsoft、少数派都在使用这一套文案排版标准。这是「每周分享」的第 5 期。上一期文章 「程序员是如何在 5 分钟内搞定公众号排版的」，介绍了如何快速对公众号文章进行排版，但是没有介绍文案排版中的另一个重点，即 「如何让文章看起来更美观」。你能看出上面两个排版哪个更好以及在好哪里么？我相信，不少人在对公众号文章进行排版时，主要是凭个人感觉或者喜好来排的，而这会导致两个问题：无论怎么排版，总感觉和网上优秀的文章相比，自己的文章看起来不是那么优雅。这种凭感觉的排版会造成排版风格不够固定，因为感觉是经常在变的，所以排版风格也就跟着会变化，而 频繁的风格变化会给读者增加阅读成本。我自己是一直深受上面两个问题困扰，所以一直在网上尝试寻找解决方法，直到最近知道了 GitHub 上一个库的存在：「中文文案排版指北」，让我终于找到了「北」。看了这篇文案排版说明之后，我发现原来 优秀的文章排版，都遵循了一套潜在的业内标准和规范。当我尝试着将这套标准应用到了自己的文章中之后，发现文章变得好看多了。下面就来介绍一下这个库规定了哪些文案排版标准。空格我以前从来没有意识到「文字之间需要适当地添加空格」这件事的重要性：「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。」中英文之间需要增加空格正确：你可以搜索 makcyun 关注我的博客。错误：你可以搜索makcyun关注我的博客。例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。中文与数字之间需要增加空格正确：拿钢琴来说，键盘有始也有终，它有 88 个键，并不是无限的。错误：拿钢琴来说，键盘有始也有终，它有88个键，并不是无限的。数字与单位之间需要增加空格正确：双十一我买了一副 2000 元的耳机。错误： 双十一我买了一副 2000元的耳机。例外1：当表示规格时，不需要添加空格：正确：我的手机有 64GB 内存错误：我的手机有 64 GB 内存例外2：度／百分比与数字之间不需要增加空格。正确：今天很冷，只有 1°，所以我几乎 90% 的时间都是在被窝里。错误：今天很冷，只有 1 °，所以我几乎 90 % 的时间都是在被窝里。全角标点与其他字符之间不加空格正确：我刚刚买了一部 iPhone，好开心！错误：我刚刚买了一部 iPhone ，好开心！标点符号说起标点符号，就要先说一下「全角」和「半角」这两种标点符号类型。首先，简单来说，中文字符是全角字符，拉丁字母和数字则是半角字符。全角标点占 2 个字节，宽一些；半角标点占 1 个字节，窄一些。比如表示中英文的逗号分别是「，」和「,」。使用全角中文标点正确：嗨！你今天去 Python 技术主题大会（PyCon）现场了么？错误：嗨!你今天去 Python 技术主题大会(PyCon)现场了么?遇到完整的英文整句、特殊名词，其內容使用半角标点正确：我发现很多人都喜欢用乔布说的那句话：「Stay hungry, stay foolish.」错误：我发现很多人都喜欢用乔布说的那句话：「Stay hungry，stay foolish。」名词专有名词使用正确的大小写正确：我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。错误：我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。不要使用不地道的缩写正确：我们需要一位熟悉 JavaScript、HTML5 的前端开发者。错误：我们需要一位熟悉 Js、h5 的前端开发者。建议简体中文使用直角引号直角引号就是「」、『』，他们才是真正的中文引号，常见的 “” 是弯角引号，是当时我国参考了英文引号之后，而制定出的简体中文引号标准。实际上，在文章中使用直角引号要比弯角引号更好看。对比一下：「老师，『有条不紊』的『紊』是什么意思？」“老师，‘有条不紊’的‘紊’是什么意思？”中文尽量不要使用斜体中文不太适合用斜体字，斜体之后效果糟糕，字形会扭曲。链接之间增加空格如果想浏览我的博客，请点击底部 阅读原文 查看。如果想浏览我的博客，请点击底部阅读原文查看。首行顶格写不用缩进这个你可能会不同意，不过这样做，效果的确更好。先来解释一下「首行缩进」的目的是为了什么：「 每段之前空两格」 是我们从小学写作文时养成的习惯，也是正式文体的格式要求，目的是为了区分自然段。但是像我们现在接触的阅读，都是没有固定的格式要求的，如微信公众号、电子文档等，所以大家一般都采用「 空出一行」 进行自然段与自然段之间的区分。有了段落区分之后，也就没有必要再再去空格了。而且这种写作方式非常省事，看起来也很整齐 。以上就是一套优秀的文案排版所遵循的标准。你可能会质疑它到底对不对，那么你可以看看 Apple 中国，Microsoft 中国官网、少数派网站 的排版，它们基本上都是采用了上面的排版标准。本文完。欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>文章排版</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 可视化(4)：WordCloud 中英文词云图绘制方法汇总]]></title>
    <url>%2F2018%2F11%2F14%2FPython_visualization04.html</url>
    <content type="text"><![CDATA[英文和中文词云图绘制总结。摘要： 当我们手中有一篇文档，比如书籍、小说、电影剧本，若想快速了解其主要内容是什么，那么可以通过绘制WordCloud 词云图，通过关键词（高频词）就可视化直观地展示出来，非常方便。本文主要介绍常见的英文和中文文本的词云图绘制，以及通过 DataFrame 数据框绘制 Frequency 频率词云图。在上一篇文章「pyspider 爬取并分析虎嗅网 5 万篇文章 」中的文本可视化部分，我们通过 WordCloud 和 jieba 两个包绘制了中文词云图，当时只是罗列出了代码，并没有详细介绍。接下来，将详细说明词云图绘制步骤，并且对词云图的种类进行拓展。1. 英文词云这里，我们先绘制英文文本的词云图，因为它相对简单一些，这里，以《海上钢琴师》这部电影的剧本为例。首先，需要准备好电影剧本的文本文件（如下图）：接下来，我们绘制一个最简单的矩形词云图，代码如下：1234567891011121314151617181920import osfrom os import pathfrom wordcloud import WordCloudfrom matplotlib import pyplot as plt# 获取当前文件路径d = path.dirname(__file__) if "__file__" in locals() else os.getcwd()# 获取文本texttext = open(path.join(d,'legend1900.txt')).read()# 生成词云wc = WordCloud(scale=2,max_font_size = 100)wc.generate_from_text(text)# 显示图像plt.imshow(wc,interpolation='bilinear')plt.axis('off')plt.tight_layout()#存储图像wc.to_file('1900_basic.png')# or# plt.savefig('1900_basic.png',dpi=200)plt.show()首先，通过 open() 方法读取文本文件，然后 WordCloud 方法设置了词云参数，generate_from_text() 生成该文本词云，然后显示和保存词云图，十几行代码就可以生成最简单的词云图。通过上面的词云图，你可能会发现有几点问题：可不可以随便更换背景，比如白色？词云图是矩形，能不能换成其他形状或者自定义图片样式？词云中最显眼的词汇 「ONE」，并没有实际含义，能不能去掉？以上这些都是可以更改的，如果你想实现以上想法，那么需要先了解一下 WordCloud 的API 参数及它的一些方法。这里，我们列出它的各项参数，并注释重要的几项：1234567891011121314151617181920212223242526wordcloud.WordCloud( font_path=None, # 字体路径，英文不用设置路径，中文需要，否则无法正确显示图形 width=400, # 默认宽度 height=200, # 默认高度 margin=2, # 边缘 ranks_only=None, prefer_horizontal=0.9, mask=None, # 背景图形，如果想根据图片绘制，则需要设置 scale=1, color_func=None, max_words=200, # 最多显示的词汇量 min_font_size=4, # 最小字号 stopwords=None, # 停止词设置，修正词云图时需要设置 random_state=None, background_color='black', # 背景颜色设置，可以为具体颜色,比如white或者16进制数值 max_font_size=None, # 最大字号 font_step=1, mode='RGB', relative_scaling='auto', regexp=None, collocations=True, colormap='viridis', # matplotlib 色图，可更改名称进而更改整体风格 normalize_plurals=True, contour_width=0, contour_color='black', repeat=False)了解各项参数后，我们就可以自定义想要的词云图了。比如更换一下背景颜色和整体风格，就可以修改以下几项：123456wc = WordCloud( scale=2,# 缩放2倍 max_font_size = 100, background_color = '#383838',# 灰色 colormap = 'Blues') # colormap名称 https://matplotlib.org/examples/color/colormaps_reference.html接下来，我们提升一点难度，通过设置 StopWords 去掉没有实际意义的「ONE」，然后将词云图绘制在我们自定义的一张图片上。代码实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546import osfrom os import pathimport numpy as npfrom wordcloud import WordCloud,STOPWORDS,ImageColorGeneratorfrom PIL import Imagefrom matplotlib import pyplot as pltfrom scipy.misc import imreadimport randomdef wc_english(): # 获取当前文件路径 d = path.dirname(__file__) if "__file__" in locals() else os.getcwd() # 获取文本text text = open(path.join(d,'legend1900.txt')).read() # 读取背景图片 background_Image = np.array(Image.open(path.join(d, "mask1900.jpg"))) # or # background_Image = imread(path.join(d, "mask1900.jpg")) # 提取背景图片颜色 img_colors = ImageColorGenerator(background_Image) # 设置英文停止词 stopwords = set(STOPWORDS) wc = WordCloud( margin = 2, # 设置页面边缘 mask = background_Image, scale = 2, max_words = 200, # 最多词个数 min_font_size = 4, # 最小字体大小 stopwords = stopwords, random_state = 42, background_color = 'white', # 背景颜色 max_font_size = 150, # 最大字体大小 ) # 生成词云 wc.generate_from_text(text) # 等价于 # wc.generate(text) # 根据图片色设置背景色 wc.recolor(color_func=img_colors) #存储图像 wc.to_file('1900pro1.png') # 显示图像 plt.imshow(wc,interpolation='bilinear') plt.axis('off') plt.tight_layout() plt.show()首先，通过 open() 方法读取文本文件，Image.open() 方法读取了背景图片，np.array 方法将图片转换为矩阵。接着设置了词云自带的英文 StopWords 停止词，用来分割筛除文本中不需要的词汇，比如：a、an、the 这些。然后，在 wordcloud 方法中，设置词云的具体参数。generate_from_text() 方法生成该词云，recolor() 则是根据图片色彩绘制词云颜色，绘制效果如下：接着，我们还是看到了显眼的「ONE」，下面我们将它去除掉，方法也很简单，几行代码搞定：123456789# 获取文本词排序，可调整 stopwordsprocess_word = WordCloud.process_text(wc,text)sort = sorted(process_word.items(),key=lambda e:e[1],reverse=True)print(sort[:50]) # 获取文本词频最高的前50个词# 结果[('one', 60), ('ship', 47), ('Nineteen Hundred', 43), ('know', 38), ('music', 36), ...]stopwords = set(STOPWORDS)stopwords.add('one')这里，我们可以对文本词频进行排序，通过输出看到 「ONE」词频最高，然后添加进 stopwords 中，就可以屏蔽该词从而不再显示。这种手动添加停止词的方法适用于词数量比较少的情况。另外，我们还可以将词云图颜色显示为黑白渐变色，也只需修改几行代码即可：12345def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs): return "hsl(0, 0%%, %d%%)" % random.randint(50, 100) # 随机设置hsl色值wc.recolor(color_func=grey_color_func)以上，就是英文词云图绘制的几种方法，下面我们介绍中文词云图的绘制。2. 中文词云相比于英文词云，中文在绘制词云图前，需要先切割词汇，这里推荐使用 jieba 包来切割分词。因为它可以说是最好的中文分词包了，GitHub 上拥有 160 K 的 Star 数。安装好 jieba 包后，我们就可以对文本进行分词然后生成词云了。这里，选取吴军老师的著作《浪潮之巅》作为中文文本的案例，仍然采用图片形式的词云图。素材准备好后，接下来就可以开始中文词云图绘制。首先，需要读取文本文件，相比于英文，这里要添加文本编码格式，否则会报错，添加几行检测代码即可：123456text = open(path.join(d,'langchao.txt'),'rb').read()text_charInfo = chardet.detect(text)print(text_charInfo)# 结果&#123;'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''&#125;text = open(path.join(d,r'langchao.txt'),encoding='UTF-8-SIG').read()接着，对文本进行分词。jieba 分词有 3 种方式：精确模式、全模式和搜索引擎模式，它们之间的差别，可以用一个例子来体现。比如，有这样的一句话：「”我来到北京清华大学”」，用 3 种模式进行分词，结果分别如下：全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学精确模式: 我/ 来到/ 北京/ 清华大学搜索引擎模式： 我/ 来/ 来到/ 北京/ 清华/ 大学/ 清华大学/根据结果可知，我们应该选择「精确模式」来分词。关于 jieba 包的详细用法，可以参考 GitHub 仓库链接：https://github.com/fxsjy/jieba分词完成后，我们还需要设置 stopwords 停止词，由于 WordCloud 没有中文停止词，所以我们需要自行构造。这里可以采取两种方式来构造：通过 stopwords.update() 方法手动添加根据已有 stopwords 词库遍历文本筛除停止词2.1. stopwords.update() 手动添加这种方法和前面的英文停止词构造的方法是一样的，目的是在词云图中不显示 stopwords 就行了 。先不设置 stopwords，而是先对文本词频进行排序，然后将不需要的词语添加为 stopwords 即可。下面用代码实现：123456# 获取文本词排序，可调整 stopwordsprocess_word = WordCloud.process_text(wc,text)sort = sorted(process_word.items(),key=lambda e:e[1],reverse=True)print(sort[:50]) # # 获取文本词频最高的前50个词[('公司', 1273), ('但是', 769), ('IBM', 668), ('一个', 616), ('Google', 429), ('自己', 396), ('因此', 363), ('微软', 358), ('美国', 344), ('没有', 334)...]可以看到，我们先输出文本词频最高的一些词汇后，发现：但是、一个、因此、没有等这些词都是不需要显示在词云图中的。因此，可以把这些词用列表的形式添加到 stopwords 中，然后再次绘制词云图就能得出比较理想的效果，完整代码如下：12345678910111213141516171819202122232425262728293031323334353637383940import chardetimport jiebatext+=' '.join(jieba.cut(text,cut_all=False)) # cut_all=False 表示采用精确模式# 设置中文字体font_path = 'C:\Windows\Fonts\SourceHanSansCN-Regular.otf' # 思源黑体# 读取背景图片background_Image = np.array(Image.open(path.join(d, "wave.png")))# 提取背景图片颜色img_colors = ImageColorGenerator(background_Image)# 设置中文停止词stopwords = set('')stopwords.update(['但是','一个','自己','因此','没有','很多','可以','这个','虽然','因为','这样','已经','现在','一些','比如','不是','当然','可能','如果','就是','同时','比如','这些','必须','由于','而且','并且','他们'])wc = WordCloud( font_path = font_path, # 中文需设置路径 margin = 2, # 页面边缘 mask = background_Image, scale = 2, max_words = 200, # 最多词个数 min_font_size = 4, # stopwords = stopwords, random_state = 42, background_color = 'white', # 背景颜色 # background_color = '#C3481A', # 背景颜色 max_font_size = 100, )wc.generate(text)# 获取文本词排序，可调整 stopwordsprocess_word = WordCloud.process_text(wc,text)sort = sorted(process_word.items(),key=lambda e:e[1],reverse=True)print(sort[:50]) # 获取文本词频最高的前50个词# 设置为背景色，若不想要背景图片颜色，就注释掉wc.recolor(color_func=img_colors)# 存储图像wc.to_file('浪潮之巅basic.png')# 显示图像plt.imshow(wc,interpolation='bilinear')plt.axis('off')plt.tight_layout()plt.show()我们对比以下 stopwords 添加前后的效果就可以大致看出效果。stopwords 添加之前：stopwords 添加之后：可以看到，stopwords.update() 这种方法需要手动去添加，比较麻烦一些，而且如果 stopwords 过多的话，添加就比较费时了。下面介绍第 2 种 自动去除 stopwords 的方法。2.2. stopwords 库自动遍历删除这种方法的思路也比较简单。主要分为 2 个步骤：利用已有的中文 stopwords 词库，对原文本进行分词后，遍历词库去除停止词，然后生成新的文本文件。根据新的文件绘制词云图，便不会再出现 stopwords，如果发现 stopwords 词库不全可以进行补充，然后再次生成词云图即可。代码实现如下：1234567891011121314151617181920212223242526272829303132# 对原文本分词def cut_words(): # 获取当前文件路径 d = path.dirname(__file__) if "__file__" in locals() else os.getcwd() text = open(path.join(d,r'langchao.txt'),encoding='UTF-8-SIG').read() text = jieba.cut(text,cut_all=False) content = '' for i in text: content += i content += " " return content# 加载stopwordsdef load_stopwords(): filepath = path.join(d,r'stopwords_cn.txt') stopwords = [line.strip() for line in open(filepath,encoding='utf-8').readlines()] # print(stopwords) # ok return stopwords# 去除原文stopwords,并生成新的文本def move_stopwwords(content,stopwords): content_after = '' for word in content: if word not in stopwords: if word != '\t'and'\n': content_after += word content_after = content_after.replace(" ", " ").replace(" ", " ") # print(content_after) # 写入去停止词后生成的新文本 with open('langchao2.txt','w',encoding='UTF-8-SIG') as f: f.write(content_after)网上有很多中文 stopwords 词库资料，这里选取了一套包含近 2000 个词汇和标点符号的词库：stopwords_cn.txt，结构形式如下：遍历该 stopwords 词库，删除停止词获得新的文本，然后利用第一种方法绘制词云图即可。首先输出一下文本词频最高的部分词汇，可以看到常见的停止词已经没有了。1[('公司', 1462), ('美国', 366), ('IBM', 322), ('微软', 320), ('市场', 287), ('投资', 263), ('世界', 236), ('硅谷', 235), ('技术', 234), ('发展', 225), ('计算机', 218), ('摩托罗拉', 203)...]最终绘制词云图，效果如下：3. Frenquency 词云图除了直接读入文本生成词云，还可以使用 DataFrame 或者 字典格式 的词频作为输入绘制词云。下面，以此前我们爬过的一份 10 年「世界大学排名 TOP500 强」 数据为例，介绍如何绘制词云图。数据为 5001行 x 6 列，我们想根据各国 TOP 500 强大学的数量之和，展示各国顶尖大学数量的一个大概情况对比。​123456789101112world_rank university score quantity year country1 哈佛大学 100 500 2009 USA2 斯坦福大学 73.1 499 2009 USA3 加州大学-伯克利 71 498 2009 USA4 剑桥大学 70.2 497 2009 UK5 麻省理工学院 69.5 496 2009 USA...496 犹他州立大学 2018 USA497 圣拉斐尔生命健康大学 2018 Italy498 早稻田大学 2018 Japan499 韦恩州立大学 2018 USA500 西弗吉尼亚大学 2018 USA有两种格式可以直接生成频率词云图，第一种是 Series 生成。123456789101112131415161718192021import pandas as pdimport matplotlib.dates as mdatefrom wordcloud import WordCloudimport matplotlib.pyplot as pltdf = pd.read_csv('university.csv',encoding = 'utf-8')df = df.groupby(by = 'country').count()df = df['world_rank'].sort_values(ascending = False)print(df[:10])# 结果如下：countryUSA 1459Germany 382UK 379China 320France 210Canada 209Japan 206Australia 199Italy 195Netherlands 122第二种转换为 dict 字典生成。1234df = dict(df)print(df)# 结果如下：&#123;'USA': 1459, 'Germany': 382, 'UK': 379, 'China': 320, 'France': 210,..&#125;这两种数据都可以快速生成词云图，代码实现如下：123456789101112131415161718font_path='C:\Windows\Fonts\SourceHanSansCN-Regular.otf' # 思源黑wordcloud = WordCloud( background_color = '#F3F3F3', font_path = font_path, width = 5000, height = 300, margin = 2, max_font_size = 200, random_state = 42, scale = 2, colormap = 'viridis', # 默认virdis )wordcloud.generate_from_frequencies(df)# or# wordcloud.fit_words(df)plt.imshow(wordcloud,interpolation = 'bilinear')plt.axis('off')plt.show()结果如下：可以看到，美国最为突出，其次是德国、英国、中国等。文中代码及素材可以在公众号后台回复 词云 或者在下面的链接中获取：本文完。推荐阅读：做 PPT 没灵感？澎湃网 1500 期信息图送给你国内创业公司的信息都在这里了欢迎扫一扫识别关注我的公众号]]></content>
      <categories>
        <category>Python可视化</category>
      </categories>
      <tags>
        <tag>WordCloud 词云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员是如何在 5 分钟内搞定公众号排版的]]></title>
    <url>%2F2018%2F11%2F10%2Fweekly_sharing4.html</url>
    <content type="text"><![CDATA[放弃 Word ，放弃 135 编辑器，用这套方法又快又好搞定公众号排版。这是每周分享的第 4 期。先简单介绍下这个栏目，顾名思义，就是会在每个周末分享一篇文章。内容主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。好，下面开始进入正题。文章主要内容：Markdown 语法介绍Markdown 编辑器推荐 TyporaVS Code 给标题添加编号Md2All 渲染 Markdown正如微信公众号的口号所言「再小的个体，都有自己的品牌」，于是越来越多的人开始拥有自己的公众号，这几乎成为了不少人写作的标配。在我的微信好友中，拥有公众号的人数比例达到了 30% ，这是很夸张的一个比例。大家在公众号书写出于各自的目的，但是有一点是共通的，就是要「排版」。虽然每个人都有自己的一套排版技巧和排版风格，但我发现有些人写作 1小时、排版也要 1 小时，才勉强能排出比较好看的文章，更有甚者排版时间也花了，文章看起来依然惨不忍睹。而有些人写作 1 小时，排版只要 1 分钟，排出来却非常好看。这些人肯定是有独到的排版技巧的。我的公众号开通时间算是比较早的 (2014年)，这 4 年，我使用了不下 3 套排版方法。可以看到，我的排版风格一直在变化，大致可以分为 3 个阶段。早期 （2014 年）那会儿还不太会玩公众号，文章内容都是直接从 Word 中复制进去然后就发布，根本谈不上排版。中期 (2016 年)，我使用了很多人推荐的 秀米 和135 这些公众号排版编辑器，套用了很多模板自带的样式，文章排版出来也变得好看很多。到今年，我换成了 Markdwon 和 Md2All 的组合方式。因为，我发现前两种方式有几个比较明显的缺点：在自己电脑上的 Word 中排版地很好看，但在别人的电脑上打开时却惨不忍睹，等于没有排版。这是 Word 的一大硬伤。使用秀米编辑器排版虽然有很多好看的样式，但是其实不需要那么多花哨的样式，而且耗时比较长。这两个平台文章可移植性差，排版好以后仅适用于公众号，如果想发布到其他平台，需要重新排版，太麻烦。如果用一句话来形容 Markdwon 和 Md2All 的这种排版的特点，那就是：Word 好比素颜，Markdown + Md2ALL 则是底妆 + 精妆。1. 关于 Markdwon几个月前，我开始搜索有什么更好用的公众号排版方式，看到很多人推荐使用 Markdwon 编辑器排版。于是赶紧了解了下是怎么玩的，原来 Markdown 语言诞生于 2004 年，起初主要是程序员使用，到现在渐渐成为了很多写作者的第一选择，这种方法最大的特点就是：写文和排版可以同步进行，从而大大节省排版时间。什么意思呢，就是在文章需要进行排版的地方使用一些诸如：「# - &gt; ! [] ()」 这几种符号，就可以自动识别为：标题、序号、引用、图片、超链接等这些排版格式。这样就省去了在 Word 中去进行：插入列表级别、插入图片、插入链接这些麻烦的操作，手要不停地在键盘和鼠标中切换。使用它，手几乎可以不用离开键盘，从而更加专注于写作。上面的这些符号就是 Markdown 语言中的固定几种语法符号，简单易学，学会以后能够节省很多排版时间。如果你此前没有使用过 Markdown 写作，那么推荐两个教程给你，以供参考。看了之后如果对这种写作方式感兴趣的话，那么强烈建议你放弃现在的 Word 开始拥抱 Markdown 吧。献给写作者的 Markdown 新手指南markdown 介绍/学习/工具/资料2. Typora 和 Markeditor使用 Markdown 写作，最好是找一款支持 Markdown 语法的编辑器。十多年来，随着 Markdwon 越来越流行，各种各样的 Makdown 编辑器如雨后春笋般涌出，多的让人目不暇接。到底哪款编辑器最好用？这个是没有固定答案的，不仅不同的人的喜欢不同，而且同一个人在不同时期喜好也是变化的，只有多去尝试几款，通过对比后就能发现你喜欢的，我个人就用过不下 5 款编辑器。这里给你推荐些获取 Markdwon 编辑器的资源：CTOLib 码库：123 款 Markdwon 编辑器详细介绍少数派：18 款优秀的 Markdown 写作工具GitHub：数百款 Markdown 编辑器最终的选用原则只有一个：「选用着最舒服的」。为了不至于让你挑花眼，我推荐两款给你：Typora 和 Markeditor 。二者都是开源的，也都支持 Mac 和 Windows。相比于 Markeditor，我更喜欢 Typora，也是我现在微信公众号排版使用的编辑器。它有这么几个优点：「What You See Is What You Mean」，也就是「所见即所得」据我所知，它是目前唯一款能够做到实时预览 Markdown 效果的编辑器。其他大多数编辑器，包括 Markeditor 在内，要么只能先写然后再预览，要么左右分屏一边写一边预览。事实上，当 Markdwon 比较熟练以后，只需要 Typora 这种能够实时预览的效果。足够多的快捷键，代替 Markdown 语法符号输入举个例子，如果你想插入一幅图片，那么基本的操作是这样来写：！，而在 Typroa 里，只需要输入快捷键：Ctrl + Shift + I，就可以快速插入图片了。同理，它还可以插入表格、超链接等。也就是说，你早先记得那些语法，在编辑器里可以不用了，取而代之的是快捷键。这样一来，Markdown 更加容易写了。不过，语法是一定要掌握的。上面，用了不少篇幅去介绍 Markdown 和编辑器。 现在，我们回到今天所要说的「又快又好地排版」这个问题上来。3. 给标题添加编号用 Markdown 编辑好文章，其实我们已经完成一多半的排版工作。接下来「标题添加编号这个功能」其实是选择性的，可有可无，不过我觉得添加上编号会让文章显得更加有秩序。你可能会问，这个编号是手动添加上去的么？当然不是，这里推荐一款可以自动添加编号的 VS Code 插件：Markdown add index。使用很简单，只需要 3 步：安装 VS Code 软件（编程的人应该都用过这款颜值很高的 IDE 吧）在里面搜索下载 Markdown add index 这个插件打开命令窗口 (Ctrl + Shift + P)，输入 Markdown add index，然后回车，就会发现文章中的标题已经自动添加上序号了。如果后期标题有顺序调整，只需要再次执行该命令，就会自动更改。这比手动去改，方便太多。4. Md2All 渲染文章完成以上工作后，我们只需要再做最后一步的「文章渲染」就大功告成了。渲染的目的就是让公众号的文章变得更加好看，也就好比女生化妆的最后一步「精妆」。之前很多人会推荐使用 Markdwon Here 插件来渲染，但用过之后发现其实并不太好用。这里，推荐一款可以说是 Markdown Here 的增强版插件： Md2All 。它有这几个优点：排版样式非常多，支持自定义 CSS 样式我的公众号排版风格，就是在里面预先自定义好 CSS 样式。CSS 样式，其实就是类似下面这种预先设置好的文章排版格式，比如：字体、间距、列表、表格这些。123456789101112131415161718192021222324p &#123; /*段落选择器*/ margin: 1.5em 5px !important; /*颜色*/ color:#565656; /*字体*/ font-family:'微软雅黑'; /*字号*/ font-size:15px; /*行间距，可用百分比，数值倍数，像素设置，还包括text-indent缩进、letter-spacing字间距、*/ line-height:2; /*段间距，一般用margin属性调整*/ margin-bottom:20px; /*页边距用padding属性调整*/ letter-spacing: 0.5px; /*文字间距*/&#125;ul, ol &#123; /*无序、有序列表 缩进15px*/ padding-left: 25px; font-family:'微软雅黑'; font-size: 11px; letter-spacing: 0.5px; /*文字间距*/&#125;支持数十种代码高亮主题样式这个就有点牛逼了，内置了很多种非常不错的颜色主题，而且还可以显示代码行数。可对接七牛云账号，支持直接上传图片这个很方便，文章要插入图片，只需把图片拖拽进去，就会自动返回 URL 链接。至于七牛云，我是非常建议去注册个账号，可以把它当作文章图片的仓库。关于这款插件更多的功能，可以参考下面这个教程：玩转公众号markdown排版5. 复制到公众号，大功告成完成以上 3 步，就可以点击 Md2All 的「复制」按钮，然后打开微信公众号后台复制进去，格式会原模原样地保留下来，然后，发布就可以了。6. 小结本文所说的微信公众号排版，看似非常复杂，要用到很多软件，好像 5 分钟之内不可能排版好。其实，这些只是一次性工作，当你前期花点时间配置好各项操作后，以后的排版工作就「一劳永逸」了。5 分钟之内完成，是可以做得到的。本文完。推荐阅读：每周分享第1期：关于 PDF 处理软件，你需要的都在这里了每周分享第 2 期：一掏出手机，就暴露了程序猿身份每周分享第 3 期：安卓最好用的电子书阅读器欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>技能 GET</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyspider 爬取并分析虎嗅网 5 万篇文章]]></title>
    <url>%2F2018%2F11%2F04%2Fweb_scraping_withpython9.html</url>
    <content type="text"><![CDATA[数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。摘要： 不少时候，一篇文章能否得到广泛的传播，除了文章本身实打实的质量以外，一个好的标题也至关重要。本文爬取了虎嗅网建站至今共 5 万条新闻标题内容，助你找到起文章标题的技巧与灵感。同时，分享一些值得关注的文章和作者。1. 分析背景1.1. 为什么选择虎嗅在众多新媒体网站中，「虎嗅」网的文章内容和质量还算不错。在「新榜」科技类公众号排名中，它位居榜单第 3 名，还是比较受欢迎的。所以选择爬取该网站的文章信息，顺便从中了解一下这几年科技互联网都出现了哪些热点信息。「关于虎嗅」虎嗅网创办于 2012 年 5 月，是一个聚合优质创新信息与人群的新媒体平台。该平台专注于贡献原创、深度、犀利优质的商业资讯，围绕创新创业的观点进行剖析与交流。虎嗅网的核心，是关注互联网及传统产业的融合、明星公司的起落轨迹、产业潮汐的动力与趋势。1.2. 分析内容分析虎嗅网 5 万篇文章的基本情况，包括收藏数、评论数等发掘最受欢迎和最不受欢迎的文章及作者分析文章标题形式（长度、句式）与受欢迎程度之间的关系展现近些年科技互联网行业的热门词汇1.3. 分析工具PythonpyspiderMongoDBMatplotlibWordCloudJieba2. 数据抓取使用 pyspider 抓取了虎嗅网的主页文章，文章抓取时期为 2012 年建站至 2018 年 11 月 1 日，共计约 5 万篇文章。抓取 了 7 个字段信息：文章标题、作者、发文时间、评论数、收藏数、摘要和文章链接。2.1. 目标网站分析这是要爬取的 网页界面，可以看到是通过 AJAX 加载的。右键打开开发者工具查看翻页规律，可以看到 URL 请求是 POST 类型，下拉到底部查看 Form Data，表单需提交参数只有 3 项。经尝试， 只提交 page 参数就能成功获取页面的信息，其他两项参数无关紧要，所以构造分页爬取非常简单。123huxiu_hash_code: 39bcd9c3fe9bc69a6b682343ee3f024apage: 4last_dateline: 1541123160接着，切换选项卡到 Preview 和 Response 查看网页内容，可以看到数据都位于 data 字段里。total_page 为 2004，表示一共有 2004 页的文章内容，每一页有 25 篇文章，总共约 5 万篇，也就是我们要爬取的数量。以上，我们就找到了所需内容，接下来可以开始构造爬虫，整个爬取思路比较简单。之前我们也练习过这一类 Ajax 文章的爬取，可以参考：抓取澎湃网建站至今 1500 期信息图栏目图片2.2. pyspider 介绍和之前文章不同的是，这里我们使用一种新的工具来进行爬取，叫做：pyspider 框架。由国人 binux 大神开发，GitHub Star 数超过 12 K，足以证明它的知名度。可以说，学习爬虫不能不会使用这个框架。网上关于这个框架的介绍和实操案例非常多，这里仅简单介绍一下。我们之前的爬虫都是在 Sublime 、PyCharm 这种 IDE 窗口中执行的，整个爬取过程可以说是处在黑箱中，内部运行的些细节并不太清楚。而 pyspider 一大亮点就在于提供了一个可视化的 WebUI 界面，能够清楚地查看爬虫的运行情况。pyspider 的架构主要分为 Scheduler(调度器)、Fetcher(抓取器)、Processer(处理器)三个部分。Monitor(监控器)对整个爬取过程进行监控，Result Worker(结果处理器)处理最后抓取的结果。我们看看该框架的运行流程大致是怎么样的：一个 pyppider 爬虫项目对应一个 Python 脚本，脚本里定义了一个 Handler 主类。爬取时首先调用 on_start() 方法生成最初的抓取任务，然后发送给 Scheduler。Scheduler 将抓取任务分发给 Fetcher 进行抓取，Fetcher 执行然后得到 Response、随后将 Response 发送给 Processer。Processer 处理响应并提取出新的 URL 然后生成新的抓取任务，然后通过消息队列的方式通知 Scheduler 当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待 Result Worker 处理。Scheduler 接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回 Fetcher 进行抓取。不断重复以上工作、直到所有的任务都执行完毕，抓取结束。抓取结束后、程序会回调 on_finished() 方法，这里可以定义后处理过程。该框架比较容易上手，网页右边是代码区，先定义类（Class）然后在里面添加爬虫的各种方法（也可以称为函数），运行的过程会在左上方显示，左下方则是输出结果的区域。这里，分享几个不错的教程以供参考：GitHub 项目地址：https://github.com/binux/pyspider官方主页：http://docs.pyspider.org/en/latest/pyspider 中文网：http://www.pyspider.cn/page/1.htmlpyspider 爬虫原理剖析：http://python.jobbole.com/81109/pyspider 爬淘宝图案例实操：https://cuiqingcai.com/2652.html安装好该框架后，下面我们可以就开始爬取了。2.3. 抓取数据CMD 命令窗口执行：pyspider all 命令，然后浏览器输入：http://localhost:5000/ 就可以启动 pyspider 。点击 Create 新建一个项目，Project Name 命名为：huxiu，因为要爬取的 URL 是 POST 类型，所以这里可以先不填写，之后可以在代码中添加，再次点击 Creat 便完成了该项目的新建。新项目建立好后会自动生成一部分模板代码，我们只需在此基础上进行修改和完善，然后就可以运行爬虫项目了。现在，简单梳理下代码编写步骤。123456789101112from pyspider.libs.base_handler import *class Handler(BaseHandler): crawl_config:&#123; "headers":&#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', 'X-Requested-With': 'XMLHttpRequest' &#125; &#125; def on_start(self): for page in range(2,3): # 先循环1页 print('正在爬取第 %s 页' % page) self.crawl('https://www.huxiu.com/v2_action/article_list',method='POST',data=&#123;'page':page&#125;, callback=self.index_page)这里，首先定义了一个 Handler 主类，整个爬虫项目都主要在该类下完成。 接着，可以将爬虫基本的一些基本配置，比如 Headers、代理等设置写在下面的 crawl_config 属性中。（如果你还没有习惯从函数（def）转换到类（Class）的代码写法，那么需要先了解一下类的相关知识，之后我也会单独用一篇文章介绍一下。）下面的 on_start() 方法是程序的入口，也就是说程序启动后会首先从这里开始运行。首先，我们将要爬取的 URL传入 crawl() 方法，同时将 URL 修改成虎嗅网的：https://www.huxiu.com/v2_action/article_list。由于 URL 是 POST 请求，所以我们还需要增加两个参数：method 和 data。method 表示 HTTP 请求方式，默认是 GET，这里我们需要设置为 POST；data 是 POST 请求表单参数，只需要添加一个 page 参数即可。接着，通过 callback 参数定义一个 index_page() 方法，用来解析 crawl() 方法爬取 URL 成功后返回的 Response 响应。在后面的 index_page() 方法中，可以使用 PyQuery 提取响应中的所需内容。具体提取方法如下：123456789101112131415161718import jsonfrom pyquery import PyQuery as pqdef index_page(self, response): content = response.json['data'] # 注意，在sublime中，json后面需要添加()，pyspider 中则不用 doc = pq(content) lis = doc('.mod-art').items() data = [&#123; 'title': item('.msubstr-row2').text(), 'url':'https://www.huxiu.com'+ str(item('.msubstr-row2').attr('href')), 'name': item('.author-name').text(), 'write_time':item('.time').text(), 'comment':item('.icon-cmt+ em').text(), 'favorites':item('.icon-fvr+ em').text(), 'abstract':item('.mob-sub').text() &#125; for item in lis ] # 列表生成式结果返回每页提取出25条字典信息构成的list print(data) return data这里，网页返回的 Response 是 json 格式，待提取的信息存放在其中的 data 键值中，由一段 HTML 代码构成。我们可以使用 response.json[‘data’] 获取该 HTML 信息，接着使用 PyQuery 搭配 CSS 语法提取出文章标题、链接、作者等所需信息。这里使用了列表生成式，能够精简代码并且转换为方便的 list 格式，便于后续存储到 MongoDB 中。我们输出并查看一下第 2 页的提取结果：12345# 由25个 dict 构成的 list[&#123;'title': '想要长生不老？杀死体内的“僵尸细胞”吧', 'url': 'https://www.huxiu.com/article/270086.html', 'name': '造就Talk', 'write_time': '19小时前', 'comment': '4', 'favorites': '28', 'abstract': '如果有了最终疗法，也不应该是每天都需要接受治疗'&#125;, &#123;'title': '日本步入下流社会，我们还在买买买', 'url': 'https://www.huxiu.com/article/270112.html', 'name': '腾讯《大家》©', 'write_time': '20小时前', 'comment': '13', 'favorites': '142', 'abstract': '我买，故我在'&#125;...]可以看到，成功得到所需数据，然后就可以保存了，可以选择输出为 CSV、MySQL、MongoDB 等方式，这里我们选择保存到 MongoDB 中。1234567891011121314151617181920import pandas as pdimport pymongoimport timeimport numpy as npclient = pymongo.MongoClient('localhost',27017)db = client.Huxiumongo_collection = db.huxiu_newsdef on_result(self,result): if result: self.save_to_mongo(result) def save_to_mongo(self,result): df = pd.DataFrame(result) #print(df) content = json.loads(df.T.to_json()).values() if mongo_collection.insert_many(content): print('存储到 mongondb 成功') # 随机暂停 sleep = np.random.randint(1,5) time.sleep(sleep)上面，定义了一个 on_result() 方法，该方法专门用来获取 return 的结果数据。这里用来接收上面 index_page() 返回的 data 数据，在该方法里再定义一个存储到 MongoDB 的方法就可以保存到 MongoDB 中。关于数据如何存储到 MongoDB 中，我们在之前的 一篇文章 中有过介绍，如果忘记了可以回顾一下。下面，我们来测试一下整个爬取和存储过程。点击左上角的 run 就可以顺利运行单个网页的抓取、解析和存储，结果如下：上面完成了单页面的爬取，接下来，我们需要爬取全部 2000 余页内容。需要修改两个地方，首先在 on_start() 方法中将 for 循环页数 3 改为 2002。改好以后，如果我们直接点击 run ，会发现还是只能爬取第 2 页的结果。这是因为，pyspider 以 URL的 MD5 值作为 唯一 ID 编号，ID 编号相同的话就视为同一个任务，便不会再重复爬取。由于 GET 请求的 分页URL 通常是有差异的，所以 ID 编号会不同，也就自然能够爬取多页。但这里 POST 请求的分页 URL 是相同的，所以爬完第 2 页，后面的页数便不会再爬取。那有没有解决办法呢？ 当然是有的，我们需要重新写下 ID 编号的生成方式，方法很简单，在 on_start() 方法前面添加下面 2 行代码即可：12def get_taskid(self,task): return md5string(task['url']+json.dumps(task['fetch'].get('data','')))这样，我们再点击 run 就能够顺利爬取 2000 页的结果了，我这里一共抓取了 49,996 条结果，耗时 2 小时左右完成。以上，就完成了数据的获取。有了数据我们就可以着手分析，不过这之前还需简单地进行一下数据的清洗、处理。3. 数据清洗处理首先，我们需要从 MongoDB 中读取数据，并转换为 DataFrame。12345client = pymongo.MongoClient(host='localhost', port=27017)db = client['Huxiu']collection = db['huxiu_news']# 将数据库数据转为DataFramedata = pd.DataFrame(list(collection.find()))下面我们看一下数据的总体情况，可以看到数据的维度是 49996 行 × 8 列。发现多了一列无用的 _id 需删除，同时 name 列有一些特殊符号，比如© 需删除。另外，数据格式全部为 Object 字符串格式，需要将 comment 和 favorites 两列更改为数值格式、 write_time 列更改为日期格式。1234567891011121314151617181920212223print(data.shape) # 查看行数和列数print(data.info()) # 查看总体情况print(data.head()) # 输出前5行# 结果：(49996, 8)Data columns (total 8 columns):_id 49996 non-null objectabstract 49996 non-null objectcomment 49996 non-null objectfavorites 49996 non-null objectname 49996 non-null objecttitle 49996 non-null objecturl 49996 non-null objectwrite_time 49996 non-null objectdtypes: object(8) _id abstract comment favorites name title url write_time0 5bdc2 “在你们看到… 22 50 普象工业设计小站© 看了苹果屌 https:// 10小时前1 5bdc2 中国”绿卡”号称“世界最难拿” 9 16 经济观察报© 递交材料厚 https:// 10小时前2 5bdc2 鲜衣怒马少年时 2 13 小马宋 金庸小说陪 https:// 11小时前3 5bdc2 预告还是预警？ 3 10 Cuba Libre 阿里即将发 https:// 11小时前4 5bdc2 库克：咋回事？ 2 3 Cuba Libre 【虎嗅早报 https:// 11小时前代码实现如下：12345678910# 删除无用_id列data.drop(['_id'],axis=1,inplace=True)# 替换掉特殊字符©data['name'].replace('©','',inplace=True,regex=True)# 字符更改为数值data = data.apply(pd.to_numeric,errors='ignore')# 更该日期格式data['write_time'] = data['write_time'].replace('.*前','2018-10-31',regex=True) # 为了方便，将write_time列，包含几小时前和几天前的行，都替换为10月31日最后1天。data['write_time'] = pd.to_datetime(data['write_time'])下面，我们看一下数据是否有重复，如果有，那么需要删除。12345678910111213# 判断整行是否有重复值print(any(data.duplicated()))# 显示True，表明有重复值，进一步提取出重复值数量data_duplicated = data.duplicated().value_counts()print(data_duplicated) # 显示2 True ，表明有2个重复值# 删除重复值data = data.drop_duplicates(keep='first')# 删除部分行后，index中断，需重新设置indexdata = data.reset_index(drop=True)#结果：True False 49994True 2然后，我们再增加两列数据，一列是文章标题长度列，一列是年份列，便于后面进行分析。123456789101112data['title_length'] = data['title'].apply(len)data['year'] = data['write_time'].dt.yearData columns (total 9 columns):abstract 49994 non-null objectcomment 49994 non-null int64favorites 49994 non-null int64name 49994 non-null objecttitle 49994 non-null objecturl 49994 non-null objectwrite_time 49994 non-null datetime64[ns]title_length 49994 non-null int64year 49994 non-null int64以上，就完成了基本的数据清洗处理过程，针对这 9 列数据可以开始进行分析了。4. 描述性数据分析通常，数据分析主要分为四类： 「描述型分析」、「诊断型分析」「预测型分析」「规范型分析」。「描述型分析」是用来概括、表述事物整体状况以及事物间关联、类属关系的统计方法，是这四类中最为常见的数据分析类型。通过统计处理可以简洁地用几个统计值来表示一组数据地集中性（如平均值、中位数和众数等）和离散型(反映数据的波动性大小，如方差、标准差等)。这里，我们主要进行描述性分析，数据主要为数值型数据（包括离散型变量和连续型变量）和文本数据。4.1. 总体情况先来看一下总体情况。12345678910print(data.describe()) comment favorites title_length count 49994.000000 49994.000000 49994.000000 mean 10.860203 34.081810 22.775333 std 24.085969 48.276213 9.540142 min 0.000000 0.000000 1.000000 25% 3.000000 9.000000 17.000000 50% 6.000000 19.000000 22.000000 75% 12.000000 40.000000 28.000000 max 2376.000000 1113.000000 224.000000这里，使用了 data.describe() 方法对数值型变量进行统计分析。从上面可以简要得出以下几个结论：读者的评论和收藏热情都不算太高，大部分文章（75 %）的评论数量为十几条，收藏数量不过几十个。这和一些微信大 V 公众号动辄百万级阅读、数万级评论和收藏量相比，虎嗅网的确相对小众一些。不过也正是因为小众，也才深得部分人的喜欢。评论数最多的文章有 2376 条，收藏数最多的文章有 1113 个收藏量，说明还是有一些潜在的比较火或者质量比较好的文章。最长的文章标题长达 224 个字，大部分文章标题长度在 20 来个字左右，所以标题最好不要太长或过短。对于非数值型变量（name、write_time），使用 describe() 方法会产生另外一种汇总统计。1234567891011121314print(data['name'].describe())print(data['write_time'].describe())# 结果：count 49994unique 3162top 虎嗅freq 10513Name: name, dtype: objectcount 49994unique 2397top 2014-07-10 00:00:00freq 274first 2012-04-03 00:00:00last 2018-10-31 00:00:00unique 表示唯一值数量，top 表示出现次数最多的变量，freq 表示该变量出现的次数，所以可以简单得出以下几个结论：在文章来源方面，3162 个作者贡献了这 5 万篇文章，其中自家官网「虎嗅」写的数量最多，超过了 1 万篇，这也很自然。在文章发表时间方面，最早的一篇文章来自于 2012年 4 月 3 日。 6 年多时间，发文数最多的 1 天 是 2014 年 7 月 10 日，一共发了 274 篇文章。4.2. 不同时期文章发布的数量变化可以看到 ，以季度为时间尺度的6 年间，前几年发文数量比较稳定，大概在1750 篇左右，个别季度数量激增到 2000 篇以上。2016 年之后文章开始增加到 2000 篇以上，可能跟网站知名度提升有关。首尾两个季度日期不全，所以数量比较少。具体代码实现如下：1234567891011121314151617181920212223242526272829def analysis1(data): # # 汇总统计 # print(data.describe()) # print(data['name'].describe()) # print(data['write_time'].describe()) data.set_index(data['write_time'],inplace=True) data = data.resample('Q').count()['name'] # 以季度汇总 data = data.to_period('Q') # 创建x,y轴标签 x = np.arange(0,len(data),1) ax1.plot(x,data.values, #x、y坐标 color = color_line , #折线图颜色为红色 marker = 'o',markersize = 4 #标记形状、大小设置 ) ax1.set_xticks(x) # 设置x轴标签为自然数序列 ax1.set_xticklabels(data.index) # 更改x轴标签值为年份 plt.xticks(rotation=90) # 旋转90度，不至太拥挤 for x,y in zip(x,data.values): plt.text(x,y + 10,'%.0f' %y,ha = 'center',color = colors,fontsize=fontsize_text ) # '%.0f' %y 设置标签格式不带小数 # 设置标题及横纵坐标轴标题 plt.title('虎嗅网文章数量发布变化(2012-2018)',color = colors,fontsize=fontsize_title) plt.xlabel('时期') plt.ylabel('文章(篇)') plt.tight_layout() # 自动控制空白边缘 plt.savefig('虎嗅网文章数量发布变化.png',dpi=200) plt.show()4.3. 文章收藏量 TOP 10接下来，到了我们比较关心的问题：几万篇文章里，到底哪些文章写得比较好或者比较火？序号titlefavoritescomment1读完这10本书，你就能站在智商鄙视链的顶端了1113132京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利867103离职创业？先读完这22本书再说86094货币如水，覆水难收784395自杀经济学77811962016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里774397真正强大的商业分析能力是怎样炼成的？746188腾讯没有梦想705329段永平连答53问，核心是“不为清单”7032710王健林的滑铁卢70192此处选取了「favorites」(收藏数量)作为衡量标准。毕竟，一般好的文章，我们都会有收藏的习惯。第一名「读完这10本书，你就能站在智商鄙视链的顶端了 」以 1113 次收藏位居第一，并且遥遥领先于后者，看来大家都怀有「想早日攀上人生巅峰，一览众人小」的想法啊。打开这篇文章的链接，文中提到了这几本书：《思考，快与慢》、《思考的技术》、《麦肯锡入职第一课：让职场新人一生受用的逻辑思考力》等。一本都没看过，看来这辈子是很难登上人生巅峰了。发现两个有意思的地方。第一，文章标题都比较短小精炼。第二，文章收藏量虽然比较高，但评论数都不多，猜测这是因为 大家都喜欢做伸手党？4.4. 历年文章收藏量 TOP3在了解文章的总体排名之后，我们来看看历年的文章排名是怎样的。这里，每年选取了收藏量最多的 3 篇文章。yeartitlefavorites2012产品的思路——来自腾讯张小龙的分享（全版）187Fab CEO：创办四家公司教给我的90件事163张小龙：微信背后的产品观1622013创业者手记：我所犯的那些入门错误473马化腾三小时讲话实录：千亿美金这个线，其实很恐怖391雕爷亲身谈：白手起家的我如何在30岁之前赚到1000万。读《MBA教不了的创富课》354201485后，突变的一代528雕爷自述：什么是我做餐饮时琢磨、而大部分“外人”无法涉猎的思考？521据说这40张PPT是蚂蚁金服的内部培训资料……4852015读完这10本书，你就能站在智商鄙视链的顶端了1113京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利867离职创业？先读完这22本书再说8602016蝗虫般的刷客大军：手握千万手机号，分秒间薅干一家平台554准CEO必读的这20本书，你读过几本？548运营简史：一文读懂互联网运营的20年发展与演变50320172016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里774真正强大的商业分析能力是怎样炼成的？746王健林的滑铁卢7012018货币如水，覆水难收784自杀经济学778腾讯没有梦想705可以看到，文章收藏量基本是逐年递增的，但 2015 年的 3 篇文章的收藏量却是最高的，包揽了总排名的前 3 名，不知道这一年的文章有什么特别之处。以上只罗列了一小部分文章的标题，可以看到标题起地都蛮有水准的。关于标题的重要性，有这样通俗的说法：「一篇好文章，标题占一半」，一个好的标题可以大大增强文章的传播力和吸引力。文章标题虽只有短短数十字，但要想起好，里面也是很有很多技巧的。好在，这里提供了 5 万个标题可以参考。如需，可以在公众号后台回复「虎嗅」得到这份 CSV 文件。代码实现如下：12345678910111213141516171819202122232425262728293031323334def analysis2(data): # # 总收藏排名 # top = data.sort_values(['favorites'],ascending = False) # # 收藏前10 # top.index = (range(1,len(top.index)+1)) # 重置index，并从1开始编号 # print(top[:10][['title','favorites','comment']]) # 按年份排名 # # 增加一列年份列 # data['year'] = data['write_time'].dt.year def topn(data): top = data.sort_values('favorites',ascending=False) return top[:3] data = data.groupby(by=['year']).apply(topn) print(data[['title','favorites']]) # 增加每年top123列，列依次值为1、2、3 data['add'] = 1 # 辅助 data['top'] = data.groupby(by='year')['add'].cumsum() data_reshape = data.pivot_table(index='year',columns='top',values='favorites').reset_index() # print(data_reshape) # ok data_reshape.plot( # x='year', y=[1,2,3], kind='bar', width=0.3, color=['#1362A3','#3297EA','#8EC6F5'] # 设置不同的颜色 # title='虎嗅网历年收藏数最多的3篇文章' ) plt.xlabel('Year') plt.ylabel('文章收藏数量') plt.title('历年 TOP3 文章收藏量比较',color = colors,fontsize=fontsize_title) plt.tight_layout() # 自动控制空白边缘，以全部显示x轴名称 # plt.savefig('历年 Top3 文章收藏量比较.png',dpi=200) plt.show()4.4.1. 最高产作者 TOP20上面，我们从收藏量指标进行了分析,下面，我们关注一下发布文章的作者（个人/媒体）。前面提到发文最多的是虎嗅官方，有一万多篇文章，这里我们筛除官媒，看看还有哪些比较高产的作者。可以看到，前 20 名作者的发文量差距都不太大。发文比较多的有「娱乐资本论」、「Eastland」、「发条橙子」这类媒体号；也有虎嗅官网团队的作者：发条橙子、周超臣、张博文等；还有部分独立作者：假装FBI、孙永杰等。可以尝试关注一下这些高产作者。代码实现如下：12345678910111213def analysis3(data): data = data.groupby(data['name'])['title'].count() data = data.sort_values(ascending=False) # pandas 直接绘制,.invert_yaxis()颠倒顺序 data[1:21].plot(kind='barh',color=color_line).invert_yaxis() for y,x in enumerate(list(data[1:21].values)): plt.text(x+12,y+0.2,'%s' %round(x,1),ha='center',color=colors) plt.xlabel('文章数量') plt.ylabel('作者') plt.title('发文数量最多的 TOP20 作者',color = colors,fontsize=fontsize_title) plt.tight_layout() plt.savefig('发文数量最多的TOP20作者.png',dpi=200) plt.show()4.4.2. 平均文章收藏量最多作者 TOP 10我们关注一个作者除了是因为文章高产以外，可能更看重的是其文章水准。这里我们选择「文章平均收藏量」（总收藏量/文章数）这个指标，来看看文章水准比较高的作者是哪些人。这里，为了避免出现「某作者只写了一篇高收藏率的文章」这种不能代表其真实水准的情况，我们将筛选范围定在至少发布过 5 篇文章的作者们。nametotal_favoritesariticls_numavg_favorites重读19476324楼台23028287彭萦24879276曹山石11875237饭统戴老板787036218笔记侠15868198辩手李慕阳1198962193李录237013182高晓松8895177宁南山282716176可以看到，前 10 名作者包括：遥遥领先的 重读、两位高产又有质量的 辩手李慕阳 和 饭统戴老板 ，还有大众比较熟悉的 高晓松、宁南山 等。如果你将这份名单和上面那份高产作者名单进行对比，会发现他们没有出现在这个名单中。相比于数量，质量可能更重要吧。下面，我们就来看看排名第一的 重读 都写了哪些高收藏量文章。ordertitlefavoriteswrite_time1我采访出200多万字素材，还原了阿里系崛起前传2312018/10/312阿里史上最强人事地震回顾：中供铁军何以被生生解体4942018/4/93马云“斩”卫哲：复原阿里史上最震撼的人事地震5782018/3/154重读一场马云发起、针对卫哲的批斗会2692017/8/315阿里“中供系”前世今生：马云麾下最神秘的子弟兵2032017/5/106揭秘马云麾下最神秘的子弟兵：阿里“中供系”的前世今生1722017/4/26居然写的都是清一色关于马老板家的文章。了解了前十名作者之后，我们顺便也看看那些处于最后十名的都是哪些作者。nametotal_favoritesariticls_numavg_favorites于斌25112朝克图33231东风日产24131董晓常1481蔡钰31161马继华12111angeljie751薛开元661pookylee15240Yang Yemeng070一对比，就能看到他们的文章收藏量就比较寒碜了。尤其好奇最后一位作者 Yang Yemeng ，他写了 7 篇文章，竟然一个收藏都没有。来看看他究竟写了些什么文章。原来写的全都是英文文章，看来大家并不太钟意阅读英文类的文章啊。具体实现代码：12345678910111213141516171819def analysis4(data): data = pd.pivot_table(data,values=['favorites'],index='name',aggfunc=[np.sum,np.size]) data['avg'] = data[('sum','favorites')]/data[('size','favorites')] # 平均收藏数取整 # data['avg'] = data['avg'].round(decimals=1) data['avg'] = data['avg'].astype('int') # flatten 平铺列 data.columns = data.columns.get_level_values(0) data.columns = ['total_favorites','ariticls_num','avg_favorites'] # 筛选出文章数至少5篇的 data=data.query('ariticls_num &gt; 4') data = data.sort_values(by=['avg_favorites'],ascending=False) # # 查看平均收藏率第一名详情 # data = data.query('name == "重读"') # # 查看平均收藏率倒数第一名详情 # data = data.query('name == "Yang Yemeng"') # print(data[['title','favorites','write_time']]) print(data[:10]) # 前10名 print(data[-10:]) # 后10名4.5. 文章评论数最多 TOP10说完了收藏量。下面，我们再来看看评论数量最多的文章是哪些。ordertitlecommentfavorites1喜瓜2.0—明星社交应用的中国式引进与创新237632百度，请给“儿子们”好好起个名字129793三星S5为什么对凤凰新闻客户端下注？115714三星Tab S：马是什么样的马？鞍又是什么样的鞍？95105三星，正在重塑你的营销观91416马化腾，你就把微信卖给运营商得了！743207【文字直播】罗永浩 VS 王自如 网络公开辩论711338看三星Hub如何推动数字内容消费变革68419三星要重新定义软件与内容商店新模式，SO?670010三星Hub——数字内容交互新模式6110基本上都是和 三星 有关的文章，这些文章大多来自 2014 年，那几年 三星 好像是挺火的，不过这两年国内基本上都见不到三星的影子了，世界变化真快。发现了两个有意思的现象。第一，上面关于 三星 和前面 阿里 的这些批量文章，它们「霸占」了评论和收藏榜，结合知乎上曾经的一篇关于介绍虎嗅这个网站的文章：虎嗅网其实是这样的 ，貌似能发现些微妙的事情。第二，这些文章评论数和收藏数两个指标几乎呈极端趋势，评论量多的文章收藏量却很少，评论量少的文章收藏量却很多。我们进一步观察下这两个参数的关系。可以看到，大多数点都位于左下角，意味着这些文章收藏量和评论数都比较低。但也存在少部分位于上方和右侧的异常值，表明这些文章呈现 「多评论、少收藏」或者「少评论、多收藏」的特点。4.6. 文章标题长度下面，我们再来看看文章标题的长度和收藏量之间有没有什么关系。大致可以看出两点现象：第一，收藏量高的文章，他们的标题都比较短（右侧的部分散点）。第二，标题很长的文章，它们的收藏量都非常低（左边形成了一条垂直线）。看来，文章起标题时最好不要起太长的。实现代码如下：1234567891011def analysis5(data): plt.scatter( x=data['favorites'], y =data['comment'], s=data['title_length']/2, ) plt.xlabel('文章收藏量') plt.ylabel('文章评论数') plt.title('文章标题长度与收藏量和评论数之间的关系',color = colors,fontsize=fontsize_title) plt.tight_layout() plt.show()文章标题形式下面，我们看看作者在起文章标题的时候，在标点符号方面有没有什么偏好。可以看到，五万篇文章中，大多数文章的标题是陈述性标题。三分之一（34.8%） 的文章标题使用了问号「？」，而仅有 5% 的文章用了叹号「！」。通常，问号会让人们产生好奇，从而想去点开文章；而叹号则会带来一种紧张或者压迫感，使人不太想去点开。所以，可以尝试多用问号而少用叹号。4.7. 文本分析最后，我们从这 5 万篇文章中的标题和摘要中，来看看虎嗅网的文章主要关注的都是哪些主题领域。这里首先运用了 jieba 分词包对标题进行了分词，然后用 WordCloud 做成了词云图，因虎嗅网含有「虎」字，故选取了一张老虎头像。（关于 jieba 和 WordCloud 两个包，之后再详细介绍）可以看到文章的主题内容侧重于：互联网、知名公司、电商、投资这些领域。这和网站本身对外宣传的核心内容，即「关注互联网与移动互联网一系列明星公司的起落轨迹、产业潮汐的动力与趋势，以及互联网与移动互联网如何改造传统产业」大致相符合。实现代码如下：1234567891011121314151617181920212223242526272829303132333435363738def analysis6(data): text='' for i in data['title'].values: symbol_to_replace = '[!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@，。?★、…【】《》？“”‘’！[\\]^_`&#123;|&#125;~]+' i = re.sub(symbol_to_replace,'',i) text+=' '.join(jieba.cut(i,cut_all=False)) d = path.dirname(__file__) if "__file__" in locals() else os.getcwd() background_Image = np.array(Image.open(path.join(d, "tiger.png"))) font_path = 'C:\Windows\Fonts\SourceHanSansCN-Regular.otf' # 思源黑字体 # 添加stopswords stopwords = set() # 先运行对text进行词频统计再排序，再选择要增加的停用词 stopwords.update(['如何','怎么','一个','什么','为什么','还是','我们','为何','可能','不是','没有','哪些','成为','可以','背后','到底','就是','这么','不要','怎样','为了','能否','你们','还有','这样','这个','真的','那些']) wc = WordCloud( background_color = 'black', font_path = font_path, mask = background_Image, stopwords = stopwords, max_words = 2000, margin =2, max_font_size = 100, random_state = 42, scale = 2, ) wc.generate_from_text(text) process_word = WordCloud.process_text(wc, text) # 下面是字典排序 sort = sorted(process_word.items(),key=lambda e:e[1],reverse=True) # sort为list print(sort[:50]) # 输出前词频最高的前50个，然后筛选出不需要的stopwords，添加到前面的stopwords.update()方法中 img_colors = ImageColorGenerator(background_Image) wc.recolor(color_func=img_colors) # 颜色跟随图片颜色 plt.imshow(wc,interpolation='bilinear') plt.axis('off') plt.tight_layout() # 自动控制空白边缘 plt.savefig('huxiu20.png',dpi=200) plt.show()上面的关键词是这几年总体的概况，而科技互联网行业每年的发展都是不同的，所以，我们再来看看历年的一些关键词，透过这些关键词看看这几年互联网行业、科技热点、知名公司都有些什么不同变化。可以看到每年的关键词都有一些相同之处，但也不同的地方：中国互联网、公司、苹果、腾讯、阿里等这些热门关键词一直都是热门，这几家公司真是稳地一批啊。每年会有新热点涌现：比如 2013 年的微信（刚开始火）、2016 年的直播（各大直播平台如雨后春笋般出现）、2017年的 iPhone（上市十周年）、2018年的小米（上市）。不断有新的热门技术出现：2013 - 2015 年的 O2O、2016 年的 VR、2017 年的 AI 、2018 年的「区块链」。这些科技前沿技术也是这几年大家口耳相传的热门词汇。通过这一幅图，就看出了这几年科技互联网行业、明星公司、热点信息的风云变化。5. 小结本文简要分析了虎嗅网 5 万篇文章信息，大致了解了近些年科技互联网的千变万化。发掘了那些优秀的文章和作者，能够节省宝贵的时间成本。一篇文章要想传播广泛，文章本身的质量和标题各占一半，文中的5 万个标题相信能够带来一些灵感。本文尚未做深入的文本挖掘，而文本挖掘可能比数据挖掘涵盖的信息量更大，更有价值。进行这些分析需要机器学习和深度学习的知识，待后期学习后再来补充。本文完。文中的完整代码和素材可以在公众号后台回复「虎嗅」 或者在下面的链接中获取：https://github.com/makcyun/web_scraping_with_python推荐阅读：做 PPT 没灵感？澎湃网 1500 期信息图送给你听说你想创业找投资？国内创业公司的信息都在这里了欢迎扫一扫识别关注我的公众号]]></content>
      <categories>
        <category>Python爬虫,Python分析,Python可视化</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Python可视化</tag>
        <tag>Python分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安卓最好用的电子书阅读器]]></title>
    <url>%2F2018%2F11%2F02%2Fweekly_sharing3.html</url>
    <content type="text"><![CDATA[推荐一款最好用的手机阅读 App。这是每周分享的第 3 期。先简单介绍下这个栏目，顾名思义，就是会在每个周末分享一篇文章。内容主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。好，下面开始进入正题。我们大多数人每天当中花时间花的最多的事情，应该就是看手机了，聊天 玩游戏、看视频、或者阅读学习等。我呢，手机阅读的时间相对多一些。自从学习 Python 以来，手机上下载了很多的 PDF 电子书，然后在 WPS 中阅读。起初一直觉得很方便，各个文件之间的切换也非常顺畅。不过，用久了之后发现有几个不方便的地方：目录跳转麻烦有时看书不是从头看到尾的，而是跳跃地阅读部分章节。需要先定位到所在目录页数，再滑动到相应位置，还要看其他章节时，则需重复同样的操作。这样一来，需要花费较多的时间在目录和内容的切换上，真正阅读的时间减少了。不支持其他电子书格式除了 PDF 格式以外，WPS 几乎不支持 txt、epub、mobi 等格式。阅读功能单一缺少诸如：蓝光过滤、字体背景设置、滚动翻页等功能。缺少自定义性功能WPS 走的是简洁路线，提供的自定义功能很少，用来查看文件还可以，但是阅读就显得力不从心了。发现了这些痛点之后，我便开始寻找替代品，前后试用了多款 App。最近，终于找到一款能够解决以上所有问题的阅读器。这款堪称 「安卓最好用的阅读器」，名叫：「静读天下」。很多非常棒的 App 都具备一个特质是：提供 高度可自定义 的使用功能以满足不同用户的喜好与需求。而 静读天下 就是其中的佼佼者之一。用了它之后，阅读体验得到了大大提升，下面一一道来它的各种好。1. 看书1.1. 目录快速跳转定位它最实用的一个功能就是「能够自动识别电子书的目录」。单击屏幕即可呼出章节目录，然后点击想查看的章节内容即可快速跳转到相应位置。看完以后如果想看其他章节内容，只需要重复操作一次即可。这样，就缩短了查找内容的时间，从而可以更加专注于阅读。1.2. 便捷的护眼功能每天看书的时间是在变化的，白天和晚上的光线会有所不同。因而护眼阅读就显得很有必要。虽然手机上有调节亮度的功能，但功能不全且需要额外操作。所幸，该 App 里只需通过触屏手势就能实现 「快速调节亮度」 和 「蓝光过滤 」功能。另外，还能够自动切换白天/晚上模式。1.3. 字体、配色、背景设置它支持的电子书格式除了 PDF 以外，还支持 txt、epub、mobi 等多种格式。这些格式支持设置字体、配色、背景等功能。比如，看惯了无衬线字体，想体验下有棱角的衬线字体，那么可以下载自己喜欢的字体然后使用。字体的大小、颜色、行距、留白都可以凭自己喜好设置，甚至还可以竖版阅读。除此之外，背景也是可以随意更换的，内置十几种背景，不够还可以下载自己自己喜欢的，可以说提供了无限选择。1.4. 添加批注有时，会边阅读边做些批注，比如添加：高亮、下划线、删除线、标注等。它提供了丰富的批注功能，批注好后可以在书签页面里看到，方便下次快速查找到。1.5. 书籍分类如果手机里有很多且类别不同的电子书，有了这个书籍分类功能会更方便。比如，我将电子书分为 Pyhon、数学、机器学习等类别，想看哪一类就筛选即可。1.6. 添加书签当需要暂停阅读时，可以添加书签，下次再看时直接点击即可快速回到上次的阅读位置。1.7. 数百种自定义组合功能可以看到，上面所实现的能都是通过设定功能所对应的触屏手势完成的。该 App 可以实现的功能包括：翻页、搜索、书签、配色、导航等 15 种事件。具体功能对应的操作包括：屏幕点击、滑动手势、手机物理按键等 21 种操作。恩，也就是说，如果你喜欢追求极端个性化操作的话，App 为你提供了 300 多种方案，足以满足你对快捷键的癖好。2. 下书要阅读，那就得有电子书。这里推荐两个非常棒的电子书下载 App：「搜书大师」 和 「小寻书」，利用它们基本都能找到你所需的书。3. 导入下载好以后，定位到电子书所在位置，然后导入即可。另外，还可以设置过滤来选择导入的文件类型。OK，还有很多好用的功能，这里就不再往下罗列了，感兴趣的话可以到官网进一步了解：https://www.moondownload.com/chinese.html如果你想体验下这 3 款 App，可以在网上搜索下载或者在公众号后台回复「阅读器」得到。本文完。推荐阅读：每周分享第1期：关于 PDF 处理软件，你需要的都在这里了每周分享第 2 期：一掏出手机，就暴露了程序猿身份欢迎长按识别关注我的公众号–&gt;]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
        <tag>佳软</tag>
        <tag>APP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周分享第 2 期：一掏出手机，就暴露了程序猿身份]]></title>
    <url>%2F2018%2F10%2F26%2Fweekly_sharing2.html</url>
    <content type="text"><![CDATA[推荐一款神级桌面 App：Aris 终端桌面。这是「每周分享」的第 2 期。先简单介绍下这个栏目。顾名思义，就是会在每个周末分享一篇文章。内容涵盖范围会比较广，主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。好，下面开始进入正题。自从我的手机桌面变成这个样子以后，每当在电梯里掏出手机时，总隐隐约约能感觉到有异样的眼神。我猜他们心里在想：「这人要么是个程序猿，要么就是个装 X 犯。」谁还没点极客精神，是吧？这一期，想向你推荐这款我用了几个月并且爱不释手的 App：「Aris 终端桌面」，有 3 个原因。首先，它能用来装 X。提供了多种桌面主题，且高度可自定义化。只要你想，就可以打造出独一无二的桌面来。其次，它体积只有 4 M大，且非常省电。光这两个特点就足以秒杀众多其他桌面。最后，也是最重要的，它可以改变我们玩手机的方式。`如果说一般的手机桌面是 Windows 的话，这个 App 桌面就是 Linux，一切操作都靠命令来完成，快捷方便。它有很多实用的功能 ，下面一一道来。1. 打开 App它最实用的功能就是通常输入不超过 3 个字母 就能够快速打开手机上的 App。不用像普通桌面需要 切换或者下拉屏幕 挨个去找那么复杂。比如打开 「微信」、「支付宝」分别只需要输入字母 W 和 ZF 就能打开，1 秒钟都要不到。对于平常不太常用的功能比如「设置」、「闹钟」这些，仍然只需要输入几个字母，很快就能打开，不用再到处去找了。(双击看高清大图)2. 打开微信「扫一扫」我们每天几乎都要打开微信「扫一扫」付款或者添加好友。常规操作需要打开微信右上角的「+ 号」，再点击扫一扫。而使用它，只需要输入「wes」就能调用 wescan 命令直接打开「扫一扫」，比常规操作快多了。3. 打电话打电话也很方便，像搜索 App 一样，可以直接输入联系人拼音字母，然后拨号就行了。省去进到电话 App 里面去拨打的步骤。上面只是常规操作，下面介绍些「骚」操作。4. 打开手电筒 / WIFI当需要打开手电筒时，那么只要输入 「to」就能打开手电筒的命令「Torch」，用完要关上就再输入一次「to」就可以；当走到一个可以连 WIFI的地方，只需要输入 WI，就能打开 WIFI命令。当然，其他很多工具都可以打开。(双击看高清大图)5. 查天气如果你想查一下某个城市的天气，比如「北京」那么只需要输入：beijing weather。(双击看高清大图)6. 查股价如果你炒股，还可以查询股票股价，比如查询下「万科」今天的股价，只需要输入：sz.000002 stock。(双击看高清大图)7. 翻译还可以翻译单词，比如遇到一个不熟悉的词： charming，那么输入：charming translating ，就会返回该词的翻译：迷人的。8. 发送 apk 到其他软件这是个非常实用的功能，可以把手机上的 App 直接打包成 Apk 文件发给别人。比如我手机上有个好用的 WPS Office，然后想通过 QQ 发送给朋友，则可以使用下面的命令：wpsoffice apk qq 就能发送了。还有很多好用的功能，这里就不再往下罗列了，可以看官方文档进一步了解这款 App 是怎么使用的：https://7doger.gitbooks.io/aris/到这里，如果你想下载下来把玩一下，可以在网上搜索下载或者在后台回复「aris」得到这款 App。最后，知乎上有一篇讲 「极客精神」的文章我觉得写地很好，分享给你。所谓的“极客精神”，描述起来就这样简单——“好奇之心与改变之力”。你不需要一定是个程序员或者是产品经理，也不一定变成一副科技宅的模样，如果你对世界充满好奇心和探索精神，并愿意自己去创造哪怕一些改变——这，其实就够了。本文完。推荐阅读：每周分享第1期：关于 PDF 处理软件，你需要的都在这里了欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>每周分享</category>
      </categories>
      <tags>
        <tag>神器</tag>
        <tag>佳软</tag>
        <tag>APP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(8)：模拟登录方法汇总]]></title>
    <url>%2F2018%2F10%2F20%2Fweb_scraping_withpython8.html</url>
    <content type="text"><![CDATA[对于很多要先登录的网站来说，模拟登录往往是爬虫的第一道坎。本文介绍 POST 请求登录、获取 Cookies 登录、Seleium 模拟登录三种方法。摘要： 在进行爬虫时，除了常见的不用登录就能爬取的网站，还有一类需要先登录的网站。比如豆瓣、知乎，以及上一篇文章中的桔子网。这一类网站又可以分为：只需输入帐号密码、除了帐号密码还需输入或点击验证码等类型。本文以只需输入账号密码就能登录的桔子网为例，介绍模拟登录常用的 3 种方法。POST 请求方法：需要在后台获取登录的 URL并填写请求体参数，然后 POST 请求登录，相对麻烦；添加 Cookies 方法：先登录将获取到的 Cookies 加入 Headers 中，最后用 GET 方法请求登录，这种最为方便；Selenium 模拟登录：代替手工操作，自动完成账号和密码的输入，简单但速度比较慢。下面，我们用代码分别实现上述 3 种方法。1. 目标网页这是我们要获取内容的网页：http://radar.itjuzi.com/investevent这个网页需要先登录才能看到数据信息，登录界面如下：可以看到，只需要输入账号和密码就可以登录，不用输验证码，比较简单。下面我们利用一个测试账号和密码，来实现模拟登录。2. POST 提交请求登录首先，我们要找到 POST 请求的 URL。有两种方法，第一种是在网页 devtools 查看请求，第二种是在 Fiddler 软件中查看。先说第一种方法。在登录界面输入账号密码，并打开开发者工具，清空所有请求，接着点击登录按钮，这时便会看到有大量请求产生。哪一个才是 POST 请求的 URL呢？这个需要一点经验，因为是登录，所以可以尝试点击带有 「login」字眼的请求。这里我们点击第四个请求，在右侧 Headers 中可以看到请求的 URL，请求方式是 POST类型，说明 URL 找对了。接着，我们下拉到 Form Data，这里有几个参数，包括 identify 和 password，这两个参数正是我们登录时需要输入的账号和密码，也就是 POST 请求需要携带的参数。参数构造非常简单，接下来只需要利用 Requests.post 方法请求登录网站，然后就可以爬取内容了。下面，我们尝试用 Fiddler 获取 POST 请求。如果你对 Fiddler 还不太熟悉或者没有电脑上没有安装，可以先了解和安装一下。Fiddler 是位于客户端和服务器端的 HTTP 代理，也是目前最常用的 HTTP 抓包工具之一 。 它能够记录客户端和服务器之间的所有 HTTP 请求，可以针对特定的 HTTP 请求，分析请求数据、设置断点、调试 web 应用、修改请求的数据，甚至可以修改服务器返回的数据，功能非常强大，是 web 调试的利器。Fiddler 下载地址：https://www.telerik.com/download/fiddler使用教程：https://zhuanlan.zhihu.com/p/37374178http://www.hangge.com/blog/cache/detail_1697.html下面，我们就通过 Fiddler 截取登录请求。当点击登录时，官场 Fiddler 页面，左侧可以看到抓取了大量请求。通过观察，第15个请求的 URL中含有「login」字段，很有可能是登录的 POST 请求。我们点击该请求，回到右侧，分别点击「inspectors」、「Headers」，可以看到就是 POST 请求，该 URL 和上面的方法获取的 URL 是一致的。接着，切换到右侧的 Webforms 选项，可以看到 Body 请求体。也和上面方法中得到的一致。获取到 URL 和请求体参数之后，下面就可以开始用 Requests.post 方法模拟登录了。代码如下：123456789101112131415import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', &#125;data = &#123; 'identity':'irw27812@awsoo.com', 'password':'test2018',&#125;url ='https://www.itjuzi.com/user/login?redirect=&amp;flag=&amp;radar_coupon='session = requests.Session()session.post(url,headers = headers,data = data)# 登录后，我们需要获取另一个网页中的内容response = session.get('http://radar.itjuzi.com/investevent',headers = headers)print(response.status_code)print(response.text)使用 session.post 方法提交登录请求，然后用 session.get 方法请求目标网页，并输出 HTML代码。可以看到，成功获取到了网页内容。下面，介绍第 2 种方法。3. 获取 Cookies，直接请求登录上面一种方法，我们需要去后台获取 POST 请求链接和参数，比较麻烦。下面，我们可以尝试先登录，获取 Cookie，然后将该 Cookie 添加到 Headers 中去，然后用 GET 方法请求即可，过程简单很多。代码如下：1234567891011import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', 'Cookie': '你的cookie',&#125;url = 'https://www.itjuzi.com/user/login?redirect=&amp;flag=&amp;radar_coupon='session = requests.Session()response = session.get('http://radar.itjuzi.com/investevent', headers=headers)print(response.status_code)print(response.text)可以看到，添加了 Cookie 后就不用再 POST 请求了，直接 GET 请求目标网页即可。可以看到，也能成功获取到网页内容。下面介绍第 3 种方法。4. Selenium 模拟登录这个方法很直接，利用 Selenium 代替手动方法去自动输入账号密码然后登录就行了。关于 Selenium 的使用，在之前的一篇文章中有详细介绍，如果你不熟悉可以回顾一下：https://www.makcyun.top/web_scraping_withpython5.html代码如下：12345678910111213141516171819202122232425262728from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()browser.maximize_window() # 最大化窗口wait = WebDriverWait(browser, 10) # 等待加载10sdef login(): browser.get('https://www.itjuzi.com/user/login') input = wait.until(EC.presence_of_element_located( (By.XPATH, '//*[@id="create_account_email"]'))) input.send_keys('irw27812@awsoo.com') input = wait.until(EC.presence_of_element_located( (By.XPATH, '//*[@id="create_account_password"]'))) input.send_keys('test2018') submit = wait.until(EC.element_to_be_clickable( (By.XPATH, '//*[@id="login_btn"]'))) submit.click() # 点击登录按钮 get_page_index()def get_page_index(): browser.get('http://radar.itjuzi.com/investevent') try: print(browser.page_source) # 输出网页源码 except Exception as e: print(str(e))login()这里，我们在网页中首先定位了账号节点位置：&#39;//*[@id=&quot;create_account_email&quot;]&#39;，然后用 input.send_keys 方法输入账号，同理，定位了密码框位置并输入了密码。接着定位 登录 按钮的位置：//*[@id=&quot;login_btn&quot;]，然后用 submit.click() 方法实现点击登录按钮操作，从而完成登录。可以看到，也能成功获取到网页内容。以上就是模拟需登录网站的几种方法。当登录进去后，就可以开始爬取所需内容了。5. 总结：本文分别实现了模拟登录的 3 种操作方法，建议优先选择第 2 种，即先获取 Cookies 再 Get 请求直接登录的方法。本文模拟登录的网站，仅需输入账号密码，不需要获取相关加密参数，比如 Authenticity_token ，同时也无需输入验证码，所以方法比较简单。但是还有很多网站模拟登录时，需要处理加密参数、验证码输入等问题。后续将会介绍。推荐阅读：Python爬虫(5)：Selenium 爬取东方财富网股票财务报表本文完。欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>模拟登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(7)：IT桔子网，国内创业公司的信息都在这里了]]></title>
    <url>%2F2018%2F10%2F17%2Fweb_scraping_withpython7.html</url>
    <content type="text"><![CDATA[以 IT 桔子网为例，介绍需登录网站的爬取方法。爬取该网站数据库中的信息：创业公司投融资情况、投资机构信息、独角兽公司。摘要： 之前爬的网站都是不需要登录就可以爬取的，但还有很多网站的内容需要先登录才能爬，比如 IT 桔子、豆瓣、知乎等。这时采用之前的方法就不行了，需要先登录再去爬。本文以 IT 桔子网站为例，介绍模拟登录方法，然后爬取该网站数据库中的数据信息，并保存到 MongoDB 数据库中。1. 网站介绍网址：https://www.itjuzi.com/对于创投圈的人来说，国内的IT桔子和国外的 Crunchbase应该算是必备网站。今天，我们要说的就是IT桔子，这个网站提供了很多有价值的信息。信息的价值体现在哪里呢，举个简单的例子。你运营了一个不错的公众号，有着 10 万粉丝群，这时候你想找投资继续做大，但你不知道该到哪里去找投资。这时候你偶然发现了IT桔子，在网站上你看到了同领域的大 V 号信息，他们得到了好几家公司上千万的投资。你看着心生羡慕，也跃跃欲试。于是，你经过综合对比分析，对自己公众号估值 200 万，然后就去找投资大 V 号的那几家公司洽谈。由于目的性明确，准备也充分，你很快就得到了融资。这个例子中，IT 桔子提供了创业公司（运营公众号也是创业）融资金额和投资机构等相关宝贵的信息。当然，这只是非常小的一点价值，该网站数据库提供了自 2000 年以来的海量数据，包括：超过11 万家的创业公司、6 万多条投融资信息、7 千多家投资机构以及其他数据，可以说是非常全了。这些数据的背后蕴含着大量有价值的信息。接下来，本文尝试爬取该网站数据库的信息，之后做一些数据分析，尝试找到一些有意思的信息。主要内容：模拟登录分析 Ajax 然后抓取存储到 MongoDB 数据库导出 csv数据分析(后期)2. 模拟登录2.1. Session 和 Cookies观察这个网站，是需要先登录才能看到数据信息的，但是好在不用输验证码。我们需要利用账号和密码，然后实现模拟登录。模拟登录的方法有好几种，比如 Post 直接提交账号、先登录获取 Cookies 再直接请求、Selenium 模拟登录等。其中：Post 方法需要在后台获取登录的 url，填写表单参数然后再请求，比较麻烦；直接复制 Cookies 的方法就是先登录账号，复制出 Cookies 并添加到 Headers 中，再用 requests.get 方法提交请求，这种方法最为方便；Selenium 模拟登录方法是直接输入账号、密码，也比较方便，但速度会有点慢。之后，会单独介绍几种方法的具体实现的步骤。这里，我们就先采用第二种方法。首先，需要先了解两个知识点：Session 和 Cookies。Session 在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。所以我们可以理解为 Cookies 里面保存了登录的凭证，有了它我们只需要在下次请求携带 Cookies 发送 Request 而不必重新输入用户名、密码等信息重新登录了。因此在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 Cookies 放在 Request Headers 里面直接请求。更多知识，可以参考崔庆才大神的文章：https://germey.gitbooks.io/python3webspider/content/2.4-Session%E5%92%8CCookies.html在了解 Cookies 知识后，我们就可以进入正题了。2.2. Requests 请求登录首先，利用已有的账号和密码，登录进网站主页，然后右键-检查，打开第一个 www.itjuzi.com 请求：向下拉到 Requests Headers 选项，可以看到有很多字段信息，这些信息之后我们都要添加到 Get 请求中去。定位到下方的 Cookie 字段，可以看到有很多 Cookie 值和名称，这是在登录后自动产生的。我们将整个 Cookies 复制 Request Headers 里，然后请求网页就可以顺利登陆然后爬取。如果不加 Cookies，那么就卡在登录界面，也就无法进行后面的爬取，所以 Cookies 很重要，需要把它放到 Request Headers 里去。下面，我们按照 json 格式开始构造 Request Headers。这里推荐一个好用的网站，可以帮我们自动构造 Request Headers：https://curl.trillworks.com/使用方法也很简单，右键复制cURL链接到这个网页中。将 cURL 复制到左边选框，默认选择语言为 Python，然后右侧就会自动构造后 requests 请求，包括 headers，复制下来直接可以用。登录好以后，我们就转到投融资速递网页中（url：http://radar.itjuzi.com/investevent），然后就可以获取该页面网页内容了。代码如下：12345678910111213141516171819202122import requestsheaders = &#123; 'Connection': 'keep-alive', 'Cache-Control': 'max-age=0', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'DNT': '1', 'Referer': 'http://radar.itjuzi.com/investevent', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7', 'If-None-Match': 'W/^\\^5bc7df15-19cdc^\\^', 'If-Modified-Since': 'Thu, 18 Oct 2018 01:17:09 GMT', # 主页cookie 'Cookie': '你的cookie', &#125;url = 'http://radar.itjuzi.com/investevent' # 投融资信息s = requests.Session()response = s.get(url,headers = headers)print(response.status_code)print(response.text)可以看到，添加 Cookie 后，我们请求投融资信息网页就成功了。这里如果不加 Cookie 的结果就什么也得不到：好，这样就算成功登录了。但是整个 headers 请求头的参数太多，是否一定需要带这么多参数呢？ 这里就去尝试看哪些参数是请求一定要的，哪些则是不用的，不用的可以去掉以精简代码。经过尝试，仅需要下面三个参数就能请求成功。123456headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' 'X-Requested-With': 'XMLHttpRequest', # 主页cookie 'Cookie': '复制你的cookie', &#125;Tips：爬取失效的时候需要重新注册帐号，然后生成新的 Cookie如果你没那么多邮箱账号，那么推荐一个可生成随机账号的免费邮箱，用来接收注册激活链接：https://10minutemail.net/3. 网页爬取分析在成功登录以后，我们就可以通过分析网页结构来采取相应的爬取方法。这里，我们将爬取投融资速递、创业公司、投资机构和千里马等几个子板块的数据。首先，以投融资速递信息为例。网址：http://radar.itjuzi.com/investevent3.1. 分析网页可以看到，投融资事件信息网页中的数据是表格形式。经尝试点击翻页，发现url不变，可以初步判定网页数据采用 Ajax 形式呈现。切换到后台，点击翻页可以发现出现有规律的 info?locatiop 开头的请求，页数随 page 参数而发生规律的变化。点击请求查分别查看 Response 和 Preview，发现表格数据是 json 格式，这就太好了。因为 json 格式的数据非常整齐也很容易抓取。上一篇文章，我们正好解决了 json 格式数据的处理方法，如果不太熟悉可以回顾一下：https://www.makcyun.top/web_scraping_withpython6.html接着，我们就可以通过构造 url 参数，然后用 Get 请求就可以获取网页中的表格数据，最后再加个循环，就可以爬取多页数据了。3.2. 构造 url下面，我们来构造一下 url，切换到 Headers 选项卡，拉到最底部可以看到 url 需带的请求参数。这里有三项，很好理解。location：in，表示国内数据； orderby：def，表示默认排序；page：1，表示第一页。所以，只需要改变 page 参数就可以查看其他页的结果，非常简单。这里，如果我们对表格进行筛选，比如行业选择教育、时间选择 2018 年，那么相应的请求参数也会增加。通过构造参数就可以爬取指定的数据，这样就不用全部爬下来了，也对网站友好点。3.3. 爬取数据到这儿，我们就可以直接开始爬了。可以使用函数，也可以用定义类（Class）的方法。考虑到，Python 是一种面向对象的编程，类（Class）是面向对象最重要的概念之一，运用类的思想编程非常重要。所以，这里我们尝试采用类的方法来实现爬虫。12345678910111213141516171819202122232425262728293031323334353637import requestsimport pymongoimport randomimport timeimport jsonimport pandas as pdfrom fake_useragent import UserAgentua = UserAgent()class ITjuzi(object): def __init__(self): self.headers = &#123; 'User-Agent': ua.random, 'X-Requested-With': 'XMLHttpRequest', 'Cookie': '你的cookie', &#125; self.url = 'http://radar.itjuzi.com/investevent/info?' # investevent self.session = requests.Session() def get_table(self, page): """ 1 获取投融资事件数据 """ params = &#123; # invsestevent 'location': 'in', 'orderby': 'def', 'page': page, 'date': 2018 # 年份 &#125; response = self.session.get( self.url, params=params, headers=self.headers).json() print(response) # self.save_to_mongo(response)if __name__ == '__main__': spider = itjuzi() spider.get_table(1)如果你之前一直是用 Def 函数的写法，而没有接触过 Class 类的写法，可能会看地比较别扭，我之前就是这样的，搞不懂为什么要有 self、为什么用__init__。这种思维的转变可以通过看教程和别人写的实际案例去揣摩。这里，我先略过，之后会单独介绍。简单解释一下上面代码的意思。首先定义了一个类（class），类名是 ITjuzi，类名通常是大写开头的单词。后面的 (object) 表示该类是从哪个类继承下来的，这个可以暂时不用管，填上就可以。然后定义了一个特殊的__init__方法，__init__方法的第一个参数永远是 self，之后是其他想要设置的属性参数。在这个方法里可以绑定些确定而且必须的属性，比如 headers、url 等。在 headers 里面，User-Agent 没有使用固定的 UA，而是借助 fake_useragent包 生成随机的 UA：ua.random。因为，这个网站做了一定的反爬措施，这样可以起到一定的反爬效果，后续我们会再说到。接着，定义了一个 get_table() 函数，这个函数和普通函数没有什么区别，除了第一个参数永远是实例变量 self。在 session.get（）方法中传入 url、请求参数和 headers，请求网页并指定获取的数据类型为 json 格式，然后就可以顺利输出 2018 年投融资信息的第 1 页数据：3.4. 存储到 MongoDB数据爬取下来了，那么我们放到哪里呢？可以选择存储到 csv 中，但 json 数据中存在多层嵌套，csv 不能够直观展现。这里，我们可以尝试之前没有用过的 MongoDB 数据库，当作练习。另外，数据存储到该数据库中，后期也可以导出 csv，一举两得。关于 MongoDB 的安装与基本使用，可以参考下面这两篇教程，之后我也会单独再写一下：安装https://germey.gitbooks.io/python3webspider/content/1.4.2-MongoDB%E7%9A%84%E5%AE%89%E8%A3%85.html使用https://germey.gitbooks.io/python3webspider/content/5.3.1-MongoDB%E5%AD%98%E5%82%A8.html可视化工具可采用 Robo 3T (之前叫 RoboMongo)https://www.mongodb.com/下面我们就将上面返回的 json 数据，存储到 MongoDB 中去：1234567891011121314151617181920212223242526272829import pymongoimport numpy as np# mongodb数据库初始化client = pymongo.MongoClient('localhost', 27017)# 指定数据库db = client.ITjuzi# 指定集合，类似于mysql中的表mongo_collection1 = db.itjuzi_investeventdef save_to_mongo(self, response): try: data = response['data']['rows'] # dict可以连续选取字典层内的内容 df = pd.DataFrame(data) table = json.loads(df.T.to_json()).values() if mongo_collection1.insert_many(table): # investment print('存储到mongodb成功') sleep = np.random.randint(3, 7) time.sleep(sleep) except Exception: print('存储到mongodb失败') def spider_itjuzi(self, start_page, end_page): for page in range(start_page, end_page): print('下载第%s页:' % (page)) self.get_table(page) print('下载完成') if __name__ == '__main__': spider = ITjuzi() spider.spider_itjuzi(1, 2)这里，安装好 MongoingDB 数据库、Robo 3T 和 pymongo 库后，我们就可以开始使用了。首先，对数据库进行初始化，然后指定（如果没有则生成）数据将要存放的数据库和集合名称。接着，定义了save_to_mongo 函数。由于表格里面的数据存储在键为 rows 的 value 值中，可使用 response[&#39;data&#39;][&#39;rows&#39;] 来获取到 json 里面的嵌套数据，然后转为 DataFrame。DataFrame 存储 MongoDB 参考了 stackoverflow 上面的一个回答：json.loads(df.T.to_json()).values()。然后，使用 mongo_collection1.insert_many(table) 方法将数据插入到 mongo_collection1，也就是 itjuzi_investevent 集合中。爬取完一页数据后，设置随机延时 3-6 s，避免爬取太频繁，这也能起到一定的反爬作用。最后，我们定义一个分页循环爬取函数 spider_itjuzi，利用 for 循环设置爬取起始页数就可以了，爬取结果如下：打开 Robo 3T，可以看到数据成功存储到 MongoDB 中了：好，以上，我们就基本上完成了 2018 年投融资信息数据表的爬取，如果你想爬其他年份或者更多页的数据，更改相应的参数即可。3.5. 导出到 csv数据存好后，如果还不太熟悉 MongoDB 的对数据的操作，那么我们可以将数据导出为 csv，在 excel 中操作。MongoDB不能直接导出 csv，但操作起来也不麻烦，利用mongoexport命令，几行代码就可以输出 csv。mongoexport导出 csv 的方法：https://docs.mongodb.com/manual/reference/program/mongoexport/#mongoexport-fields-example首先，运行 cmd，切换路径到 MongoDB 安装文件夹中的 bin 目录下，我这里是：1cd C:\Program Files\MongoDB\Server\4.0\bin接着，在桌面新建一个txt文件，命名为fields，在里面输入我们需要输出的表格列名，如下所示：然后，利用mongoexport命令，按照：表格所在的数据库、集合、输出格式、导出列名文件位置和输出文件名的格式，编写好命令并运行就可以导出了：1mongoexport --db ITjuzi --collection itjuzi_investevent --type=csv --fieldFile C:\Users\sony\Desktop\fields.txt --out C:\Users\sony\Desktop\investevent.csvcmd 命令：导出 csv 结果如下：Tips：直接用 excel 打开可能会是乱码，需先用 Notepad++ 转换为 UTF-8 编码，然后 excel 再打开就正常了。以上，我们就完成了整个数据表的爬取。3.6. 完整代码下面，可以再尝试爬取创业公司、投资机构和千里马的数据。他们的数据结构形式是一样的，只需要更换相应的参数就可以了，感兴趣的话可以尝试下。将上面的代码再稍微整理一下，完整代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import requestsimport reimport pymongoimport randomimport timeimport jsonimport randomimport numpy as npimport csvimport pandas as pdfrom fake_useragent import UserAgentimport socket # 断线重试from urllib.parse import urlencode# 随机uaua = UserAgent()# mongodb数据库初始化client = pymongo.MongoClient('localhost', 27017)# 获得数据库db = client.ITjuzi# 获得集合mongo_collection1 = db.itjuzi_investeventmongo_collection2 = db.itjuzi_companymongo_collection3 = db.itjuzi_investmentmongo_collection4 = db.itjuzi_horseclass itjuzi(object): def __init__(self): self.headers = &#123; 'User-Agent': ua.random, 'X-Requested-With': 'XMLHttpRequest', # 主页cookie 'Cookie': '你的cookie', &#125; self.url = 'http://radar.itjuzi.com/investevent/info?' # investevent # self.url = 'http://radar.itjuzi.com/company/infonew?' # company # self.url = 'http://radar.itjuzi.com/investment/info?' # investment # self.url = 'https://www.itjuzi.com/horse' # horse self.session = requests.Session() def get_table(self, page): """ 1 获取投融资事件网页内容 """ params = &#123; # 1 invsestevent 'location': 'in', 'orderby': 'def', 'page': page, 'date':2018 #年份 &#125; # # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # # # params = &#123; # 2 company # 'page': page, # # 'scope[]': 1, # 行业 1教育 # 'orderby': 'pv', # 'born_year[]': 2018, # 只能单年，不能多年筛选，会保留最后一个 # &#125; # # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # # # params = &#123; # 3 investment # 'orderby': 'num', # 'page': page # &#125; # # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # # # params = &#123; # 4 horse # &#125; # 可能会遇到请求失败，则设置3次重新请求 retrytimes = 3 while retrytimes: try: response = self.session.get( self.url, params=params, headers=self.headers,timeout = (5,20)).json() self.save_to_mongo(response) break except socket.timeout: print('下载第&#123;&#125;页，第&#123;&#125;次网页请求超时' .format(page,retrytimes)) retrytimes -=1 def save_to_mongo(self, response): try: data = response['data']['rows'] # dict可以连续选取字典层内的内容 # data =response # 爬取千里马时需替换为此data df = pd.DataFrame(data) table = json.loads(df.T.to_json()).values() if mongo_collection1.insert_many(table): # investevent # if mongo_collection2.insert_many(table): # company # if mongo_collection3.insert_many(table): # investment # if mongo_collection4.insert_many(table): # horse print('存储到mongodb成功') sleep = np.random.randint(3, 7) time.sleep(sleep) except Exception: print('存储到mongodb失败') def spider_itjuzi(self, start_page, end_page): for page in range(start_page, end_page): print('下载第%s页:' % (page)) self.get_table(page) print('下载完成')if __name__ == '__main__': spider = itjuzi() spider.spider_itjuzi(1, 2)源代码也可以在公众号后台回复：「it桔子」，或者在下面的链接中获取：https://github.com/makcyun/web_scraping_with_python后续文章，我们将对这些数据进行分析，尝试从中找出一些有意思的发现。4. 总结：本文以 IT 桔子网为例，介绍了需登录网站的爬取方法。即：先模拟登录再爬取数据信息。但是还有一些网站登录时需要输入验证码，这让爬取难度又增加，后期会再进行介绍。IT 桔子相比之前的爬虫网站，反爬措施高了很多。本文通过设置随机延时、随机 UserAgent，可一定程度上增加爬虫的稳定性。但是仍然会受到反爬措施的限制，后期可尝试通过设置 IP 代理池进一步提升爬虫效率。上面的爬虫程序在爬取过程容易中断，接着再进行爬取即可。但是手动修改非常不方便，也容易造成数据重复爬取或者漏爬。所以，为了更完整地爬取，需增加断点续传的功能。本文完。欢迎扫一扫关注我的微信公众号]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>模拟登录</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(6)：50 行代码爬取东方财富网上市公司 10 年近百万行财务报表数据]]></title>
    <url>%2F2018%2F10%2F12%2Fweb_scraping_withpython6.html</url>
    <content type="text"><![CDATA[通过分析网址 JavaScript 请求，以比 Selenium 快 100 倍的方法，快速爬取东方财富网各上市公司历年的财务报表数据。摘要： 上一篇文章，我们用Selenium成功爬取了东方财富网的财务报表数据，但是速度非常慢，爬取 70 页需要好几十分钟。为了加快速度，本文分析网页JavaScript请求，找到数据接口然后快速爬取财务报表数据。1. JavaScript请求分析上一篇文章，我们简单分了东方财富网财务报表网页后台的js请求，文章回顾：（https://www.makcyun.top/web_scraping_withpython5.html）接下来，我们深入分析。首先，点击报表底部的下一页，然后观察左侧Name列，看会弹出什么新的请求来：可以看到，当不断点击下一页时，会相应弹出以get？type开头的请求。点击右边Headers选项卡，可以看到请求的URL，网址非常长，先不管它，后续我们会分析各项参数。接着，点击右侧的Preview和Response，可以看到里面有很多整齐的数据，尝试猜测这可能是财务报表中的数据，经过和表格进行对比，发现这正是我们所需的数据，太好了。然后将URL复制到新链接中打开看看，可以看到表格中的数据完美地显示出来了。竟然不用添加Headers、UA去请求就能获取到，看来东方财富网很大方啊。到这里，爬取思路已经很清晰了。首先，用Request请求该URL，将获取到的数据进行正则匹配，将数据转变为json格式，然后写入本地文件，最后再加一个分页循环爬取就OK了。这比之前的Selenium要简单很多，而且速度应该会快很多倍。下面我们就先来尝试爬一页数据看看。2. 爬取单页2.1. 抓取分析这里仍然以2018年中报的利润表为例，抓取该网页的第一页表格数据，网页url为：http://data.eastmoney.com/bbsj/201806/lrb.html表格第一页的js请求的url为：http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?type=CWBB_LRB&amp;token=70f12f2f4f091e459a279469fe49eca5&amp;st=noticedate&amp;sr=-1&amp;p=2&amp;ps=50&amp;js=var%20spmVUpAF={pages:(tp),data:%20(x)}&amp;filter=(reportdate=^2018-06-30^)&amp;rt=51312886,data:%20(x)}&amp;filter=(reportdate=^2018-06-30^)&amp;rt=51312886)下面，我们通过分析该url，来抓取表格内容。1234567891011121314151617import requestsdef get_table(): params = &#123; 'type': 'CWBB_LRB', # 表格类型,LRB为利润表缩写，必须 'token': '70f12f2f4f091e459a279469fe49eca5', # 访问令牌，必须 'st': 'noticedate', # 公告日期 'sr': -1, # 保持-1不用改动即可 'p': 1, # 表格页数 'ps': 50, # 每页显示多少条信息 'js': 'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;', # js函数，必须 'filter': '(reportdate=^2018-06-30^)', # 筛选条件 # 'rt': 51294261 可不用 &#125; url = 'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?' response = requests.get(url, params=params).text print(response)get_table()这里我们定义了一个get_table()方法，来输出抓取的第一页表格内容。params为url请求中所包含的参数。这里对重要参数进行简单说明：type为7个表格的类型说明，将type拆成两部分：’CWBB_‘ 和’LRB’，资产负债表等后3个表是以’CWBB_’ 开头，业绩报表至预约披露时间表等前4个表是以’YJBB20_‘开头的；’LRB’为利润表的首字母缩写，同理业绩报表则为’YJBB’。所以，如果要爬取不同的表格，就需要更改type参数。’filter’为表格筛选参数，这里筛选出年中报的数据。不同的表格筛选条件会不一样，所以当type类型更改的时候，也要相应修改filter类型。params参数设置好之后，将url和params参数一起传进requests.get()方法中，这样就构造好了请求连接。几行代码就可以成功获取网页第一页的表格数据了：可以看到，表格信息存储在LFtlXDqn变量中，pages表示表格有72页。data为表格数据，是一个由多个字典构成的列表，每个字典是表格的一行数据。我们可以通过正则表达式分别提取出pages和data数据。2.2. 正则表达式提取表格1234567# 确定页数import re pat = re.compile('var.*?&#123;pages:(\d+),data:.*?')page_all = re.search(pat, response)print(page_all.group(1))结果：72这里用\d+匹配页数中的数值，然后用re.search()方法提取出来。group(1)表示输出第一个结果，这里就是()中的页数。1234567891011# 提取出list，可以使用json.dumps和json.loadsimport jsonpattern = re.compile('var.*?data: (.*)&#125;', re.S)items = re.search(pattern, response)data = items.group(1)print(data)print(type(data))结果如下：[&#123;'scode': '600478', 'hycode': '016040', 'companycode': '10001305', 'sname': '科力远', 'publishname': '材料行业'...'sjltz': 10.466665, 'kcfjcxsyjlr': 46691230.93, 'sjlktz': 10.4666649042, 'eutime': '2018/9/6 20:18:42', 'yyzc': 14238766.31&#125;]&lt;class 'str'&gt;这里在匹配表格数据用了(.*)表示贪婪匹配，因为data中有很多个字典，每个字典都是以’}’结尾，所以我们利用贪婪匹配到最后一个’}’，这样才能获取data所有数据。多数情况下，我们可能会用到(.*?)，这表示非贪婪匹配，意味着之多匹配一个’}’，这样的话，我们只能匹配到第一行数据，显然是不对的。2.3. json.loads()输出表格这里提取出来的list是str字符型的，我们需要转换为list列表类型。为什么要转换为list类型呢，因为无法用操作list的方法去操作str，比如list切片。转换为list后，我们可以对list进行切片，比如data[0]可以获取第一个{}中的数据，也就是表格第一行，这样方便后续构造循环从而逐行输出表格数据。这里采用json.loads()方法将str转换为list。1234567data = json.loads(data)# print(data) 和上面的一样print(type(data))print(data[0])结果如下：&lt;class 'list'&gt;&#123;'scode': '600478', 'hycode': '016040', 'companycode': '10001305', 'sname': '科力远', 'publishname': '材料行业', 'reporttimetypecode': '002', 'combinetypecode': '001', 'dataajusttype': '2', 'mkt': 'shzb', 'noticedate': '2018-10-13T00:00:00', 'reportdate': '2018-06-30T00:00:00', 'parentnetprofit': -46515200.15, 'totaloperatereve': 683459458.22, 'totaloperateexp': 824933386.17, 'totaloperateexp_tb': -0.0597570689015973, 'operateexp': 601335611.67, 'operateexp_tb': -0.105421872593886, 'saleexp': 27004422.05, 'manageexp': 141680603.83, 'financeexp': 33258589.95, 'operateprofit': -94535963.65, 'sumprofit': -92632216.61, 'incometax': -8809471.54, 'operatereve': '-', 'intnreve': '-', 'intnreve_tb': '-', 'commnreve': '-', 'commnreve_tb': '-', 'operatetax': 7777267.21, 'operatemanageexp': '-', 'commreve_commexp': '-', 'intreve_intexp': '-', 'premiumearned': '-', 'premiumearned_tb': '-', 'investincome': '-', 'surrenderpremium': '-', 'indemnityexp': '-', 'tystz': -0.092852, 'yltz': 0.178351, 'sjltz': 0.399524, 'kcfjcxsyjlr': -58082725.17, 'sjlktz': 0.2475682609, 'eutime': '2018/10/12 21:01:36', 'yyzc': 601335611.67&#125;接下来我们就将表格内容输入到csv文件中。123456# 写入csv文件import csvfor d in data: with open('eastmoney.csv', 'a', encoding='utf_8_sig', newline='') as f: w = csv.writer(f) w.writerow(d.values())通过for循环，依次取出表格中的每一行字典数据{}，然后用with…open的方法写入’eastmoney.csv’文件中。tips：’a’表示可重复写入；encoding=’utf_8_sig’ 能保持csv文件的汉字不会乱码；newline为空能避免每行数据中产生空行。这样，第一页50行的表格数据就成功输出到csv文件中去了：这里，我们还可以在输出表格之前添加上表头：12345678# 添加列标题def write_header(data): with open('eastmoney.csv', 'a', encoding='utf_8_sig', newline='') as f: headers = list(data[0].keys()) print(headers) print(len(headers)) # 输出list长度，也就是有多少列 writer = csv.writer(f) writer.writerow(headers)这里，data[0]表示list的一个字典中的数据，data[0].keys()表示获取字典中的key键值，也就是列标题。外面再加一个list序列化（结果如下），然后将该list输出到’eastmoney.csv’中作为表格的列标题即可。12['scode', 'hycode', 'companycode', 'sname', 'publishname', 'reporttimetypecode', 'combinetypecode', 'dataajusttype', 'mkt', 'noticedate', 'reportdate', 'parentnetprofit', 'totaloperatereve', 'totaloperateexp', 'totaloperateexp_tb', 'operateexp', 'operateexp_tb', 'saleexp', 'manageexp', 'financeexp', 'operateprofit', 'sumprofit', 'incometax', 'operatereve', 'intnreve', 'intnreve_tb', 'commnreve', 'commnreve_tb', 'operatetax', 'operatemanageexp', 'commreve_commexp', 'intreve_intexp', 'premiumearned', 'premiumearned_tb', 'investincome', 'surrenderpremium', 'indemnityexp', 'tystz', 'yltz', 'sjltz', 'kcfjcxsyjlr', 'sjlktz', 'eutime', 'yyzc']44 # 一共有44个字段，也就是说表格有44列。以上，就完成了单页表格的爬取和下载到本地的过程。3. 多页表格爬取将上述代码整理为相应的函数，再添加for循环，仅50行代码就可以爬取72页的利润报表数据：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import requestsimport reimport jsonimport csvimport timedef get_table(page): params = &#123; 'type': 'CWBB_LRB', # 表格类型,LRB为利润表缩写，必须 'token': '70f12f2f4f091e459a279469fe49eca5', # 访问令牌，必须 'st': 'noticedate', # 公告日期 'sr': -1, # 保持-1不用改动即可 'p': page, # 表格页数 'ps': 50, # 每页显示多少条信息 'js': 'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;', # js函数，必须 'filter': '(reportdate=^2018-06-30^)', # 筛选条件，如果不选则默认下载全部时期的数据 # 'rt': 51294261 可不用 &#125; url = 'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?' response = requests.get(url, params=params).text # 确定页数 pat = re.compile('var.*?&#123;pages:(\d+),data:.*?') page_all = re.search(pat, response) # 总页数 pattern = re.compile('var.*?data: (.*)&#125;', re.S) items = re.search(pattern, response) data = items.group(1) data = json.loads(data) print('\n正在下载第 %s 页表格' % page) return page_all,datadef write_header(data): with open('eastmoney.csv', 'a', encoding='utf_8_sig', newline='') as f: headers = list(data[0].keys()) writer = csv.writer(f) writer.writerow(headers)def write_table(data): for d in data: with open('eastmoney.csv', 'a', encoding='utf_8_sig', newline='') as f: w = csv.writer(f) w.writerow(d.values()) def main(page): data = get_table(page)[1] write_table(data)if __name__ == '__main__': start_time = time.time() # 下载开始时间 # 写入表头 write_header(get_table(1)) page_all = get_table(1)[0] page_all = int(page_all.group(1)) for page in range(1, page_all): main(page) end_time = time.time() - start_time # 结束时间 print('下载用时: &#123;:.1f&#125; s' .format(end_time))整个下载只用了20多秒，而之前用selenium花了几十分钟，这效率提升了足有100倍！这里，如果我们想下载全部时期（从2007年-2018年）利润报表数据，也很简单。只要将type中的filter参数注释掉，意味着也就是不筛选日期，那么就可以下载全部时期的数据。这里当我们取消注释filter列，将会发现总页数page_all会从2018年中报的72页增加到2528页，全部下载完成后，表格有超过12万行的数据。基于这些数据，可以尝试从中进行一些有价值的数据分析。4. 通用代码构造以上代码实现了2018年中报利润报表的爬取，但如果不想局限于该报表，还想爬取其他报表或者其他任意时期的数据，那么就需要手动地去修改代码中相应的字段，很不方便。所以上面的代码可以说是简短但不够强大。为了能够灵活实现爬取任意类别和任意时期的报表数据，需要对代码再进行一些加工，就可以构造出通用强大的爬虫程序了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198"""e.g: http://data.eastmoney.com/bbsj/201806/lrb.html"""import requestsimport refrom multiprocessing import Poolimport jsonimport csvimport pandas as pdimport osimport time# 设置文件保存在D盘eastmoney文件夹下file_path = 'D:\\eastmoney'if not os.path.exists(file_path): os.mkdir(file_path)os.chdir(file_path)# 1 设置表格爬取时期、类别def set_table(): print('*' * 80) print('\t\t\t\t东方财富网报表下载') print('作者：高级农民工 2018.10.10') print('--------------') year = int(float(input('请输入要查询的年份(四位数2007-2018)：\n'))) # int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会 # https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10 while (year &lt; 2007 or year &gt; 2018): year = int(float(input('年份数值输入错误，请重新输入：\n'))) quarter = int(float(input('请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：\n'))) while (quarter &lt; 1 or quarter &gt; 4): quarter = int(float(input('季度数值输入错误，请重新输入：\n'))) # 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充， # http://www.runoob.com/python/att-string-format.html quarter = '&#123;:02d&#125;'.format(quarter * 3) # quarter = '%02d' %(int(month)*3) # 确定季度所对应的最后一天是30还是31号 if (quarter == '06') or (quarter == '09'): day = 30 else: day = 31 date = '&#123;&#125;-&#123;&#125;-&#123;&#125;' .format(year, quarter, day) # print('date:', date) # 测试日期 ok # 2 设置财务报表种类 tables = int( input('请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): \n')) dict_tables = &#123;1: '业绩报表', 2: '业绩快报表', 3: '业绩预告表', 4: '预约披露时间表', 5: '资产负债表', 6: '利润表', 7: '现金流量表'&#125; dict = &#123;1: 'YJBB', 2: 'YJKB', 3: 'YJYG', 4: 'YYPL', 5: 'ZCFZB', 6: 'LRB', 7: 'XJLLB'&#125; category = dict[tables] # js请求参数里的type，第1-4个表的前缀是'YJBB20_'，后3个表是'CWBB_' # 设置set_table()中的type、st、sr、filter参数 if tables == 1: category_type = 'YJBB20_' st = 'latestnoticedate' sr = -1 filter = "(securitytypecode in ('058001001','058001002'))(reportdate=^%s^)" %(date) elif tables == 2: category_type = 'YJBB20_' st = 'ldate' sr = -1 filter = "(securitytypecode in ('058001001','058001002'))(rdate=^%s^)" %(date) elif tables == 3: category_type = 'YJBB20_' st = 'ndate' sr = -1 filter=" (IsLatest='T')(enddate=^2018-06-30^)" elif tables == 4: category_type = 'YJBB20_' st = 'frdate' sr = 1 filter = "(securitytypecode ='058001001')(reportdate=^%s^)" %(date) else: category_type = 'CWBB_' st = 'noticedate' sr = -1 filter = '(reportdate=^%s^)' % (date) category_type = category_type + category # print(category_type) # 设置set_table()中的filter参数 yield&#123; 'date':date, 'category':dict_tables[tables], 'category_type':category_type, 'st':st, 'sr':sr, 'filter':filter &#125;# 2 设置表格爬取起始页数def page_choose(page_all): # 选择爬取页数范围 start_page = int(input('请输入下载起始页数：\n')) nums = input('请输入要下载的页数，（若需下载全部则按回车）：\n') print('*' * 80) # 判断输入的是数值还是回车空格 if nums.isdigit(): end_page = start_page + int(nums) elif nums == '': end_page = int(page_all.group(1)) else: print('页数输入错误') # 返回所需的起始页数，供后续程序调用 yield&#123; 'start_page': start_page, 'end_page': end_page &#125;# 3 表格正式爬取def get_table(date, category_type,st,sr,filter,page): # 参数设置 params = &#123; # 'type': 'CWBB_LRB', 'type': category_type, # 表格类型 'token': '70f12f2f4f091e459a279469fe49eca5', 'st': st, 'sr': sr, 'p': page, 'ps': 50, # 每页显示多少条信息 'js': 'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;', 'filter': filter, # 'rt': 51294261 可不用 &#125; url = 'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?' response = requests.get(url, params=params).text # 确定页数 pat = re.compile('var.*?&#123;pages:(\d+),data:.*?') page_all = re.search(pat, response) # print(page_all.group(1)) # ok # 提取出list，可以使用json.dumps和json.loads pattern = re.compile('var.*?data: (.*)&#125;', re.S) items = re.search(pattern, response) # 等价于 # items = re.findall(pattern,response) # print(items[0]) data = items.group(1) data = json.loads(data) return page_all, data,page# 4 写入表头# 方法1 借助csv包，最常用def write_header(data,category): with open('&#123;&#125;.csv' .format(category), 'a', encoding='utf_8_sig', newline='') as f: headers = list(data[0].keys()) # print(headers) # 测试 ok writer = csv.writer(f) writer.writerow(headers)# 5 写入表格def write_table(data,page,category): print('\n正在下载第 %s 页表格' % page) # 写入文件方法1 for d in data: with open('&#123;&#125;.csv' .format(category), 'a', encoding='utf_8_sig', newline='') as f: w = csv.writer(f) w.writerow(d.values())def main(date, category_type,st,sr,filter,page): func = get_table(date, category_type,st,sr,filter,page) data = func[1] page = func[2] write_table(data,page,category)if __name__ == '__main__': # 获取总页数，确定起始爬取页数 for i in set_table(): date = i.get('date') category = i.get('category') category_type = i.get('category_type') st = i.get('st') sr = i.get('sr') filter = i.get('filter') constant = get_table(date,category_type,st,sr,filter, 1) page_all = constant[0] for i in page_choose(page_all): start_page = i.get('start_page') end_page = i.get('end_page') # 先写入表头 write_header(constant[1],category) start_time = time.time() # 下载开始时间 # 爬取表格主程序 for page in range(start_page, end_page): main(date,category_type,st,sr,filter, page) end_time = time.time() - start_time # 结束时间 print('下载完成') print('下载用时: &#123;:.1f&#125; s' .format(end_time))以爬取2018年中业绩报表为例，感受一下比selenium快得多的爬取效果（视频链接）：https://v.qq.com/x/page/a0519bfxajc.html利用上面的程序，我们可以下载任意时期和任意报表的数据。这里，我下载完成了2018年中报所有7个报表的数据。文中代码和素材资源可以在下面的链接中获取：https://github.com/makcyun/eastmoney_spider本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Ajax</tag>
        <tag>图片下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(5)：Selenium 爬取东方财富网股票财务报表]]></title>
    <url>%2F2018%2F10%2F02%2Fweb_scraping_withpython5.html</url>
    <content type="text"><![CDATA[利用Selenium爬取东方财富网各上市公司历年的财务报表数据。摘要： 现在很多网页都采取JavaScript进行动态渲染，其中包括Ajax技术。上一篇文章通过分析Ajax接口数据，顺利爬取了澎湃新闻网动态网页中的图片。但有的网页虽然也Ajax技术，但接口参数可能是加密的无法直接获得，比如淘宝；有的动态网页也采用JavaScript，但不是Ajax技术，比如Echarts官网。所以，当遇到这两类网页时，上一篇文章中的方法便不再奏效，需要新的采取新的方法，这其中包括干脆、直接、好用的的Selenium大法。东方财富网的财务报表网页也是通过JavaScript动态加载的，本文利用Selenium方法爬取该网站上市公司的财务报表数据。[TOC]1. 实战背景很多网站都提供上市公司的公告、财务报表等金融投资信息和数据，比如：腾讯财经、网易财经、新浪财经、东方财富网等。这之中，发现东方财富网的数据非常齐全。东方财富网有一个数据中心：http://data.eastmoney.com/center/，该数据中心提供包括特色数据、研究报告、年报季报等在内的大量数据（见下图）。以年报季报类别为例，我们点开该分类查看一下2018年中报（见下图），可以看到该分类下又包括：业绩报表、业绩快报、利润表等7个报表的数据。以业绩报表为例，报表包含全部3000多只股票的业绩报表数据，一共有70多页。假如，我们想获取所有股票2018年中的业绩报表数据，然后对该数据进行一些分析。采取手动复制的方法，70多页可以勉强完成。但如果想获取任意一年、任意季度、任意报表的数据，要再通过手动复制的方法，工作量会非常地大。举个例子，假设要获取10年间（40个季度）、所有7个报表的数据，那么手动复制的工作量大约将是：40×7×70（每个报表大约70页），差不多要重复性地复制2万次！！！可以说是人工不可能完成的任务。所以，本文的目标就是利用Selenium自动化技术，爬取年报季报类别下，任意一年（网站有数据至今）、任意财务报表数据。我们所需要做的，仅是简单输入几个字符，其他就全部交给电脑，然后过一会儿打开excel，就可以看到所需数据”静静地躺在那里”，是不是挺酷的？好，下面我们就开始实操一下。首先，需要分析要爬取的网页对象。2. 网页分析之前，我们已经爬过表格型的数据，所以对表格数据的结构应该不会太陌生，如果忘了，可以再看一下这篇文章：https://www.makcyun.top/web_scraping_withpython2.html我们这里以上面的2018年中报的业绩报表为例，查看一下表格的形式。网址url：http://data.eastmoney.com/bbsj/201806/lrb.html，bbsj代表年报季报，201803代表2018年一季报，类似地，201806表示年中报；lrb是利润表的首字母缩写，同理，yjbb表示业绩报表。可以看出，该网址格式很简单，便于构造url。接着，我们点击下一页按钮，可以看到表格更新后，url没有发生改变，可以判定是采用了Javscript。那么，我们首先判断是不是采用了Ajax加载的。方法也很简单，右键检查或按F12，切换到network并选择下面的XHR，再按F5刷新。可以看到只有一个Ajax请求，点击下一页也并没有生成新的Ajax请求，可以判断该网页结构不是常见的那种点击下一页或者下拉会源源不断出现的Ajax请求类型，那么便无法构造url来实现分页爬取。XHR选项里没有找到我们需要的请求，接下来试试看能不能再JS里找到表格的数据请求。将选项选为JS，再次F5刷新，可以看到出现了很多JS请求，然后我们点击几次下一页，会发现弹出新的请求来，然后右边为响应的请求信息。url链接非常长，看上去很复杂。好，这里我们先在这里打住不往下了。可以看到，通过分析后台元素来爬取该动态网页的方法，相对比较复杂。那么有没有干脆、直截了当地就能够抓取表格内容的方法呢？有的，就是本文接下来要介绍的Selenium大法。3. Selenium知识Selenium 是什么？一句话，自动化测试工具。它是为了测试而出生的，但在近几年火热的爬虫领域中，它摇身一变，变成了爬虫的利器。直白点说， Seleninm能控制浏览器, 像人一样”上网”。比如，可以实现网页自动翻页、登录网站、发送邮件、下载图片/音乐/视频等等。举个例子，写几行python代码就可以用Selenium实现登录IT桔子，然后浏览网页的功能。怎么样，仅用几行代码就能实现自动上网操作，是不是挺神奇的？当然，这仅仅是Selenium最简单的功能，还有很多更加丰富的操作，可以参考以下几篇教程：参考网站：Selenium官网： https://selenium-python.readthedocs.io/SeleniumPython文档（英文版）：http://selenium-python.readthedocs.org/index.htmlSeleniumPython文档（中文版）：https://selenium-python-zh.readthedocs.io/en/latest/faq.htmlSelenium 基本操作：https://www.yukunweb.com/2017/7/python-spider-Selenium-PhantomJS-basic/Selenium爬取淘宝信息实战：https://cuiqingcai.com/2852.html只需要记住重要的一点就是：Selenium能做到&quot;可见即可爬&quot;。也就是说网页上你能看到的东西，Selenium基本上都能爬取下来。包括上面我们提到的东方财富网的财务报表数据，它也能够做到，而且非常简单直接，不用去后台查看用了什么JavaScript技术或者Ajax参数。下面我们就实际来操练下吧。4. 编码实现4.1. 思路安装配置好Selenium运行的相关环境，浏览器可以用Chrome、Firefox、PhantomJS等，我用的是Chrome；东方财富网的财务报表数据不用登录可直接获得，Selenium更加方便爬取；先以单个网页中的财务报表为例，表格数据结构简单，可先直接定位到整个表格，然后一次性获取所有td节点对应的表格单元内容；接着循环分页爬取所有上市公司的数据，并保存为csv文件。重新构造灵活的url，实现可以爬取任意时期、任意一张财务报表的数据。根据上述思路，下面就用代码一步步来实现。4.2. 爬取单页表格我们先以2018年中报的利润表为例，抓取该网页的第一页表格数据，网页url：http://data.eastmoney.com/bbsj/201806/lrb.html快速定位到表格所在的节点：id = dt_1，然后可以用Selenium进行抓取了，方法如下：12345678910111213141516171819from selenium import webdriverbrowser = webdriver.Chrome()# 当测试好能够顺利爬取后，为加快爬取速度可设置无头模式，即不弹出浏览器# 添加无头headlesss 1使用chrome headless,2使用PhantomJS# 使用 PhantomJS 会警告高不建议使用phantomjs，建议chrome headless# chrome_options = webdriver.ChromeOptions()# chrome_options.add_argument('--headless')# browser = webdriver.Chrome(chrome_options=chrome_options)# browser = webdriver.PhantomJS()# browser.maximize_window() # 最大化窗口,可以选择设置browser.get('http://data.eastmoney.com/bbsj/201806/lrb.html')element = browser.find_element_by_css_selector('#dt_1') # 定位表格，element是WebElement类型# 提取表格内容tdtd_content = element.find_elements_by_tag_name("td") # 进一步定位到表格内容所在的td节点lst = [] # 存储为listfor td in td_content: lst.append(td.text)print(lst) # 输出表格内容这里，使用Chrome浏览器构造一个Webdriver对象，赋值给变量browser，browser调用get()方法请求想要抓取的网页。接着使用find_element_by_css_selector方法查找表格所在的节点：‘#dt_1’。这里推荐一款小巧、快速定位css/xpath的Chrome插件：SelectorGadget，使用这个插件就不用再去源代码中手动定位节点那么麻烦了。插件地址：https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb紧接着再向下定位到td节点，因为网页中有很多个td节点，所以要用find_elements方法。然后，遍历数据节点存储到list中。打印查看一下结果：12345# list形式:['1', '002161', '远望谷', ...'-7960万', '09-29', '2','002316', '亚联发展', ...'1.79亿', '09-29', '3',... '50', '002683', '宏大爆破',...'1.37亿', '09-01']是不是很方便，几行代码就能抓取下来这一页表格，除了速度有点慢。为了便于后续存储，我们将list转换为DataFrame。首先需要把这一个大的list分割为多行多列的子list，实现如下：1234567891011121314151617import pandas as pd# 确定表格列数col = len(element.find_elements_by_css_selector('tr:nth-child(1) td'))# 通过定位一行td的数量，可获得表格的列数，然后将list拆分为对应列数的子listlst = [lst[i:i + col] for i in range(0, len(lst), col)]# 原网页中打开"详细"链接可以查看更详细的数据，这里我们把url提取出来，方便后期查看lst_link = []links = element.find_elements_by_css_selector('#dt_1 a.red')for link in links: url = link.get_attribute('href') lst_link.append(url)lst_link = pd.Series(lst_link)# list转为dataframedf_table = pd.DataFrame(lst)# 添加url列df_table['url'] = lst_linkprint(df_table.head()) # 查看DataFrame这里，要将list分割为子list，只需要确定表格有多少列即可，然后将每相隔这么多数量的值划分为一个子list。如果我们数一下该表的列数，可以发现一共有16列。但是这里不能使用这个数字，因为除了利润表，其他报表的列数并不是16，所以当后期爬取其他表格可能就会报错。这里仍然通过find_elements_by_css_selector方法，定位首行td节点的数量，便可获得表格的列数，然后将list拆分为对应列数的子list。同时，原网页中打开”详细”列的链接可以查看更详细的数据，这里我们把url提取出来，并增加一列到DataFrame中，方便后期查看。打印查看一下输出结果：可以看到，表格所有的数据我们都抓取到了，下面只需要进行分页循环爬取就行了。这里，没有抓取表头是因为表头有合并单元格，处理起来就非常麻烦。建议表格抓取下来后，在excel中复制表头进去就行了。如果，实在想要用代码完成，可以参考这篇文章：https://blog.csdn.net/weixin_39461443/article/details/754569624.3. 分页爬取上面完成了单页表格的爬取，下面我们来实现分页爬取。首先，我们先实现Selenium模拟翻页跳转操作，成功后再爬取每页的表格内容。123456789101112131415161718192021222324252627282930313233343536373839from selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitimport timebrowser = webdriver.Chrome()browser.maximize_window() # 最大化窗口,可以选择设置wait = WebDriverWait(browser, 10)def index_page(page): try: browser.get('http://data.eastmoney.com/bbsj/201806/lrb.html') print('正在爬取第： %s 页' % page) wait.until( EC.presence_of_element_located((By.ID, "dt_1"))) # 判断是否是第1页，如果大于1就输入跳转，否则等待加载完成。 if page &gt; 1: # 确定页数输入框 input = wait.until(EC.presence_of_element_located( (By.XPATH, '//*[@id="PageContgopage"]'))) input.click() input.clear() input.send_keys(page) submit = wait.until(EC.element_to_be_clickable( (By.CSS_SELECTOR, '#PageCont &gt; a.btn_link'))) submit.click() time.sleep(2) # 确认成功跳转到输入框中的指定页 wait.until(EC.text_to_be_present_in_element( (By.CSS_SELECTOR, '#PageCont &gt; span.at'), str(page))) except Exception: return Nonedef main(): for page in range(1,5): # 测试翻4页 index_page(page)if __name__ == '__main__': main()这里，我们先加载了相关包，使用WebDriverWait对象，设置最长10s的显式等待时间，以便网页加载出表格。判断表格是否加载出来，用到了EC.presence_of_element_located条件。表格加载出来后，设置一个页面判断，如果在第1页就等待页面加载完成，如果大于第1页就开始跳转。要完成跳转操作，我们需要通过获取输入框input节点，然后用clear()方法清空输入框，再通过send_keys()方法填写相应的页码，接着通过submit.click()方法击下一页完成翻页跳转。这里，我们测试一下前4页跳转效果，可以看到网页成功跳转了。下面就可以对每一页应用第一页爬取表格内容的方法，抓取每一页的表格，转为DataFrame然后存储到csv文件中去。4.4. 通用爬虫构造上面，我们完成了2018年中报利润表： http://data.eastmoney.com/bbsj/201806/lrb.html，一个网页表格的爬取。但如果我们想爬取任意时期、任意一张报表的表格，比如2017年3季度的利润表、2016年全年的业绩报表、2015年1季度的现金流量表等等。上面的代码就行不通了，下面我们对代码进行一下改造，变成更通用的爬虫。从图中可以看到，东方财富网年报季报有7张表格，财务报表最早从2007年开始每季度一次。基于这两个维度，可重新构造url的形式，然后爬取表格数据。下面，我们用代码进行实现：1234567891011121314151617181920212223242526# 重构url# 1 设置财务报表获取时期year = int(float(input('请输入要查询的年份(四位数2007-2018)： ')))# int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会while (year &lt; 2007 or year &gt; 2018): year = int(float(input('年份数值输入错误，请重新输入：')))quarter = int(float(input('请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)： ')))while (quarter &lt; 1 or quarter &gt; 4): quarter = int(float(input('季度数值输入错误，请重新输入： ')))# 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充quarter = '&#123;:02d&#125;'.format(quarter * 3)# quarter = '%02d' %(int(month)*3)date = '&#123;&#125;&#123;&#125;' .format(year, quarter)# 2 设置财务报表种类tables = int( input('请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): '))dict_tables = &#123;1: '业绩报表', 2: '业绩快报表', 3: '业绩预告表', 4: '预约披露时间表', 5: '资产负债表', 6: '利润表', 7: '现金流量表'&#125;dict = &#123;1: 'yjbb', 2: 'yjkb/13', 3: 'yjyg', 4: 'yysj', 5: 'zcfz', 6: 'lrb', 7: 'xjll'&#125;category = dict[tables]# 3 设置urlurl = 'http://data.eastmoney.com/&#123;&#125;/&#123;&#125;/&#123;&#125;.html' .format('bbsj', date, category)print(url) # 测试输出的url经过上面的设置，我们通过输入想要获得指定时期、制定财务报表类型的数值，就能返回相应的url链接。将该链接应用到前面的爬虫中，就可以爬取相应的报表内容了。另外，除了从第一页开始爬取到最后一页的结果以外，我们还可以自定义设置想要爬取的页数。比如起始页数从第1页开始，然后爬取10页。1234567891011121314151617181920212223# 4 选择爬取页数范围start_page = int(input('请输入下载起始页数：\n'))nums = input('请输入要下载的页数，（若需下载全部则按回车）：\n')# 确定网页中的最后一页browser.get(url)# 确定最后一页页数不直接用数字而是采用定位，因为不同时间段的页码会不一样try: page = browser.find_element_by_css_selector('.next+ a') # next节点后面的a节点except: page = browser.find_element_by_css_selector('.at+ a')else: print('没有找到该节点')# 上面用try.except是因为绝大多数页码定位可用'.next+ a'，但是业绩快报表有的只有2页，无'.next+ a'节点end_page = int(page.text)if nums.isdigit(): end_page = start_page + int(nums)elif nums == '': end_page = end_pageelse: print('页数输入错误')# 输入准备下载表格类型print('准备下载:&#123;&#125;-&#123;&#125;' .format(date, dict_tables[tables]))经过上面的设置，我们就可以实现自定义时期和财务报表类型的表格爬取了，将代码再稍微整理一下，可实现下面的爬虫效果：视频截图：视频地址：https://v.qq.com/x/page/y07335thsn2.html背景中类似黑客帝国的代码雨效果，其实是动态网页效果。素材来源于下面这个网站，该网站还有很多酷炫的动态背景可以下载下来。http://wallpaper.upupoo.com/store/paperDetail-1783830052.htm4.5. 完整代码整个爬虫的完整代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183from selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitimport timeimport pandas as pdimport os# 先chrome，后phantomjs# browser = webdriver.Chrome()# 添加无头headlessschrome_options = webdriver.ChromeOptions()chrome_options.add_argument('--headless')browser = webdriver.Chrome(chrome_options=chrome_options)# browser = webdriver.PhantomJS() # 会报警高提示不建议使用phantomjs，建议chrome添加无头browser.maximize_window() # 最大化窗口wait = WebDriverWait(browser, 10)def index_page(page): try: print('正在爬取第： %s 页' % page) wait.until( EC.presence_of_element_located((By.ID, "dt_1"))) # 判断是否是第1页，如果大于1就输入跳转，否则等待加载完成。 if page &gt; 1: # 确定页数输入框 input = wait.until(EC.presence_of_element_located( (By.XPATH, '//*[@id="PageContgopage"]'))) input.click() input.clear() input.send_keys(page) submit = wait.until(EC.element_to_be_clickable( (By.CSS_SELECTOR, '#PageCont &gt; a.btn_link'))) submit.click() time.sleep(2) # 确认成功跳转到输入框中的指定页 wait.until(EC.text_to_be_present_in_element( (By.CSS_SELECTOR, '#PageCont &gt; span.at'), str(page))) except Exception: return Nonedef parse_table(): # 提取表格第一种方法 # element = wait.until(EC.presence_of_element_located((By.ID, "dt_1"))) # 第二种方法 element = browser.find_element_by_css_selector('#dt_1') # 提取表格内容td td_content = element.find_elements_by_tag_name("td") lst = [] for td in td_content: # print(type(td.text)) # str lst.append(td.text) # 确定表格列数 col = len(element.find_elements_by_css_selector('tr:nth-child(1) td')) # 通过定位一行td的数量，可获得表格的列数，然后将list拆分为对应列数的子list lst = [lst[i:i + col] for i in range(0, len(lst), col)] # 原网页中打开"详细"链接，可以查看更详细的数据，这里我们把url提取出来，方便后期查看 lst_link = [] links = element.find_elements_by_css_selector('#dt_1 a.red') for link in links: url = link.get_attribute('href') lst_link.append(url) lst_link = pd.Series(lst_link) # list转为dataframe df_table = pd.DataFrame(lst) # 添加url列 df_table['url'] = lst_link # print(df_table.head()) return df_table# 写入文件def write_to_file(df_table, category): # 设置文件保存在D盘eastmoney文件夹下 file_path = 'D:\\eastmoney' if not os.path.exists(file_path): os.mkdir(file_path) os.chdir(file_path) df_table.to_csv('&#123;&#125;.csv' .format(category), mode='a', encoding='utf_8_sig', index=0, header=0)# 设置表格获取时间、类型def set_table(): print('*' * 80) print('\t\t\t\t东方财富网报表下载') print('作者：高级农民工 2018.10.6') print('--------------') # 1 设置财务报表获取时期 year = int(float(input('请输入要查询的年份(四位数2007-2018)：\n'))) # int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会 # https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10 while (year &lt; 2007 or year &gt; 2018): year = int(float(input('年份数值输入错误，请重新输入：\n'))) quarter = int(float(input('请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：\n'))) while (quarter &lt; 1 or quarter &gt; 4): quarter = int(float(input('季度数值输入错误，请重新输入：\n'))) # 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充， # http://www.runoob.com/python/att-string-format.html quarter = '&#123;:02d&#125;'.format(quarter * 3) # quarter = '%02d' %(int(month)*3) date = '&#123;&#125;&#123;&#125;' .format(year, quarter) # print(date) 测试日期 ok # 2 设置财务报表种类 tables = int( input('请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): \n')) dict_tables = &#123;1: '业绩报表', 2: '业绩快报表', 3: '业绩预告表', 4: '预约披露时间表', 5: '资产负债表', 6: '利润表', 7: '现金流量表'&#125; dict = &#123;1: 'yjbb', 2: 'yjkb/13', 3: 'yjyg', 4: 'yysj', 5: 'zcfz', 6: 'lrb', 7: 'xjll'&#125; category = dict[tables] # 3 设置url # url = 'http://data.eastmoney.com/bbsj/201803/lrb.html' eg. url = 'http://data.eastmoney.com/&#123;&#125;/&#123;&#125;/&#123;&#125;.html' .format( 'bbsj', date, category) # # 4 选择爬取页数范围 start_page = int(input('请输入下载起始页数：\n')) nums = input('请输入要下载的页数，（若需下载全部则按回车）：\n') print('*' * 80) # 确定网页中的最后一页 browser.get(url) # 确定最后一页页数不直接用数字而是采用定位，因为不同时间段的页码会不一样 try: page = browser.find_element_by_css_selector('.next+ a') # next节点后面的a节点 except: page = browser.find_element_by_css_selector('.at+ a') # else: # print('没有找到该节点') # 上面用try.except是因为绝大多数页码定位可用'.next+ a'，但是业绩快报表有的只有2页，无'.next+ a'节点 end_page = int(page.text) if nums.isdigit(): end_page = start_page + int(nums) elif nums == '': end_page = end_page else: print('页数输入错误') # 输入准备下载表格类型 print('准备下载:&#123;&#125;-&#123;&#125;' .format(date, dict_tables[tables])) print(url) yield&#123; 'url': url, 'category': dict_tables[tables], 'start_page': start_page, 'end_page': end_page &#125;def main(category, page): try: index_page(page) # parse_table() #测试print df_table = parse_table() write_to_file(df_table, category) print('第 %s 页抓取完成' % page) print('--------------') except Exception: print('网页爬取失败，请检查网页中表格内容是否存在')# 单进程if __name__ == '__main__': for i in set_table(): # url = i.get('url') category = i.get('category') start_page = i.get('start_page') end_page = i.get('end_page') for page in range(start_page, end_page): # for page in range(44,pageall+1): # 如果下载中断，可以尝试手动更改网页继续下载 main(category, page) print('全部抓取完成')这里，我下载了所有上市公司的部分报表。2018年中报业绩报表：2017年报的利润表：如果你想下载更多的报表，可以使用文中的代码，代码和素材资源可以在下面的链接中获取：https://github.com/makcyun/eastmoney_spider另外，爬虫还可以再完善一下，比如增加爬取上市公司的公告信息，设置可以爬任意一家（数家/行业）的公司数据而不用全部。还有一个问题是，Selenium爬取的速度很慢而且很占用内存，建议尽量先尝试采用Requests请求的方法，抓不到的时候再考虑这个。文章开头在进行网页分析的时候，我们初步分析了表格JS的请求数据，是否能从该请求中找到我们需要的表格数据呢？ 后续文章，我们换一个思路再来尝试爬取一次。本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Ajax</tag>
        <tag>图片下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pyhton可视化(3): 中国66家环保股上市公司市值Top20强]]></title>
    <url>%2F2018%2F09%2F20%2FPython_visualization03.html</url>
    <content type="text"><![CDATA[Tushare包提取中国环保股上市公司近10年市值排名，并结合D3.js做动态数据可视化表。摘要： 之前介绍过Tushare包和D3.js可视化动态表格，本文将二者进行结合，制作中国上市公司环境保护股Top20强，近十年的市值变化动态表。制作近10年期间，环保板块市值最高的20只股票动态变化，需要获得各只股票在不同年份的市值。获取特定股票的市值可以利用pro.daily_basic接口获取到每日的市值，然后利用Resample函数获得年均市值。但获取环保板块所有几十只股票的数据，用手动输入股票代码就不是很方便，此时，可以利用该包另外一个接口ts.get_stock_basics()接口获取所有股票基本数据，该接口能够返回股票代码、行业类别等数据。两个接口合二为一就可以提取出所需的数据，下面开始详细实现步骤。1. 提取所有股票代码123456789101112import tushare as ts# 获取所有股票列表data = ts.get_stock_basics()print(data.head())# 返回数据如下，所有列值可以参考：http://tushare.org/fundamental.html name industry area pe outstanding totals totalAssets \code 002936 N郑银 银行 河南 8.27 6.00 59.22 44363604.00 600856 中天能源 供气供热 吉林 21.28 13.43 13.67 1712831.63 300021 大禹节水 农业综合 甘肃 35.27 6.48 7.97 359294.91 603111 康尼机电 运输设备 江苏 0.00 7.38 9.93 734670.69 000498 山东路桥 建筑施工 山东 19.52 4.41 11.20 1926262.38可以看到，index是股票代码，name股票名称，industry是行业分类。我们需要获取环保类（可以获取任意行业类别，也可以全部获取所有股票，为了后期数据提取量耗时短一些，所以选择提取环保类股票）的股票代码和股票名称，代码如下：123456789101112data = data[data.industry =='环境保护']print(data.head()) #返回的环保股数据print('环保股股票数量为'：len(data.industry)) #计算环保股股票数量结果如下： name industry area pe outstanding totals totalAssets \code 300056 三维丝 环境保护 福建 0.00 2.37 3.85 266673.63 002549 凯美特气 环境保护 湖南 34.44 6.20 6.24 122630.13 300422 博世科 环境保护 广西 19.22 2.64 3.56 509822.44 601330 绿色动力 环境保护 深圳 59.83 1.16 11.61 784969.25 000820 神雾节能 环境保护 辽宁 0.00 2.88 6.37 284674.34 环保股股票数量为： 66可以看到，环境保护股一共有66只，下面我们将用这66只股票的代码和名称，输入到pro.daily_basic()接口中，获取每只股票的每日数据，其中包括每日市值。时间期限从2009年1月1日至2018年9月10日，共10年的逐日数据。2. 提股票每日市值每日基本指标的数据接口：https://tushare.pro/document/2?doc_id=321234pro = ts.pro_api()pro.daily_basic(ts_code='', trade_date='',start_date = '',end_date = '')# ts_code是股票代码，格式为000002.SZ,可以为一只股票，也可以是列表组成的多支股票# 后面三个是交易日期，可以为固定日期，也可以为一个时期，格式'20180919'该接口股票代码的格式是000002.SZ，而上面股票代码格式是：000002，没有带后缀.SZ，由此需要添加上，然后就可获取每只股票近10年的逐日市值数据。12345678910data['code2'] = data.index# apply方法添加.SZ后缀data['code2'] = data['code2'].apply(lambda i:i+'.SZ')data = data.set_index(['code2'])# 将code和name转为dict，因为我们只需要表格中的代码和名称列data = data['name']data = data.to_dict()# print(data) #测试返回的环保股字典数据 ok&#123;'300056.SZ': '三维丝', '002549.SZ': '凯美特气', '300422.SZ': '博世科', '601330.SZ': '绿色动力', '000820.SZ': '神雾节能', '300072.SZ': '三聚环保', '300055.SZ': '万邦达', '002717.SZ': '岭南股份', '300070.SZ': '碧水源', '000504.SZ': '南华生物', '300203.SZ': '聚光科技', '002672.SZ': '东江环保', '000967.SZ': '盈峰环境', '002322.SZ': '理工环科', '300272.SZ': '开能健康', '300495.SZ': '美尚生态', '603717.SZ': '天域生态', '300266.SZ': '兴源环境', '603126.SZ': '中材节能', '002200.SZ': '云投生态', '300385.SZ': '雪浪环境', '603200.SZ': '上海洗霸', '000826.SZ': '启迪桑德', '300262.SZ': '巴安水务', '002887.SZ': '绿茵生态', '603568.SZ': '伟明环保', '300631.SZ': '久吾高科', '002616.SZ': '长青集团', '300156.SZ': '神雾环保', '000920.SZ': '南方汇通', '600008.SZ': '首创股份', '601200.SZ': '上海环境', '603955.SZ': '大千生态', '603177.SZ': '德创环保', '600481.SZ': '双良节能', '300190.SZ': '维尔利', '603588.SZ': '高能环境', '002034.SZ': '旺能环境', '603817.SZ': '海峡环保', '002499.SZ': '科林环保', '603822.SZ': '嘉澳环保', '300664.SZ': '鹏鹞环保', '300332.SZ': '天壕环境', '600526.SZ': '菲达环保', '600874.SZ': '创业环保', '600292.SZ': '远达环保', '603903.SZ': '中持股份', '300172.SZ': '中电环保', '000544.SZ': '中原环保', '300692.SZ': '中环环保', '600388.SZ': '龙净环保', '300425.SZ': '环能科技', '300388.SZ': '国祯环保', '300362.SZ': '天翔环境', '300197.SZ': '铁汉生态', '300187.SZ': '永清环保', '300090.SZ': '盛运环保', '002573.SZ': '清新环境', '000035.SZ': '中国天楹', '603797.SZ': '联泰环保', '603603.SZ': '博天环境', '300137.SZ': '先河环保', '300355.SZ': '蒙草生态', '300152.SZ': '科融环境', '002658.SZ': '雪迪龙', '600217.SZ': '中再资环'&#125;可以看到，很完整地显示了环保股的股票代码和名称，下面通过for循环即可获取每日数据。为了方便，将上式代码命名为一个函数get_code()，return data 为上面的dict。3. 提取环保股公司数据123456789101112ts_codes = get_code()start = '20090101'end = '201809010'for key,value in ts_codes.items(): data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) # 获取每只股票时间段数据 # 添加代码列和名称列 # 替换掉末尾的.SZ,regex设置为true才行 data['code'] = data['ts_code'].replace('.SZ','',regex = True) data['name'] = value # 存储结果 data.to_csv('environment.csv',mode='a',encoding = 'utf_8_sig',index = False,header = 0) print('数据提取完毕')表格结果如下，66只股票10年一共产生了75933行数据。如果提前全部3000多家股票的数据，那么数据量会达到几百万行，量太大，所以这里仅提取了66支。其中，选中的列为每日市值（万元）。下面就可以根据日期、市值得到各只股票每年的市值均值，然后绘制股票动态表。4. 绘制Top20强动态表首先读取上面的表格，获取DataFrame信息：1234567891011121314151617181920212223df = pd.read_csv('environment.csv',encoding = 'utf-8',converters = &#123;'code':str&#125;)# converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示print(df.info())Data columns (total 17 columns):ts_code 75932 non-null objecttrade_date 75932 non-null int64close 75932 non-null float64turnover_rate 75932 non-null float64volume_ratio 0 non-null float64pe 70861 non-null float64e_ttm 69254 non-null float64pb 73713 non-null float64ps 75932 non-null float64ps_ttm 75840 non-null float64total_share 75932 non-null float64float_share 75932 non-null float64free_share 75932 non-null float64total_mv 75932 non-null float64circ_mv 75932 non-null float64code 75932 non-null int64name 75932 non-null objectdtypes: float64(13), int64(2), object(2)可以看到trade_date交易日期是整形，需将交易日期先转换为字符型再转换为datetime日期型。1234567891011121314151617181920212223242526from datetime import datetime# trade_date是int型，需转为字符型df['trade_date'] = df['trade_date'].apply(str)# 或者# df['trade_date'] = df['trade_date'].astype(str)# 将object转为datatimedf['trade_date'] = pd.to_datetime(df['trade_date'],format = '%Y%m%d',errors = 'ignore') #errors忽略无法转换的数据，不然会报错# 结果如下：Data columns (total 17 columns):ts_code 75932 non-null objecttrade_date 75932 non-null datetime64[ns]close 75932 non-null float64turnover_rate 75932 non-null float64volume_ratio 0 non-null float64pe 70861 non-null float64e_ttm 69254 non-null float64pb 73713 non-null float64ps 75932 non-null float64ps_ttm 75840 non-null float64total_share 75932 non-null float64float_share 75932 non-null float64free_share 75932 non-null float64total_mv 75932 non-null float64circ_mv 75932 non-null float64code 75932 non-null int64name 75932 non-null object1234567891011121314# 设置总市值数字格式由万元变为亿元df['total_mv'] = (df['total_mv']/10000)# 保留四列,并将交易日期设为indexdf = df[['ts_code','trade_date','total_mv','name']]df = df.set_index('trade_date')print(df.head())# 结果如下: ts_code total_mv nametrade_date 2018-08-30 300090.SZ 36.034715 盛运环保2018-08-29 300090.SZ 38.014644 盛运环保2018-08-28 300090.SZ 39.202602 盛运环保2018-08-27 300090.SZ 40.126569 盛运环保2018-08-24 300090.SZ 38.938611 盛运环保接下来，求出每只股票每年的市值平均值：12345678910111213141516171819202122232425求平均市值时需切片同一股票，这里股票名称切片赋值为value变量，也就是dict字典里66只股票名称df = df[df.name == value]# 不能用query方法,会报错 df = df.query('name == value')# resampe按年统计数据df = df.resample('AS').mean() #年平均市值print(df.head())# 结果如下： total_mv codetrade_date 2009-01-01 25.184678 三维丝2010-01-01 50.672849 三维丝2011-01-01 46.488004 三维丝2012-01-01 39.214508 三维丝2013-01-01 59.110332 三维丝# 再用to_period按年显示市值数据df = df.to_period('A')print(df.head())# 结果如下: total_mv codetrade_date 2009 25.184678 三维丝2010 50.672849 三维丝2011 46.488004 三维丝2012 39.214508 三维丝2013 59.110332 三维丝经过以上处理，基本就获得了想要的数据。为了能够满足D3.js模板表格条件，再做一点修改：12345678910# 增加code列df['code'] = value# 重置indexdf = df.reset_index()# 重命名为d3.js格式# 增加一列空typedf['type'] = ''df = df[['code','type','total_mv','trade_date']]df.rename(columns = &#123;'code':'name','total_mv':'value','type':'type','trade_date':'date'&#125;)# df.to_csv('parse_environment.csv',mode='a',encoding = 'utf_8_sig',index = False,float_format = '%.1f',header = 0) # float_format = '%.1f' #设置输出浮点数格式为1位小数最终，生成parse_environment.csv文件如下：123456789101112name type value date中国天楹 8.8 2009中国天楹 9.8 2010中国天楹 15 2011中国天楹 18.8 2012中国天楹 22.5 2013...东方园林 177.1 2014东方园林 320.5 2015东方园林 288.4 2016东方园林 481 2017东方园林 461.5 2018可绘制出动态可视化表格，见下面视频。可以看到前几年市值龙头由东方园林、碧桂园轮流坐庄，近两年三聚环保强势崛起，市值增长迅猛，跃居头名。https://www.bilibili.com/video/av32087716/本文仅对比了环保股企业的市值变化，你还可以分析互联网股、金融股等100多种行业的企业市值对比。另外，Tushare包返回的参数还可以做更多其他分析。文章完整的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import pandas as pdimport tushare as tsfrom datetime import datetimeimport matplotlib.pyplot as pltts.set_token('404ba015bd44c01cf09c8183dcd89bb9b25749057ff72b5f8671b9e6')pro = ts.pro_api()def get_code(): # 所有股票列表 data = ts.get_stock_basics() # data = data.query('industry == "环境保护"') # 或者 data = data[data.industry =='环境保护'] # 提取股票代码code并转化为list data['code2'] = data.index # apply方法添加.SZ后缀 data['code2'] = data['code2'].apply(lambda i:i+'.SZ') data = data.set_index(['code2']) # 将code和name转为dict data = data['name'] data = data.to_dict() # 增加东方园林 data['002310.SZ'] = '东方园林' # print(data) #测试返回的环保股dict ok return datadef stock(key,start,end,value): data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) # 获取每只股票时间段数据 # 替换掉末尾的.SZ,regex设置为true才行 data['code'] = data['ts_code'].replace('.SZ','',regex = True) data['name'] = value # print(data) data.to_csv('environment.csv',mode='a',encoding = 'utf_8_sig',index = False,header = 0)def parse_code(): df = pd.read_csv('environment.csv',encoding = 'utf-8',converters = &#123;'code':str&#125;) # converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示 df.columns = ['ts_code','trade_date','close','turnover_rate','volume_ratio','pe','e_ttm','pb','ps','ps_ttm','total_share','float_share','free_share','total_mv','circ_mv', 'code','name'] # trade_date是int型，需转为字符型 df['trade_date'] = df['trade_date'].apply(str) # 或者df['trade_date'] = df['trade_date'].astype(str) # 将object转为datatime df['trade_date'] = pd.to_datetime(df['trade_date'],format = '%Y%m%d',errors = 'ignore') #errors忽略无法转换的数据，不然会报错 ## 设置总市值数字格式由万元变为亿元 df['total_mv'] = (df['total_mv']/10000) # 保留四列,并将交易日期设为index df = df[['ts_code','trade_date','total_mv','name']] df = df.set_index('trade_date') df = df[df.name == value] # # 不能用query方法 # # df = df.query('name == ') df = df.resample('AS').mean()/10000 #年平均市值 df = df.to_period('A') # # 增加code列 df['code'] = value # # 重置index df = df.reset_index() # 重命名为d3.js格式 # 增加一列空type df['type'] = '' df = df[['code','type','total_mv','trade_date']] df.rename(columns = &#123;'code':'name','total_mv':'value','type':'type','trade_date':'date'&#125;) df.to_csv('parse_environment.csv',mode='a',encoding = 'utf_8_sig',index = False,float_format = '%.1f',header = 0) float_format = '%.1f' #设置输出浮点数格式 # print(df) # print(df.info())def main(): # get_code() #提取环保股dict start = '20090101' end = '201809010' ts_codes = get_code() # dict_values转list keys = list(ts_codes.keys()) values = list(ts_codes.values()) for key,value in ts_codes.items(): stock(key,start,end,value) for value in values: parse_code(value)if __name__ == '__main__': main()文件素材可以在这里获得：https://github.com/makcyun/web_scraping_with_python/tree/master/Tushare本文完。]]></content>
      <categories>
        <category>Python可视化</category>
      </categories>
      <tags>
        <tag>Python可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的白血病妻子]]></title>
    <url>%2F2018%2F09%2F15%2Flife01.html</url>
    <content type="text"><![CDATA[世界骨髓捐献者日。当夜晚来临，人们结束一天的忙碌，正赶着回家之际，我的一天才刚刚开始。守夜时间是晚七时至凌晨七时，我称之为”深夜医院”。你问我有没有人，我会说人还挺少的。1. 前言这是一篇关于一个年轻的家庭和白血病作斗争的生活随笔。人物介绍：我，苏克，27岁。妻子，”23”，25岁。儿子，馍馍，2岁半。时间 ： 2018年5月至今地点 ：东莞—广州—北京2. 天使与恶魔5月18日23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”我：”老婆，太棒了，我就说你可以的，你都找不到工作，那很多人更找不到了！”23：”一直说想换工作，这次终于是跨出这一步了，很紧张啊！”我：”不要紧张，你肯定能很快适应新工作的。”23：”好吧，我答应 HR 6月1号去签合同，再上几天班就把年假休了，放松一下。”我：”对啊，好好休息休息，我要是也有年假，就陪你出去玩了。”23：”你在深圳，周末能有空回来就不错了…”我：”以后，回东莞来就有时间了…”一周后 5月25日 周五23：”老公，我住院了。”我：”啊？怎么回事？”23：” 背痛了两天，痛地不行了，还发烧…”我：”还是没有缓解啊，我刚下班，现在从梅林关坐大巴回来，到了马上去医院…”一个多小时后…我：”怎么样？医生怎么说?”23：”医生看了血常规，说白细胞很高(29万多，正常是1万以内)，让我转到血液科来了，现在打消炎针退烧…”我：”希望没大碍，早点退烧吧，这几天晚上我陪你。”半夜…23：”老公，我很热，出了好多汗，衣服全湿了…”我：”我给你换，你额头不烫了，多出点汗烧就退下来了。”23：”我背好疼…”我：”忍受不了的痛么？我刚去找了医生，医生说忍一忍，坚强一点。”3天后 周一我：” 老婆，我得赶去深圳上班了，希望你这周能退烧。”23：”我也希望，我这周五还要去公司签劳动合同呢。”6月1日 周五我：”老婆，今天儿童节，我在网上买了好多书给儿子，以后周末回来给他讲故事，争取做个会讲故事的好爸爸。”23：”好，你快回来吧，我这几天反复高烧到40度，医生也找不到原因，让做骨穿还建议转院去广州”我：”啊？太奇怪了，这次发烧怎么这么顽固，骨穿是做什么？我马上坐车回来…”6月2日 转院到广州一家医院的血液科当地医院的医生找不到病因，让我们转院到广州去。我一直不明白，发烧为什么要住到血液科病区。到了之后，医生给23抽了骨髓，说是查找病因。一天后医生单独跟我我和家里人说：”确诊病人患了急性淋巴细胞白血病，之前反复高烧和骨骼疼痛是这种病的常见临床表现。这种病得马上开始化疗，不然会有生命危险。” 听完，我一时没有反应过来。后来了解到，如果一切顺利的话，治疗和康复时长大概要2年，治疗费用在100万左右；如果不顺利，生命可能只剩不到2年。我突然明白了：原来这不是普通地发烧和背疼啊。不过，”白血病是什么病? 严重么？好治么？”23那几天一直问：”检测结果有没有出来？到底是怎么回事？”我说：”医生说需要十几天才能检测出来，别着急，这边的医生肯定会把你治好的。”我偷偷百度了下，大致了解到白血病俗称”血癌”，是癌症的一种。后面一直在想，为什么23会得上这个病？不管怎样，首先要做好长期住院治疗的准备，那么得尽快辞职了。6月8日我回到深圳，火速辞了职、退了租房。晚上，关了灯，望着窗外灯火通明的深圳，看着这个刚刚来了3个月的城市。各种思绪开始涌上心头：23会很顺利地治好病么？未来的生活会是怎样？2年后进入社会我们还有竞争力么？很多刚毕业的人感叹生活艰难，一出来就要变成”房奴”。那我，如果不靠父母亲友，岂不是要变成”病奴”了？想了好一会儿却什么没想出来，那么就顺其自然、虔诚祈祷吧。转而，开始回忆起这几年的走过的点点滴滴，想从曾面对过的种种困难中找到信心。3. 时光倒流3.1. 2014年-浪漫邂逅2014年夏天，我本科毕业，从成都开启了筹划已久的70天毕业旅行。对于一个在新疆长大的内地人来说，22、3岁才第一次见到大海，那是一种难以言表的心情。整个旅途，我都尽可能地享受和体验每一处风土人情。70天，不知不觉就快要结束，来到了最后一站：南亚岛国斯里兰卡。在一个青旅里我第一次见到了23。那种感觉用一个词形容就够了：”一见钟情”。聊了几次天之后，得知她在广州念大三，来斯里兰卡是到一个孤儿院做义工。一周后，我到了她所在的城市”加勒”——一座濒临印度洋的小城镇。距离回国倒数的那几天里，我用一顿大盘鸡把她”收买”了。后来，我认为这是我人生中最重要的决定之一。3.2. 2015年-约定终身回国后，我们开始异地，我在成都读研，她在广州读大四。这一年，我们的感情状况可以用”打飞的”来形容，她常来成都，我也常过去，还参加了她的毕业典礼。最终，我们决定在8月，认识的一周年的那天领证结婚。婚礼仪式很简单，仅是请亲人朋友吃了一顿饭。3.3. 2016年-馍馍出生2016年过年期间，感谢伟大的妈妈，把儿子馍馍带到了这个世界上。坦诚地说，那种感觉是惊喜与压力并存。我们俩都没有什么经济来源，只能靠家里”救济”，我仅能每个月回来几天。3.4. 2017年-毕业工作就这样，异地状态维持了近3年，我终于毕业，可以回去抱娃和工作。曾经，八卦地转发过一篇文章给23说：”居然有人带娃参加毕业典礼”，没想到，说的也算是自己啊。家也从西北搬到了东南，走过这一路，仅仅花了多数人二到三分之一的时间。3.5. 2018年-再次异地在东莞工作了半年，2018年春节后，我决定还是去隔壁更大的深圳看看。4月的某一天，我和23说：”要不你还是从政府出来吧，趁还年轻换个更有活力和挑战的工作环境。”23：”是啊，为了馍馍在里面待了3年，我也想出来了。”我：”那我们就开始找找看吧。”一个月后的5月18日23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”(回到了文章的开头）窗外的霓虹灯渐渐地黯淡下来，发现已经半夜，赶紧睡吧，明天得赶去广州的医院，要开始新的人生节奏了。4. 难熬的化疗4.1. 治疗过程6月2日，23在广州医院住下来，仍然在发烧并伴随强烈的骨痛。前3天体温持续超过38度，半夜最高烧到了40.3度。6月4日下午，瞒着23马上开始进行了化疗药物注射。说起化疗，我脑子里最先想到的是赵本山和范伟演过的一个小品，里面戏称化疗为”谈话治疗”。可实际中的化疗远比这难受地多。23对药物反应特别大，不停呕吐、浑身乏力、也没有任何食欲，体重开始暴瘦。连续打了三天化疗针以后，总算压制住了体内的恶性白细胞，23不再发烧和骨痛，整个人才渐渐恢复精神，开始有胃口喝粥和下床活动一下。随着在病区看到和听到医生、护士、病友关于白血病的治疗和讲解，23慢慢接受了自己得病的事实。化疗的第14天，出现了掉头发现象后，她同意剪发，过肩的长发被剪成了齐耳短发。好在，她很看得开，说：”这样很方便，至少出汗不用那么难受了”。没有料到，头发脱落的速度是那么地快。仅仅2天过后，头发稍微一抓就是一把，整个床上和地下到处都是，打扫卫生的阿姨说”快点剪了吧，打扫卫生不方便”。坚持了几天后，在出院的那一天，给23彻底剪光了所有的头发。她依然很释怀，还在朋友圈让大家推荐戴哪个假发好，我心里感到很意外。在病房，23每天要经历的三件事就是输液、抽血和用药。大部分的时间都是在输液，各种各样的药物通过一袋袋的氯化钠和葡萄糖注射进23的血液里，有时是单手臂，有时双手都要打。同时，几乎每天要扎针抽血，各种颜色的抽血管，到后期手背已全部瘀青。至于服药，就像一日三餐一样稀松平常。就这样，每一天都仿佛在重复着昨天。整个疗程22天下来，一共输入了114袋液体，平均每天5袋，重达23200 ml，大致和一个5岁的小孩体重差不多。一共抽了81管血液，平均每天4管。注射和服用各种各样的药物加起来，有60种之多。化疗两周后，身高168 cm 的23体重降到了79斤，搀扶她上厕所，摸到的都是骨骼。多数女生喜欢越瘦越好，瘦到这种程度可能就不喜欢了吧。4.2. 23的感受尽管生理和心理都遭受着从未体验过的艰难考验，23的表现大大超出了我的心理预估。不难受的时候，她会发一些朋友圈，每个人看到都会情不自禁地被感动到。要说，会让23难受的一件事，那就是见不到儿子。从他出生下来至今，从来没有这么多天看不到他。所以，每次和馍馍视频，她都特别开心。有一天晚上，打电话回家，发现突然找不到馍馍，23急得要命。后来，终于在一个小剧场找到了他，原来和小伙伴在一起。23想哭又有点生气，训了他几句。之后，我妈说原来儿子是想跟着小伙伴的妈妈去进剧场看节目，但他没有票就赖在那里不想走。他在那里哭着说：”我妈妈在就好了，妈妈在就会给我买票。”听完23就哭了，我也鼻头一酸，2岁半的他竟然会讲出这样的话。6月23日住了3周院后，23终于能踏出病区，呼吸到外面的空气。回家的路上，她一直看着车窗外，没有说一句话。回到家后第一件事就是抱馍馍，馍馍居然很自然地接受了光头的妈妈。回到房间之后，23突然大哭了一场，吓了我一跳，急忙问怎么了，她说：”活着真好。”4.3. 陪伴我一直都是一个大老粗，不怎么会照顾人，更不用说病人。好在，家里人给予了非常强力的支持。在23身边，做的最多的仅仅是陪伴。5. 关于白血病5.1. 什么是白血病在此之前，我的医学常识差到连感冒、发烧该吃什么药都不太清楚。白血病？我甚至有百度过：是不是血液是白色的病？(最好不要去百度)在医院住久了之后，慢慢地开始对这个病有所了解。白血病，是一种骨髓中的白细胞大量、异常增生的癌症。这些白细胞会破坏骨髓的造血功能，转而取代正常细胞。初期会导致病人出现发烧、贫血、感染等症状，具体发病原因至今没有完全弄清楚。虽然是癌症，但受益于科技发展，目前白血病是可以根治的。5年的生存率总体在50%以上，儿童高于成年人，且不同类型的白血病存活率也会不一样。根据细胞类型和发病程度，可将白血病分为四种主要类型：细胞类型急性慢性淋巴细胞白血病急性淋巴细胞白血病慢性淋巴细胞白血病（ALL）（CLL ）髓细胞白血病急性髓性白血病慢性粒细胞白血病（AML ）（CML）儿童中，四分之三患上的是ALL；成人中，AML 和CLL 最为常见，《我不是药神》里面说的是CML。23患上的是ALL，并且属于其中的高危型，存活率为50%。ALL接受造血干细胞移植且5年内没有复发的话，之后终身基本上就不会再复发，也就是会和正常人一样。普通个体患这种病的概率是十万分之1-1.5，所以很多人对这个病比较陌生，觉得离自己很遥远。其实，不用觉得陌生和遥远。像宋庆龄、肯德基创始人、居里夫人等这些名人就是罹患这种病而逝世的。根据中国红十字会统计，2012年中国内地有400万患者，并且每年新增4万人。5.2. 治疗方法白血病的治疗通常是采用：化疗、放疗、干细胞移植中的一种或者组合。以23所属的ALL高危类型来说，治疗主要分为三个阶段：化疗缓解、造血干细胞移植和排异控制和恢复。5.2.1. 化疗刚患病时，骨髓中会产生超出常人数十倍的白细胞，这么多的白细胞需要采用高强度的化疗进行治疗。最终将骨髓中的白血病原始细胞减少至&lt;5％，并从血液中清除肿瘤细胞。这个阶段的治疗通常需要3-6个疗程，然后为下一个阶段做准备。5.2.2. 造血干细胞捐献移植干细胞具有自我更新、繁殖并能分化成不同种类的成熟细胞的能力。造血干细胞是一群未分化的血液细胞，可以制造运输氧气的红血球、帮助凝血的血小板、抵抗感染的白血球等。所以，造血干细胞可用来治疗许多包括白血病在内的血液疾病、肿瘤等。它存在于骨髓、婴儿脐带血、以及成人周边血液中。不少人会认为捐献造血干细胞就是抽取骨髓，而抽取骨髓会有很大风险甚至会残疾。所以常出现志愿者反悔的情形，给出的理由常常是”父母不同意”、”儿女不同意”，甚至是”丈母娘不同意”、”领导不同意”。其实，骨髓捐献造血干细胞已经是很久以前的方法了，现在90%以上的捐献者采用的捐献周边血干细胞。其实，这就是一个加强版的抽血过程，血液中左手臂抽出经过一个血液分离机，提取出干细胞，然后再经过右手臂回输到自己身体中。整个过程持续半天，最终抽取出200-300ml的血液就算完成捐献了。对于病人来说，造血干细胞移植，就是利用供者的干细胞的造血能力全部换掉自身体内的血液，从而彻底清除癌变的白细胞，然后重新造血。因此，病人的血型在移植完后会变成供体的血型。供体的选择，通常是按照：兄弟姐妹、直系亲属、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序去筛选寻找。判断供体和不合适，其实和常人输液一样，如果你是A型血，那你需要同样是A型血的人，B型血的人就爱莫能助。只不过移植不是用血型来判断，而是根据HLA（人类白细胞抗原）。HLA（人类白细胞抗原）是编码人类的主要组织相容性复合体的基因，负责调节人体免疫系统。简单地说，HLA是人体生物学的”身份证”，由父母遗传，能识别”自己”和”非己”，会通过免疫反应排除”非己”。所以，HLA在造血干细胞移植的成败中起着重要的作用。造血干细胞移植就需要供体和受体HLA配型相同，如果不相同就会发生致命的排斥反应。由于不同人种、不同种族、不同个体的HLA差别很大，所以要采用一种方法来确定HLA。判断两个人的HLA是否相同，只需要抽8ml的血，做HLA高分辨检测化验，然后将HLA上5组基因的数字进行对比就能判定。简单地举个例子，供体1的10组数字和受体全部对应，称为”全相合”，这是最理想的，移植效果也最好；供体2有5组数字和受体对应，称为”半相合”，如果是直系亲属，那么可以移植，如果是非血缘供提，那么无法移植；供体3一组数字都对不上，称为”零相合”，完全不可用。类似地还会有9组数字对应的情况，称为9个点匹配，以此类推。尽管判断HLA是否相同很简单，但是供体的选择范围其实很有限。按照兄弟姐妹、父母子女、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序来解释一下。兄弟姐妹根据染色体的基本常识，正常情况下，子女的染色体一半来自父亲、一半来自母亲。可以知道，兄弟姐妹有25%的概率是全相合、50%的概率是半相合、剩下25%则是零相合。可以说，只要有兄弟姐妹，供体基本就找到了，但很多人没有兄弟姐妹，尤其90后。父母子女没有兄弟姐妹的人，可以将父母子、女身作为潜在的供体对象。绝大多数情况下，父母和子女之间是半相合，这是最常见的。目前，国内半相合移植技术发展地很好，很多患者才得以生存下来。但是也有条件限制，比如在健康的条件下，父母年龄最好不要超过60岁，子女体重要达到一定要求才行。堂（表）兄弟姐妹在堂（表）兄弟姐妹中，这个概率其实就比较低了，但如果前两类人群中无法找到，也只能去试一下。骨髓库志愿者上面三类人群都无法找到，那么只能寄希望于骨髓库中的志愿者了。没有血缘关系的人，HLA能匹配上的概率非常低，只有数十万到上百万分之一！但依然得抱着希望去尝试，因为这是最后一根救命稻草。5.2.3. 控制排异移植完成后，供体的干细胞进入病人身体，自身细胞会视为外来敌人然后会发生排斥。可以比喻成战争，视战役的惨烈程度分为两种情况：第一种小规模或者和平停战，那么移植算成功，慢慢恢复就能康复起来；第二种是拼到了两败俱伤的境地，就会引起严重的排异反应，这就会致命。控制排异和恢复的过程需要持续数年。5.3. 治疗费用根据治疗方案就可以大致计算白血病的治疗费用。23的治疗费用主要包括：化疗费用+移植费用+排异控制费用+服药费用。简单做了一下费用预估：化疗费用15-40万移植费用30-50万排异控制费用：可能几万，也可能几百万口服药（几万到数十万）。拿口服药举个例子，服用进口药，60片共3g要8500元。目前，千足金（99.99%）的价格是265元/g，这种药的单价是其10倍不止。一瓶够服用一个月，服用时间按年计算。所以，白血病可能50万能治好，也可能花了500万，仍然留不住病人。6. 希望之光很多亲朋好友在得知我们的情况后，非常关心并且给予了很多的帮助，让我们不要有压力。我觉得，压力大不大最好是去对比一下。一对比，很容易就能得出答案。3个多月以来，住了3家医院，接触和听说过不少病友的故事。比如：有怀身孕6个月的妻子照顾患病的丈夫、这辈子几乎无法再当妈妈的未婚女生、撇下2岁多孩子消失的父亲等等。相比之下，我们的压力不算大，唯一的压力来自于供体的寻找。9月13日医生告诉了一个久违的好消息：在台湾骨髓库找到了一个全相合且有意愿的供者。全家听到后都特别的开心，祈祷这位同胞体检能够通过，最终顺利捐出造血干细胞。2015年，世界骨髓捐献者协会宣布：每年9月的第3个周六，为”世界骨髓捐献者日”。截至2018年9月，全球骨髓库有3250万志愿者，占全世界75亿人的千分之四左右。中国内地排在美国、德国和巴西之后，位列世界第四，志愿者的人数为250万，仅占总人口比例的0.18%，比其他国家（地区）低一个数量级。250万这个数字，还只是登记在库内的初查人数，实际上真正愿意做志愿者的人，会比这低很多。从2001年成立到现在，17年只捐献了7600例，意味着最多有7600个患者得到了救治，每年只有平均450人。第一大骨髓库美国，30年来也不过，只有8万人得到了救治。可以说，不仅国内，全世界骨髓库都很需要志愿者的加入。骨髓库成立时间（年）库容志愿者人数（万）人口（亿）占人口比例捐献造血干细胞例数美国19889003.32.73%80,000德国19934000.839.37%-巴西-5002.122.36%-中国200125013.90.18%7,600台湾1993430.241.79%5,100参考来源：WMDA global trend report 2017、 http://lnmdp.org.admin.yuntai.net/readnews.aspx?n_id=1817&amp;type=4假如，大陆志愿者的人口比例，能够达到台湾的比例，那么，这一数字将会从250万变成2500万！成为志愿者的了解渠道：http://www.cmdp.org.cn/show/1020166.html本文完。链接：世界骨髓捐献者协会WMDA：https://www.wmda.info/世界骨髓捐献者日网站WMDD：https://worldmarrowdonorday.org/中华骨髓库CMDP：http://www.cmdp.org.cn/台湾慈濟骨髓幹細胞中心BTCSCC：http://btcscc.tzuchi.com.tw/香港骨髓庫：https://www5.ha.org.hk/rcbts/hkarticle.asp?bid=56&amp;MenuID=6#.W52iwfl4nDc美国骨髓库NMDP：https://bethematch.org/欧洲骨髓库EMDIS：www.emdis.net/德国骨髓库ZKRD：https://www.zkrd.de/de/日本骨髓库JMDP： http://www.jmdp.or.jp/白血病介绍：https://en.m.wikipedia.org/wiki/Leukemia急性淋巴细胞白血病介绍： https://en.m.wikipedia.org/wiki/Acute_lymphoblastic_leukemiaHLA介绍：https://en.m.wikipedia.org/wiki/Human_leukocyte_antigen]]></content>
      <categories>
        <category>生活随笔</category>
      </categories>
      <tags>
        <tag>生活随笔</tag>
        <tag>白血病</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python可视化(2)：折线图]]></title>
    <url>%2F2018%2F09%2F11%2FPython__visualization02.html</url>
    <content type="text"><![CDATA[日期型数据的折线图绘制。摘要： 利用matplotlib绘制横轴为日期格式的折线图时，存在不少技巧。本文借助Tushare包返回的股票数据，介绍日期折线图绘制的方法。上一篇文章的最后讲到了折线图的绘制，本文详细介绍绘制方法。折线图绘制的数据源，采用Tushare包获取上市公司基本数据表，格式如下：123456789import pandas as pddata = pd.read('get_stock_basics.csv',encoding = 'utf8')print(data.head())ts_code symbol name list_status list_date is_hs000001.SZ 1 平安银行 L 19910403 S000002.SZ 2 万科A L 19910129 S000004.SZ 4 国农科技 L 19910114 N000005.SZ 5 世纪星源 L 19901210 N然后利用resample和to.period方法汇总各年度的上市公司数量数据，格式为Pandas.Series数组格式。1234567891011121314151617181920# 汇总各年上市公司数量data = data.set_index(['list_date'])data = data.resample('AS').count()['ts_code']data = data.to_period('A')print(data.head())print(data.tail())# 结果如下：list_date1990 71991 41992 371993 1061994 99...list_date2014 1242015 2232016 2272017 4382018 781. Series直接绘制折线图首先，我们可以直接利用pandas的数组Series绘制折线图：123456789101112import matplotlib.pyplot as pltplt.style.use('ggplot') # 设置绘图风格fig = plt.figure(figsize = (10,6)) # 设置图框的大小ax1 = fig.add_subplot(1,1,1)data.plot() # 绘制折线图# 设置标题及横纵坐标轴标题colors1 = '#6D6D6D' #设置标题颜色为灰色plt.title('历年中国内地上市公司数量变化',color = colors1,fontsize = 18)plt.xlabel('年份')plt.ylabel('数量(家)')plt.show()可以发现，图中存在两个问题：一是缺少数值标签，二是横坐标年份被自动分割了。我们希望能够添加上数值标签，然后坐标轴显示每一年的年份值。接下来，需要采用新的方法重新绘制折线图。2. 折线图完善12345678910111213141516171819# 创建x,y轴标签x = np.arange(0,len(data),1) ax1.plot(x,data.values, #x、y坐标 color = '#C42022', #折线图颜色为红色 marker = 'o',markersize = 4 #标记形状、大小设置 )ax1.set_xticks(x) # 设置x轴标签为自然数序列ax1.set_xticklabels(data.index) # 更改x轴标签值为年份plt.xticks(rotation=90) # 旋转90度，不至太拥挤for x,y in zip(x,data.values): plt.text(x,y + 10,'%.0f' %y,ha = 'center',color = colors1,fontsize = 10 ) # '%.0f' %y 设置标签格式不带小数# 设置标题及横纵坐标轴标题plt.title('历年中国内地上市公司数量变化',color = colors1,fontsize = 18)plt.xlabel('年份')plt.ylabel('数量(家)')# plt.savefig('stock.png',bbox_inches = 'tight',dpi = 300)plt.show()完善后的折线图如下：可以看到，x轴逐年的数据都显示并且数值标签也添加上了。3. 多元折线图上面介绍了一元折线图的绘制，当需要绘制多元折线图时，方法也很简单，只要重复绘图函数即可。这里我们以二元折线图为例，绘制国内两家知名地产公司万科和保利地产2017年的市值变化对比折线图。3.1. 数据来源数据源仍然采用tushare包的pro.daily_basic()接口，该接口能够返回股票的每日股市数据，其中包括每日市值total_mv。我们需要的两只股票分别是万科地产(000002.SZ)和保利地产(600048.SH)，下面就来获取两只股票2017年的市值数据。123456789101112131415161718192021222324252627import tushare as tsts.set_token('你的token') # 官网注册后可以获得pro = ts.pro_api()def get_stock(): lst = [] ts_codes = ['000002.SZ', '600048.SH'] for ts_code in ts_codes: data = pro.daily_basic( ts_code=ts_code, start_date='20170101', end_date='20180101') print(lst) reutrn lst # 结果如下，total_mv为当日市值（万元）： #万科地产数据 ts_code trade_date close … total_mv circ_mv0 000002.SZ 20171229 31.06 … 3.43E+07 3.02E+071 000002.SZ 20171228 30.7 … 3.39E+07 2.98E+072 000002.SZ 20171227 30.79 … 3.40E+07 2.99E+073 000002.SZ 20171226 30.5 … 3.37E+07 2.96E+074 000002.SZ 20171225 30.37 … 3.35E+07 2.95E+07 #保利地产数据 ts_code trade_date close … total_mv circ_mv0 600048.SH 20171229 14.15 … 1.68E+07 1.66E+071 600048.SH 20171228 13.71 … 1.63E+07 1.61E+072 600048.SH 20171227 13.65 … 1.62E+07 1.60E+073 600048.SH 20171226 13.85 … 1.64E+07 1.63E+074 600048.SH 20171225 13.55 … 1.61E+07 1.59E+07下面对数据作进一步修改，从DataFrame中提取total_mv列，index设置为日期。1234567891011121314151617181920212223242526272829303132333435data['trade_date'] = pd.to_datetime(data['trade_date'])# 设置index为日期data = data.set_index(data['trade_date']).sort_index(ascending=True)# 按月汇总和显示data = data.resample('m')data = data.to_period()# 市值改为亿元market_value = data['total_mv']/10000# 二者结果分别如下，万科地产：2017-01 2291.9732702017-02 2286.3310372017-03 2306.8947902017-04 2266.3379062017-05 2131.0530982017-06 2457.7166592017-07 2686.9821642017-08 2524.4620772017-09 2904.0854872017-10 2976.9995502017-11 3263.3740432017-12 3317.107474# 保利地产：2017-01 1089.0082862017-02 1120.0233502017-03 1145.7316402017-04 1153.7604352017-05 1108.2306092017-06 1157.2760442017-07 1244.9669052017-08 1203.5802092017-09 1290.7066062017-10 1244.4387562017-11 1336.6619162017-12 1531.1506163.2. 绘制二元折线图利用上面的Series数据就可以作图了。12345678910111213141516171819202122232425262728# 设置绘图风格plt.style.use(&apos;ggplot&apos;)fig = plt.figure(figsize = (10,6))colors1 = &apos;#6D6D6D&apos; #标题颜色# data1万科，data2保利data1 = lst[0]data2 = lst[1]# 绘制第一条折线图data1.plot(color = &apos;#C42022&apos;, #折线图颜色marker = &apos;o&apos;,markersize = 4, #标记形状、大小设置label = &apos;万科&apos;)# 绘制第二条折线图data2.plot(color = &apos;#4191C0&apos;, #折线图颜色marker = &apos;o&apos;,markersize = 4, #标记形状、大小设置label = &apos;保利&apos;)# 还可以绘制更多条# 设置标题及横纵坐标轴标题plt.title(&apos;2017年万科与保利地产市值对比&apos;,color = colors1,fontsize = 18)plt.xlabel(&apos;月份&apos;)plt.ylabel(&apos;市值(亿元)&apos;)plt.savefig(&apos;stock1.png&apos;,bbox_inches = &apos;tight&apos;,dpi = 300)plt.legend() # 显示图例plt.show()绘图结果如下：如果想添加数值标签，则可以使用下面的代码：1234567891011121314151617181920212223242526272829303132333435363738# 绘制第一条折线图# 创建x,y轴标签x = np.arange(0,len(data1),1)ax1.plot(x,data1.values, #x、y坐标color = '#C42022', #折线图颜色红色marker = 'o',markersize = 4, #标记形状、大小设置label = '万科')ax1.set_xticks(x) # 设置x轴标签ax1.set_xticklabels(data1.index) # 设置x轴标签值# plt.xticks(rotation=90)for x,y in zip(x,data1.values): plt.text(x,y + 10,'%.0f' %y,ha = 'center',color = colors1,fontsize = 10 ) # '%.0f' %y 设置标签格式不带小数# 绘制第二条折线图x = np.arange(0,len(data2),1)ax1.plot(x,data2.values, #x、y坐标color = '#4191C0', #折线图颜色蓝色marker = 'o',markersize = 4, #标记形状、大小设置label = '保利')ax1.set_xticks(x) # 设置x轴标签ax1.set_xticklabels(data2.index) # 设置x轴标签值# plt.xticks(rotation=90)for x,y in zip(x,data2.values): plt.text(x,y + 10,'%.0f' %y,ha = 'center',color = colors1,fontsize = 10 ) # '%.0f' %y 设置标签格式不带小数# 设置标题及横纵坐标轴标题plt.title('2017年万科与保利地产市值对比',color = colors1,fontsize = 18)plt.xlabel('月份')plt.ylabel('市值(亿元)')plt.savefig('stock1.png',bbox_inches = 'tight',dpi = 300)plt.legend() # 显示图例plt.show()结果如下图所示：可以看到，两只股票市值从2017年初开始一直在上涨，万科的市值是保利的2倍左右。本文仅简单提取了两只股票的市值数据，只要你愿意，3000多只股票的数据都可以拿来绘图。文章代码及素材可在下面链接中获得：https://github.com/makcyun/web_scraping_with_python/tree/master/date_plot本文完。]]></content>
      <categories>
        <category>Python可视化</category>
      </categories>
      <tags>
        <tag>Python可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据处理分析(1)：日期型数据处理]]></title>
    <url>%2F2018%2F09%2F11%2Fpython_data_analysis%26mining01.html</url>
    <content type="text"><![CDATA[以调用大名鼎鼎的Tushare包案例，介绍日期数据的处理。摘要： Python数据处理分析中，日期型数据的处理是相对复杂且非常重要的一环。本文以调用Tushare包获得股票的各种信息数据为案例，介绍日期数据的处理。之前的一篇文章用爬虫实现了上市公司信息的抓取。但还有更简单的方法，就是调用Tushare包，可以很便捷地拿到干净的各种股市数据。强烈推荐一下这款由国内团队开发的包，Github上目前Star数 6000+。Tushare是一个开源免费、强大的python金融财经数据接口包。调用该包返回的数据格式基本是Pandas DataFrame类型，非常便于后续处理分析。包的数据来源于新浪财经、腾讯财经、上交所和深交所，比较齐全，质量也很可靠。参考：https://tushare.pro/document/2https://github.com/waditu/Tushare下面我们就来简单体检一下这款包的便利，然后利用它返回的数据处理其中的日期型数据。1. 获取数据接口使用前提：首先在官网注册成功后获得token，然后通过下面命令下载Tushare包，然后在程序中调用就可以使用了。1pip instasll tushare可以获得的信息接口非常多，包括：行情数据、基础数据、财务数据板块等。下面就简单使用下部分接口。首先，获取国内股票列表数据。123456import tushare as tsts.set_token('你的token')pro = ts.pro_api()data = pro.stock_basic(exchange_id='', is_hs='', fields='symbol,name,is_hs,list_date,list_status')print(data)# ''表示获取全部exchange_id表示股票代码，可以获取特定股票的基础信息，为空则获取全部;is_hs表示是否沪深港通，为空表示提取所有股市；fields表示想要提取的信息列表。结果如下：1234567891011121314151617181920212223 ts_code symbol name list_status list_date is_hs0 000001.SZ 000001 平安银行 L 19910403 S1 000002.SZ 000002 万科A L 19910129 S2 000004.SZ 000004 国农科技 L 19910114 N3 000005.SZ 000005 世纪星源 L 19901210 N4 000006.SZ 000006 深振业A L 19920427 S5 000007.SZ 000007 全新好 L 19920413 N6 000008.SZ 000008 神州高铁 L 19920507 S7 000009.SZ 000009 中国宝安 L 19910625 S8 000010.SZ 000010 美丽生态 L 19951027 N9 000011.SZ 000011 深物业A L 19920330 S10 000012.SZ 000012 南玻A L 19920228 S···3532 603987.SH 603987 康德莱 L 20161121 N3533 603988.SH 603988 中电电机 L 20141104 N3534 603989.SH 603989 艾华集团 L 20150515 H3535 603990.SH 603990 麦迪科技 L 20161208 N3536 603991.SH 603991 至正股份 L 20170308 N3537 603993.SH 603993 洛阳钼业 L 20121009 H3538 603996.SH 603996 中新科技 L 20151222 N3539 603997.SH 603997 继峰股份 L 20150302 H3540 603998.SH 603998 方盛制药 L 20141205 N3541 603999.SH 603999 读者传媒 L 20151210 N很轻松地就能获得3542家上市公司的基本情况。下面就将这个数据作为日期型处理的基础数据。2. 日期型数据处理查看一下数据结构：123456789RangeIndex: 3542 entries, 0 to 3541Data columns (total 6 columns):ts_code 3542 non-null objectsymbol 3542 non-null objectname 3542 non-null objectlist_status 3542 non-null objectlist_date 3542 non-null objectis_hs 3542 non-null objectdtypes: object(6)所有列都是object字符型。这里想对日期做数据分析，比如可以统计一下历年上市公司数量。需更改日期型数据字符型为日期型。1data['list_date'] = pd.to_datetime(data['list_date'])pd.to_datetime将’list_date’列格式改为datetime格式，再来看一下：123456789RangeIndex: 3542 entries, 0 to 3541Data columns (total 6 columns):ts_code 3542 non-null objectsymbol 3542 non-null objectname 3542 non-null objectlist_status 3542 non-null objectlist_date 3542 non-null datetime64[ns]is_hs 3542 non-null objectdtypes: object(6)2.1. 按日期切片筛选数据有时候我们需要按年、季度、月、日这样的日期格式来筛选提取相应的数据。2.1.1. 按年度获取单一年份数据，比如2017年1234567891011data = data.set_index(data['list_date'])data = data['2017']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2017-12-25 001965.SZ 001965 招商公路 L 2017-12-25 S2017-03-24 002774.SZ 002774 快意电梯 L 2017-03-24 N2017-01-12 002824.SZ 002824 和胜股份 L 2017-01-12 N2017-01-06 002838.SZ 002838 道恩股份 L 2017-01-06 N2017-01-24 002839.SZ 002839 张家港行 L 2017-01-24 S获取多个年份，比如2015-201712345678910data = data['2015':'2017']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2015-01-26 000166.SZ 000166 申万宏源 L 2015-01-26 S2017-12-25 001965.SZ 001965 招商公路 L 2017-12-25 S2015-12-30 001979.SZ 001979 招商蛇口 L 2015-12-30 S2015-01-27 002734.SZ 002734 利民股份 L 2015-01-27 N2015-01-22 002739.SZ 002739 万达电影 L 2015-01-22 S2.1.2. 按月度12345678910data = data['2017-1']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2017-01-12 002824.SZ 002824 和胜股份 L 2017-01-12 N2017-01-06 002838.SZ 002838 道恩股份 L 2017-01-06 N2017-01-24 002839.SZ 002839 张家港行 L 2017-01-24 S2017-01-10 002840.SZ 002840 华统股份 L 2017-01-10 N2017-01-19 002841.SZ 002841 视源股份 L 2017-01-19 S2.1.3. 按具体天123456789data = data['2017-1-12']print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 2017-01-12 002824.SZ 002824 和胜股份 L 2017-01-12 N2017-01-12 300584.SZ 300584 海辰药业 L 2017-01-12 N2017-01-12 603628.SH 603628 清源股份 L 2017-01-12 H2017-01-12 603639.SH 603639 海利尔 L 2017-01-12 H2.2. to_period按日期显示数据dataframe.to_period方法只是用于显示数据，但不会进行统计。2.2.1. 按年度12345678910data = data.to_period('A') # 'A'默认是从'A-DEC'开始算,也可以根据情况设置为'A-JAN'print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 1991 000001.SZ 000001 平安银行 L 1991-04-03 S1991 000002.SZ 000002 万科A L 1991-01-29 S1991 000004.SZ 000004 国农科技 L 1991-01-14 N1990 000005.SZ 000005 世纪星源 L 1990-12-10 N1992 000006.SZ 000006 深振业A L 1992-04-27 S可以看到，相比上面筛选数据时是按原始的日期，这里利用to_period方法，设置参数为’A’后，可以直接显示为年，这在后期可视化绘图时非常有用。2.2.2. 按季度12345678910data = data.to_period('Q') # 'Q'默认是从'Q-DEC'开始算,也可以根据情况设置为“Q-SEP”，“Q-FEB”等print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 1991Q2 000001.SZ 000001 平安银行 L 1991-04-03 S1991Q1 000002.SZ 000002 万科A L 1991-01-29 S1991Q1 000004.SZ 000004 国农科技 L 1991-01-14 N1990Q4 000005.SZ 000005 世纪星源 L 1990-12-10 N1992Q2 000006.SZ 000006 深振业A L 1992-04-27 S2.2.3. 按月度12345678910data = data.to_period('M')print(data.head())# 结果 ts_code symbol name list_status list_date is_hslist_date 1991-04 000001.SZ 000001 平安银行 L 1991-04-03 S1991-01 000002.SZ 000002 万科A L 1991-01-29 S1991-01 000004.SZ 000004 国农科技 L 1991-01-14 N1990-12 000005.SZ 000005 世纪星源 L 1990-12-10 N1992-04 000006.SZ 000006 深振业A L 1992-04-27 S2.3. resample按日期统计数据按日期进行统计数据，可以利用resample方法。2.3.1. 按年度123456789data = data.resample('AS').count()['name'] # count对各年上市公司数量进行计数print(data.head())# 结果list_date1990-01-01 71991-01-01 41992-01-01 371993-01-01 1061994-01-01 992.3.2. 按季度123456789data = data.resample('Q').count()['name'] print(data.head())# 结果list_date1990-12-31 71991-03-31 21991-06-30 21991-09-30 01991-12-31 02.3.3. 按月度123456789data = data.resample('M').count()['name'] print(data.head())# 结果list_date1990-12-31 71991-01-31 21991-02-28 01991-03-31 01991-04-30 12.4. 统计和显示结合利用前面的resample和to.period方法，可以按年、季度、月份汇总数据。123456# 汇总各年上市公司数量data = data.set_index(['list_date'])data = data.resample('AS').count()['ts_code']data = data.to_period('A')print(data.head())print(data.tail())结果如下：1234567891011121314list_date1990 71991 41992 371993 1061994 99Freq: A-DEC, Name: name, dtype: int64list_date2014 1242015 2232016 2272017 4382018 78Freq: A-DEC, Name: name, dtype: int64基于上述数据，可以利用matplotlib绘制出历年上市公司数量的折线图：折线图的具体绘制方法，见后续文章。以上就是简单利用了Tushare的一个接口返回的数据，介绍了日期型数据的转换和处理。本文完。]]></content>
      <categories>
        <category>Python数据清洗处理</category>
      </categories>
      <tags>
        <tag>Python数据分析</tag>
        <tag>Python包</tag>
        <tag>Tushare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每周分享第 1 期：关于 PDF 阅读处理软件，你需要的都在这里了]]></title>
    <url>%2F2018%2F09%2F08%2Ffuli01.html</url>
    <content type="text"><![CDATA[分享几款佳软。从今天起，我将开一个新的栏目「每周分享」，顾名思义，就是会在每个周末分享一篇文章。内容涵盖范围会比较广，主要集中在软件、App、PPT、摄影Ps等几个方面。之后，你可以在公众号里面的 「不务正业」菜单里集中查看。插一句，另一个「不误正业」菜单会是以 Python 为主的编程文章。为什么会想起开这个栏目呢，主要有两个原因。第一，我的公众号前不久更名为「第2大脑」，是因为我想在这里分享一些一直存在我大脑里的好用的软件、App等黑科技玩意儿。一直以来，我对软件、App这块儿保有很强的好奇心，总喜欢尝试些新鲜有趣或者能提高效率的东西。时间长了，发现积累了蛮多，但是都散乱地分布在电脑、手机或者其他地方，而没有好好地进行总结，也没拿出来分享过。本着开源精神，接下来我想陆陆续续把一些可能会对你用的东西，分享出来。第二，大多数 Python 类公众号的内容基本上都是跟 Python 相关的，我觉得太单调。虽然「人生苦短，快用 Python」没错，但是同时关注一些别的，既不枯燥也能让生活更加有意思，是不是。好，说完了原因，那么下面就直奔主题吧。第 1 期，就先分享几款「PDF 软件」。为什么呢？ 因为，之前遇到过很多人在处理和 PDF 文件相关的问题时，总是很头疼，到处求助软件。这里我就分享几款，个人觉得非常好用的 PDF 软件，几乎能满足你对 PDF 所有的需求。有了它们，之后再遇到格式转换、提取分割、复制、压缩等问题时，很轻松就能解决了。先推荐一款难得的国产良心软件。1. Foxit Reader从网站全英文 Style 就可以看出这款国产软件是走国际路线的。口碑非常好，足以媲美大名鼎鼎的 Adobe Acrobat，但体积要小地多。这款软件的功能主要是浏览 PDF ，同时打开几十个 PDF 文件也不会卡。除了正常浏览 PDF以外，更多的需求是对 PDF进行处理，最常见的就是 PDF 转换为 Word/Excel/PPT。2. Solid PDF Converter在 PDF文件是可以 复制 的前提下，这款软件可以说是 PDF 转换为 Office 格式最好用的软件。价格不便宜，官方售价 100 美元。当 PDF 文件不能复制（扫描件或者图片）时，上面的软件就无能为力了，这时就需要下面这款大名鼎鼎的 OCR （文字识别）软件了。3. 最好用的OCR文字识别软件它到底有多好，一句话就够了：知乎上 PDF 话题中排名第一的软件。当你需要从 PDF 中复制，但却无法选择时，这款软件能帮你搞定一切。另外，手机拍的文字照片，也能转成 Word。这么好用，自然价格也不便宜，官方售价 500 RMB。4. Small PDF有时，我们得到的 PDF 文件体积很大，打开或者传输给别人很不方便。那么就需要进行压缩了，压缩效果最好的软件就是这款 Small PDF了。 它的网页版是我们经常在搜索时会遇到的。除此之外，它还有很多其他非常实用的功能。5. PDFdo除上面的之外，还会有几类需求，比如：只需要 PDF 文件中的某一部分，那么需要从 PDF 中进行提取或者分割；要将多个 PDF 合并成一个文件，那么需要合并 PDF；要将好几个 Word 文件同时转换为 PDF，那么有批量转换就最好了。如果有上面的需求，那么这款软件是最佳选择，速度非常快。官方售价 99 RMB。以上这 5 款软件就足以应对和 PDF 相关的大部分问题了，但他们都是电脑软件。这里再推荐一款手机上的软件。6. WPS Office别看到是「WPS」就摇头，手机版的和电脑版的 WPS 可谓 「天壤之别」。除了阅读以外，还拥有：转换为 Word、提取 PDF、合并 PDF、转为图片、拍照扫描等几项核心功能。这款软件有免费版，不过高级版是需要付费的。以上就是我一直在用的几款 PDF 软件，推荐给你。有条件请支持正版，没有条件，就百度/谷歌/某宝搜索吧，实在找不到可以给我留言。本文完。欢迎长按识别关注我的公众号]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>pdf</tag>
        <tag>佳软</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pyhton可视化(1): 20秒纵览十年中国大学排行榜变化]]></title>
    <url>%2F2018%2F09%2F05%2FPython_visualization01.html</url>
    <content type="text"><![CDATA[Python爬取近十年中国大学Top20强并结合D3.js做动态数据可视化表。摘要：：最近在朋友圈看到一个很酷炫的动态数据可视化表，介绍了新中国成立后各省GDP的发展历程，非常惊叹竟然还有这种操作，也想试试。于是，照葫芦画瓢虎，在网上爬取了历年中国大学学术排行榜，制作了一个中国大学排名Top20强动态表。1. 作品介绍这里先放一下这个动态表是什么样的：https://www.bilibili.com/video/av24503002不知道你看完是什么感觉，至少我是挺震惊的，想看看作者是怎么做出来的，于是追到了作者的B站主页，发现了更多有意思的动态视频：这些作品的作者是：@Jannchie见齐，他的主页：https://space.bilibili.com/1850091/#/video这些会动的图表是如何做出来的呢？他用到的是一个动态图形显示数据的JavaScript库：D3.js，一种前端技术。难怪不是一般地酷炫。那么，如果不会D3.js是不是就做不出来了呢？当然不是，Jannchie非常Open地给出了一个手把手简单教程：https://www.bilibili.com/video/av28087807他同时还开放了程序源码，你只需要做2步就能够实现：到他的Github主页下载源码到本地电脑：https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js打开dist文件夹里面的exampe.csv文件，放进你想要展示的数据，再用浏览器打开bargraph.html网页，就可以实现动态效果了。下面，我们稍微再说详细一点，实现这种效果的关键点。最重要的是要有数据。观察一下上面的作品可以看到，横向柱状图中的数据要满足两个条件：一是要有多个对比的对象，二是要在时间上连续。这样才可以做出动态效果来。看完后我立马就有了一个想法：想看看近十年中国的各个大学排名是个什么情况。下面我们就通过实实例来操作下。2. 案例操作：中国大学Top20强2.1. 数据来源世界上最权威的大学排名有4类，分别是：原上海交通大学的ARWUhttp://www.shanghairanking.com/ARWU2018.html英国教育组织的QShttps://www.topuniversities.com/university-rankings/world-university-rankings/2018泰晤士的THEhttps://www.timeshighereducation.com/world-university-rankings美国的usnewshttps://www.usnews.com/best-colleges/rankings关于，这四类排名的更多介绍，可以看这个：https://www.zhihu.com/question/20825030/answer/71336291这里，我们选取相对比较权威也比较符合国情的第一个ARWU的排名结果。打开官网，可以看到有英文版和中文版排名，这里选取中文版。排名非常齐全，从2003年到最新的2018年都有，非常好。同时，可以看到这是世界500强的大学排名，而我们需要的是中国（包括港澳台）的大学排名。怎么办呢？ 当然不能一年年地复制然后再从500条数据里一条条筛选出中国的，这里就要用爬虫来实现了。可以参考不久前的一篇爬取表格的文章：https://www.makcyun.top/web_scraping_withpython2.html2.2. 抓取数据2.2.1. 分析url首先，分析一下URL:1234http://www.zuihaodaxue.com/ARWU2018.htmlhttp://www.zuihaodaxue.com/ARWU2017.html...http://www.zuihaodaxue.com/ARWU2009.html可以看到，url非常有规律，只有年份数字在变，很简单就能构造出for循环。格式如下：1url = 'http://www.zuihaodaxue.com/ARWU%s.html' % (str(year))下面就可以开始写爬虫了。2.2.2. 获取网页内容1234567891011import requeststry: url = 'http://www.zuihaodaxue.com/ARWU%s.html' % (str(year)) response = requests.get(url,headers = headers) # 2009-2015用'gbk'，2016-2018用'utf-8' if response.status_code == 200: # return response.text # text会乱码，content没有问题 return response.content return Noneexcept RequestException:print('爬取失败')上面需要注意的是，不同年份网页采用的编码不同，返回response.test会乱码，返回response.content则不会。关于编码乱码的问题，以后单独写一篇文章。2.2.3. 解析表格用read_html函数一行代码来抓取表格，然后输出：12tb = pd.read_html(html)[0]print(tb)可以看到，很顺利地表格就被抓取了下来：但是表格需要进行处理，比如删除掉不需要的评分列，增加年份列等，代码实现如下：1234567891011121314tb = pd.read_html(html)[0]# 重命名表格列，不需要的列用数字表示tb.columns = ['world rank','university', 2,3, 'score',5,6,7,8,9,10]tb.drop([2,3,5,6,7,8,9,10],axis = 1,inplace = True)# 删除后面不需要的评分列# rank列100名后是区间，需需唯一化，增加一列index作为排名tb['index_rank'] = tb.indextb['index_rank'] = tb['index_rank'].astype(int) + 1# 增加一列年份列tb['year'] = i# read_html没有爬取country，需定义函数单独爬取tb['country'] = get_country(html)return tb需要注意的是，国家没有被抓取下来，因为国家是用的图片表示的，定位到国家代码位置：可以看到美国是用英文的USA表示的，那么我们可以单独提取出src属性，然后用正则提取出国家名称就可以了，代码实现如下：1234567891011# 提取国家名称def get_country(html): soup = BeautifulSoup(html,'lxml') countries = soup.select('td &gt; a &gt; img') lst = [] for i in countries: src = i['src'] pattern = re.compile('flag.*\/(.*?).png') country = re.findall(pattern,src)[0] lst.append(country) return lst然后，我们就可以输出一下结果：12345678910111213141516171819202122 world rank university score index_rank year country0 1 哈佛大学 100.0 1 2018 USA1 2 斯坦福大学 75.6 2 2018 USA2 3 剑桥大学 71.8 3 2018 UK3 4 麻省理工学院 69.9 4 2018 USA4 5 加州大学-伯克利 68.3 5 2018 USA5 6 普林斯顿大学 61.0 6 2018 USA6 7 牛津大学 60.0 7 2018 UK7 8 哥伦比亚大学 58.2 8 2018 USA8 9 加州理工学院 57.4 9 2018 USA9 10 芝加哥大学 55.5 10 2018 USA10 11 加州大学-洛杉矶 51.2 11 2018 USA11 12 康奈尔大学 50.7 12 2018 USA12 12 耶鲁大学 50.7 13 2018 USA13 14 华盛顿大学-西雅图 50.0 14 2018 USA14 15 加州大学-圣地亚哥 47.8 15 2018 USA15 16 宾夕法尼亚大学 46.4 16 2018 USA16 17 伦敦大学学院 46.1 17 2018 UK17 18 约翰霍普金斯大学 45.4 18 2018 USA18 19 苏黎世联邦理工学院 43.9 19 2018 Switzerland19 20 华盛顿大学-圣路易斯 42.1 20 2018 USA20 21 加州大学-旧金山 41.9 21 2018 USA数据很完美，接下来就可以按照D3.js模板中的example.csv文件的格式作进一步的处理了。2.3. 数据处理这里先将数据输出为university.csv文件，结果见下表：10年一共5011行×6列数据。接着，读入该表作进一步数据处理，代码如下：123456789101112131415161718192021222324252627df = pd.read_csv('university.csv')# 包含港澳台# df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]# 只包括内地df = df.query("(country == 'China')")df['index_rank_score'] = df['index_rank']# 将index_rank列转为整形df['index_rank'] = df['index_rank'].astype(int)# 美国# df = df.query("(country == 'UnitedStates')|(country == 'USA')") #求topn名def topn(df): top = df.sort_values(['year','index_rank'],ascending = True) return top[:20].reset_index()df = df.groupby(by =['year']).apply(topn)# 更改列顺序df = df[['university','index_rank_score','index_rank','year']]# 重命名列df.rename (columns = &#123;'university':'name','index_rank_score':'type','index_rank':'value','year':'date'&#125;,inplace = True)# 输出结果df.to_csv('university_ranking.csv',mode ='w',encoding='utf_8_sig', header=True, index=False)# index可以设置上面需要注意两点：可以提取包含港澳台在内的大中华区所有的大学，也可以只提取内地的大学，还可以提取世界、美国等各种排名。定义了一个求Topn的函数，能够按年份分别求出各年的前20名大学名单。打开输出的university_ranking.csv文件：结果非常好，可以直接作为D3.js的导入文件了。2.3.1. 完整代码将代码再稍微完善一下，完整地代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import pandas as pdimport csvimport requestsfrom requests.exceptions import RequestExceptionfrom bs4 import BeautifulSoupimport timeimport restart_time = time.time() #计算程序运行时间# 获取网页内容def get_one_page(year): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; # 英文版 # url = 'http://www.shanghairanking.com/ARWU%s.html' % (str(year)) # 中文版 url = 'http://www.zuihaodaxue.com/ARWU%s.html' % (str(year)) response = requests.get(url,headers = headers) # 2009-2015用'gbk'，2016-2018用'utf-8' if response.status_code == 200: # return response.text # text会乱码，content没有问题 # https://stackoverflow.com/questions/17011357/what-is-the-difference-between-content-and-text return response.content return None except RequestException: print('爬取失败')# 解析表格def parse_one_page(html,i): tb = pd.read_html(html)[0] # 重命名表格列，不需要的列用数字表示 tb.columns = ['world rank','university', 2,3, 'score',5,6,7,8,9,10] tb.drop([2,3,5,6,7,8,9,10],axis = 1,inplace = True) # 删除后面不需要的评分列 # rank列100名后是区间，需需唯一化，增加一列index作为排名 tb['index_rank'] = tb.index tb['index_rank'] = tb['index_rank'].astype(int) + 1 # 增加一列年份列 tb['year'] = i # read_html没有爬取country，需定义函数单独爬取 tb['country'] = get_country(html) # print(tb) # 测试表格ok return tb # print(tb.info()) # 查看表信息 # print(tb.columns.values) # 查看列表名称# 提取国家名称def get_country(html): soup = BeautifulSoup(html,'lxml') countries = soup.select('td &gt; a &gt; img') lst = [] for i in countries: src = i['src'] pattern = re.compile('flag.*\/(.*?).png') country = re.findall(pattern,src)[0] lst.append(country) return lst # print(lst) # 测试提取国家是否成功ok# 保存表格为csvdef save_csv(tb): tb.to_csv(r'university.csv', mode='a', encoding='utf_8_sig', header=True, index=0) endtime = time.time()-start_time # print('程序运行了%.2f秒' %endtime)def analysis(): df = pd.read_csv('university.csv') # 包含港澳台 # df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']] # 只包括内地 df = df.query("(country == 'China')")[['university','year','index_rank']] df['index_rank_score'] = df['index_rank'] # 将index_rank列转为整形 df['index_rank'] = df['index_rank'].astype(int) # 美国 # df = df.query("(country == 'UnitedStates')|(country == 'USA')")[['university','year','index_rank']] #求topn名 def topn(df): top = df.sort_values(['year','index_rank'],ascending = True) return top[:20].reset_index() df = df.groupby(by =['year']).apply(topn) # 更改列顺序 df = df[['university','index_rank_score','index_rank','year']] # 重命名列 df.rename (columns = &#123;'university':'name','index_rank_score':'type','index_rank':'value','year':'date'&#125;,inplace = True) # 输出结果 df.to_csv('university_ranking.csv',mode ='w',encoding='utf_8_sig', header=True, index=False) # index可以设置def main(year): # generate_mysql() for i in range(2009,year): #抓取10年 # get_one_page(i) html = get_one_page(i) # parse_one_page(html,i) # 测试表格ok tb = parse_one_page(html,i) save_csv(tb) print(i,'年排名提取完成完成') analysis()# # 单进程if __name__ == '__main__': main(2019) # 2016-2018采用gb2312编码，2009-2015采用utf-8编码至此，我们已经有university_ranking.csv基础数据，下面就可以进行可视化呈现了。2.4. 可视化呈现首先，到见齐的github主页：https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js2.4.1. 克隆仓库文件如果你平常使用github或者Git软件的话，那么就找个合适文件存放目录，然后直接在 GitBash里分别输入下面3条命令就搭建好环境了：123456# 克隆项目仓库git clone https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js# 切换到项目根目录cd Historical-ranking-data-visualization-based-on-d3.js# 安装依赖npm install如果你此前没有用过上面的软件，你可以直接点击Download Zip下载下来然后解压即可，不过还是强烈建议使用第一种方法，因为后面如果要自定义可视化效果的话，需要修改代码然后执行npm run build命令才能够看到效果。2.4.2. 效果呈现好，所有基本准备都已完成，下面就可以试试看效果了。任意浏览器打开bargraph.html网页，点击选择文件，然后选择：前面输出的university_ranking.csv文件，看下效果吧：https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0可以看到，有了大致的可视化效果，但还存在很多瑕疵，比如：表顺序颠倒了、字体不合适、配色太花哨等。可不可以修改呢？当然是可以的，只需要分别修改文件夹中这几个文件的参数就可以了：config.js 全局设置各项功能的开关，比如配色、字体、文字名称、反转图表等等功能；color.css 修改柱形图的配色；stylesheet.css 具体修改配色、字体、文字名称等的css样式；visual.js 更进一步的修改，比如图表的透明度等。知道在哪里修改了以后，那么，如何修改呢？很简单，只需要简单的几步就可以实现：打开网页，右键-检查，箭头指向想要修改的元素，然后在右侧的css样式表里，双击各项参数修改参数，修改完元素就会发生变化，可以不断微调，直至满意为止。把参数复制到四个文件中对应的文件里并保存。Git Bash不断重复运行npm run build，之后刷新网页就可以看到优化后的效果。最后，再添加一个合适的BGM就可以了。以下是我优化之后的效果：https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0BGM：ツナ覚醒如果你不太会调整，没有关系，我会分享优化后的配置文件。以上，就是实现动态可视化表的步骤。 同样地，只要更改数据源可以很方便地做出世界、美国等大学的动态效果，可以看看：中国（含港澳台）大学排名：http://media2.makcyun.top/Greater_China_uni_ranking.mp4美国大学排名：http://media2.makcyun.top/USA_uni_ranking.mp4文章所有的素材，在公众号后台回复大学排名就可以得到，或者到我的github下载：https://github.com/makcyun/web_scraping_with_python感兴趣的话就动手试试吧。本文完。]]></content>
      <categories>
        <category>Python可视化</category>
      </categories>
      <tags>
        <tag>Python可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(4)：图片批量下载-以澎湃网信息图为例]]></title>
    <url>%2F2018%2F09%2F02%2Fweb_scraping_withpython4.html</url>
    <content type="text"><![CDATA[澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。摘要： 上一篇文章介绍了单页图片的爬取，但是当爬取多页时，难度会增加。同时，前几篇爬虫文章中的网站有一个明显的特点是：可以通过点击鼠标实现网页的翻页，并且url会发生相应的变化。除了此类网站以外，还有一类非常常见的网站特点是：没有”下一页”这样的按钮，而是”加载更多”或者会不断自动刷新从而呈现出更多的内容，同时网页url也不发生变化。这种类型的网页通常采用的是Ajax技术，要抓取其中的网页内容需要采取一定的技巧。本文以信息图做得非常棒的澎湃”美数课”为例，抓取该栏目至今所有文章的图片。栏目网址：https://www.thepaper.cn/list_25635本文知识点：Ajax知识多页图片爬取1. Ajax知识在该主页上尝试不断下拉，会发现网页不断地加载出新的文章内容来，而并不需要通过点击”下一页”来实现，而且网址url也保持不变。也就是说在同一个网页中通过下拉源源不断地刷新出了网页内容。这种形式的网页在今天非常常见，它们普遍是采用了Ajax技术。Ajax 全称是 Asynchronous JavaScript and XML（异步 JavaScript 和 XML）。它不是一门编程语言，而是利用 JavaScript 在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。Ajax更多参考：https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html采用了Ajax的网页和普通的网页有一定的区别，普通网页的爬虫代码放在这种类型的网页上就行不通了，必须另辟出路。下面我们就来尝试一下如何爬取网易”数读”所有的文章。主页右键-检查，然后按f5刷新，会弹出很多链接文件。鼠标上拉回到第一个文件：list_25635，在右侧按ctrl+f搜索一下第一篇文章的标题：”娃娃机生意经”，可以看到在html网页中找到了对应的源代码。接着，我们拖动下拉鼠标，显示出更多文章。然后再次搜索一篇文章的标题：”金砖峰会”，会发现搜不到相应的内容了。是不是感觉很奇怪？其实，这里就是用了Ajax的技术，和普通网页翻页是刷新整个网页不同，这种类型网页可以再保持url不变的前提下只刷新部分内容。这就为我们进行爬虫带来了麻烦。因为，我们通过解析网页的url：https://www.thepaper.cn/list_25635只能爬取前面部分的内容而后面通过下拉刷新出来的内容是爬取不到的。这显然不完美，那么怎么才能够爬取到后面不断刷新出来的网页内容呢？2. url分析我们把右侧的选项卡从ALL切换到Network，然后按再次按f5刷新，可以发现Name列有4个结果。选择第3个链接打开并点击Response，通过滑动可以看到一些文本内容和网页中的文章标题是一一对应的。比如第一个是：娃娃机生意经｜有没有好奇过抓娃娃机怎么又重新火起来了？，一直往下拖拽可以看到有很多篇文章。此时，再切换到headers选项卡，复制Request URL后面的链接并打开，会显示一部分文章的标题和图片内容。数一下的话，可以发现一共有20个文章标题，也就是对应着20篇文章。这个链接其实和上面的list_25635链接的内容是一致的。这样看来，好像发现不了什么东西，不过不要着急。接下来，回到Name列，尝试滚动下拉鼠标，会发现弹出好几个新的开头为load_index的链接来。选中第一个load_index的链接，点击Response查看一下html源代码，尝试在网页中搜索一下：十年金砖峰这个文章的标题，惊奇地发现，在网页中找到了对于的文章标题。而前面，我们搜索这个词时，是没有搜索到的。这说明了什么呢？说明十年金砖峰这篇文章的内容不在第一个list_25635链接中，而在这个load_index的链接里。鼠标点击headers，复制Request URL后面的链接并打开，就可以再次看到包括这篇文章在内的新的20篇文章。是不是发现了点了什么？接着，我们继续下拉，会发现弹出更多的load_index的链接。再搜索一个标题：地图湃｜海外港口热，可以发现在网页中也同样找到了文章标题。回到我们的初衷：下载所有网页的图片内容。那么现在就有解决办法礼：一个个地把出现的这些url网址中图片下载下来就大功告成了。好，我们先来分析一下这些url，看看有没有相似性，如果有很明显的相似性，那么就可以像普通网页那样，通过构造翻页页数的url，实现for循环就可以批量下载所有网页的图片了。复制前3个链接如下：123https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=2&amp;isList=true&amp;lastTime=1533169319712 https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=3&amp;isList=true&amp;lastTime=1528625875167 https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=4&amp;isList=true&amp;lastTime=1525499007926发现pageidx键的值呈现规律的数字递增变化，看起来是个好消息。但同时发现后面的lastTime键的值看起来是随机变化的，这个有没有影响呢？ 来测试一下，复制第一个链接，删掉&amp;lastTime=1533169319712这一串字符，会发现网页一样能够正常打开，就说明着一对参数不影响网页内容，那就太好了。我们可以删除掉，这样所有url的区别只剩pageidx的值了，这时就可以构造url来实现for循环了。构造的url形式如下：123https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=2https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=3https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=4同时，尝试把数字2改成1并打开链接看看会有什么变化，发现呈现的内容就是第1页的内容。这样，我们就可以从第一页开始构造url循环了。https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=1既然确定了首页，那么也要相应地确定一下尾页。很简单，我们把数字改大然后打开链接看是否有内容即可。比如改为10 ，打开发现有内容显示，很好。接着，再改为30，发现没有内容了。说明该栏目的页数介于这两个数之间，尝试几次后，发现25是最后一个有内容的网页，也意味着能够爬取的页数一共是25页。确定了首页和尾页后，下面我们就可以开始构造链接，先爬取第一篇文章网页里的图片（这个爬取过程，我们上一篇爬取网易”数读”已经尝试过了），然后爬取这一整页的图片，最后循环25页，爬取所有图片，下面开始吧。3. 程序代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130import requestsfrom bs4 import BeautifulSoupimport reimport osfrom hashlib import md5from requests.exceptions import RequestExceptionfrom multiprocessing import Poolfrom urllib.parse import urlencodeheaders = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125;# 1 获取索引界面网页内容def get_page_index(i): # 下载1页 # url = 'https://www.thepaper.cn/newsDetail_forward_2370041' # 2下载多页，构造url paras = &#123; 'nodeids': 25635, 'pageidx': i &#125; url = 'https://www.thepaper.cn/load_index.jsp?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text # print(response.text) # 测试网页内容是否提取成功ok# 2 解析索引界面网页内容def parse_page_index(html): soup = BeautifulSoup(html,'lxml') # 获取每页文章数 num = soup.find_all(name = 'div',class_='news_li') for i in range(len(num)): yield&#123; # 获取title 'title':soup.select('h2 a')[i].get_text(), # 获取图片url，需加前缀 'url':'https://www.thepaper.cn/' + soup.select('h2 a')[i].attrs['href'] # print(url) # 测试图片链接 &#125;# 3 获取每条文章的详情页内容def get_page_detail(item): url = item.get('url') # 增加异常捕获语句 try: response = requests.get(url,headers = headers) if response.status_code == 200: return response.text # print(response.text) # 测试网页内容是否提取成功 except RequestException: print('网页请求失败') return None # 4 解析每条文章的详情页内容def parse_page_detail(html): soup = BeautifulSoup(html,'lxml') # 获取title if soup.h1: #有的网页没有h1节点，因此必须要增加判断，否则会报错 title = soup.h1.string # 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一 items = soup.find_all(name='img',width =['100%','600']) # 有的图片节点用width='100%'表示，有的用600表示，因此用list合并选择 # https://blog.csdn.net/w_xuechun/article/details/76093950 # print(items) # 测试返回的img节点ok for i in range(len(items)): pic = items[i].attrs['src'] # print(pic) #测试图片链接ok yield&#123; 'title':title, 'pic':pic, 'num':i # 图片添加编号顺序 &#125;# 5 下载图片def save_pic(pic): title = pic.get('title') # 标题规范命名：去掉符号非法字符| 等 title = re.sub('[\/:*?"&lt;&gt;|]','-',title).strip() url = pic.get('pic') # 设置图片编号顺序 num = pic.get('num') if not os.path.exists(title): os.mkdir(title) # 获取图片url网页信息 response = requests.get(url,headers = headers) try: # 建立图片存放地址 if response.status_code == 200: file_path = '&#123;0&#125;\&#123;1&#125;.&#123;2&#125;' .format(title,num,'jpg') # 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest() if not os.path.exists(file_path): # 开始下载图片 with open(file_path,'wb') as f: f.write(response.content) print('文章"&#123;0&#125;"的第&#123;1&#125;张图片下载完成' .format(title,num)) else: print('该图片%s 已下载' %title) except RequestException as e: print(e,'图片获取失败') return Nonedef main(i): # get_page_index(i) # 测试索引界面网页内容是否获取成功ok html = get_page_index(i) data = parse_page_index(html) # 测试索引界面url是否获取成功ok for item in data: # print(item) #测试返回的dict html = get_page_detail(item) data = parse_page_detail(html) for pic in data: save_pic(pic)# 单进程if __name__ == '__main__': for i in range(1,26): main(i)# 多进程if __name__ == '__main__': pool = Pool() pool.map(main,[i for i in range(1,26)]) pool.close() pool.join()结果：文章代码和栏目从2015年至今437篇文章共1509张图片资源，可在下方链接中得到。https://github.com/makcyun/web_scraping_with_python本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Ajax</tag>
        <tag>图片下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫(3)：单页图片下载-网易"数读"信息图]]></title>
    <url>%2F2018%2F09%2F01%2Fweb_scraping_withpython3.html</url>
    <content type="text"><![CDATA[下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。本文知识点：单张图片下载单页图片下载Ajax技术介绍1. 单张图片下载以一篇最近比较热的涨房价的文章为例：暴涨的房租，正在摧毁中国年轻人的生活，从文章里随意挑选一张北京房租地图图片，通过Requests的content属性来实现单张图片的下载。12345import requestsurl = 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'response = requests.get(url)with open('北京房租地图.jpg', 'wb') as f: f.write(response.content)5行代码就能将这张图片下载到电脑上。不只是该张图片，任意图片都可以下载，只要替换图片的url即可。这里用到了Requests的content属性，将图片存储为二进制数据。至于，图片为什么可以用二进制数据进行存储，可以参考这个教程：https://www.zhihu.com/question/36269548/answer/667345825行代码看起来很短，但如果只是下载一张图片显然没有必要写代码，”右键另存为”更快。现在，我们放大一下范围，去下载这篇文章中的所有图片。粗略数一下，网页里有超过15张图片，这时，如果再用”右键另存为”的方法，显然就比较繁琐了。下面，我们用代码来实现下载该网页中的所有图片。2. 单页图片下载2.1. Requests获取网页内容首先，用堪称python”爬虫利器”的Requests库来获取该篇文章的html内容。Requests库可以说是一款python爬虫的利器，它的更多用法，可参考下面的教程：http://docs.python-requests.org/zh_CN/latest/index.htmlhttps://cuiqingcai.com/2556.html1234567891011import requestsheaders = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125;url = 'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'response = requests.get(url,headers = headers)if response.status_code == 200: # return response.text print(response.text) # 测试网页内容是否提取成功ok2.2. 解析网页内容通过上面方法可以获取到html内容，接下来解析html字符串内容，从中提取出网页内的图片url。解析和提取url的方法有很多种，常见的有5种，分别是：正则表达式、Xpath、BeautifulSoup、CSS、PyQuery。任选一种即可，这里为了再次加强练习，5种方法全部尝试一遍。首先，在网页中定位到图片url所在的位置，如下图所示：从外到内定位url的位置：&lt;p&gt;节点-&lt;a&gt;节点-&lt;img&gt;节点里的src属性值。2.2.1. 正则表达式12345678import repattern =re.compile('&lt;p&gt;.*?&lt;img alt="房租".*?src="(.*?)".*?style',re.S) items = re.findall(pattern,html) # print(items) for item in items: yield&#123; 'url':item &#125;运行结果如下:12345678910111213141516&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/425eca61322a4f99837988bb78a001ac.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/d6cb58a6bb014b8683b232f3c00f0e39.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/88d2e535765a4ed09e03877238647aa5.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/09/01/98d2f9579e9e49aeb76ad6155e8fc4ea.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7410ed4041a94cab8f30e8de53aaaaa1.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/49a0c80a140b4f1aa03724654c5a39af.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3070964278bf4637ba3d92b6bb771cea.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/812b7a51475246a9b57f467940626c5c.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/8bcbc7d180f74397addc74e47eaa1f63.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/e593efca849744489096a77aafd10d3e.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7653feecbfd94758a8a0ff599915d435.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/edbaa24a17dc4cca9430761bfc557ffb.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/f768d440d9f14b8bb58e3c425345b97e.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3430043fd305411782f43d3d8635d632.png'&#125;&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/111ba73d11084c68b8db85cdd6d474a7.png'&#125;2.2.2. Xpath语法12345678from lxml import etree parse = etree.HTML(html) items = parse.xpath('*//p//img[@alt = "房租"]/@src') print(items) for item in items: yield&#123; 'url':item &#125;结果同上。2.2.3. CSS选择器1234567soup = BeautifulSoup(html,'lxml') items = soup.select('p &gt; a &gt; img') #&gt;表示下级绝对节点 # print(items) for item in items: yield&#123; 'url':item['src'] &#125;2.2.4. BeautifulSoup find_all方法123456789soup = BeautifulSoup(html,'lxml')# 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一item = soup.find_all(name='img',width =['100%'])for i in range(len(item)): url = item[i].attrs['src'] yield&#123; 'url':url &#125; # print(pic) #测试图片链接ok2.2.5. PyQuery123456789from pyquery import PyQuery as pqdata = pq(html)data2 = data('p &gt; a &gt; img')# print(items)for item in data2.items(): #注意这里和BeautifulSoup 的css用法不同 yield&#123; 'url':item.attr('src') # 或者'url':item.attr.src &#125;以上用了5种方法提取出了该网页的url地址，任选一种即可。这里假设选择了第4种方法，接下来就可以下载图片了。提取出的网址是一个dict字典，通过dict的get方法调用里面的键和值。1234567891011121314151617181920title = pic.get('title')url = pic.get('pic')# 设置图片编号顺序num = pic.get('num')# 建立文件夹if not os.path.exists(title): os.mkdir(title)# 获取图片url网页信息response = requests.get(url,headers = headers)# 建立图片存放地址file_path = '&#123;0&#125;\&#123;1&#125;.&#123;2&#125;' .format(title,num,'jpg')# 文件名采用编号方便按顺序查看# 开始下载图片with open(file_path,'wb') as f: f.write(response.content) print('该图片已下载完成',title)很快，15张图片就按着文章的顺序下载下来了。将上述代码整理一下，增加一点异常处理和图片的标题、编号的代码以让爬虫更健壮，完整的代码如下所示：2.3. 全部代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import requestsfrom bs4 import BeautifulSoupimport reimport osfrom hashlib import md5from requests.exceptions import RequestExceptionfrom multiprocessing import Poolfrom urllib.parse import urlencodeheaders = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125;def get_page(): # 下载1页 url = 'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html' # 增加异常捕获语句 try: response = requests.get(url,headers = headers) if response.status_code == 200: return response.text # print(response.text) # 测试网页内容是否提取成功 except RequestException: print('网页请求失败') return Nonedef parse_page(html): soup = BeautifulSoup(html,'lxml') # 获取title title = soup.h1.string # 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一 item = soup.find_all(name='img',width =['100%']) # print(item) # 测试 for i in range(len(item)): pic = item[i].attrs['src'] yield&#123; 'title':title, 'pic':pic, 'num':i # 图片添加编号顺序 &#125; # print(pic) #测试图片链接okdef save_pic(pic): title = pic.get('title') url = pic.get('pic') # 设置图片编号顺序 num = pic.get('num') if not os.path.exists(title): os.mkdir(title) # 获取图片url网页信息 response = requests.get(url,headers = headers) try: # 建立图片存放地址 if response.status_code == 200: file_path = '&#123;0&#125;\&#123;1&#125;.&#123;2&#125;' .format(title,num,'jpg') # 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest() if not os.path.exists(file_path): # 开始下载图片 with open(file_path,'wb') as f: f.write(response.content) print('该图片已下载完成',title) else: print('该图片%s 已下载' %title) except RequestException as e: print(e,'图片获取失败') return Nonedef main(): # get_page() # 测试网页内容是获取成功ok html = get_page() # parse_page(html) # 测试网页内容是否解析成功ok data = parse_page(html) for pic in data: # print(pic) #测试dict save_pic(pic)# 单进程if __name__ == '__main__': main()小结上面通过爬虫实现下载一张图片延伸到下载一页图片，相比于手动操作，爬虫的优势逐渐显现。那么，能否实现多页循环批量下载更多的图片呢，当然可以，下一篇文章将进行介绍。你也可以尝试一下，这里先放上”福利”：网易”数读”栏目从2012年至今350篇文章的全部图片已下载完成。如果你需要，可以到我的github下载。https://github.com/makcyun/web_scraping_with_python本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>Ajax</tag>
        <tag>图片下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(2)：10行代码爬取全国所有A股/港股/新三板上市公司信息]]></title>
    <url>%2F2018%2F08%2F27%2Fweb_scraping_withpython2.html</url>
    <content type="text"><![CDATA[python爬虫第2篇利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。摘要： 我们平常在浏览网页中会遇到一些表格型的数据信息，除了表格本身体现的内容以外，你可能想透过表格再更进一步地进行汇总、筛选、处理分析等操作从而得到更多有价值的信息，这时可用python爬虫来实现。本文采用pandas库中的read_html方法来快速准确地抓取表格数据。本文知识点：Table型表格抓取DataFrame.read_html函数使用爬虫数据存储到mysql数据库Navicat数据库的使用1. table型表格我们在网页上会经常看到这样一些表格，比如：QS2018世界大学排名：财富世界500强企业排名：IMDB世界电影票房排行榜：中国上市公司信息：他们除了都是表格以外，还一个共同点就是当你点击右键-定位时，可以看到他们都是table类型的表格形式。从中可以看到table类型的表格网页结构大致如下：123456789101112131415161718192021&lt;table class="..." id="..."&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;...&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;...&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; ... &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;tr&gt;...&lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;先来简单解释一下上文出现的几种标签含义：123456&lt;table&gt; : 定义表格&lt;thead&gt; : 定义表格的页眉&lt;tbody&gt; : 定义表格的主体&lt;tr&gt; : 定义表格的行&lt;th&gt; : 定义表格的表头&lt;td&gt; : 定义表格单元这样的表格数据，就可以利用pandas模块里的read_html函数方便快捷地抓取下来。下面我们就来操作一下。2. 快速抓取下面以中国上市公司信息这个网页中的表格为例，感受一下read_html函数的强大之处。12345678import pandas as pdimport csvfor i in range(1,178): # 爬取全部177页数据 url = 'http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s' % (str(i)) tb = pd.read_html(url)[3] #经观察发现所需表格是网页中第4个表格，故为[3] tb.to_csv(r'1.csv', mode='a', encoding='utf_8_sig', header=1, index=0) print('第'+str(i)+'页抓取完成')只需不到十行代码，1分钟左右就可以将全部178页共3536家A股上市公司的信息干净整齐地抓取下来。比采用正则表达式、xpath这类常规方法要省心省力地多。如果采取人工一页页地复制粘贴到excel中，就得操作到猴年马月去了。上述代码除了能爬上市公司表格以外，其他几个网页的表格都可以爬，只需做简单的修改即可。因此，可作为一个简单通用的代码模板。但是，为了让代码更健壮更通用一些，接下来，以爬取177页的A股上市公司信息为目标，讲解一下详细的代码实现步骤。3. 详细代码实现3.1. read_html函数先来了解一下read_html函数的api:1234567891011pandas.read_html(io, match='.+', flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, tupleize_cols=None, thousands=', ', encoding=None, decimal='.', converters=None, na_values=None, keep_default_na=True, displayed_only=True)常用的参数：io:可以是url、html文本、本地文件等；flavor：解析器；header：标题行；skiprows：跳过的行；attrs：属性，比如 attrs = &#123;'id': 'table'&#125;；parse_dates：解析日期注意：返回的结果是**DataFrame**组成的**list**。参考：1 http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html2 http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html3.2. 分析网页url首先，观察一下中商情报网第1页和第2页的网址：12http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=1#QueryConditionhttp://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=2#QueryCondition可以发现，只有pageNum的值随着翻页而变化，所以基本可以断定pageNum=1代表第1页，pageNum=10代表第10页，以此类推。这样比较容易用for循环构造爬取的网址。试着把#QueryCondition删除，看网页是否同样能够打开，经尝试发现网页依然能正常打开，因此在构造url时，可以使用这样的格式：http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i再注意一下其他参数：a：表示A股，把a替换为h，表示港股；把a替换为xsb，则表示新三板。那么，在网址分页for循环外部再加一个for循环，就可以爬取这三个股市的股票了。3.3. 定义函数将整个爬取分为网页提取、内容解析、数据存储等步骤，依次建立相应的函数。123456789101112131415161718192021222324252627282930313233343536373839404142# 网页提取函数def get_one_page(i): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; paras = &#123; 'reportTime': '2017-12-31', #可以改报告日期，比如2018-6-30获得的就是该季度的信息 'pageNum': i #页码 &#125; url = 'http://s.askci.com/stock/a/?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None except RequestException: print('爬取失败')# beatutiful soup解析然后提取表格def parse_one_page(html): soup = BeautifulSoup(html,'lxml') content = soup.select('#myTable04')[0] #[0]将返回的list改为bs4类型 tbl = pd.read_html(content.prettify(),header = 0)[0] # prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame tbl.rename(columns = &#123;'序号':'serial_number', '股票代码':'stock_code', '股票简称':'stock_abbre', '公司名称':'company_name', '省份':'province', '城市':'city', '主营业务收入(201712)':'main_bussiness_income', '净利润(201712)':'net_profit', '员工人数':'employees', '上市日期':'listing_date', '招股书':'zhaogushu', '公司财报':'financial_report', '行业分类':'industry_classification', '产品类型':'industry_type', '主营业务':'main_business'&#125;,inplace = True) print(tbl) # return tbl # rename将表格15列的中文名改为英文名，便于存储到mysql及后期进行数据分析 # tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本# 主函数def main(page): for i in range(1,page): # page表示提取页数 html = get_one_page(i) parse_one_page(html)# 单进程if __name__ == '__main__': main(178) #共提取n页上面两个函数相比于快速抓取的方法代码要多一些，如果需要抓的表格很少或只需要抓一次，那么推荐快速抓取法。如果页数比较多，这种方法就更保险一些。解析函数用了BeautifulSoup和css选择器，这种方法定位提取表格所在的id为#myTable04的table代码段，更为准确。3.4. 存储到MySQL接下来，我们可以将结果保存到本地csv文件，也可以保存到MySQL数据库中。这里为了练习一下MySQL，因此选择保存到MySQL中。首先，需要先在数据库建立存放数据的表格，这里命名为listed_company。代码如下：12345678910111213141516171819import pymysqldef generate_mysql(): conn = pymysql.connect( host='localhost', # 本地服务器 user='root', password='******', # 你的数据库密码 port=3306, # 默认端口 charset = 'utf8', db = 'wade') cursor = conn.cursor() sql = 'CREATE TABLE IF NOT EXISTS listed_company2 (serial_number INT(30) NOT NULL,stock_code INT(30) ,stock_abbre VARCHAR(30) ,company_name VARCHAR(30) ,province VARCHAR(30) ,city VARCHAR(30) ,main_bussiness_income VARCHAR(30) ,net_profit VARCHAR(30) ,employees INT(30) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(30) ,financial_report VARCHAR(30) , industry_classification VARCHAR(255) ,industry_type VARCHAR(255) ,main_business VARCHAR(255) ,PRIMARY KEY (serial_number))' # listed_company是要在wade数据库中建立的表，用于存放数据 cursor.execute(sql) conn.close() generate_mysql()上述代码定义了generate_mysql()函数，用于在MySQL中wade数据库下生成一个listed_company的表。表格包含15个列字段。根据每列字段的属性，分别设置为INT整形（长度为30）、VARCHAR字符型(长度为30) 、DATETIME(0) 日期型等。在Navicat中查看建立好之后的表格：接下来就可以往这个表中写入数据，代码如下：1234567891011import pymysqlfrom sqlalchemy import create_enginedef write_to_sql(tbl, db = 'wade'): engine = create_engine('mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'.format(db)) # db = 'wade'表示存储到wade这个数据库中,root后面的*是密码 try: tbl.to_sql('listed_company',con = engine,if_exists='append',index=False) # 因为要循环网页不断数据库写入内容，所以if_exists选择append，同时该表要有表头，parse_one_page（）方法中df.rename已设置 except Exception as e: print(e)以上就完成了单个页面的表格爬取和存储工作，接下来只要在main()函数进行for循环，就可以完成所有总共178页表格的爬取和存储，完整代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import requestsimport pandas as pdfrom bs4 import BeautifulSoupfrom lxml import etreeimport timeimport pymysqlfrom sqlalchemy import create_enginefrom urllib.parse import urlencode # 编码 URL 字符串start_time = time.time() #计算程序运行时间def get_one_page(i): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' &#125; paras = &#123; 'reportTime': '2017-12-31', #可以改报告日期，比如2018-6-30获得的就是该季度的信息 'pageNum': i #页码 &#125; url = 'http://s.askci.com/stock/a/?' + urlencode(paras) response = requests.get(url,headers = headers) if response.status_code == 200: return response.text return None except RequestException: print('爬取失败')def parse_one_page(html): soup = BeautifulSoup(html,'lxml') content = soup.select('#myTable04')[0] #[0]将返回的list改为bs4类型 tbl = pd.read_html(content.prettify(),header = 0)[0] # prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame tbl.rename(columns = &#123;'序号':'serial_number', '股票代码':'stock_code', '股票简称':'stock_abbre', '公司名称':'company_name', '省份':'province', '城市':'city', '主营业务收入(201712)':'main_bussiness_income', '净利润(201712)':'net_profit', '员工人数':'employees', '上市日期':'listing_date', '招股书':'zhaogushu', '公司财报':'financial_report', '行业分类':'industry_classification', '产品类型':'industry_type', '主营业务':'main_business'&#125;,inplace = True) # print(tbl) return tbl # rename将中文名改为英文名，便于存储到mysql及后期进行数据分析 # tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本def generate_mysql(): conn = pymysql.connect( host='localhost', user='root', password='******', port=3306, charset = 'utf8', db = 'wade') cursor = conn.cursor() sql = 'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))' # listed_company是要在wade数据库中建立的表，用于存放数据 cursor.execute(sql) conn.close() def write_to_sql(tbl, db = 'wade'): engine = create_engine('mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'.format(db)) try: # df = pd.read_csv(df) tbl.to_sql('listed_company2',con = engine,if_exists='append',index=False) # append表示在原有表基础上增加，但该表要有表头 except Exception as e: print(e)def main(page): generate_mysql() for i in range(1,page): html = get_one_page(i) tbl = parse_one_page(html) write_to_sql(tbl) # # 单进程if __name__ == '__main__': main(178) endtime = time.time()-start_time print('程序运行了%.2f秒' %endtime) # 多进程# from multiprocessing import Pool# if __name__ == '__main__':# pool = Pool(4)# pool.map(main, [i for i in range(1,178)]) #共有178页# endtime = time.time()-start_time# print('程序运行了%.2f秒' %(time.time()-start_time))最终，A股所有3535家企业的信息已经爬取到mysql中，如下图：最后，需说明不是所有表格都可以用这种方法爬取，比如这个网站中的表格，表面是看起来是表格，但在html中不是前面的table格式，而是list列表格式。这种表格则不适用read_html爬取。得用其他的方法，比如selenium，以后再进行介绍。本文完。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>pandas</tag>
        <tag>数据抓取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫(1):多种方法爬取猫眼top100电影]]></title>
    <url>%2F2018%2F08%2F20%2Fweb_scraping_withpython1.html</url>
    <content type="text"><![CDATA[python爬虫第1篇利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。摘要： 作为小白，爬虫可以说是入门python最快和最容易获得成就感的途径。因为初级爬虫的套路相对固定，常见的方法只有几种，比较好上手。最近，跟着崔庆才大佬的书：python3网络爬虫开发实战 学习爬虫。选取网页结构较为简单的猫眼top100电影为案例进行练习。 重点是用上述所说的4种方法提取出关键内容。一个问题采用不同的解决方法有助于拓展思维，通过不断练习就能够灵活运用。本文知识点：Requsts 请求库的使用beautiful+lxml两大解析库使用正则表达式 、xpath、css选择器的使用1. 为什么爬取该网页？比较懒，不想一页页地去翻100部电影的介绍，想在一个页面内进行总体浏览（比如在excel表格中）；想深入了解一些比较有意思的信息，比如：哪部电影的评分最高？哪位演员的作品数量最多？哪个国家/地区上榜的电影数量最多？哪一年上榜的电影作品最多等。这些信息在网页上是不那么容易能直接获得的，所以需要爬虫。2. 爬虫目标从网页中提取出top100电影的电影名称、封面图片、排名、评分、演员、上映国家/地区、评分等信息，并保存为csv文本文件。根据爬取结果，进行简单的可视化分析。平台：windows7 + SublimeText33. 爬取步骤3.1. 网址URL分析首先，打开猫眼Top100的url网址： http://maoyan.com/board/4?offset=0。页面非常简单，所包含的信息就是上述所说的爬虫目标。下拉页面到底部，点击第2页可以看到网址变为：http://maoyan.com/board/4?offset=10。因此，可以推断出url的变化规律：offset表示偏移，10代表一个页面的电影偏移数量，即：第一页电影是从0-10，第二页电影是从11-20。因此，获取全部100部电影，只需要构造出10个url，然后依次获取网页内容，再用不同的方法提取出所需内容就可以了。下面，用requests方法获取第一个页面。3.2. Requests获取首页数据先定义一个获取单个页面的函数：get_one_page()，传入url参数。12345678910111213def get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; # 不加headers爬不了 response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except RequestException: return None # try-except语句捕获异常接下来在main()函数中设置url。12345678def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) print(html)if __name__ == '__main__': main()运行上述程序后，首页的源代码就被爬取下来了。如下图所示：接下来就需要从整个网页中提取出几项我们需要的内容，用到的方法就是上述所说的四种方法，下面分别进行说明。3.3. 4种内容解析提取方法3.3.1. 正则表达式提取第一种是利用正则表达式提取。什么是正则表达式？ 下面这串看起来乱七八糟的符号就是正则表达式的语法。1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&apos;它是一种强大的字符串处理工具。之所以叫正则表达式，是因为它们可以识别正则字符串（regular string）。可以这么定义：“ 如果你给我的字符串符合规则，我就返回它”；“如果字符串不符合规则，我就忽略它”。通过requests抓取下来的网页是一堆大量的字符串，用它处理后便可提取出我们想要的内容。如果还不了解它，可以参考下面的教程：http://www.runoob.com/regexp/regexp-syntax.htmlhttps://www.w3cschool.cn/regexp/zoxa1pq7.html正则表达式常用语法：table th:nth-of-type(1){width:60px}模式描述\w匹配字母数字及下划线\W匹配非字母数字及下划线\s匹配任意空白字符，等价于 [\t\n\r\f]\S匹配任意非空字符\d匹配任意数字，等价于 [0-9]\D匹配任意非数字\n匹配一个换行符\t匹配一个制表符^匹配字符串开始位置的字符$匹配字符串的末尾.匹配任意字符，除了换行符[…]用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’[^…]不在 [ ] 中的字符*匹配前面的字符、子表达式或括号里的字符 0 次或多次+同上，匹配至少一次?同上，匹配0到1次{n}匹配前面的字符、子表达式或括号里的字符 n 次{n, m}同上，匹配 m 到n 次（包含 m 或 n）( )匹配括号内的表达式，也表示一个组下面，开始提取关键内容。右键网页-检查-Network选项，选中左边第一个文件然后定位到电影信息的相应位置，如下图：可以看到每部电影的相关信息都在dd这个节点之中。所以就可以从该节点运用正则进行提取。第1个要提取的内容是电影的排名。它位于class=”board-index”的i节点内。不需要提取的内容用’.*?’替代，需要提取的数字排名用（）括起来，（）里面的数字表示为（\d+）。正则表达式可以写为：1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;&apos;接着，第2个需要提取的是封面图片，图片网址位于img节点的’data-src’属性中，正则表达式可写为：1&apos;data-src=&quot;(.*?)&quot;.*?&apos;第1和第2个正则之间的代码是不需要的，用’.*?’替代，所以这两部分合起来写就是：1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;同理，可以依次用正则写下主演、上映时间和评分等内容,完整的正则表达式如下：1&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;正则表达式写好以后，可以定义一个页面解析提取方法：parse_one_page（），用来提取内容：12345678910111213141516171819def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) # re.S表示匹配任意字符，如果不加，则无法匹配换行符 items = re.findall(pattern, html) # print(items) for item in items: yield &#123; 'index': item[0], 'thumb': get_thumb(item[1]), # 定义get_thumb()方法进一步处理网址 'name': item[2], 'star': item[3].strip()[3:], # 'time': item[4].strip()[5:], # 用两个方法分别提取time里的日期和地区 'time': get_release_time(item[4].strip()[5:]), 'area': get_release_area(item[4].strip()[5:]), 'score': item[5].strip() + item[6].strip() # 评分score由整数+小数两部分组成 &#125;Tips:re.S:匹配任意字符，如果不加，则无法匹配换行符；yield:使用yield的好处是作为生成器，可以遍历迭代，并且将数据整理形成字典，输出结果美观。具体用法可参考：https://blog.csdn.net/zhangpinghao/article/details/18716275；.strip():用于去掉字符串中的空格。上面程序为了便于提取内容，又定义了3个方法：get_thumb（）、get_release_time（）和 get_release_area（）：1234567891011121314151617181920212223242526# 获取封面大图def get_thumb(url): pattern = re.compile(r'(.*?)@.*?') thumb = re.search(pattern, url) return thumb.group(1)# http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c# 去掉@160w_220h_1e_1c就是大图 # 提取上映时间函数def get_release_time(data): pattern = re.compile(r'(.*?)(\(|$)') items = re.search(pattern, data) if items is None: return '未知' return items.group(1) # 返回匹配到的第一个括号(.*?)中结果即时间# 提取国家/地区函数def get_release_area(data): pattern = re.compile(r'.*\((.*)\)') # $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?) items = re.search(pattern, data) if items is None: return '未知' return items.group(1)Tips:‘r’：正则前面加上’r’ 是为了告诉编译器这个string是个raw string，不要转意’\’。当一个字符串使用了正则表达式后，最好在前面加上’r’；‘|’ ‘$’： 正则’|’表示或’，’$’表示匹配一行字符串的结尾；.group(1)：意思是返回search匹配的第一个括号中的结果，即(.*?)，gropup()则返回所有结果2013-12-18(，group(1)返回’（’。接下来，修改main()函数来输出爬取的内容：12345678910def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) for item in parse_one_page(html): print(item)if __name__ == '__main__': main()Tips:if name == ‘_ _main__’:当.py文件被直接运行时，if name == ‘_ main__’之下的代码块将被运行；当.py文件以模块形式被导入时，if name == ‘ _main__’之下的代码块不被运行。参考：https://blog.csdn.net/yjk13703623757/article/details/77918633。运行程序，就可成功地提取出所需内容，结果如下：123456789&#123;'index': '1', 'thumb': 'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg', 'name': '霸王别姬', 'star': '张国荣,张丰毅,巩俐', 'time': '1993-01-01', 'area': '中国香港', 'score': '9.6'&#125;&#123;'index': '2', 'thumb': 'http://p0.meituan.net/movie/54617769d96807e4d81804284ffe2a27239007.jpg', 'name': '罗马假日', 'star': '格利高里·派克,奥黛丽·赫本,埃迪·艾伯特', 'time': '1953-09-02', 'area': '美国', 'score': '9.1'&#125;&#123;'index': '3', 'thumb': 'http://p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg', 'name': '肖申克的救赎', 'star': '蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿', 'time': '1994-10-14', 'area': '美国', 'score': '9.5'&#125;&#123;'index': '4', 'thumb': 'http://p0.meituan.net/movie/e55ec5d18ccc83ba7db68caae54f165f95924.jpg', 'name': '这个杀手不太冷', 'star': '让·雷诺,加里·奥德曼,娜塔莉·波特曼', 'time': '1994-09-14', 'area': '法国', 'score': '9.5'&#125;&#123;'index': '5', 'thumb': 'http://p1.meituan.net/movie/f5a924f362f050881f2b8f82e852747c118515.jpg', 'name': '教父', 'star': '马龙·白兰度,阿尔·帕西诺,詹姆斯·肯恩', 'time': '1972-03-24', 'area': '美国', 'score': '9.3'&#125;...&#125;[Finished in 1.9s]以上是第1种提取方法，如果还不习惯正则表达式这种复杂的语法，可以试试下面的第2种方法。3.3.2. lxml结合xpath提取该方法需要用到lxml这款解析利器，同时搭配xpath语法，利用它的的路径选择表达式，来高效提取所需内容。lxml包为第三方包，需要自行安装。如果对xpath的语法还不太熟悉，可参考下面的教程：http://www.w3school.com.cn/xpath/xpath_syntax.aspxpath常用的规则表达式描述nodename选取此节点的所有子节点/从当前节点选取直接子节点//从当前节点选取子孙节点.选取当前节点..选取当前节点的父节点@选取属性12345678910111213141516171819202122232425262728293031323334&lt;/div&gt; &lt;div class="container" id="app" class="page-board/index" &gt;&lt;div class="content"&gt; &lt;div class="wrapper"&gt; &lt;div class="main"&gt; &lt;p class="update-time"&gt;2018-08-18&lt;span class="has-fresh-text"&gt;已更新&lt;/span&gt;&lt;/p&gt; &lt;p class="board-content"&gt;榜单规则：将猫眼电影库中的经典影片，按照评分和评分人数从高到低综合排序取前100名，每天上午10点更新。相关数据来源于“猫眼电影库”。&lt;/p&gt; &lt;dl class="board-wrapper"&gt; &lt;dd&gt; &lt;i class="board-index board-index-1"&gt;1&lt;/i&gt; &lt;a href="/films/1203" title="霸王别姬" class="image-link" data-act="boarditem-click" data-val="&#123;movieId:1203&#125;"&gt; &lt;img src="//ms0.meituan.net/mywww/image/loading_2.e3d934bf.png" alt="" class="poster-default" /&gt; &lt;img data-src="http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c" alt="霸王别姬" class="board-img" /&gt; &lt;/a&gt; &lt;div class="board-item-main"&gt; &lt;div class="board-item-content"&gt; &lt;div class="movie-item-info"&gt; &lt;p class="name"&gt;&lt;a href="/films/1203" title="霸王别姬" data-act="boarditem-click" data-val="&#123;movieId:1203&#125;"&gt;霸王别姬&lt;/a&gt;&lt;/p&gt; &lt;p class="star"&gt; 主演：张国荣,张丰毅,巩俐 &lt;/p&gt;&lt;p class="releasetime"&gt;上映时间：1993-01-01(中国香港)&lt;/p&gt; &lt;/div&gt; &lt;div class="movie-item-number score-num"&gt;&lt;p class="score"&gt;&lt;i class="integer"&gt;9.&lt;/i&gt;&lt;i class="fraction"&gt;6&lt;/i&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/dd&gt; &lt;dd&gt;根据截取的部分html网页，先来提取第1个电影排名信息，有两种方法。第一种：直接复制。右键-Copy-Copy Xpath，得到xpath路径为：//*[@id=”app”]/div/div/div[1]/dl/dd[1]/i,为了能够提取到页面所有的排名信息，需进一步修改为：//*[@id=”app”]/div/div/div[1]/dl/dd/i/text()，如果想要再精简一点，可以省去中间部分绝对路径’/‘然后用相对路径’//‘代替，最后进一步修改为：//*[@id=”app”]//div//dd/i/text()。第二种：观察网页结构自己写。首先注意到id = app的div节点，因为在整个网页结构id是唯一的不会有第二个相同的，所有可以将该div节点作为xpath语法的起点，然后往下观察分别是3级div节点，可以省略写为：//div,再往下分别是是两个并列的p节点、dl节点、dd节点和最后的i节点文本。中间可以随意省略，只要保证该路径能够选择到唯一的文本值‘1’即可，例如省去p和dl节点，只保留后面的节点。这样，完整路径可以为：//*[@id=”app”]//div//dd/i/text()，和上式一样。根据上述思路，可以写下其他内容的xpath路径。观察到路径的前一部分：//*[@id=”app”]//div//dd都是一样的，从后面才开始不同，因此为了能够精简代码，将前部分路径赋值为一个变量items，最终提取的代码如下：12345678910111213141516171819202122232425# 2 用lxml结合xpath提取内容def parse_one_page2(html): parse = etree.HTML(html) items = parse.xpath('//*[@id="app"]//div//dd') # 完整的是//*[@id="app"]/div/div/div[1]/dl/dd # print(type(items)) # *代表匹配所有节点，@表示属性 # 第一个电影是dd[1],要提取页面所有电影则去掉[1] # xpath://*[@id="app"]/div/div/div[1]/dl/dd[1] for item in items: yield&#123; 'index': item.xpath('./i/text()')[0], #./i/text()前面的点表示从items节点开始 #/text()提取文本 'thumb': get_thumb(str(item.xpath('./a/img[2]/@data-src')[0].strip())), # 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。 'name': item.xpath('./a/@title')[0], 'star': item.xpath('.//p[@class = "star"]/text()')[0].strip(), 'time': get_release_time(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'area': get_release_area(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'score' : item.xpath('.//p[@class = "score"]/i[1]/text()')[0] + \ item.xpath('.//p[@class = "score"]/i[2]/text()')[0] &#125;Tips:[0]：xpath后面添加了[0]是因为返回的是只有1个字符串的list，添加[0]是将list提取为字符串，使其简洁；Network：要在最原始的Network选项卡中定位，而不是Elements中，不然提取不到相关内容；class属性：p[@class = “star”]/text()表示提取class属性为”star”的p节点的文本值；提取属性值：img[2]/@data-src’：提取img节点的data-src属性值，属性值后面无需添加’/text()’运行程序，就可成功地提取出所需内容，结果和第一种方法一样。以上是第2种提取方法，如果也不太习惯xpath语法，可以试试下面的第3种方法。3.3.3. Beautiful Soup + css选择器Beautiful Soup 同lxml一样，是一个非常强大的python解析库，可以从HTML或XML文件中提取效率非常高。关于它的用法，可参考下面的教程：https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/css选择器选是一种模式，用于选择需要添加样式的元素，使用它的语法同样能够快速定位到所需节点，然后提取相应内容。使用方法可参考下面的教程：http://www.w3school.com.cn/cssref/css_selectors.aspcss选择器常用的规则table th:nth-of-type(1){width:30%}选择器例子例子描述.class.intro选择 class=”intro” 的所有元素。#id#firstname选择 id=”firstname” 的所有元素。**选择所有元素。elementp选择所有p元素。element,elementdiv,p选择所有div元素和所有p元素。element?elementdiv p选择div元素内部的所有p元素。element&gt;elementdiv&gt;p选择父元素为div元素的所有p元素。element+elementdiv+p选择紧接在div元素之后的所有p元素。[attribute][target]选择带有 target 属性所有元素。[attribute=value][target=_blank]选择 target=”_blank” 的所有元素。下面就利用这种方法进行提取：12345678910111213141516171819202122# 3 用beautifulsoup + css选择器提取def parse_one_page3(html): soup = BeautifulSoup(html, 'lxml') # print(content) # print(type(content)) # print('------------') items = range(10) for item in items: yield&#123; 'index': soup.select('dd i.board-index')[item].string, # iclass节点完整地为'board-index board-index-1',写board-index即可 'thumb': get_thumb(soup.select('a &gt; img.board-img')[item]["data-src"]), # 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值 'name': soup.select('.name a')[item].string, 'star': soup.select('.star')[item].string.strip()[3:], 'time': get_release_time(soup.select('.releasetime')[item].string.strip()[5:]), 'area': get_release_area(soup.select('.releasetime')[item].string.strip()[5:]), 'score': soup.select('.integer')[item].string + soup.select('.fraction')[item].string &#125;运行上述程序，结果同第1种方法一样。3.3.4. Beautiful Soup + find_all函数提取Beautifulsoup除了和css选择器搭配，还可以直接用它自带的find_all函数进行提取。find_all，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。它的API如下：1find_all(name , attrs , recursive , text , **kwargs)常用的语法规则如下：soup.find_all(name=’ul’)： 查找所有ul节点，ul节点内还可以嵌套；li.string和li.get_text()：都是获取li节点的文本，但推荐使用后者；soup.find_all(attrs={‘id’: ‘list-1’}))：传入 attrs 参数，参数的类型是字典类型，表示查询 id 为 list-1 的节点；常用的属性比如 id、class 等，可以省略attrs采用更简洁的形式，例如：soup.find_all(id=’list-1’)soup.find_all(class_=’element’)根据上述常用语法，可以提取网页中所需内容：12345678910111213141516def parse_one_page4(html): soup = BeautifulSoup(html,'lxml') items = range(10) for item in items: yield&#123; 'index': soup.find_all(class_='board-index')[item].string, 'thumb': soup.find_all(class_ = 'board-img')[item].attrs['data-src'], # 用.get('data-src')获取图片src链接，或者用attrs['data-src'] 'name': soup.find_all(name = 'p',attrs = &#123;'class' : 'name'&#125;)[item].string, 'star': soup.find_all(name = 'p',attrs = &#123;'class':'star'&#125;)[item].string.strip()[3:], 'time': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'area': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'score':soup.find_all(name = 'i',attrs = &#123;'class':'integer'&#125;)[item].string.strip() + soup.find_all(name = 'i',attrs = &#123;'class':'fraction'&#125;)[item].string.strip() &#125;以上就是4种不同的内容提取方法。3.4. 数据存储上述输出的结果为字典格式，可利用csv包的DictWriter函数将字典格式数据存储到csv文件中。123456789# 数据存储到csvdef write_to_file3(item): with open('猫眼top100.csv', 'a', encoding='utf_8_sig',newline='') as f: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] w = csv.DictWriter(f,fieldnames = fieldnames) # w.writeheader() w.writerow(item)然后修改一下main()方法：1234567891011def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_one_page(url) for item in parse_one_page(html): # print(item) write_to_csv(item)if __name__ == '__main__': main()结果如下图：再将封面的图片下载下来：1234567891011def download_thumb(name, url,num): try: response = requests.get(url) with open('封面图/' + name + '.jpg', 'wb') as f: f.write(response.content) print('第%s部电影封面下载完毕' %num) print('------') except RequestException as e: print(e) pass # 不能是w，否则会报错，因为图片是二进制数据所以要用wb3.5. 分页爬取上面完成了一页电影数据的提取，接下来还需提取剩下9页共90部电影的数据。对网址进行遍历，给网址传入一个offset参数即可，修改如下：123456789101112def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) for item in parse_one_page(html): # print(item) write_to_csv(item)if __name__ == '__main__': for i in range(10): main(offset = i*10)这样就完成了所有电影的爬取。结果如下：4. 可视化分析俗话说“文不如表，表不如图”。下面根据excel的数据结果，进行简单的数据可视化分析，并用图表呈现。4.1. 电影评分最高top10首先，想看一看评分最高的前10部电影是哪些？程序如下：123456789101112131415161718192021222324252627282930import pandas as pdimport matplotlib.pyplot as pltimport pylab as pl #用于修改x轴坐标plt.style.use('ggplot') #默认绘图风格很难看，替换为好看的ggplot风格fig = plt.figure(figsize=(8,5)) #设置图片大小colors1 = '#6D6D6D' #设置图表title、text标注的颜色columns = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] #设置表头df = pd.read_csv('maoyan_top100.csv',encoding = "utf-8",header = None,names =columns,index_col = 'index') #打开表格# index_col = 'index' 将索引设为indexdf_score = df.sort_values('score',ascending = False) #按得分降序排列name1 = df_score.name[:10] #x轴坐标score1 = df_score.score[:10] #y轴坐标 plt.bar(range(10),score1,tick_label = name1) #绘制条形图，用range()能搞保持x轴正确顺序plt.ylim ((9,9.8)) #设置纵坐标轴范围plt.title('电影评分最高top10',color = colors1) #标题plt.xlabel('电影名称') #x轴标题plt.ylabel('评分') #y轴标题# 为每个条形图添加数值标签for x,y in enumerate(list(score1)): plt.text(x,y+0.01,'%s' %round(y,1),ha = 'center',color = colors1)pl.xticks(rotation=270) #x轴名称太长发生重叠，旋转为纵向显示plt.tight_layout() #自动控制空白边缘，以全部显示x轴名称# plt.savefig('电影评分最高top10.png') #保存图片plt.show()结果如下图：可以看到：排名最高的分别是两部国产片”霸王别姬”和”大话西游”，其他还包括”肖申克的救赎”、”教父”等。嗯，还好基本上都看过。4.2. 各国家的电影数量比较然后，想看看100部电影都是来自哪些国家？程序如下：1234567891011121314151617area_count = df.groupby(by = 'area').area.count().sort_values(ascending = False)# 绘图方法1area_count.plot.bar(color = '#4652B1') #设置为蓝紫色pl.xticks(rotation=0) #x轴名称太长重叠，旋转为纵向# 绘图方法2# plt.bar(range(11),area_count.values,tick_label = area_count.index)for x,y in enumerate(list(area_count.values)): plt.text(x,y+0.5,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('各国/地区电影数量排名',color = colors1)plt.xlabel('国家/地区')plt.ylabel('数量(部)')plt.show()# plt.savefig('各国(地区)电影数量排名.png')结果如下图：可以看到，除去网站自身没有显示国家的电影以外，上榜电影被10个国家/地区”承包”了。其中，美国以30部电影的绝对优势占据第1名，其次是8部的日本，韩国第3，居然有7部上榜。不得不说的是香港有5部，而内地一部都没有。。。4.3. 电影作品数量集中的年份接下来站在漫长的百年电影史的时间角度上，分析一下哪些年份”贡献了”最多的电影数量，也可以说是”电影大年”。1234567891011121314151617181920212223# 从日期中提取年份df['year'] = df['time'].map(lambda x:x.split('/')[0])# print(df.info())# print(df.head())# 统计各年上映的电影数量grouped_year = df.groupby('year')grouped_year_amount = grouped_year.year.count()top_year = grouped_year_amount.sort_values(ascending = False)# 绘图top_year.plot(kind = 'bar',color = 'orangered') #颜色设置为橙红色for x,y in enumerate(list(top_year.values)): plt.text(x,y+0.1,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('电影数量年份排名',color = colors1)plt.xlabel('年份(年)')plt.ylabel('数量(部)')plt.tight_layout()# plt.savefig('电影数量年份排名.png')plt.show()结果如下图：可以看到，100部电影来自37个年份。其中2011年上榜电影数量最多，达到9部；其次是前一年的7部。回忆一下，那会儿正是上大学的头两年，可怎么感觉除了阿凡达之外，没有什么其他有印象的电影了。。。另外，网上传的号称”电影史奇迹年”的1994年仅排名第6。这让我进一步对猫眼榜单的权威性产生了质疑。再往后看，发现遥远的1939和1940年也有电影上榜。那会儿应该还是黑白电影时代吧，看来电影的口碑好坏跟外在的技术没有绝对的关系，质量才是王道。4.4 拥有电影作品数量最多的演员最后，看看前100部电影中哪些演员的作品数量最多。程序如下：1234567891011121314151617181920212223242526272829303132333435363738#表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中starlist = []star_total = df.starfor i in df.star.str.replace(' ','').str.split(','): starlist.extend(i) # print(starlist)# print(len(starlist))# set去除重复的演员名starall = set(starlist)# print(starall)# print(len(starall))starall2 = &#123;&#125;for i in starall: if starlist.count(i)&gt;1: # 筛选出电影数量超过1部的演员 starall2[i] = starlist.count(i)starall2 = sorted(starall2.items(),key = lambda starlist:starlist[1] ,reverse = True)starall2 = dict(starall2[:10]) #将元组转为字典格式# 绘图x_star = list(starall2.keys()) #x轴坐标y_star = list(starall2.values()) #y轴坐标plt.bar(range(10),y_star,tick_label = x_star)pl.xticks(rotation = 270)for x,y in enumerate(y_star): plt.text(x,y+0.1,'%s' %round(y,1),ha = 'center',color = colors1)plt.title('演员电影作品数量排名',color = colors1)plt.xlabel('演员')plt.ylabel('数量(部)')plt.tight_layout()plt.show() # plt.savefig('演员电影作品数量排名.png')结果如下图：张国荣排在了第一位，这是之前没有猜到的。其次是梁朝伟和星爷，再之后是布拉德·皮特。惊奇地发现，前十名影星中，香港影星居然占了6位。有点严重怀疑这是不是香港版的top100电影。。。对张国荣以7部影片的巨大优势雄霸榜单第一位感到好奇，想看看是哪7部电影。12345df['star1'] = df['star'].map(lambda x:x.split(',')[0]) #提取1号演员df['star2'] = df['star'].map(lambda x:x.split(',')[1]) #提取2号演员star_most = df[(df.star1 == '张国荣') | (df.star2 == '张国荣')][['star','name']].reset_index('index')# |表示两个条件或查询，之后重置索引print(star_most)可以看到包括排名第1的”霸王别姬”、第17名的”春光乍泄”、第27名的”射雕英雄传之东成西就”等。突然发现，好像只看过”英雄本色”。。。有时间，去看看他其他的作品。12345678 index star name0 1 张国荣,张丰毅,巩俐 霸王别姬1 17 张国荣,梁朝伟,张震 春光乍泄2 27 张国荣,梁朝伟,张学友 射雕英雄传之东成西就3 37 张国荣,梁朝伟,刘嘉玲 东邪西毒4 70 张国荣,王祖贤,午马 倩女幽魂5 99 张国荣,张曼玉,刘德华 阿飞正传6 100 狄龙,张国荣,周润发 英雄本色由于数据量有限，故仅作了上述简要的分析。5. 完整程序最后，将前面爬虫的所有代码整理一下，完整的代码如下：一、爬虫部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193import urllibimport requestsfrom requests.exceptions import RequestExceptionimport refrom bs4 import BeautifulSoupimport jsonimport timefrom lxml import etree# -----------------------------------------------------------------------------def get_one_page(url): try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'&#125; # 不加headers爬不了 response = requests.get(url, headers=headers) if response.status_code == 200: return response.text else: return None except RequestException: return None# 1 用正则提取内容def parse_one_page(html): pattern = re.compile( '&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;', re.S) # re.S表示匹配任意字符，如果不加.无法匹配换行符 items = re.findall(pattern, html) # print(items) for item in items: yield &#123; 'index': item[0], 'thumb': get_thumb(item[1]), 'name': item[2], 'star': item[3].strip()[3:], # 'time': item[4].strip()[5:], # 用函数分别提取time里的日期和地区 'time': get_release_time(item[4].strip()[5:]), 'area': get_release_area(item[4].strip()[5:]), 'score': item[5].strip() + item[6].strip() &#125; # 2 用lxml结合xpath提取内容def parse_one_page2(html): parse = etree.HTML(html) items = parse.xpath('//*[@id="app"]//div//dd') # 完整的是//*[@id="app"]/div/div/div[1]/dl/dd # print(type(items)) # *代表匹配所有节点，@表示属性 # 第一个电影是dd[1],要提取页面所有电影则去掉[1] # xpath://*[@id="app"]/div/div/div[1]/dl/dd[1] # lst = [] for item in items: yield&#123; 'index': item.xpath('./i/text()')[0], #./i/text()前面的点表示从items节点开始 #/text()提取文本 'thumb': get_thumb(str(item.xpath('./a/img[2]/@data-src')[0].strip())), # 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。 'name': item.xpath('./a/@title')[0], 'star': item.xpath('.//p[@class = "star"]/text()')[0].strip(), 'time': get_release_time(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'area': get_release_area(item.xpath( './/p[@class = "releasetime"]/text()')[0].strip()[5:]), 'score' : item.xpath('.//p[@class = "score"]/i[1]/text()')[0] + \ item.xpath('.//p[@class = "score"]/i[2]/text()')[0] &#125; # 3 用beautifulsoup + css选择器提取def parse_one_page3(html): soup = BeautifulSoup(html, 'lxml') # print(content) # print(type(content)) # print('------------') items = range(10) for item in items: yield&#123; 'index': soup.select('dd i.board-index')[item].string, # iclass节点完整地为'board-index board-index-1',写board-inde即可 'thumb': get_thumb(soup.select('a &gt; img.board-img')[item]["data-src"]), # 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值 'name': soup.select('.name a')[item].string, 'star': soup.select('.star')[item].string.strip()[3:], 'time': get_release_time(soup.select('.releasetime')[item].string.strip()[5:]), 'area': get_release_area(soup.select('.releasetime')[item].string.strip()[5:]), 'score': soup.select('.integer')[item].string + soup.select('.fraction')[item].string &#125;# 4 用beautifulsoup + find_all提取def parse_one_page4(html): soup = BeautifulSoup(html,'lxml') items = range(10) for item in items: yield&#123; 'index': soup.find_all(class_='board-index')[item].string, 'thumb': soup.find_all(class_ = 'board-img')[item].attrs['data-src'], # 用.get('data-src')获取图片src链接，或者用attrs['data-src'] 'name': soup.find_all(name = 'p',attrs = &#123;'class' : 'name'&#125;)[item].string, 'star': soup.find_all(name = 'p',attrs = &#123;'class':'star'&#125;)[item].string.strip()[3:], 'time': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'area': get_release_time(soup.find_all(class_ ='releasetime')[item].string.strip()[5:]), 'score':soup.find_all(name = 'i',attrs = &#123;'class':'integer'&#125;)[item].string.strip() + soup.find_all(name = 'i',attrs = &#123;'class':'fraction'&#125;)[item].string.strip() &#125;# -----------------------------------------------------------------------------# 提取时间函数def get_release_time(data): pattern = re.compile(r'(.*?)(\(|$)') items = re.search(pattern, data) if items is None: return '未知' return items.group(1) # 返回匹配到的第一个括号(.*?)中结果即时间# 提取国家/地区函数def get_release_area(data): pattern = re.compile(r'.*\((.*)\)') # $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?) items = re.search(pattern, data) if items is None: return '未知' return items.group(1)# 获取封面大图# http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c# 去掉@160w_220h_1e_1c就是大图def get_thumb(url): pattern = re.compile(r'(.*?)@.*?') thumb = re.search(pattern, url) return thumb.group(1)# 数据存储到csvdef write_to_file3(item): with open('猫眼top100.csv', 'a', encoding='utf_8_sig',newline='') as f: # 'a'为追加模式（添加） # utf_8_sig格式导出csv不乱码 fieldnames = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] w = csv.DictWriter(f,fieldnames = fieldnames) # w.writeheader() w.writerow(item)# 封面下载def download_thumb(name, url,num): try: response = requests.get(url) with open('封面图/' + name + '.jpg', 'wb') as f: f.write(response.content) print('第%s部电影封面下载完毕' %num) print('------') except RequestException as e: print(e) pass # 存储格式是wb,因为图片是二进制数格式，不能用w，否则会报错# -----------------------------------------------------------------------------def main(offset): url = 'http://maoyan.com/board/4?offset=' + str(offset) html = get_one_page(url) # print(html) # parse_one_page2(html) for item in parse_one_page(html): # 切换内容提取方法 print(item) write_to_file(item) # 下载封面图 download_thumb(item['name'], item['thumb'],item['index'])# if __name__ == '__main__':# for i in range(10):# main(i * 10) # time.sleep(0.5) # 猫眼增加了反爬虫，设置0.5s的延迟时间# 2 使用多进程提升抓取效率from multiprocessing import Poolif __name__ == '__main__': pool = Pool() pool.map(main, [i * 10 for i in range(10)])二、可视化部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#-*- coding: utf-8 -*-# 可视化分析# -------------------------------import pandas as pdimport matplotlib.pyplot as pltimport pylab as pl # 用于修改x轴坐标plt.style.use('ggplot') # 默认绘图风格很难看，替换为好看的ggplot风格fig = plt.figure(figsize=(8, 5)) # 设置图片大小colors1 = '#6D6D6D' # 设置图表title、text标注的颜色columns = ['index', 'thumb', 'name', 'star', 'time', 'area', 'score'] # 设置表头df = pd.read_csv('maoyan_top100.csv', encoding="utf-8", header=None, names=columns, index_col='index') # 打开表格# index_col = 'index' 将索引设为index# 1电影评分最高top10def annalysis_1(): df_score = df.sort_values('score', ascending=False) # 按得分降序排列 name1 = df_score.name[:10] # x轴坐标 score1 = df_score.score[:10] # y轴坐标 plt.bar(range(10), score1, tick_label=name1) # 绘制条形图，用range()能搞保持x轴正确顺序 plt.ylim((9, 9.8)) # 设置纵坐标轴范围 plt.title('电影评分最高top10', color=colors1) # 标题 plt.xlabel('电影名称') # x轴标题 plt.ylabel('评分') # y轴标题 # 为每个条形图添加数值标签 for x, y in enumerate(list(score1)): plt.text(x, y + 0.01, '%s' % round(y, 1), ha='center', color=colors1) pl.xticks(rotation=270) # x轴名称太长发生重叠，旋转为纵向显示 plt.tight_layout() # 自动控制空白边缘，以全部显示x轴名称 # plt.savefig('电影评分最高top10.png') #保存图片 plt.show()# ------------------------------# 2各国家的电影数量比较def annalysis_2(): area_count = df.groupby( by='area').area.count().sort_values(ascending=False) # 绘图方法1 area_count.plot.bar(color='#4652B1') # 设置为蓝紫色 pl.xticks(rotation=0) # x轴名称太长重叠，旋转为纵向 # 绘图方法2 # plt.bar(range(11),area_count.values,tick_label = area_count.index,color # = '#4652B1') for x, y in enumerate(list(area_count.values)): plt.text(x, y + 0.5, '%s' % round(y, 1), ha='center', color=colors1) plt.title('各国/地区电影数量排名', color=colors1) plt.xlabel('国家/地区') plt.ylabel('数量(部)') plt.show()# plt.savefig('各国(地区)电影数量排名.png')# ------------------------------# 3电影作品数量集中的年份# 从日期中提取年份def annalysis_3(): df['year'] = df['time'].map(lambda x: x.split('/')[0]) # print(df.info()) # print(df.head()) # 统计各年上映的电影数量 grouped_year = df.groupby('year') grouped_year_amount = grouped_year.year.count() top_year = grouped_year_amount.sort_values(ascending=False) # 绘图 top_year.plot(kind='bar', color='orangered') # 颜色设置为橙红色 for x, y in enumerate(list(top_year.values)): plt.text(x, y + 0.1, '%s' % round(y, 1), ha='center', color=colors1) plt.title('电影数量年份排名', color=colors1) plt.xlabel('年份(年)') plt.ylabel('数量(部)') plt.tight_layout() # plt.savefig('电影数量年份排名.png') plt.show()# ------------------------------# 4拥有电影作品数量最多的演员# 表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中def annalysis_4(): starlist = [] star_total = df.star for i in df.star.str.replace(' ', '').str.split(','): starlist.extend(i) # print(starlist) # print(len(starlist)) # set去除重复的演员名 starall = set(starlist) # print(starall) # print(len(starall)) starall2 = &#123;&#125; for i in starall: if starlist.count(i) &gt; 1: # 筛选出电影数量超过1部的演员 starall2[i] = starlist.count(i) starall2 = sorted(starall2.items(), key=lambda starlist: starlist[1], reverse=True) starall2 = dict(starall2[:10]) # 将元组转为字典格式 # 绘图 x_star = list(starall2.keys()) # x轴坐标 y_star = list(starall2.values()) # y轴坐标 plt.bar(range(10), y_star, tick_label=x_star) pl.xticks(rotation=270) for x, y in enumerate(y_star): plt.text(x, y + 0.1, '%s' % round(y, 1), ha='center', color=colors1) plt.title('演员电影作品数量排名', color=colors1) plt.xlabel('演员') plt.ylabel('数量(部)') plt.tight_layout() plt.show()# plt.savefig('演员电影作品数量排名.png')def main(): annalysis_1() annalysis_2() annalysis_3() annalysis_4()if __name__ == '__main__': main()]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python爬虫</tag>
        <tag>requests</tag>
        <tag>正则表达式</tag>
        <tag>beautifulsoup</tag>
        <tag>css</tag>
        <tag>xpath</tag>
        <tag>lxml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4块钱,用Github+Hexo搭建你的个人博客：美化篇]]></title>
    <url>%2F2018%2F07%2F17%2Fhexo02.html</url>
    <content type="text"><![CDATA[上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。摘要：搭建博客相对简单，而美化博客则要复杂一些，因为涉及到修改和增删源代码，对于没有前端基础的人来说，会比较费时间精力。为了尽可能在最短的时间里，打造一个总体看得过去的博客，本文以我的博客为例，介绍一些比较实用的博客美化操作和技巧。1. 选择新的模板首先，是要更换非常难看的初始的博客界面。重新挑选一个好看的主题模板，然后在此基础上进行美化。主题寻找：https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories该网站按照模板的受欢迎程度进行排名，可以看到遥遥领先的第一名是一款叫作：next的主题，选用这款即可。进入到这个主题，可以阅读README.md模板使用说明，还可以查看模板示例网站。模板使用：打开博客根目录下的themes文件夹(注：后文所说的根目录指：D:\blog)，右键Git Bash运行下述命令：git clone https://github.com/iissnan/hexo-theme-next themes/next就可以把这款主题的安装文件下载到电脑中。接着，打开D:\blog_config.yml文件，找到 theme字段，修改参数为：theme: hexo-theme-next，然后根目录运行下述命令：12hexo cleanhexo s -g这样，便成功应用新的next主题，浏览器访问 :http://localhost:4000，查看一下新的博客页面。可以看到，博客变得非常清爽了，（可能和你实际看到的，略有不同，没有关系）。这款主题包含4种风格，默认的是Muse，也可以尝试其他风格。具体操作：打开D:\blog\_config.yml，定位到Schemes，想要哪款主题就取消前面的#，我的博客使用的是Pisces风格。12345# Schemes#scheme: Muse#scheme: Mistscheme: Pisces#scheme: Gemini2. 模板美化接下来进行模板的美化。根据网页的结构布局，将从以下几个部分进行针对性地美化：总体侧边栏页脚文章重要的文件美化需要主要是对几个模板文件进行修改和增删。为了便于后续进行操作，先列出文件名和所在的位置：站点文件。位于站点文件夹根目录内：D:/blog/_config.yml主题文件。位于主题文件夹根目录内：D:/blog/themes/next/_config.yml自定义样式文件。位于主题文件夹内：D:\blog\themes\hexo-theme-next\source\css_custom\custom.styl2.1. 总体布置2.1.1. 设置中文界面站点文件: language: zh-Hans如果中文乱码，记事本另存为utf-8，最好不要用记事本编辑，用notepad。2.1.2. 动态背景主题文件： canvas_nest: true背景的几何线条是采用的nest效果，一个基于html5 canvas绘制的网页背景效果，非常赞！来自github的开源项目canvas-nest：https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md如果感觉默认的线条太多的话，可以这么设置：打开 next/layout/_layout.swig，在 &lt; /body&gt;之前添加代码(注意不要放在&lt; /head&gt;的后面)：1234&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot;color=&quot;233,233,233&quot; opacity=&apos;0.9&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125;说明：color ：线条颜色, 默认: ‘0,0,0’；三个数字分别为(R,G,B)opacity: 线条透明度（0~1）, 默认: 0.5count: 线条的总数量, 默认: 150zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -12.2. 侧边栏美化2.2.1. 添加博客名字和slogan修改站点文件如下：123456789101112# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: 高级农民工 # 更改为你自己的subtitle: Beginner's Mind description:keywords: python,hexo,神器,软件author: 高级农民工language: zh-Hanstimezone:2.2.2. 菜单设置文件路径：D:\blog\themes\hexo-theme-next\languages\zh-Hans.yml修改如下：1234567891011menu: home: 首&amp;emsp;&amp;emsp;页 archives: 归&amp;emsp;&amp;emsp;档 categories: 分&amp;emsp;&amp;emsp;类 tags: 标&amp;emsp;&amp;emsp;签 about: 关于博主 search: 站内搜索 top: 最受欢迎 schedule: 日程表 sitemap: 站点地图 # commonweal: 公益404注意：两字的中间添加&amp;emsp;&amp;emsp;可实现列对齐。2.2.3. 新建标签、分类、关于页面分别运行命令：123hexo new page &quot;tags&quot; hexo new page &quot;categories&quot; hexo new page &quot;about&quot;然后，打开D:\blog\source就可以看到上述三个文件夹。要添加关于博主的介绍，只需要在/about/index.md文件中，用markdown书写内容即可，写完后运行：hexo d -g，便可看到效果。2.2.4. 侧栏社交链接图标设置可以添加你的github、Email、知乎、简书等社交网站账号。主题文件：12345678910111213141516171819202122232425# ---------------------------------------------------------------# Sidebar Settings 侧栏社交链接图标设置# ---------------------------------------------------------------# Social Links.# Usage: `Key: permalink || icon`# Key is the link label showing to end users.# Value before `||` delimeter is the target permalink.# Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.social: GitHub: https://github.com/makcyun || github E-Mail: mailto:johnny824lee@gmail.com || envelope #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skypesocial_icons: enable: true icons_only: false transition: false2.2.5. 添加头像并美化博客添加头像有两种方法：第一种是放在本地文件夹中：D:\blog\public\uploads，并且命名为avatar.jpg。第二种是将图片放在七牛云中，然后传入链接。推荐这种方式，可以加快网页打开速度。站点文件任意行添加下面代码：123456# 添加头像# avatar: /uploads/avatar.jpg #方法1本地图片avatar: http://media.makcyun.top/18-8-3/40685653.jpg # 方法2网络图片注意：uppoads文件夹是在主题里的文件夹，没有则新建D:\blog\themes\hexo-theme-next\source\uploads\avatar.jpg头像变圆形可参考：http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.htmlD:\blog\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;/*再进一步想点击产生旋转效果，就继续在该文件下方添加代码：*/img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125;2.3. 页脚美化建站时间设置12345678910# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link.# Set rss to specific value if you have burned your feed already.rss:footer: # Specify the date when the site was setup. # If not defined, current year will be used. # 建站年份 since: 2018 #根据实际情况修改2.3.1. 隐藏powered By Hexo/主题文件路径： D:\blog\themes\hexo-theme-next\layout_partials\ footer.swig更改该文件下面的代码：123456&lt;div class="theme-info"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;用注释两行如下语句，也可以直接删除掉这段代码：123456&lt;!--&lt;div class="theme-info"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;--&gt;2.3.2. next版本隐藏继续在上面文件中修改代码如下：123456789# 用&lt;!--注释语句--&gt;&#123;% if theme.footer.theme.enable %&#125; &lt;!--&lt;div class="theme-info"&gt;&#123;# #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt;--&gt;&#123;% endif %&#125;2.3.3. 时间和用户名之间添加心形主题文件：建站时间下面修改icon: heart12345678910111213footer: # Specify the date when the site was setup. # If not defined, current year will be used. # 建站年份 since: 2018 # Icon between year and copyright info. # 年份后面的图标，为 Font Awesome 图标 # 自己去纠结 http://fontawesome.io/icons/ # 然后更改名字就行，下面的有关图标的设置都一样 # Icon between year and copyright info. #icon: user icon: heart如果还想让心变成跳动的红心，则继续在:上面的footer.swig文件中修改：&lt;span class=&quot;with-love&quot;&gt;为 &lt;span class=&quot;with-love&quot; id=&quot;heart&quot;&gt; #一定要加id=”heart”12345678&lt;div class="copyright"&gt;&#123;##&#125;&#123;% set current = date(Date.now(), "YYYY") %&#125;&#123;##&#125;&amp;copy; &#123;% if theme.footer.since and theme.footer.since != current %&#125;&#123;&#123; theme.footer.since &#125;&#125; &amp;mdash; &#123;% endif %&#125;&#123;##&#125;&lt;span itemprop="copyrightYear"&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt; &lt;span class="with-love"&gt; &lt;i class="fa fa-&#123;&#123; theme.footer.icon &#125;&#125;"&gt;&lt;/i&gt; &lt;/span&gt; &lt;span class="author" itemprop="copyrightHolder"&gt;&#123;&#123; theme.footer.copyright || config.author &#125;&#125;&lt;/span&gt;在自定义文件中添加如下代码：1234567891011121314// 1 页脚加闪烁红心// 自定义页脚跳动的心样式@keyframes heartAnimate &#123; 0%,100%&#123;transform:scale(1);&#125; 10%,30%&#123;transform:scale(0.9);&#125; 20%,40%,60%,80%&#123;transform:scale(1.1);&#125; 50%,70%&#123;transform:scale(1.1);&#125;&#125;#heart &#123; animation: heartAnimate 1.33s ease-in-out infinite;&#125;.with-love &#123; color: rgb(192, 0, 39);&#125;接着在自定义custom.styl文件中，添加以下代码：1234567891011121314// 1 页脚加闪烁红心// 自定义页脚跳动的心样式@keyframes heartAnimate &#123; 0%,100%&#123;transform:scale(1);&#125; 10%,30%&#123;transform:scale(0.9);&#125; 20%,40%,60%,80%&#123;transform:scale(1.1);&#125; 50%,70%&#123;transform:scale(1.1);&#125;&#125;#heart &#123; animation: heartAnimate 1.33s ease-in-out infinite;&#125;.with-love &#123; color: rgb(192, 0, 39); # rgb可随意修改&#125;2.3.4. 页脚显示总访客数和总浏览量首先，在上述footer.swig文件首行添加如下代码：1234567891011121314151617181920212223242526272829303132333435363738394041&lt;script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt;#接着修改相应代码：# 添加总访客量&lt;span id="busuanzi_container_site_uv"&gt; 访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次&lt;/span&gt;&#123;% if theme.footer.powered %&#125; &lt;!--&lt;div class="powered-by"&gt;&#123;# #&#125;&#123;&#123; __('footer.powered', '&lt;a class="theme-link" target="_blank" href="https://hexo.io"&gt;Hexo&lt;/a&gt;') &#125;&#125;&#123;##&#125;&lt;/div&gt;--&gt;&#123;% endif %&#125;# 添加'|'符号&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.custom_text %&#125; &lt;div class="footer-custom"&gt;&#123;# #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;# 添加总访问量&lt;span id="busuanzi_container_site_pv"&gt; 总访问量:&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次&lt;/span&gt;# 添加'|'符号&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125;# 添加博客全站共：&lt;div class="theme-info"&gt; &lt;div class="powered-by"&gt;&lt;/div&gt; &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt;2.4. 文章美化2.4.1. 显示统计字数和估计阅读时长修改主题文件：1234567891011# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcount# 显示统计字数和估计阅读时长# 注意：这个要安装插件，先进入站点文件夹根目录# 然后：npm install hexo-wordcount --savepost_wordcount: item_text: true wordcount: true min2read: true totalcount: false separated_meta: false注意，做了以上修改后，发现字数只显示了数字并没有带相应的单位:字和分钟。因此，还需做如下修改：打开D:\blog\themes\hexo-theme-next\layout\_macro\ **post.swig**文件，添加单位：1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123;% if theme.post_wordcount.wordcount or theme.post_wordcount.min2read %&#125; &lt;div class="post-wordcount"&gt; &#123;% if theme.post_wordcount.wordcount %&#125; &#123;% if not theme.post_wordcount.separated_meta %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt; &#123;% endif %&#125; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-file-word-o"&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.post_wordcount.item_text %&#125; &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.wordcount') &#125;&#125;&amp;#58;&lt;/span&gt; &#123;% endif %&#125; &lt;span title="&#123;&#123; __('post.wordcount') &#125;&#125;"&gt; &#123;&#123; wordcount(post.content) &#125;&#125; 字 &lt;/span&gt; &#123;% endif %&#125; &#123;% if theme.post_wordcount.wordcount and theme.post_wordcount.min2read %&#125; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt; &#123;% endif %&#125; &#123;% if theme.post_wordcount.min2read %&#125; &lt;span class="post-meta-item-icon"&gt; &lt;i class="fa fa-clock-o"&gt;&lt;/i&gt; &lt;/span&gt; &#123;% if theme.post_wordcount.item_text %&#125; &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.min2read') &#125;&#125; &amp;asymp;&lt;/span&gt; &#123;% endif %&#125; &lt;span title="&#123;&#123; __('post.min2read') &#125;&#125;"&gt; &#123;&#123; min2read(post.content) &#125;&#125; 分钟 &lt;/span&gt; &#123;% endif %&#125; &lt;/div&gt; &#123;% endif %&#125; &#123;% if post.description and (not theme.excerpt_description or not is_index) %&#125; &lt;div class="post-description"&gt; &#123;&#123; post.description &#125;&#125; &lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt; &lt;/header&gt; &#123;% endif %&#125;2.4.2. 添加阅读全文实现在主页只展示部分文字，其他文字隐藏起来，通过点击’阅读更多’来阅读全文。方法就是写每一篇文章的时候，在必要的地方添加&lt;!-- more --&gt;即可。例如：1234567891011121314---title: 4块钱,用Github+Hexo搭建你的个人博客：搭建篇id: hexo01images: http://media.makcyun.top/18-8-3/89578286.jpgcategories: hexo博客tags: [hexo,个人博客,github]keywords: hexo,搭建博客,github pages,next---4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。 &lt;!-- more --&gt;摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。2.4.3. 显示每篇文章的阅读量参考这个教程即可：http://www.jeyzhang.com/hexo-next-add-post-views.html在这个过程中发现了一个问题：pc端正常显示阅读量，但是移动端没有显示具体的阅读量。解决办法：在leancloud网站上，进入安全中心，检查web安全域名列表中是否添加了http：开头的域名，如果没有，则添加上应该就能解决，例如，我的：1http://makcyun.top/2.4.4. 文章摘要配图参考这个教程即可：http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/附上我的设置：在自定义文件中添加如下代码：1234567891011121314151617181920212223242526// img.img-topic &#123;// width: 100%;//&#125;//图片外部的容器方框.out-img-topic &#123; display: block; max-height:350px; //图片显示高度，如果不设置则每篇文章的图片高度会不一样，看起来不协调 margin-bottom: 24px; overflow: hidden;&#125;//图片img.img-topic &#123; display: block ; margin-left: .7em; margin-right: .7em; padding: 0; float: right; clear: right;&#125;// 去掉图片边框.posts-expand .post-body img &#123; border: none; padding: 0px;&#125;2.4.5. 添加打赏功能参考下面的教程：https://www.cnblogs.com/mrwuzs/p/7943337.htmlhttps://blog.csdn.net/lcyaiym/article/details/76796545以上，包括了博客美化的大部分操作。如果，你觉得还不够，想做得更精致一些，那么推荐一个非常详细的美化教程：https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl本文完。]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4块钱,用Github+Hexo搭建你的个人博客：搭建篇]]></title>
    <url>%2F2018%2F07%2F06%2Fhexo01.html</url>
    <content type="text"><![CDATA[4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。【更新于2018/7/14】摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。好在参考了很多大佬的教程后顺利搭建完成，但过程中还是踩了一些坑。这里及时进行总结，作为博客的第一篇文章。一、前言1 网上有很多现成的博客不用，为什么要自己搭建?可能有人会说：很多网站都能写博客，而且博客现在其实都有点过时了，为什么还要自己去搞？这里我说一下我想自己搭建的两点原因：一、网上的多数博客大家都共用一套相同的模板界面，没有特点、界面也杂乱充斥着很多不相关的东西，不论是自己写还是给人看，体验都不好。二、拥有一个你自己可以起名字的博客网站，里面的任何内容完全由你自己决定，这感觉是件很酷的事。这些普通的网站博客和一些个人博客，哪个好看，高下立判吧。vs&nbsp;vs更多个人博客：litten &nbsp; http://litten.me/Ryan &nbsp; http://ryane.top/liyin &nbsp; https://liyin.date/reuixiy &nbsp; https://reuixiy.github.io/Tranquilpeak &nbsp; https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/2 搭建博客难不难？我之前认为搭建博客是一件只有能程序猿才能做出来的高大上的活。其实，只要跟着网上的教程一步步做下去，一个小时不到就可以搭建好你自己的个人博客。所以，搭建博客其实很简单。只不过，如果你想把博客做得好看一些的话，才会花费一些精力。二、开始搭建博客如果看到上面那些精美的博客，你已经心动了，那就开始动手吧。下面正式开始博客搭建步骤。搭建教程参考搭建博客的教程网上一搜一大堆，为了节省你的搜索时间，这里我筛选出了下面几篇很棒的教程，我基本都是跟着一步步做下来的。小白独立搭建博客2018，你该搭建自己的博客了！手把手教你用Hexo+Github 搭建属于自己的博客操作平台:Win7 64位。相关名词解释：Hexo：一种常用的博客框架，有了它建立博客非常简单。你可以认为它是一种博客模板，只不过它比普通网站的那种博客模板要好看地多，并且有很高度的自定义性，只要你愿意，你可以建立一个独一无二的博客来。若想详细了解Hexo的使用，移步 Hexo官方网站 https://hexo.io/zh-cn/docs/。Github：一个全世界程序猿聚集的知名网站。免费的远程仓库和开源协作社区。我们需要利用网站里的Github Pages功能来托管需发布到网上的博客的相关文件和代码。Git： 一种版本控制系统。我们在自己的本地电脑写博客，如何把博客同步到Github，然后发布到网上去？就需要用这个软件去写几行代码然后就能搞定，后期用的最多的就是它。Node.js： 提供JavaScript的开发环境，安装好以后就不用跟它再打交道，所以不用太关注它。1 软件安装配置搭建博客需要先下载2个软件：Git和Nodejs。软件安装过程很简单，一直点击Next默认直到安装完成就行了。Git官网：https://git-scm.com/download/win安装完，打开cmd窗口运行下面命令，如果有返回版本信息说明安装成功。git –version Nodejs官网：https://nodejs.org/en/download/同样，安装完有返回版本信息说明安装成功，见下图。node -v npm -v 至此，软件安装步骤完成。2 安装Hexo博客框架安装hexo这里开始就要用到使用频率最高的Git软件了。桌面右键点击git bash here选项，会打开Git软件界面，输入下面每行命令并回车：12npm install hexo-cli -gnpm install hexo-deployer-git --save第一句是安装hexo，第二句是安装hexo部署到git page的deployer。代码命令看不懂没关系，一是这些命令之后几乎不再用到，二是用多了你会慢慢记住。设置博客存放文件夹你需要预想一下你要把博客文件新建在哪个盘下的文件夹里，除了c盘都可以，例如d盘根目录下的blog文件夹，紧接着在桌面输入下面命令并回车：hexo init /d/blog cd /d/blog npm install *注：/d/bog可以更改为你自己的文件夹* 有的教程是先新建博客文件夹，在该文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，再执行以下操作。但是我操作过程中出现过这样的失败提示：hexo:conmand not found，但我执行上面的命令时就没有出现该问题。​hexo initnpm install查看博客效果至此，博客初步搭建好，输入下面一行本地部署生成的命令：hexo s -g 然后打开浏览器在网址栏输入：localhost:4000就可以看到博客的样子，如果无法打开，则继续输入下面命令：npm install hexo-deployer-git --save hexo clean hexo s -g 打开该网址，你可以看到第一篇默认的博客：Hello World。但看起来很难看，后续会通过重新选择模板来对博客进行美化。现在你就可以开始写博客了，但是博客只能在你自己的电脑上看到，别人无法在网上看到你的博客。接下来需要利用前面提到的的Github Pages功能进行设置，设置完成之后别人通过搜索就可以看到你的博客。3 把你的博客部署到Github Pages上去这是搭建博客相对比较复杂也是容易出错的一部分。1. Github账号注册及配置如果你没有github帐号，就新建一个，然后去邮箱进行验证；如果你有帐号则直接登录。官网：https://github.com/配置步骤：建立new repository只填写username.github.io即可，然后点击create repositrory。注意：username.github.io 的username要和用户名保持一致，不然后面会失败。以我的为例：开启gh-pages功能点击github主页点击头像下面的profile,找到新建立的username.github.io文件打开，点击settings，往下拉动鼠标到GitHub Pages。如果你看到上方出现以下警告：GitHub Pages is currently disabled. You must first add content to your repository before you can publish a GitHub Pages site不用管他，点击选择choose a theme，随便选择一个，（之后我们要更改这些丑陋的模板），然后select theme保存就行了。接下来的几个步骤参考教程1即可。主要步骤包括：git创建SSH密钥在GitHub账户中添加你的公钥测试成功并设置用户信息将本地的Hexo文件更新到Github库中hexo部署更新博客经过以上几步的操作，顺利的话，你的博客可以发布到网上，其他人也可以通过你的网址username.github.io（我的是makcyun.github.io）访问到你的博客。4 赶紧新建个博客试试接下来你可以自己新建一个文档来写下你的第一篇博客并在网页上测试。同样在根目录D:\blog中运行下面命令：hexo new 第一篇博客 *注：第一篇博客名称可以随便修改* 然后打开D:\blog\source\_posts文件夹，就可以看到一个第一篇博客.md的文件。用支持markdown语法的软件打开该文件进行编辑即可。编辑好以后，运行下述命令：​hexo cleanhexo d -g然后，在网址中输入username.github.io即可看到你的博客上，出现第一篇博客这篇新的文章。至此，你的个人博客初步搭建过程就完成了。但是，现在还存在两个问题你可能想解决：markdown语法是什么，如何用软件编写博客？网址是username.github.io，感觉很奇怪，而我的博客网址怎么是www开头的？好，下面来讲解一下。第一个问题关于markdown语法介绍：markdown——入门指南当你大致了解markdown语法后，如何用markdown写博客呢？不妨参考这两篇详细教程：Markdown語法說明HEXO下的Markdown语法(GFM)写博客接下来你要一个可以写markdown语法的软件，这里推荐两款软件。Windows下使用Markdown Pad2, Mac下使用Mou。我用的是Markdown Pad2，这款软件是付费软件，但网上有很多破解版，我这里将软件上传到了百度网盘，如需请取。MarkdownPad2： https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA 密码：y9zh安装好后，就可以打开刚才的第一篇博客.md，开始尝试写你的第一篇博客了。比如这是我用markdownpad写的博客原稿。可以看到文档里面都是字符，没有图片这些。所以只需要用键盘专注于打字就行了，不需要像word那么复杂，还要用鼠标插入标题样式、图片这些操作。第二个问题我的网址不是默认的username.github.io，是因为我购买了一个域名，然后和username.github.io进行了关联，这样我的博客网址变成了我的域名。在哪里购买域名呢？首推去 阿里云官网 购买。你可以随意起你喜欢的名字，然后在该网站进行搜索，没有人占用的话你就可以购买该域名。不同的后缀价格不同。可以看到.com、 .net等会比较贵，最便宜的这两年新出的.top域名，只要4块钱一年，我购买的就是这种。购买完域名以后，需要做以下几个步骤：实名认证修改DNS域名解析新建CNAME文件1 实名认证在修改DNS之前，必须要阿里云官网实名认证成功，用淘宝账户登录然后填写相关信息即可。2 修改DNS实名认证成功后，进入管理界面，依次点击：修改DNS为：f1g1ns1.dnspod.netf1g1ns2.dnspod.net3 域名解析DNS修改好以后，到DNSPOD这个网站去解析你的域名。首先，微信登录并注册 https://www.dnspod.cn/，点击域名解析，添加上你的域名。接着，添加以下两条记录即可。注意：makcyun.github.io.需换成你自己的名称，另外最后有一个“.”4 新建CNAME文件在博客根目录文件夹下,例如我的D:\blog\source，新建名为CNAME的记事本文件，去掉后缀。在里面输入你的域名，例如我的：www.makcyun.top即可，保存并关闭。注意：这里填不填写www前缀都是可以的，区别在于填写www，那么博客网址就会以www开头，例如：www.makcyun.top；如果不填写，博客网址是：makcyun.top，二者都可以，看你喜欢。完成以上4步之后，根目录下再次运行：hexo d -g 这时，输入你在记事本里的域名网址，即可打开你的博客。至此，你的博客就换成了你想要的网址，别人也可以通过这个网址访问到你的博客。到这里，博客的初步搭建就算完成了，顺利的话不到1小时就能完成，如果中间出现差错，保持耐心多试几次应该就没问题。此时，还有一个比较重要的问题就是，你可能会觉得你的博客不够美观，不如我博客里前面提到的那几个博客，甚至也没有我的好看。如果你还愿意折腾的话，下一篇文章，我会以我的博客为例，讲一讲如何进行博客的美化。]]></content>
      <categories>
        <category>hexo博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
