<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>高级农民工</title>
  
  <subtitle>Beginner&#39;s Mind</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.makcyun.top/"/>
  <updated>2019-05-31T02:39:09.960Z</updated>
  <id>https://www.makcyun.top/</id>
  
  <author>
    <name>高级农民工</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>毕业两年内落户深圳攻略</title>
    <link href="https://www.makcyun.top/2019/05/31/life12.html"/>
    <id>https://www.makcyun.top/2019/05/31/life12.html</id>
    <published>2019-05-31T08:16:24.000Z</published>
    <updated>2019-05-31T02:39:09.960Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri May 31 2019 10:40:35 GMT+0800 (GMT+08:00) --><p>深圳落户二三事。</p><a id="more"></a><p><strong>摘要：</strong>大学生深圳落户攻略。</p><p>上周回深圳办完了落户的最后一道手续，几经辗转，身份从新疆人、四川人、东莞人终于又变成了深圳人。</p><p>转过这么多次户口，深圳落户效率最高、流程手续最简单。20 分钟拿到户口本，一周后拿到身份证。下面试着梳理下落户流程，分享给有同样需求的人。</p><p>网上关于深圳落户的攻略已有不少，个人建议<strong>看最新和符合自身实际情况的攻略，以免走弯路</strong>。</p><p>先简单说下个人情况，如果发现跟你差别很大，建议就不要作参考了。</p><p>本人 2017 年 6 月硕士毕业，大学期间户口迁到了学校，毕业后到东莞工作，报到证、档案、户口都跟着迁到了东莞。深圳落户主要分两种，应届生和在职落户，应届生落户相对容易，所以我打算走应届生落户。深圳对于应届生的定义是：<strong>即将毕业和毕业两年内的都算应届生</strong>，所以我即将 2 年到期，再不办就不能走应届生落户了。</p><p>2019 年 5 月份开始着手办理，5 月 21 号顺利完成落户。由于我人在北京，所以全部事务都是找人代办，只有 21 号去深圳派出所是自己去的。<strong>去派出所是必须本人亲自到场办理</strong>。</p><p><strong>户口性质是派出所集体户口，没有时间期限，挂多久都可以，只要你自己不迁出来，派出所是不会主动撵你走的</strong>。</p><p>先简单说下落户流程。</p><p>落户深圳不难，时间快，需要的材料也不多。正常情况下半个月能搞好，也只需要做两件事：<strong>通过资格审核、办理落户材料。</strong></p><p>怎么样才能通过资格审核呢？很简单，只要你有下面这四样东西就可以了：</p><ul><li>身份证</li><li>毕业证</li><li>教育部《学历证书电子注册备案表》</li><li>《毕业生接收申请表》</li></ul><p>身份证和毕业证是手上就有的，《学历证书电子注册备案表》花几分钟就能在学信网上下载下来。这样就只剩下《毕业生接收申请表》了，这个表是落户深圳必须要填写的一个表，表的内容就是一些个人基本情况以及想落户到深圳哪个区哪个派出所，半个小时差不多也就能填好。</p><p>有了这四项材料，上传到系统中随即就能通过系统审核。这样就算完成了第一件事，一天的时间就能搞定。</p><p>审核通过表示有落户资格，接下来就只需做第二件事：准备带去派出所办理落户的几项材料。</p><p>具体需要哪几项材料呢，也只有寥寥这几样：</p><ul><li>身份证</li><li>结婚证（已婚的话）</li><li>《毕业生接收申请表》</li><li>身份证照相回执</li><li>户口迁移证</li><li><strong>不需要一寸证件照照片、也不需要毕业证和学位证。</strong></li></ul><p>身份证和结婚证也是现成的，《毕业生接收申请表》第一步已经有了，打印出来带上即可。身份证照相回执也很容易获得，去照相馆花几十块前就能拿到，还有一种既便宜又省事的方法（后续说）。</p><p>插一句，<strong>身份证照相回执照片就是你之后的身份证头像</strong>，尽量弄好看些，毕竟这张身份证会跟你 20 年。</p><p>最后就只剩下「户口迁移证」这一样证件，这个证是办理户口最难拿到的东西，<strong>落户的大部分时间就是为了得到它</strong>，后面会详说。</p><p>到这里，就得到了落户全部需要的材料，把这些材料带到派出所办理，快的话半个小时就能成深圳人了。</p><p>是不是不难？下面展开说说如何实操。</p><hr><p>仍然从「资格审核」和「办理派出所落户材料」这两方面来说。</p><p>要想通过资格审核需要做这几件事：</p><ul><li><p>第一步：注册账号</p></li><li><p>第二步：填写《毕业生接收申请表》</p></li><li><p>第三步：打印扫描申请表（后面几个地方会用到）</p></li><li><p>第四步：上传材料</p></li></ul><p>在开始之前，最重要的事就是：<strong>用 IE 浏览器！用 IE 浏览器！用 IE 浏览器！</strong>，不然会出现很多 Bug。</p><h2 id="通过资格审核"><a href="#通过资格审核" class="headerlink" title="通过资格审核"></a>通过资格审核</h2><h3 id="第一步：注册账号"><a href="#第一步：注册账号" class="headerlink" title="第一步：注册账号"></a>第一步：注册账号</h3><p>打开 <a href="http://hrss.sz.gov.cn/" target="_blank" rel="noopener">深圳人力资源和社会保障局网</a> 这个网站，选择：<a href="https://hrsspub.sz.gov.cn/rcyj/" target="_blank" rel="noopener">人才引进（毕业生、在职人才引进）测评与申报系统</a></p><p><img src="http://media.makcyun.top/FvFC-LZOKtFqJV2hoBbfq8hpA3Fr" alt=""></p><p>此时会跳转到到广东政务网，填写账号密码登陆，如果还没有注册就注册一下。<strong>账号和密码一定要记住！因为后续会经常要登陆这个网站。</strong></p><p><img src="http://media.makcyun.top/FkHk97UsA1Lm0Ncm99OzA9Ki36_w" alt=""></p><p>选择「接收毕业生」：</p><p><img src="http://media.makcyun.top/FlUq2NJ_2rIFjJNI9PX2sI1SQ1Wb" alt=""></p><p>（另外可以看到，如果走「在职落户」，也是在这里面操作）</p><h3 id="第二步：填写《毕业生接收申请表》"><a href="#第二步：填写《毕业生接收申请表》" class="headerlink" title="第二步：填写《毕业生接收申请表》"></a>第二步：填写《毕业生接收申请表》</h3><p>填写必须要填的红色 * 号的内容即可。</p><p>个人信息没什么说的，<strong>重点注意落户地点</strong>，需要<strong>选择落户到哪个区和该区上具体的派出所</strong>。落到哪个区跟后面能拿多少补贴是直接挂钩的，因为不同的区补贴政策不同。如果你不在意补贴的话，落户哪个区都没有关系了，自行斟酌。</p><p>我落的是：<strong>福田区莲花派出所</strong>。确定好区之后，具体落哪个派出所则没有区别。推荐莲花派出所，离地铁站近，日后办事方便，办事效率挺高。</p><p>在深圳十区中，南山区实力最强，IT 企业和人才都多，所以没有补贴。福田区次之，本科生没有补贴，硕士有 12,500 元的补贴。排名靠后的其他区诸如宝安、龙华、龙岗等，硕士补贴则为 25,000 元，本科生也有补贴，具体多少没太关注，可以关注《深圳本地宝》公众号上查，之后补贴内容会详细讲。</p><h3 id="第三步：打印《毕业生接收申请表》"><a href="#第三步：打印《毕业生接收申请表》" class="headerlink" title="第三步：打印《毕业生接收申请表》"></a>第三步：打印《毕业生接收申请表》</h3><p>《毕业生接收申请表》填写好之后，右键另存为 PDF 保存下来，找个打印店打印两份，签上自己名字再扫描。打印出来的两份表之后会用到。扫描件需要上传，见下一步。</p><p><img src="http://media.makcyun.top/FuySRLfMxJNC50iKYRyiASKn0_5H" alt=""></p><p>注意红色箭头处：一旦你按照上面的步骤注册填写好了这份申请表，那么 90 天之内必须要完成下一步的证件上传，否则就失效了，需要重新申请。</p><h3 id="第四步：上传材料"><a href="#第四步：上传材料" class="headerlink" title="第四步：上传材料"></a>第四步：上传材料</h3><p><img src="http://media.makcyun.top/FtDe00Yg_nyCvxFliI_yK7es76uv" alt=""></p><p>接下来就要在系统中上传四份材料，分别是：</p><ul><li>《毕业生接收申请表》</li><li>身份证正反面</li><li>毕业证（不是学位证，办户口全程都用不到学位证）</li><li><a href="http://www.chsi.com.cn" target="_blank" rel="noopener">教育部学历证书电子注册备案表</a></li></ul><p>前三项资料你有现成的了，最后一个电子注册备案表在<a href="http://www.chsi.com.cn" target="_blank" rel="noopener">学信网</a>上下载下来就行，有效期尽可能选 180 天。</p><p>到这里就可以提交材料了，审核通过的话，很快（我是半个小时）就会收到一条来自深圳公安的短信：</p><p><img src="http://media.makcyun.top/FugcDg1s0OMWWof9uARMKXPjPpUq" alt=""></p><p>意思就是通过系统审核了，即上面说的第一件事。给你发了一个指标卡编号，有这个编号才能去派出所落户，有效期到年底，也就是最晚要在 12 月 30 之前去到派出所办理落户，这之间哪天去都行。</p><p>另外还一个就是去办理之前，需要关注「深圳公安」公众号进行取号预约，一般头天预约第二天就行，如果是想预约周末，那可能要很早预约。</p><hr><h2 id="办理派出所落户材料"><a href="#办理派出所落户材料" class="headerlink" title="办理派出所落户材料"></a>办理派出所落户材料</h2><p>到这里就要准备前面说过的派出所落户材料：</p><ul><li>身份证</li><li>结婚证（已婚的话）</li><li>《毕业生接收申请表》</li><li>户口迁移证</li><li>身份证照相回执</li></ul><p>前三个已有，只剩下「户口迁移证」和「身份证照相回执」。</p><p>先说如何办理「户口迁移证」。以我户口所在地东莞为例，去公安局领取户口迁移证需要这几项材料：</p><ul><li>《毕业生接收申请表》</li><li>户口本</li><li>身份证</li><li>改派到深圳的报到证</li></ul><p>同样，前三样是已有的，只剩下一个「改派到深圳的报到证」没有。当初毕业时学校发了报到证，到东莞工作后交给了当地的人才管理办公室，现在要迁到深圳，那就需要一个新的报到证。</p><h3 id="第一步：改派报到证"><a href="#第一步：改派报到证" class="headerlink" title="第一步：改派报到证"></a>第一步：改派报到证</h3><p>怎么获得新的报到证呢，最好直接联系学校就业办，我们学校是叫「就业指导服务中心」，问过学校老师后说，要想开新的报到证需要两样材料：</p><ul><li>原来的报到证</li><li>离职证明</li></ul><p>也就是说，得拿回原来的旧报到证才能开新的报到证，另外还需要离职证明。</p><p>于是打电话给当地人才办，说明我的情况（从原先公司离职，要迁户口到深圳）后，问如何拿回报到证，工作人员说需要这两项材料：</p><ul><li>离职证明</li><li>人事代理协议</li></ul><p>还好我保留着当时公司的离职证明，不然就很难办了。由于我的档案是保管在人才办的，所以当时签了人事代理协议以让他们代为保管档案。</p><p>把这两项材料交给他们就可以拿回报到证，把报到证再寄回学校，让同学去就业指导中心就能拿到新的报到证，也就是改派的报到证。这时，需要填写报到证抬头，填写：<strong>深圳市人力资源和社会保障局</strong>就行了。</p><p><img src="http://media.makcyun.top/luEED585ZXqKax1CJ-c1JS4jMnvv" alt=""></p><p>这里注意几点：</p><ul><li>报到地址是默认填写为罗湖区的，这个跟要迁的户口所在区没关系，不用管</li><li>报到证有效期是两个月，两个月没有用就作废</li></ul><p>有了报到证其实落户就完成了一半。</p><h3 id="第二步：办户口迁移证"><a href="#第二步：办户口迁移证" class="headerlink" title="第二步：办户口迁移证"></a>第二步：办户口迁移证</h3><p>拿到改派《报到证》后连同这三项材料的<strong>原件和复印件</strong>就可以去<strong>户口所在公安局</strong>办理户口迁移证了。</p><ul><li>《毕业生接收申请表》</li><li>户口本</li><li>身份证</li></ul><p>工作人员会问你户口迁往地址是哪里，填你所要落的派出所地址就行，比如我的是：<strong>深圳市福田区莲花派出所</strong>。</p><p>很快当场就能拿到户口迁移证，注意一点，户口迁移证原则上是要本人办理的，如果本人办不了那就只有两类人可以代办。一类是和你在一个户口本上的亲属，一类是夫妻有结婚证。</p><h3 id="第三步：身份证照相回执"><a href="#第三步：身份证照相回执" class="headerlink" title="第三步：身份证照相回执"></a>第三步：身份证照相回执</h3><p>有了户口迁移证就只需要办理最后一样证件：身份证照相回执，有两种方法可以拿到。</p><p>第一种是去照相馆。到深圳随便找家照相馆说要办理身份证，让他照个相给个回执就可以了，加急一般当天能拿到。</p><p>还一种更好的方法是自己制作，只需花 1.5 元很快就可以做好。</p><p>打开这个网站：<a href="https://www.rzzx.com.cn/webs/PhotoViews/UploadPrint.aspx" target="_blank" rel="noopener">证件数码相片质量检测系统</a>，注册填写基本信息，然后上传照片，照片要符合下面两个要求才能通过审核：</p><ul><li>大小为 14K 至28 K，小于14K 或大于 28K 都过不了审核</li><li>长宽分别为 358 * 441 像素</li></ul><p>通过审核之后就可以保存回执再打印出来就可以了，类似下面这样：</p><p><img src="http://media.makcyun.top/FkR_7y_rUSYzqMfCYkM4ihZVq0tx" alt=""></p><p>插一句，此处有商机，<strong>在淘宝上办这个照相回执要 5 块钱，利用信息差赚钱</strong>。</p><p>到这儿我们就有了办理户口所需要的全部资料，最后一步就是带着这些资料去落户。</p><h3 id="第三步：派出所迁入户口"><a href="#第三步：派出所迁入户口" class="headerlink" title="第三步：派出所迁入户口"></a>第三步：派出所迁入户口</h3><p>去派出所落户之前，确保已经早先已经收到了深圳公安发的短信。</p><p>然后微信关注「深圳公安」微信公众号，依次选择：业务办理—业务预约申请—普通户政业务—“毕业生入户（不需本市准迁）”。预约办理时间，然后按照指定时间提前 15 分钟去派出所办理就行。</p><p>工作人员会当场给两张户口页，一张是自己的，一张是集体户口首页。保存好，万一丢了需要本人到派出所补办，费用 20 元。</p><p>身份证不能当场拿到，需要十天左右，可以到派出所领，也可以选择邮寄，邮寄只能邮寄到深圳本地地址，可以找在深圳的朋友代领，否则就只能本人回派出所领。</p><h2 id="后续事项"><a href="#后续事项" class="headerlink" title="后续事项"></a>后续事项</h2><p>到这里落户口这件事就算完成，不过之后还有几件重要的事：</p><ul><li>报到</li><li>迁移档案</li><li>申请市区补贴</li></ul><p><strong>这几件事如果有时间就尽快办，如果暂时没时间晚点办也没关系。</strong>比如我暂时不在深圳，便打算晚点回去办。</p><h3 id="报到"><a href="#报到" class="headerlink" title="报到"></a>报到</h3><p>报到的意思就是需要把<strong>《就业报到证》和《毕业生介绍信》交到档案保管单位</strong>，他们会存入个人档案。</p><p>《就业报到证》已经有了，《毕业生介绍信》还没有，在哪里获取呢，很简单。</p><p>当天办完落户后，就可以到申报系统里打印《毕业生介绍信》，大致是这个样子：</p><p><img src="http://media.makcyun.top/Fum6zIN_2UXy_zWB8qWIfRyVexzf" alt=""></p><p>有了材料就可以交到档案保管单位了，这个单位又在哪儿呢，很简单。</p><p>在《毕业生介绍申请表》下面的小字中有写：</p><p><img src="http://media.makcyun.top/Fp0Qw3GyE5MdIcv2u4zXvqEY7dOu" alt=""></p><p>打电话给福田区人力资源服务中心（0755-82918158），咨询报到材料交到哪里就可以了。</p><h3 id="迁移档案"><a href="#迁移档案" class="headerlink" title="迁移档案"></a>迁移档案</h3><p>户口迁到深圳后，档案也要跟着迁过去。迁到哪里呢，上面的红色箭头处就是。我落的福田区，所以档案保管在福田人力资源中心。</p><p><strong>如何迁移档案呢？咨询两个单位：档案保管单位和接收单位。</strong></p><p>先打电话给档案保管单位，也就是我原户口所在地东莞的人才办，工作人员说<strong>需要两项材料：「调档函」和档案迁移地址</strong>。「调档函」需要档案接收单位开具。</p><p>接着又打电话给档案接收单位：福田人力资源中心，工作人员说需要两项材料：<strong>《毕业生介绍信》和《毕业生接收申请表》</strong>，这两样手上是有的，去现场办理就可以拿到调档函。</p><p>拿着调档函回档案接收单位，他们会把档案寄给接收单位。不需要自己操心。</p><p>额外说一下，户口办好之后，有时间就顺便把档案调了，如果没时间那晚点调也没关系，我不在深圳就打算晚点回去再调。</p><p>到这儿就完成了报到和迁档案这两件事。</p><h3 id="领取市、区两级补贴"><a href="#领取市、区两级补贴" class="headerlink" title="领取市、区两级补贴"></a>领取市、区两级补贴</h3><p>如果你目前有在深圳工作，公司给你缴纳了社保（缴纳多久没有限制，建议至少一个月，且是公司缴纳，个人缴纳无效），就可以计划申请深圳市的「新引进人才租房和生活补贴」。补贴分市里和区里两级补贴。<strong>先申请到市里补贴再申请区级补贴</strong>。</p><p>市级补贴所有人是一样的：</p><ul><li>全日制本科15000元/人</li><li>全日制硕士25000元/人</li><li>全日制博士30000元/人</li></ul><p><strong>市级补贴申请资格条件</strong></p><p><img src="http://media.makcyun.top/Fp1Svx4iNvLK5anvxKQONnHulJMZ" alt=""></p><p>注意这里的<strong>市级补贴分两种：「人才租房补贴」和「人才租房和生活补贴」。后者补贴多，学历要求高</strong>。</p><ul><li><p>具有全日制普通高等教育本科及以上学历，学历以办理引进手续时申报的为准</p></li><li><p>具有深圳户籍</p></li><li><p>在深圳缴纳了社保，申请是社保仍在深圳市</p></li><li><p>本人未享受过购房优惠政策、未正在租住公租房；<br>新调入的在职人才除符合上述规定条件外，还应当符合下列年龄条件：本科的未满 30 周岁、硕士的未满 35 周岁、博士未满 40 周岁。</p></li></ul><p><strong>申请时间限制</strong></p><p><strong>申请市级补贴是有时间限制的，也就是《毕业生介绍信》签发之日起一年内</strong>。超过这个时间就领不了补贴了，一定要注意。</p><p>那就需要搞清楚「《毕业生介绍信》签发之日起」是什么时候。通过上面的《毕业生介绍信》截图可以看到，时间是 2019 年 5 月 15 日。我不太清楚为什么是这个日期，但应该跟两件事有关。</p><p>第一个，我通过资格审核是 5 月 15 日，也是这一天收到了深圳公安的短信。</p><p>第二个，我 5 月 21 日去派出所办理的落户，这一天才有的《毕业生介绍信》。</p><p>符合上面两点后，就可以申请补贴了。</p><p><strong>补贴申请网址</strong></p><p><a href="http://www.gdzwfw.gov.cn/portal/guide/11440300695583248530511026000#matters-part4" target="_blank" rel="noopener">深圳市新引进人才租房和生活补贴</a></p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>尽管我自认上面说的清楚，但你可能还会遇到其他问题，可以选择下面这三种方式咨询，得到的答案是最权威的。</p><p>深圳社保电话：0755-12333 ，注意了拨通后默认是智能语音，<strong>想转接人工服务，依次选择 1-2-2-0 就可以</strong>。</p><p>深圳户政电话：0755-84465000</p><p>工作人员QQ：1840886088</p><p>另推荐一个公众号和一篇攻略。</p><p>一个公众号：<strong>深圳本地宝</strong></p><p>这个公众号有对落户、补贴等事项的详细教程。</p><p>一篇文章：<a href="https://zhuanlan.zhihu.com/p/59507428?utm_source=wechat_session&amp;utm_medium=social&amp;s_s_i=qP8GAGsfBQdqSdH%2FBZeikfXGkGUcxH2bAPMS9wIjSRk%3D&amp;s_r=1&amp;from=groupmessage&amp;isappinstalled=0&amp;wechatShare=1" target="_blank" rel="noopener">2019 年应届毕业生深圳户口自助落户攻略</a></p><p>这篇文章是答主亲身落户经历，内容非常详尽。</p><p>好，以上就是一点深圳落户心得。仔细看完上面说的，落户应该没问题。还有问题，我也木办法。想交流可以加我微信：qqguai001 ，备注（落户）。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri May 31 2019 10:40:35 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;深圳落户二三事。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>pythonchallenge 一个边玩游戏边学 Python 的通关网站</title>
    <link href="https://www.makcyun.top/2019/05/17/Python_learning05.html"/>
    <id>https://www.makcyun.top/2019/05/17/Python_learning05.html</id>
    <published>2019-05-17T08:16:16.000Z</published>
    <updated>2019-05-17T09:41:13.079Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Python 趣味学习网站。</p><a id="more"></a><p>这里是「每周分享」的第 28 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>一个学习 Python 的趣味网站</strong> 。</p><p>最近在网上看到一个非常有意思的 Python 游戏通关网站叫 ：<a href="http://www.pythonchallenge.com/" target="_blank" rel="noopener">pythonchallenge</a>。一共有 33 关，每一关都需要利用 Python 知识解题找到答案，然后进入下一关。很考验对 Python 的综合掌握能力，比如有的闯关需要用到正则表达式，有的要用到简单的爬虫。</p><p>平常学 Python 都是按章节顺序、包或者模块来学，容易前学后忘。正好可以拿这个网站来综合测试一下对 Python 的掌握情况，以便查缺补漏。</p><p><img src="http://media.makcyun.top/win/20190517/zxEI9qCgGEX7.png?imageslim" alt="mark"></p><p>来说说这个网站怎么玩。</p><p><img src="http://media.makcyun.top/win/20190517/ouAsVeqHH94B.png?imageslim" alt="mark"></p><p>这是网站主页面，很有历史感对吧，诞生了已有十几年了。但千万不要因为看着像老古董而小瞧它。</p><p>我们来玩玩看，点击「get challenged」开始挑战。</p><p>第 0 关是 Warming up 热身环节：</p><p>这一关要求是修改 URL 链接，给的提示是电脑上的数学表达式： 2 的 38 次方，所以大概就是需要计算出数值，然后修改url 进入下一关。</p><p>所以这关就是考Python 的基本数值运算，你知道怎么算么？</p><p>随意打开个 IDE，比如 Python 自带终端，只要一行代码就能计算出结果：</p><p><img src="http://media.makcyun.top/win/20190517/jsSvU886Y3xw.png?imageslim" alt="mark"></p><p>把原链接中的 <code>0</code>替换为 <code>274877906944</code>回车就会进入下一关：</p><p><img src="http://media.makcyun.top/win/20190517/ypFIjlp1IuLF.png?imageslim" alt="mark"></p><p>游戏这就正式开始了。图片中的笔记本给了三组字母，很容易发现规律：前面的字母往后移动两位就是后面的字母。</p><p>那么需要做的就是根据这个规律把下面的提示字符串，做位移解密得到真正的句子含义：</p><p>这道题考察字符串编码和 for 循环相关知识，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'''g fmnc wms bgblr rpylqjyrc gr zw fylb. rfyrq ufyr amknsrcpq</span></span><br><span class="line"><span class="string">    ypc dmp. bmgle gr gl zw fylb gq glcddgagclr ylb rfyr'q</span></span><br><span class="line"><span class="string">    ufw rfgq rcvr gq qm jmle. sqgle qrpgle.kyicrpylq()</span></span><br><span class="line"><span class="string">    gq pcamkkclbcb. lmu ynnjw ml rfc spj.'''</span></span><br><span class="line"></span><br><span class="line">text_translate = <span class="string">''</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> text:</span><br><span class="line">    <span class="keyword">if</span> str.isalpha(i):</span><br><span class="line">        n = ord(i)</span><br><span class="line">        <span class="keyword">if</span> i &gt;= <span class="string">'y'</span>:</span><br><span class="line">            n = ord(i) + <span class="number">2</span> - <span class="number">26</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            n = ord(i) + <span class="number">2</span></span><br><span class="line">        text_translate += chr(n)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        text_translate += i</span><br><span class="line">print(text_translate)</span><br></pre></td></tr></table></figure><p>得到结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">i hope you didnt translate it by hand. </span><br><span class="line">thats what computers are for. </span><br><span class="line">doing it in by hand is inefficient and that&apos;s why this text is so long. </span><br><span class="line">using string.maketrans()is recommended. now apply on the url.</span><br></pre></td></tr></table></figure><p>作者很风趣，当然不能手动去一个推算了，推荐用 string.maketrans() 这个方法解决，我们上面采取的是比较直接的方法，官方给出了更为精简的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> string</span><br><span class="line">l = string.lowercase</span><br><span class="line">t = string.maketrans(l, l[<span class="number">2</span>:] + l[:<span class="number">2</span>])</span><br><span class="line"><span class="keyword">print</span> (text.translate(t))</span><br></pre></td></tr></table></figure><p>然后把 url 中的 <code>map</code> 改为<code>ocr</code>回车就来到了第 2 关：</p><p><img src="http://media.makcyun.top/win/20190517/ehGvHsSHtovF.png?imageslim" alt="mark"></p><p>作者接着说过关的提示可能在书里（当然不可能了）也可能在网页源代码里。那就右键查看源代码往下拉看到绿色区域，果然找到了问题：</p><p><img src="http://media.makcyun.top/win/20190517/g39fqrlV1JYP.png?imageslim" alt="mark"></p><p>意思就是：<strong>要在下面这一大串字符里找到出现次数最少的几个字符</strong></p><p>考察了这么几个知识点：</p><ul><li>正则表达式提取字符串</li><li>list 计数</li><li>条件语句</li></ul><p>如果是你，你会怎么做？</p><p>来看下，十行代码怎么实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://www.pythonchallenge.com/pc/def/ocr.html'</span></span><br><span class="line">res = requests.get(url).text</span><br><span class="line">text = re.findall(<span class="string">'.*?&lt;!--.*--&gt;.*&lt;!--(.*)--&gt;'</span>,res,re.S)</span><br><span class="line"><span class="comment"># list转为str便于遍历字符</span></span><br><span class="line">str = <span class="string">''</span>.join(text)</span><br><span class="line"></span><br><span class="line">lst = []</span><br><span class="line">key=[]</span><br><span class="line"><span class="comment">#遍历字符</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> str:</span><br><span class="line">    <span class="comment">#将字符存到list中</span></span><br><span class="line">    lst.append(i)</span><br><span class="line">    <span class="comment">#如果字符是唯一的，则添加进key</span></span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> key:</span><br><span class="line">        key.append(i)</span><br><span class="line"><span class="comment"># 将list列表中的字符出现字数统计出来</span></span><br><span class="line"><span class="keyword">for</span> items <span class="keyword">in</span> key:</span><br><span class="line">    print(items,lst.count(items))</span><br></pre></td></tr></table></figure><p>首先，用 Requests 请求网页然后用正则提取出字符串，接着 for 循环计算每个字符出现的次数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">% <span class="number">6104</span></span><br><span class="line">$ <span class="number">6046</span></span><br><span class="line"><span class="meta">@ 6157</span></span><br><span class="line">_ <span class="number">6112</span></span><br><span class="line">^ <span class="number">6030</span></span><br><span class="line"><span class="comment"># 6115</span></span><br><span class="line">) <span class="number">6186</span></span><br><span class="line">&amp; <span class="number">6043</span></span><br><span class="line">! <span class="number">6079</span></span><br><span class="line">+ <span class="number">6066</span></span><br><span class="line">] <span class="number">6152</span></span><br><span class="line">* <span class="number">6034</span></span><br><span class="line">&#125; <span class="number">6105</span></span><br><span class="line">[ <span class="number">6108</span></span><br><span class="line">( <span class="number">6154</span></span><br><span class="line">&#123; <span class="number">6046</span></span><br><span class="line"></span><br><span class="line">e <span class="number">1</span></span><br><span class="line">q <span class="number">1</span></span><br><span class="line">u <span class="number">1</span></span><br><span class="line">a <span class="number">1</span></span><br><span class="line">l <span class="number">1</span></span><br><span class="line">i <span class="number">1</span></span><br><span class="line">t <span class="number">1</span></span><br><span class="line">y <span class="number">1</span></span><br></pre></td></tr></table></figure><p>可以看到出现次数最少的就是最后几个字符，合起来是「equality」，替换 url 字符就闯过过了第 2 关进入下一关继续挑战。是不是有点意思？</p><p>后面每一关都需要用到相关的 Python 技巧解决，比如第 4 关：</p><p><img src="http://media.makcyun.top/win/20190517/jbXJRTGYHxSc.png?imageslim" alt="mark"></p><p>这一关作者弄了个小恶作剧，需要手动输入数值到 url 中然后回车，你以为这样就完了么？并没有它有会不断重复弹出新的数值让你输入，貌似没完没了。</p><p>到这儿就能发现肯定不能靠手动去完成，要用到 Python 实现自动填充修改 url 回车跳转到新 url，循环直到网页再也无法跳转为止。</p><p>如果是你，你会怎么做？</p><p>其实一段简单的爬虫加正则就能搞定。思路很简单，把每次网页中的数值提取出来替换成新的 url 再请求网页，循环下去，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首页url</span></span><br><span class="line">resp = requests.get(</span><br><span class="line">    <span class="string">'http://www.pythonchallenge.com/pc/def/linkedlist.php?nothing=12345'</span>).text</span><br><span class="line">url = <span class="string">'http://www.pythonchallenge.com/pc/def/linkedlist.php?nothing='</span></span><br><span class="line"><span class="comment"># 计数器</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 提取下一页动态数值</span></span><br><span class="line">        nextid = re.search(<span class="string">'\d+'</span>, resp).group()</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        nextid = int(nextid)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'最后一个url为：%s'</span> % nexturl)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取下一页url</span></span><br><span class="line">    nexturl = url + str(nextid)</span><br><span class="line">    print(<span class="string">'url %s:%s'</span> % (count, nexturl))</span><br><span class="line">    <span class="comment"># 重复请求</span></span><br><span class="line">    resp = requests.get(nexturl).text</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><p>gif</p><p>可以看到，最终循环了 85 次找到了最后一个数字<code>16044</code>，输入到 url 中就闯关成功。</p><p>33 关既有趣又能锻炼使用 Python 解决问题的技巧，感兴趣的话去玩玩看。</p><p>网址：<a href="http://www.pythonchallenge.com/" target="_blank" rel="noopener">http://www.pythonchallenge.com/</a></p><p>如果遇到不会做的题，可以在这里找到参考答案：</p><p>中参考文教程：</p><p><a href="https://www.cnblogs.com/jimnox/archive/2009/12/08/tips-to-python-challenge.html" target="_blank" rel="noopener">https://www.cnblogs.com/jimnox/archive/2009/12/08/tips-to-python-challenge.html</a></p><p><a href="https://blog.csdn.net/Jurbo/article/details/52136323" target="_blank" rel="noopener">https://blog.csdn.net/Jurbo/article/details/52136323</a></p><p>官方参考教程：</p><p><a href="http://garethrees.org/2007/05/07/python-challenge/" target="_blank" rel="noopener">http://garethrees.org/2007/05/07/python-challenge/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Python 趣味学习网站。&lt;/p&gt;
    
    </summary>
    
      <category term="Python学习" scheme="https://www.makcyun.top/categories/Python%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python入门" scheme="https://www.makcyun.top/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>线性回归 Statsmodels 模型预测波士顿房价</title>
    <link href="https://www.makcyun.top/2019/05/12/Machine_learning05.html"/>
    <id>https://www.makcyun.top/2019/05/12/Machine_learning05.html</id>
    <published>2019-05-12T08:16:16.000Z</published>
    <updated>2019-05-19T10:11:45.338Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>机器学习之线性回归。</p><a id="more"></a><p>线性回归是入门机器学习的第一课。波士顿房价数据集是建立线性回归的一个很好案例，下面将分别使用 Statsmodels 和 Sklearn 模型建立线性回归模型，最终预测该地区房价。本文先使用 Statsmodels 模型。</p><p>这份 <a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html" target="_blank" rel="noopener">数据来源</a> 于 1978 年发表的一份杂志上，数据量很小一共只有 506 行 * 14 列，各特征变量含义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CRIM: 城镇人均犯罪率，百分比%</span><br><span class="line">ZN: 住宅用地所占比例，百分比%</span><br><span class="line">INDUS: 城镇中非住宅用地所占比例，百分比%</span><br><span class="line">CHAS: CHAS 虚拟变量，用于回归分析</span><br><span class="line">NOX: 环保指数</span><br><span class="line">RM: 每栋住宅的房间数</span><br><span class="line">AGE: <span class="number">1940</span> 年以前建成的自住单位的比例，百分比%</span><br><span class="line">DIS: 距离 <span class="number">5</span> 个波士顿的就业中心的加权距离</span><br><span class="line">RAD: 距离高速公路的便利指数</span><br><span class="line">TAX: 每一万美元的不动产税率,百分比%</span><br><span class="line">PTRATIO: 城镇中的教师学生比例，百分比%</span><br><span class="line">B: 关于黑人比例的一个参数</span><br><span class="line">LSTAT: 地区中有多少房东属于低收入人群，百分比%</span><br><span class="line">MEDV: 自住房屋房价中位数（也就是均价），单位 $<span class="number">1000</span> 美元</span><br></pre></td></tr></table></figure><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>数据集包含 sklearn 包中，只需要两行代码就可以加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">data = datasets.load_boston()</span><br></pre></td></tr></table></figure><p>查看数据集相关特征，可以用这些方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.DESCR <span class="comment"># 查看数据集描述</span></span><br><span class="line">data.feature_names <span class="comment"># 查看特征变量名 </span></span><br><span class="line">data.target <span class="comment"># 查看目标变量数据</span></span><br><span class="line">data.data <span class="comment"># 查看自变量数据</span></span><br></pre></td></tr></table></figure><p>大致结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Boston House Prices dataset</span><br><span class="line">===========================</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">------</span><br><span class="line">Data Set Characteristics:  </span><br><span class="line"></span><br><span class="line">    :Number of Instances: <span class="number">506</span> </span><br><span class="line"></span><br><span class="line">    :Number of Attributes: <span class="number">13</span> numeric/categorical predictive</span><br><span class="line">    </span><br><span class="line">    :Median Value (attribute <span class="number">14</span>) <span class="keyword">is</span> usually the target</span><br><span class="line"></span><br><span class="line">    :Attribute Information (<span class="keyword">in</span> order):</span><br><span class="line">        - CRIM     per capita crime rate by town</span><br><span class="line">        - ZN       proportion of residential land zoned <span class="keyword">for</span> lots over <span class="number">25</span>,<span class="number">000</span> sq.ft.</span><br><span class="line">        - INDUS    proportion of non-retail business</span><br><span class="line"> ...</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">[<span class="string">'CRIM'</span> <span class="string">'ZN'</span> <span class="string">'INDUS'</span> <span class="string">'CHAS'</span> <span class="string">'NOX'</span> <span class="string">'RM'</span> <span class="string">'AGE'</span> <span class="string">'DIS'</span> <span class="string">'RAD'</span> <span class="string">'TAX'</span> <span class="string">'PTRATIO'</span></span><br><span class="line"> <span class="string">'B'</span> <span class="string">'LSTAT'</span>]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>为便于后续建模，把数据集转换为 DataFrame 格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据集转为 dataframe 格式 </span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line">boston = pd.DataFrame(data.data,columns=data.feature_names)</span><br><span class="line">boston[<span class="string">'Price'</span>] = data.data <span class="comment"># 因变量也加入dataframe</span></span><br><span class="line">boston.head()</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/Fl8FE1-L0i9YbLBhtb-YHj_kPCRl" alt=""></p><p>13 个自变量中有 6 个的单位是百分比：</p><ul><li>ZN</li><li>INDUS</li><li>AGE</li><li>TAX</li><li>PTRATIO</li><li>LSTAT</li></ul><p>接下来把数据拆分为训练集和测试集，可以用 train_test_split ，也可以用 shuffle：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">方法1 train_test_split</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">col_x = boston.columns[:<span class="number">-1</span>]</span><br><span class="line">train, test = train_test_split(boston, test_size=<span class="number">0.2</span>, random_state=<span class="number">123</span>)</span><br><span class="line">print(train.shape)</span><br><span class="line">print(test.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">(<span class="number">404</span>, <span class="number">14</span>)</span><br><span class="line">(<span class="number">102</span>, <span class="number">14</span>)</span><br></pre></td></tr></table></figure><p>用 shuffle：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">方法2 shuffle</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle </span><br><span class="line">x,y = shuffle(boston[col_x],boston[<span class="string">'Price'</span>],random_state=<span class="number">124</span>)</span><br><span class="line">offset = int(x.shape[<span class="number">0</span>]*<span class="number">0.8</span>)</span><br><span class="line">x_train ,y_train= x[:offset],y[:offset]</span><br><span class="line">x_test ,y_test= x[offset:],y[offset:]</span><br></pre></td></tr></table></figure><p>数据集划分好后，测试集就放在一边不动，直到模型型建立好后再来用它。</p><h3 id="探索性数据分析"><a href="#探索性数据分析" class="headerlink" title="探索性数据分析"></a>探索性数据分析</h3><p>下面对测试集数据做简单的探索性分析，即：EDA（Exploratory Data Analysis），以便快速了解特征变量情况，比如是数值/类别/还是字符变量，为后面特征工程做准备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">boston.info()</span><br><span class="line"><span class="comment"># output：</span></span><br><span class="line">RangeIndex: <span class="number">506</span> entries, <span class="number">0</span> to <span class="number">505</span></span><br><span class="line">Data columns (total <span class="number">14</span> columns):</span><br><span class="line">CRIM       <span class="number">506</span> non-null float64</span><br><span class="line">ZN         <span class="number">506</span> non-null float64</span><br><span class="line">INDUS      <span class="number">506</span> non-null float64</span><br><span class="line">CHAS       <span class="number">506</span> non-null float64</span><br><span class="line">NOX        <span class="number">506</span> non-null float64</span><br><span class="line">RM         <span class="number">506</span> non-null float64</span><br><span class="line">AGE        <span class="number">506</span> non-null float64</span><br><span class="line">DIS        <span class="number">506</span> non-null float64</span><br><span class="line">RAD        <span class="number">506</span> non-null float64</span><br><span class="line">TAX        <span class="number">506</span> non-null float64</span><br><span class="line">PTRATIO    <span class="number">506</span> non-null float64</span><br><span class="line">B          <span class="number">506</span> non-null float64</span><br><span class="line">LSTAT      <span class="number">506</span> non-null float64</span><br><span class="line">Price      <span class="number">506</span> non-null float64</span><br><span class="line">dtypes: float64(<span class="number">14</span>)</span><br></pre></td></tr></table></figure><p>可以看到全部自变量都是数值值，且没有缺失值，属于很理想的数据集，实际项目中会比这要复杂很多，不过鉴于我们用这个数据集是初步学习线性回归，数据集简单些也好，重心好放在后面的模型建立和检验部分。</p><p>先了解数据集分布情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">boston.describe().round(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># output：</span></span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/FpVfZiuyvI5RqCSVx82Yo-7TT-sD" alt=""></p><p>重点看一下因变量 Price 房价情况，平均房价是 22.53，单位是千美元，也就是房价均值是 2 万多美元，数据来源于 1978 年，可见房价并不便宜。</p><p>绘制直方图进一步查看房价的分布：</p><p><img src="http://media.makcyun.top/Fg4jwfUcPG4JieAvAoX1hDuJZPbD" alt=""></p><p>同红色标准正态分布曲线对比，可以看到数据集呈右偏趋势，且峰度较高，另外数据集在 50 处被截断了。在后续建模时可能需要处理转化为标准正态分布。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> stats </span><br><span class="line"></span><br><span class="line">sns.set(rc=&#123;<span class="string">'figure.figsize'</span>:(<span class="number">8</span>,<span class="number">6</span>),<span class="string">'font.sans-serif'</span>:[<span class="string">'simhei'</span>,<span class="string">'Arial'</span>]&#125;)  <span class="comment">#设置字体，避免中文不显示</span></span><br><span class="line">sns.distplot(train.Price,bins=<span class="number">20</span>,</span><br><span class="line">            fit = stats.norm, <span class="comment"># 拟合标准正态分布</span></span><br><span class="line">            kde_kws=&#123;<span class="string">'label'</span>:<span class="string">'核密度曲线'</span>&#125;,</span><br><span class="line">            fit_kws=&#123;<span class="string">'color'</span>:<span class="string">'red'</span>,<span class="string">'label'</span>:<span class="string">'标准正态分布曲线'</span>&#125;</span><br><span class="line">            )</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><blockquote><p>seaborn 绘图中文容易变成方框不显示，需要额外设置中文字体</p></blockquote><p>查看各变量同因变量之间的相关性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train.corrwith(train.Price).sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">Price      <span class="number">1.000000</span></span><br><span class="line">RM         <span class="number">0.716973</span></span><br><span class="line">ZN         <span class="number">0.360129</span></span><br><span class="line">B          <span class="number">0.327933</span></span><br><span class="line">DIS        <span class="number">0.255739</span></span><br><span class="line">CHAS       <span class="number">0.119402</span></span><br><span class="line">AGE       <span class="number">-0.380633</span></span><br><span class="line">CRIM      <span class="number">-0.386443</span></span><br><span class="line">RAD       <span class="number">-0.392065</span></span><br><span class="line">NOX       <span class="number">-0.436879</span></span><br><span class="line">TAX       <span class="number">-0.493560</span></span><br><span class="line">INDUS     <span class="number">-0.496211</span></span><br><span class="line">PTRATIO   <span class="number">-0.506643</span></span><br><span class="line">LSTAT     <span class="number">-0.746059</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>可以看到跟因变量相关系数比较高的自变量有：</p><ul><li>RM：每栋住宅房间数，呈正相关，很好理解，房间数越多房价一般越高；</li><li>LSTAT：所在地区房东属于低收入人群比例，呈负相关，也好理解，低收入比例越高说明该地区整体比较穷，房价自然就低；</li><li>PTRATIO：教师与学生的比例，呈负相关，也好理解，类似国内的学区房，资源越好的地方，学生比老师多地多，该比例低，而房价会很高。</li></ul><p>还可以详细查看一下各变量之间的相关系数：</p><p><img src="http://media.makcyun.top/win/20190512/azgy1nhzdwt9.jpg?imageslim" alt="mark"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line">sns.set(style=<span class="string">'ticks'</span>,rc=&#123;<span class="string">'figure.figsize'</span>:(<span class="number">12</span>,<span class="number">12</span>)&#125;)</span><br><span class="line"></span><br><span class="line">train_corr = train.corr()</span><br><span class="line">sns.set(style=<span class="string">'ticks'</span>,rc=&#123;<span class="string">'figure.figsize'</span>:(<span class="number">10</span>,<span class="number">10</span>)&#125;)</span><br><span class="line">sns.heatmap(train_corr,cbar=<span class="keyword">True</span>,annot=<span class="keyword">True</span>,square=<span class="keyword">True</span>,fmt=<span class="string">'0.2f'</span>,annot_kws=&#123;<span class="string">'size'</span>:<span class="number">12</span>&#125;)</span><br></pre></td></tr></table></figure><blockquote><p>annot=True 可以添加相关系数</p></blockquote><p>以上初步确定了跟因变量相关系数比较高的几个变量，另外我们还可以使用 SelectKBest 方法直接筛选自变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest,f_regression</span><br><span class="line">X1 = train.iloc[:,:<span class="number">13</span>]</span><br><span class="line">selector = SelectKBest(f_regression,k=<span class="number">3</span>) <span class="comment"># k：筛选k个变量</span></span><br><span class="line">selector.fit_transform(X1,train[<span class="string">'Price'</span>])</span><br><span class="line">X1.columns[selector.get_support(indices=<span class="keyword">True</span>)].tolist()</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">[<span class="string">'RM'</span>, <span class="string">'PTRATIO'</span>, <span class="string">'LSTAT'</span>]</span><br></pre></td></tr></table></figure><p>我们这里假设筛选出了 3 个最相关的变量，结果和前面一致。</p><p>由于下面我们需要先练习一元线性回归，那么只需要一个自变量，这里更改 k 值为 1能够得到最相关的自变量为：LSTAT。下面，就利用这个自变量使用 StatsModels 模型建立房价的一元线性回归。</p><h3 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h3><p>ChangeLog</p><ul><li>2019/5/12 建立</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;机器学习之线性回归。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://www.makcyun.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.makcyun.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>微信朋友圈的一些新奇玩法</title>
    <link href="https://www.makcyun.top/2019/05/11/weekly_sharing27.html"/>
    <id>https://www.makcyun.top/2019/05/11/weekly_sharing27.html</id>
    <published>2019-05-11T08:16:16.000Z</published>
    <updated>2019-05-11T03:27:49.264Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>新鲜有趣。</p><a id="more"></a><p>这里是「每周分享」的第 27 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：微信朋友圈的一些高（装）级（逼）玩法。</p><p>大多数人每天都要刷朋友圈，但很多人可能觉得朋友圈只有三个主要功能：发图文、视频以及转发动态。其实，朋友圈还有不少新奇玩法，下面来说道说道。</p><h3 id="01-朋友圈文字防止被折叠"><a href="#01-朋友圈文字防止被折叠" class="headerlink" title="01 朋友圈文字防止被折叠"></a>01 朋友圈文字防止被折叠</h3><p>文字被折叠是很多人发朋友圈会遇到的问题，而大部分人会选择在评论区重发一遍，其实没有必要。用下面的方法基本可以保证文字不被折叠。</p><p><img src="http://media.makcyun.top/Ft_X1Q_iWDwy-mzARPjwD-WwgZZI" alt=""></p><p><strong>安卓手机</strong></p><p>复制好想发的文字 →进入朋友圈编辑界面 →随便输入几个字符并全选 →覆盖粘贴再发布。</p><p><strong>苹果手机</strong></p><p>朋友圈编辑页面粘贴好文字→点击左上角「取消」并保留草稿→重新进入编辑界面再发布。</p><h3 id="02-朋友圈发九宫格图片"><a href="#02-朋友圈发九宫格图片" class="headerlink" title="02 朋友圈发九宫格图片"></a>02 朋友圈发九宫格图片</h3><p>你有时可能会见到别人发出这样的九宫格图片，一张照片被切割成了九部分，其实做起来很简单。</p><p><img src="http://media.makcyun.top/FuDH6mxD9gq5F1Z6cH513lIdYTGW" alt=""></p><p>微信搜索「九宫格照片」小程序，添加想要切割的图片然后保存，发朋友圈按照顺序选择就可以了。除了正方形，还可以切割成其他多种形状。</p><p><img src="http://media.makcyun.top/FikVCHlEs8C7xTpn_E0yGlpIwDSW" alt=""></p><h3 id="03-朋友圈图片评论"><a href="#03-朋友圈图片评论" class="headerlink" title="03 朋友圈图片评论"></a>03 朋友圈图片评论</h3><p>通常我们评论朋友圈都是文字/表情评论，如果你想用表情包或者图片评论的话就不行，其实可以采用曲线方式评论，比如像下面这样：</p><p><img src="http://media.makcyun.top/Ft1zm3A78T4mkUuR7QsRqVgDV1pL" alt=""></p><p>别人打开链接就能看到你发的图片，这种操作实际上就是把表情/图片上传到图床再生成链接得到，实现起来也简单：</p><p><strong>安卓手机</strong></p><p>搜索「图床神器」小程序，上传图片复制 http 链接，觉得链接长可以用「短网址生成」小程序生成短连接，最后复制到朋友圈评论中就可以了。</p><p><img src="http://media.makcyun.top/FjfwOvuCzImDrZVea-LARcXTv5j_" alt=""></p><p><strong>苹果手机</strong></p><p>安装 「快捷指令」或者「捷径」App，参考这个教程就可以了：<a href="https://sharecuts.cn/shortcut/1133" target="_blank" rel="noopener">https://sharecuts.cn/shortcut/1133</a></p><p>需要注意一点，图片是上传到公共图床上的，所以不要上传一些隐私图片。</p><h3 id="04-朋友圈发九宫格动态图片-视频"><a href="#04-朋友圈发九宫格动态图片-视频" class="headerlink" title="04 朋友圈发九宫格动态图片/视频"></a>04 朋友圈发九宫格动态图片/视频</h3><p>上面的九宫格图片还有另外一种更高级的玩法，就是图片是动态的，像下面这样：</p><p><img src="http://media.makcyun.top/FllTxO5KC4kmP9o6UAR3omkABVmV" alt=""></p><p>实现起来也很简单，使用「Cinepic」款 App 就可以了。依次选择：背景图片、图片、背景音乐，然后保存到本地视频。发到朋友圈就可以了。除了动态图片还可以发送视频，适合那些看现场演唱会爱连发朋友圈的人。</p><p><img src="http://media.makcyun.top/Fv2Bt5X6BjWlWDEldJRzemJDSmZw" alt=""></p><h3 id="05-朋友圈发语音"><a href="#05-朋友圈发语音" class="headerlink" title="05 朋友圈发语音"></a>05 朋友圈发语音</h3><p>你大概见过很多人在朋友圈分享音乐链接，但朋友圈其实还可以发语音。</p><p><img src="http://media.makcyun.top/FvZ4rwReRw04xf14jAe26sIFsZk-" alt=""></p><p>实现方法也很简单，微信中打开这个链接，选择录音就可以了，最长 1 分钟。</p><p><a href="https://sharecuts.cn/shortcut/1133" target="_blank" rel="noopener">https://flhd.maikaolin.club/huodong/wxvoice/</a></p><p>最后，分享一个旧版本微信。</p><p>自从微信升级到 7.0 之后，你可能会觉得界面使用起来很别扭，比如说公众号订阅列表，变得很乱，不再像以前那么干净，比如像我还在用的 6.7 版本：</p><p><img src="http://media.makcyun.top/Fis9G6TzJNuHbjrYLNyVaA-vL0Wl" alt=""></p><p>在我公众号：「高级农民工」后台回复「<strong>微信</strong>」就可以得到旧版本安装包和上面的 Cinepic App。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;新鲜有趣。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>漫威 DC 宇宙英雄综合实力可视化对比分析</title>
    <link href="https://www.makcyun.top/2019/05/06/data_analysis&amp;mining05.html"/>
    <id>https://www.makcyun.top/2019/05/06/data_analysis&amp;mining05.html</id>
    <published>2019-05-06T08:16:16.000Z</published>
    <updated>2019-05-07T00:49:06.494Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>一生漫威粉。</p><a id="more"></a><p><strong>摘要</strong>：数据采集、分析实战。</p><p><img src="http://media.makcyun.top/FjC-dqDPXyXpXv1Ai5RnaCT9SnTy" alt=""></p><p>昨天借最近持续火爆的的《复联4》说了说漫威电影宇宙票房话题，今天票房就上升到了全球第二，超越保持 20 多年记录的《泰坦尼克号》，有生之年能见到也是难得了。</p><p>另外，文末卖了个关子：<strong>那么多英雄到底谁最强</strong>？今天就来用 Python 对比分析一下各位英雄的综合实力，结果绝对超出你预料。</p><p>先说说下漫威电影和漫威漫画的关系。我们看的电影叫「漫改电影」，意思就是从漫画中改编过来搬上荧幕的。这些电影出现不过十年，而漫画五十年前就出现了，大多数数角色由斯坦·李创造，所以你可以看到每部漫威电影他都有客串。</p><p>电影中为了呈现更好的视觉效果以刺激观众感官，会刻意加强或者弱化某些英雄的能力，尤其是精彩的打斗场景，让我们以为这就是他们的真实实力。</p><p>比如：</p><ul><li>美队跟谁都能五五分</li><li>最强之人是灭霸</li><li>正面对决猩红女巫能手撕灭霸</li><li>惊奇队长貌似是唯一一个能单挑不怵灭霸的</li></ul><p>而在漫画中的实际情况并不完全是这样。漫画里对每个角色都设定了能力值，能力值包括六个方面，分别是</p><ul><li>Intelligence / 智力</li><li>Power / 能量</li><li>Strength / 力量</li><li>Speed / 速度</li><li>Durability / 耐力</li><li>Combat / 格斗技</li></ul><p>比如钢铁侠的能力值是这样的：</p><p><img src="http://media.makcyun.top/FhY7WD-sRtGeZ_8LGxCZL_ZjLxBq" alt=""></p><p>可以看到他的智力和能量值是满分，很贴合电影中 Tony Stark 演的钢铁侠形象。而速度和格斗技巧不过刚及格，可电影中给我们看到的钢铁侠上天入地速度杠杠的，打斗也很强。唯一的解释就是，电影作了美化。</p><p>在权威漫画人物网站：<a href="https://www.superherodb.com/" target="_blank" rel="noopener">superherodb</a>上，给每位角色都标出了能力值。孰强孰弱一对比就一目了然。</p><p><img src="http://media.makcyun.top/FsxAOxeYY4gfDfX3k3ByTer9iisM" alt=""></p><p>不只是上面这些热门角色，该网站拥有包括漫威 、DC 在内的上百家漫画公司的数千位漫画角色详细信息，可以说是非常强大。</p><p><img src="http://media.makcyun.top/Flomh3235xRHHvQLj9R22RUmPZLe" alt=""></p><p>当然，一个个去对比很麻烦而且很难发现深层次关系，这时候就需要 Python 出场了。</p><p>首先需要获取这些数据，怎么获取呢？当然是爬虫。鉴于以前爬过类似的网站，这里就不爬了感兴趣可以自行尝试。</p><p>还有一种更为取巧的方法就是找现成的 API 接口然后调用即可。网上找了一圈，最终找到了 <a href="https://superheroapi.com/ids.html" target="_blank" rel="noopener">superheroapi</a> 这个网站。</p><p>该网站上提供了 700 多位角色的详细信息，数量虽不多但也够用。</p><p><img src="http://media.makcyun.top/FgdQgYTBmCvmrYXm7fyoqGp3HlDk" alt=""></p><p>API 返回结果是 JSON 格式，包括能力值、身高体重等信息，例如：钢铁侠的信息如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"response"</span>: <span class="string">"success"</span>,</span><br><span class="line">  <span class="string">"results-for"</span>: <span class="string">"Iron Man"</span>, </span><br><span class="line">  <span class="string">"results"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"id"</span>: <span class="string">"346"</span>,</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"Iron Man"</span>,</span><br><span class="line">      <span class="string">"powerstats"</span>: &#123;  <span class="comment"># 能力值</span></span><br><span class="line">        <span class="string">"intelligence"</span>: <span class="string">"100"</span>,</span><br><span class="line">        <span class="string">"strength"</span>: <span class="string">"85"</span>,</span><br><span class="line">        <span class="string">"speed"</span>: <span class="string">"58"</span>,</span><br><span class="line">        <span class="string">"durability"</span>: <span class="string">"85"</span>,</span><br><span class="line">        <span class="string">"power"</span>: <span class="string">"100"</span>,</span><br><span class="line">        <span class="string">"combat"</span>: <span class="string">"64"</span></span><br><span class="line">      &#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><p>下面我们用 Python 先获取网站全部 700 多位角色信息然后保存到本地数据库。</p><p>代码见文末，几分钟就可以下载好结果如下：</p><p><img src="http://media.makcyun.top/Ftiiar7HqVlsRWL9qp8JcXUZZzhn" alt=""></p><p>简单的清洗处理后就可以着手分析。</p><h3 id="可视化分析"><a href="#可视化分析" class="headerlink" title="可视化分析"></a>可视化分析</h3><p>先看漫威复联系列。说起复联最重要的人物自然是六位初代英雄。</p><p><img src="https://d13ezvd6yrslxm.cloudfront.net/wp/wp-content/images/avengers-1.jpg" alt=""></p><h3 id="初代六人组实力对比"><a href="#初代六人组实力对比" class="headerlink" title="初代六人组实力对比"></a>初代六人组实力对比</h3><p>凭电影中的印象对这六人的实力排序的话，你会怎么排？</p><p>按图上从左到右的顺序来看看六人的实际实力。</p><h4 id="雷神"><a href="#雷神" class="headerlink" title="雷神"></a><strong>雷神</strong></h4><p>通过雷达图可以看到雷神很全面，多项数据都是满分，几乎没有弱点，然而智力这块儿只有中等水平。如果你看过雷神系列就会知道他的智商的确很捉急。</p><p><img src="http://media.makcyun.top/win/20190506/9sBJ4pn2mH8R.jpg?imageslim" alt="mark"></p><p><img src="http://media.makcyun.top/Fqbxv19742r7vVK2SmquCmxywq3X" alt=""></p><h4 id="黑寡妇"><a href="#黑寡妇" class="headerlink" title="黑寡妇"></a><strong>黑寡妇</strong></h4><p>寡姐身为六人组里唯一的女性，不会飞也没有什么道具，最拿手的就是肉搏，《钢铁侠2》首次出场表现就奠定了她的风格。</p><p><img src="http://media.makcyun.top/win/20190506/zuHERiqdeU17.jpg?imageslim" alt="mark"></p><h4 id="鹰眼"><a href="#鹰眼" class="headerlink" title="鹰眼"></a>鹰眼</h4><p>箭筒里永远射不完箭的鹰眼在《雷神1》中首次亮相，实力差不多是最弱的，感觉《复联1》中演反派更厉害。</p><p><img src="http://media.makcyun.top/win/20190506/x4M3H7XX10qy.jpg?imageslim" alt="mark"></p><h4 id="绿巨人"><a href="#绿巨人" class="headerlink" title="绿巨人"></a>绿巨人</h4><p>终于出现个和雷神实力匹配的对手，三项能力满分，格斗技和速度中等，浩克的确格斗能力没那么强，在《复联3》开头分分钟被灭霸给收拾了。智商比雷神高，毕竟是拥有 7 个博士头衔的人。</p><p><img src="http://media.makcyun.top/win/20190506/O9ARPl0fsRSh.jpg?imageslim" alt="mark"></p><h4 id="美国队长"><a href="#美国队长" class="headerlink" title="美国队长"></a>美国队长</h4><p>整个系列一共说了三次「I can do this all day.」 的美队给人最大的错觉就是：和谁都能五五开，然而毕竟肉身，实际没有那么强。</p><p><img src="http://media.makcyun.top/win/20190506/16ltFsW2syia.jpg?imageslim" alt="mark"></p><h4 id="钢铁侠"><a href="#钢铁侠" class="headerlink" title="钢铁侠"></a>钢铁侠</h4><p>最后是最帅气最聪明的托尼了，感觉没有电影中想的那么强大，主要他演得好印象分高。</p><p><img src="http://media.makcyun.top/win/20190506/hcoCCI9FGlKx.jpg?imageslim" alt="mark"></p><p>上面的雷达图绘制代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">5</span>, <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">names = [<span class="string">'Thor'</span>,<span class="string">'Iron Man'</span>,<span class="string">'Captain America'</span>,<span class="string">'Hulk'</span>,<span class="string">'Black Widow'</span>,<span class="string">'Hawkeye'</span>]</span><br><span class="line">names_cn = [<span class="string">'雷神'</span>,<span class="string">'钢铁侠'</span>,<span class="string">'美队'</span>,<span class="string">'绿巨人'</span>,<span class="string">'黑寡妇'</span>,<span class="string">'鹰眼'</span>]   <span class="comment">#标签</span></span><br><span class="line">title = [<span class="string">'智力'</span>, <span class="string">'能量'</span>, <span class="string">'力量'</span>, <span class="string">'速度'</span>, <span class="string">'耐力'</span>, <span class="string">'格斗技'</span>]  <span class="comment"># 标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从data查询 A6 数据</span></span><br><span class="line">scores = pd.DataFrame(&#123;<span class="string">'name'</span>:names&#125;)</span><br><span class="line">scores = scores.merge(data,how=<span class="string">'left'</span>,on=<span class="string">'name'</span>)[[<span class="string">'intelligence'</span>,<span class="string">'power'</span>,<span class="string">'strength'</span>,<span class="string">'speed'</span>,<span class="string">'durability'</span>,<span class="string">'combat'</span>]]</span><br><span class="line"><span class="comment"># 转换为list</span></span><br><span class="line">scores = scores.values.tolist()</span><br><span class="line"></span><br><span class="line">zipped = zip(scores,names_cn)</span><br><span class="line"><span class="comment"># for循环生成A6成员雷达图</span></span><br><span class="line"><span class="keyword">for</span> i,(value, name) <span class="keyword">in</span> enumerate(zipped): </span><br><span class="line">    ax= <span class="string">'ax%s'</span>%i</span><br><span class="line">    theta = np.linspace(<span class="number">0</span>, <span class="number">2</span>*np.pi, len(value), endpoint=<span class="keyword">False</span>)  <span class="comment"># 将圆根据标签的个数等比分</span></span><br><span class="line">    theta = np.concatenate((theta, [theta[<span class="number">0</span>]]))  <span class="comment"># 闭合</span></span><br><span class="line">    </span><br><span class="line">    value = np.concatenate((value, [value[<span class="number">0</span>]]))  <span class="comment"># 闭合</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里要设置为极坐标格式</span></span><br><span class="line">    ax = plt.subplot2grid(shape=(<span class="number">6</span>,<span class="number">1</span>), loc=(i,<span class="number">0</span>),polar=<span class="keyword">True</span>)</span><br><span class="line">    ax.plot(theta, value, lw=<span class="number">2</span>, alpha=<span class="number">1</span>,label=name)  <span class="comment"># 绘图</span></span><br><span class="line">    ax.fill(theta, value, alpha=<span class="number">0.25</span>)  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line">    ax.set_thetagrids(theta*<span class="number">180</span>/np.pi,title)         <span class="comment">#替换标签</span></span><br><span class="line">    ax.set_ylim(<span class="number">0</span>,<span class="number">110</span>)                          <span class="comment">#设置极轴的区间</span></span><br><span class="line">    ax.set_theta_zero_location(<span class="string">'N'</span>)         <span class="comment">#设置极轴方向</span></span><br><span class="line">    ax.set_title(<span class="string">'%s战力'</span>%name,fontsize = <span class="number">15</span>)   <span class="comment">#添加图描述</span></span><br><span class="line"> </span><br><span class="line">plt.tight_layout(h_pad=<span class="number">5.0</span>)</span><br><span class="line">plt.savefig(<span class="string">'A6单人战力.jpg'</span>,dpi=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><p>来个汇总看得更清楚，初代六人组孰强孰弱这下有答案了吧？</p><p><img src="http://media.makcyun.top/Frz2Z8w3upqgF513Bai07N1gn9WD" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> value, name <span class="keyword">in</span> zipped: </span><br><span class="line">    theta = np.linspace(<span class="number">0</span>, <span class="number">2</span>*np.pi, len(value), endpoint=<span class="keyword">False</span>)  <span class="comment"># 将圆根据标签的个数等比分</span></span><br><span class="line">    theta = np.concatenate((theta, [theta[<span class="number">0</span>]]))  <span class="comment"># 闭合</span></span><br><span class="line">    value = np.concatenate((value, [value[<span class="number">0</span>]]))  <span class="comment"># 闭合</span></span><br><span class="line"></span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>, polar=<span class="keyword">True</span>)</span><br><span class="line">    ax.plot(theta, value, lw=<span class="number">2</span>, alpha=<span class="number">1</span>,label=name)  <span class="comment"># 绘图</span></span><br><span class="line"><span class="comment">#     ax.fill(theta, value, alpha=0.25)  # 填充</span></span><br><span class="line"></span><br><span class="line">ax.set_thetagrids(theta*<span class="number">180</span>/np.pi,title)         <span class="comment">#替换标签</span></span><br><span class="line">ax.set_ylim(<span class="number">0</span>,<span class="number">110</span>)                          <span class="comment">#设置极轴的区间</span></span><br><span class="line">ax.set_theta_zero_location(<span class="string">'N'</span>)         <span class="comment">#设置极轴方向</span></span><br><span class="line">ax.set_title(<span class="string">'复联六位初代英雄战力对比'</span>,fontsize = <span class="number">20</span>)   <span class="comment">#添加图描述</span></span><br><span class="line"> </span><br><span class="line">plt.legend(loc=<span class="string">'lower center'</span>,ncol=<span class="number">6</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">fig.savefig(<span class="string">'A6.jpg'</span>,dpi=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><h3 id="十位重要英雄实力"><a href="#十位重要英雄实力" class="headerlink" title="十位重要英雄实力"></a>十位重要英雄实力</h3><p>除了六位初代英雄，陆陆续续还出现了很多其他英雄，挑选十位露脸最多的来看看。</p><h4 id="洛基"><a href="#洛基" class="headerlink" title="洛基"></a>洛基</h4><p>有「锤」必有「基」，虽然电影中洛基饰演的是反派，但其实不坏，跟雷神相爱相杀带来不少笑料，所以重要人物中必须「Loki」的名字。</p><p><img src="http://media.makcyun.top/FrIBSBkG2ZEDB4gI8etjb_iLCVX6" alt=""></p><h4 id="惊奇队长"><a href="#惊奇队长" class="headerlink" title="惊奇队长"></a>惊奇队长</h4><p>很多人都说惊奇队长应该是《复联》中最牛逼的人，在《复联4》打了个酱油。战斗力的确很惊人，但 DC 中还有一个比她还厉害的男性「惊奇队长」，一会儿说。</p><p><img src="http://media.makcyun.top/FlqKG81F7XhsXQ1g7hQETmZbl9Ol" alt=""></p><h4 id="绯红女巫"><a href="#绯红女巫" class="headerlink" title="绯红女巫"></a>绯红女巫</h4><p>不得不说绯红女巫是又美又能打，差点把灭霸撕了。我不会告诉你他们俩早在另外一部电影《老男孩》里也上演了一出别样「大战」。</p><p><img src="http://media.makcyun.top/Ft7uRXY_GmMdPfyLsP-BgHYZRW8X" alt=""></p><h4 id="幻视"><a href="#幻视" class="headerlink" title="幻视"></a>幻视</h4><p>《复联2》中诞生就拥有心灵宝石的幻视着实牛逼，把奥创打得满地找牙，但到了后面怎么就沦落到被保护的境地了。</p><p><img src="http://media.makcyun.top/FtRf6OOHQuPcTZuD66EBDzS2kxgA" alt=""></p><h4 id="奇异博士"><a href="#奇异博士" class="headerlink" title="奇异博士"></a>奇异博士</h4><p>卷福饰演的奇异博士还是很牛逼，有时间宝石、有斗篷还有酷炫的阿戈摩托之眼。《复联4》最后对着托尼竖起了一根手指，大概是说：「福尔摩斯，只能有一个。」</p><p><img src="http://media.makcyun.top/FlJlgzR-2jPaKU6jVzQLR5QhtQLv" alt=""></p><h4 id="蚁人-amp-蜘蛛侠"><a href="#蚁人-amp-蜘蛛侠" class="headerlink" title="蚁人 &amp; 蜘蛛侠"></a>蚁人 &amp; 蜘蛛侠</h4><p>蚁人和蜘蛛侠差不多，飞来飞去。蚁人是复联少数几个绝顶聪明的人，可以说《复联4》能够逆转，蚁人功劳很大。蜘蛛侠实力均衡，早在《钢铁侠》系列中就出场了，虽然身为托尼的小跟班，但漫画中蜘蛛侠是漫威最大的 IP。</p><p><img src="http://media.makcyun.top/Fux-4chHfN3ReqgzKP3VeiL4ukzi" alt=""></p><p><img src="http://media.makcyun.top/Flz9tG5ZIGsVgP55K8UscNAj24_b" alt=""></p><h4 id="黑豹-amp-冬兵"><a href="#黑豹-amp-冬兵" class="headerlink" title="黑豹 &amp; 冬兵"></a>黑豹 &amp; 冬兵</h4><p>要问谁比钢铁侠还有钱，那必然是「振金王国」瓦坎达的国王黑豹了。在去年的独立电影中大放异彩，复联中到没有太多施展拳脚的机会。</p><p>要论复联有哪几对相爱相杀组合，除了雷神和洛基，就是美队和冬兵了，《美队1》中还是挺感人的。</p><p><img src="http://media.makcyun.top/FmW545GSBfpQniJ30GQ8I8sC2iQG" alt=""></p><p><img src="http://media.makcyun.top/FrKA8Mgu9HDCvN800z9Xv15gaUQF" alt=""></p><h4 id="星爵"><a href="#星爵" class="headerlink" title="星爵"></a>星爵</h4><p>最后隆重出场的是星爵，也是我本人最喜欢的复联英雄。《银河护卫队1》打养父，《银河护卫队2》打生父，《复联34》打岳父，他才是最牛逼的「灭爸」。现实中的岳父是位了不得的人物：施瓦辛格。</p><p>虽然综合实力不怎么样但银河系尬舞天团的能力不是吹的。</p><p><img src="http://media.makcyun.top/FryCDYypeLH4YJ4DPhI9KBhQiz65" alt=""></p><p>来听听这首星伴随着 Walkman 尬舞的歌。</p><p>以上就介绍了十位重要英雄。</p><p>去掉四位稍弱人物，来对比下六人组综合实力。</p><p><img src="http://media.makcyun.top/Fum9psp0q4EVjHbNwbxtgd4bXpZm" alt=""></p><p>惊队除了智商稍微弱点，其他基本无敌，这点和雷神很像，二者综合实力也差不多，可以说是新老成员中最厉害的了。</p><h4 id="灭霸"><a href="#灭霸" class="headerlink" title="灭霸"></a>灭霸</h4><p>正派说完轮到大 BOSS 灭霸出场了。</p><p><img src="http://media.makcyun.top/FvgtL_3FaForkLaIAlHHD30hFRyT" alt=""></p><p>看到灭霸就会想起电影中被他那宝石手套支配的恐惧，五一终于理解灭霸的初心了。而灭霸真实的实力如何呢，来看看他和雷神、惊奇队长三人对比。</p><p><img src="http://media.makcyun.top/FuA-Pd6bhC-7WaXvy13XaFM7YnIl" alt=""></p><p>可以看到灭霸的优缺点非常明显，优点是和托尼一样绝顶聪明，缺点就是速度慢，难怪电影中要靠宝石跑到地球来。综合来看，三者单挑的话基本五五开，灭霸戴上手套的话就另算了。</p><h3 id="漫威和-DC-英雄比"><a href="#漫威和-DC-英雄比" class="headerlink" title="漫威和 DC 英雄比"></a>漫威和 DC 英雄比</h3><p>作为两大漫画巨头，漫威和 DC 一直在明争暗斗，早些年 DC 要比漫威混得好，漫威这十几年才起来。两大公司手上都握有大量漫画角色，对比一下这两家当家英雄应该会很有意思。</p><p>DC 比较熟知的就是超人了，这里来拿雷神和超人对比下看看。可以看到超人近乎完美，比雷神聪明速度也更快，除了格斗稍弱雷神，总体来说是碾压雷神的。</p><p><img src="http://media.makcyun.top/FnOHbfcQY5Oz6vP8ncSLpLIhpH9N" alt=""></p><p>DC 其他英雄又如何呢，把 700 多位英雄六项能力值汇总得到综合实力，然后取前十名来看看。</p><ul><li>标红色的是 DC 家的</li><li>浅灰色是其他公司的</li><li>深黑色的是漫威家的</li></ul><p><img src="http://media.makcyun.top/FjU7ItEKlPyv1bdqPK05nm6u_pdC" alt=""></p><p>完全没有想到，综合实力最强的 10 位竟然有 8 位都来自 DC，漫威完全被碾压，唯一登榜的是超越者（<em>Beyonder</em>），雷神都上不了榜。</p><p>然而问题来了，<strong>拥有如此众多实力超强的英雄，DC 近些年为什么风头全被漫威压住了？</strong></p><p>榜单上排名第一得到了 600 满分无敌了，来揭晓下 TA 是谁。</p><p>就是这位 Man of Miracles，别名 Mother of Existence 宇宙的创造者，上帝是他儿子。</p><p><img src="http://media.makcyun.top/FrcxMqaKrEADbIjL_kOGrDGhJwTG" alt=""></p><h3 id="其他有意思的"><a href="#其他有意思的" class="headerlink" title="其他有意思的"></a>其他有意思的</h3><h4 id="最高的人"><a href="#最高的人" class="headerlink" title="最高的人"></a>最高的人</h4><p>很多漫威英雄五大三粗，就来扒一扒哪些角色最高。排第一的是 「Ymir」超过 300 米，他是冰霜巨人的祖先，即洛基的祖先。范迪塞尔配音的 Groot 在银护中非常高，也仅能排第 10。</p><table><thead><tr><th>num</th><th>name</th><th>height</th><th>total_score</th></tr></thead><tbody><tr><td>0</td><td>Ymir</td><td>304.80</td><td>403.0</td></tr><tr><td>1</td><td>Godzilla</td><td>108.00</td><td>418.0</td></tr><tr><td>2</td><td>Giganta</td><td>62.50</td><td>352.0</td></tr><tr><td>3</td><td>Anti-Monitor</td><td>61.00</td><td>528.0</td></tr><tr><td>4</td><td>King Kong</td><td>30.50</td><td>424.0</td></tr><tr><td>5</td><td>Bloodwraith</td><td>30.50</td><td>72.0</td></tr><tr><td>6</td><td>Utgard-Loki</td><td>15.20</td><td>386.0</td></tr><tr><td>7</td><td>Fin Fang Foom</td><td>9.75</td><td>399.0</td></tr><tr><td>8</td><td>Galactus</td><td>8.76</td><td>533.0</td></tr><tr><td>9</td><td>Groot</td><td>7.01</td><td>427.0</td></tr></tbody></table><h4 id="最壮的人"><a href="#最壮的人" class="headerlink" title="最壮的人"></a>最壮的人</h4><p>复联中浩克、灭霸都很壮，可在诸多大神面前就是小巫见大巫了。排第一的就是熟悉的哥斯拉，重达九万吨，不得不说日本人脑洞真大。第二的金刚也有九千吨。</p><table><thead><tr><th>name</th><th>weight</th><th>total_score</th><th></th></tr></thead><tbody><tr><td>0</td><td>Godzilla</td><td>90000000</td><td>418.0</td></tr><tr><td>1</td><td>King Kong</td><td>9000000</td><td>424.0</td></tr><tr><td>2</td><td>Utgard-Loki</td><td>58000</td><td>386.0</td></tr><tr><td>3</td><td>Fin Fang Foom</td><td>18000</td><td>399.0</td></tr><tr><td>4</td><td>Galactus</td><td>16000</td><td>533.0</td></tr><tr><td>5</td><td>Groot</td><td>4000</td><td>427.0</td></tr><tr><td>6</td><td>Iron Monger</td><td>2000</td><td>379.0</td></tr><tr><td>7</td><td>Sasquatch</td><td>900</td><td>291.0</td></tr><tr><td>8</td><td>Juggernaut</td><td>855</td><td>441.0</td></tr><tr><td>9</td><td>Darkseid</td><td>817</td><td>566.0</td></tr></tbody></table><p>以上就是对宇宙英雄的一些简单分析，感兴趣的话可以自己试试。Python 源代码和数据在公众号后台回复：「漫威」就可以得到。</p><p>数据采集部分的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Mon May 5 12:57:10 2019</span></span><br><span class="line"><span class="string">@author: 高级农民工</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, Pool</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://superheroapi.com/，facebook 登陆即可自动获取token</span></span><br><span class="line">token = <span class="string">'输入你的token'</span> <span class="comment"># 如果获取不到，可以微信找我提供给你</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getapi</span><span class="params">(i)</span>:</span></span><br><span class="line">    url = <span class="string">'https://superheroapi.com/api/%s/%s'</span> % (token, i)</span><br><span class="line">    data = requests.get(url).json()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseapi</span><span class="params">(item)</span>:</span></span><br><span class="line">    lst = &#123;</span><br><span class="line">        <span class="string">'id'</span>: item[<span class="string">'id'</span>],</span><br><span class="line">        <span class="string">'name'</span>: item[<span class="string">'name'</span>],</span><br><span class="line">        <span class="comment"># 提取人物战斗力值</span></span><br><span class="line">        <span class="string">'intelligence'</span>: item[<span class="string">'powerstats'</span>][<span class="string">'intelligence'</span>],</span><br><span class="line">        <span class="string">'strength'</span>: item[<span class="string">'powerstats'</span>][<span class="string">'strength'</span>],</span><br><span class="line">        <span class="string">'speed'</span>: item[<span class="string">'powerstats'</span>][<span class="string">'speed'</span>],</span><br><span class="line">        <span class="string">'durability'</span>: item[<span class="string">'powerstats'</span>][<span class="string">'durability'</span>],</span><br><span class="line">        <span class="string">'power'</span>: item[<span class="string">'powerstats'</span>][<span class="string">'power'</span>],</span><br><span class="line">        <span class="string">'combat'</span>: item[<span class="string">'powerstats'</span>][<span class="string">'combat'</span>],</span><br><span class="line">        <span class="comment"># 提取人物特征</span></span><br><span class="line">        <span class="string">'gender'</span>: item[<span class="string">'appearance'</span>][<span class="string">'gender'</span>],</span><br><span class="line">        <span class="string">'race'</span>: item[<span class="string">'appearance'</span>][<span class="string">'race'</span>],</span><br><span class="line">        <span class="string">'height'</span>: item[<span class="string">'appearance'</span>][<span class="string">'height'</span>][<span class="number">1</span>],  <span class="comment"># 取cm</span></span><br><span class="line">        <span class="string">'weight'</span>: item[<span class="string">'appearance'</span>][<span class="string">'weight'</span>][<span class="number">1</span>],  <span class="comment"># 取kg</span></span><br><span class="line">        <span class="comment"># 提取人物头像url</span></span><br><span class="line">        <span class="string">'image'</span>: item[<span class="string">'image'</span>][<span class="string">'url'</span>],</span><br><span class="line">        <span class="string">'publisher'</span>: item[<span class="string">'biography'</span>][<span class="string">'publisher'</span>],  <span class="comment"># 出版方 Marvel/DC</span></span><br><span class="line">        <span class="string">'alignment'</span>: item[<span class="string">'biography'</span>][<span class="string">'alignment'</span>]  <span class="comment"># 正派/反派</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 写入csv</span></span><br><span class="line">    write_csv(lst)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 或者写入MongoDB</span></span><br><span class="line">    <span class="comment"># write_mongodb(lst)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 下载图片拼图片墙</span></span><br><span class="line">    image = item[<span class="string">'image'</span>][<span class="string">'url'</span>]</span><br><span class="line">    save(image)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_mongodb</span><span class="params">(lst)</span>:</span></span><br><span class="line">    client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">    db = client.marvel</span><br><span class="line">    mongo_collection = db.marvel_stats</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mongo_collection.update_one(lst, &#123;<span class="string">'$set'</span>: lst&#125;, upsert=<span class="keyword">True</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'存储失败'</span>)</span><br><span class="line">    print(<span class="string">'id:%s 存储完成'</span> % lst[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_csv</span><span class="params">(lst)</span>:</span></span><br><span class="line">    content = pd.DataFrame([lst])</span><br><span class="line">    content.to_csv(<span class="string">'./marvel.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,</span><br><span class="line">                   index=<span class="keyword">False</span>, header=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="comment"># 获取头像编号</span></span><br><span class="line">    num = re.search(<span class="string">'https:.*\/(.*?).jpg'</span>, image).group(<span class="number">1</span>)</span><br><span class="line">    dir = os.getcwd() + <span class="string">'\\marvel\\'</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dir):</span><br><span class="line">        os.mkdir(dir)</span><br><span class="line">    file_path = <span class="string">'&#123;0&#125;\\&#123;1&#125;.&#123;2&#125;'</span>.format(dir, num, <span class="string">'jpg'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(image)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">with</span> open(file_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(response.content)</span><br><span class="line">                print(<span class="string">'编号：%s下载完成'</span> % num)</span><br><span class="line">    <span class="keyword">except</span> exceptions:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(i)</span>:</span></span><br><span class="line">    data = getapi(i)</span><br><span class="line">    parseapi(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    pool = Pool()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">732</span>):</span><br><span class="line">        <span class="comment"># 多进程</span></span><br><span class="line">        pool.apply_async(main, args=[i, ])</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">'总共用时&#123;&#125;s'</span>.format((end - start)))</span><br></pre></td></tr></table></figure><p>最后，欢迎加入我的知识星球，还有更多干货。</p><p><img src="http://media.makcyun.top/FmK9TW8XT7fcwSc1fQnlSixQUUpj" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;一生漫威粉。&lt;/p&gt;
    
    </summary>
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>漫威电影宇宙，宇宙票房收割机</title>
    <link href="https://www.makcyun.top/2019/05/05/data_analysis&amp;mining04.html"/>
    <id>https://www.makcyun.top/2019/05/05/data_analysis&amp;mining04.html</id>
    <published>2019-05-05T08:16:16.000Z</published>
    <updated>2019-05-05T13:05:40.596Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>漫威电影宇宙已经无敌，霸占全球前十票房中的五席。</p><a id="more"></a><p><strong>摘要</strong>：看的不是电影是青春。</p><p>《复仇者联盟4：终局之战》上映已十天有余，刷了两遍 IMAX，在家又重温了之前的 21 部电影，感觉就是一个字：爽，两个字：难舍。</p><p>入坑漫威不早也不算晚，2011 年大一的时候偶然看了《钢铁侠》和《绿巨人》后被圈粉。12 年复联上映时和室友一起去了学校附近的影城看，晚上看完点个铁板烧边吃边讨论剧情。</p><p>八九年一晃而过，电影迎来了终局，人也从大一新生变成了社会狗。</p><p>其实，追漫威跟追星没什么区别，看电影跟看演唱会也没什么区别。</p><p>你要很早开始追周杰伦，就会期待他的新专辑和演唱会，曲子一出你可能就知道是什么歌名，出自哪张专辑，会想去看他的演唱会。如果你没有追过或者很晚才追他，那真的很遗憾，因为你错过的不仅是他，更是自己的青春。</p><p>多年下来，才发现一直不变的是唯有 NBA 和漫威电影。然而今年是悲伤的一年，喜欢的球星退役，喜欢的电影迎来终局。</p><p>一部「着看着就笑了，笑着笑着就泪了的电影」值得说道几句。</p><p>自上映之日起，这部电影就不断刷爆国内外各种记录，相信下映之时必将名留史册。</p><p>在国内，上映 12 天票房超过 38 亿，飙升到中国电影历史总票房第三位，离前不久才上升到第二位 的《流浪地球》只有 8 亿只差，目测最终二者排名会交换。</p><p>《复联4》上映之前，国内前五名都是国产电影，进入前十名的只有两部，由速度与激情系列包揽，影片一上映很快就飙到了前三甲。</p><p><img src="http://media.makcyun.top/FhU95o66886hjPsxIKabDPenCxt1" alt="数据来源：猫眼票房"></p><p>国际上，目前票房超过了 19 亿美元，上升到第五名，大概率会超过 21 亿的《泰坦尼克号》，勇夺全球票房亚军，甚至能挑战票房纪录保持了十年之久的《阿凡达》。</p><p>作为漫威电影宇宙布局 11 年、22 部电影的最后一部，以这样的成绩结束太完美了。</p><p><img src="https://uploader.shimo.im/f/hzjkp78NOK0cVHWe!thumbnail" alt="最爱银河护卫队和雷神3"></p><p>这 22 部电影很恐怖，<strong>票房最高的 5 部进入了全球历史总票房的前 10 名，占据半壁江山。</strong></p><table><thead><tr><th style="text-align:left"><strong>排名</strong></th><th style="text-align:center"><strong>电影</strong></th><th style="text-align:center"><strong>票房(亿美元)</strong></th><th style="text-align:center"><strong>发行商</strong></th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:center">阿凡达</td><td style="text-align:center">27.88</td><td style="text-align:center">二十世纪福斯</td></tr><tr><td style="text-align:left">2</td><td style="text-align:center">泰坦尼克号</td><td style="text-align:center">21.87</td><td style="text-align:center">派拉蒙影业 / 二十世纪福斯</td></tr><tr><td style="text-align:left">3</td><td style="text-align:center">星球大战：原力觉醒</td><td style="text-align:center">20.68</td><td style="text-align:center">华特迪士尼影业</td></tr><tr><td style="text-align:left">4</td><td style="text-align:center"><strong>复仇者联盟3：无限战争</strong></td><td style="text-align:center">20.48</td><td style="text-align:center">华特迪士尼影业</td></tr><tr><td style="text-align:left">5</td><td style="text-align:center"><strong>复仇者联盟4：终局之战</strong></td><td style="text-align:center">17.86</td><td style="text-align:center">华特迪士尼影业</td></tr><tr><td style="text-align:left">6</td><td style="text-align:center">侏罗纪世界</td><td style="text-align:center">16.72</td><td style="text-align:center">环球影业</td></tr><tr><td style="text-align:left">7</td><td style="text-align:center"><strong>复仇者联盟</strong></td><td style="text-align:center">15.19</td><td style="text-align:center">华特迪士尼影业</td></tr><tr><td style="text-align:left">8</td><td style="text-align:center">速度与激情7</td><td style="text-align:center">15.16</td><td style="text-align:center">环球影业</td></tr><tr><td style="text-align:left">9</td><td style="text-align:center"><strong>复仇者联盟2：奥创纪元</strong></td><td style="text-align:center">14.05</td><td style="text-align:center">华特迪士尼影业</td></tr><tr><td style="text-align:left">10</td><td style="text-align:center"><strong>黑豹</strong></td><td style="text-align:center">13.47</td><td style="text-align:center">华特迪士尼影业</td></tr></tbody></table><p>条形图看着更清楚：</p><p><img src="http://media.makcyun.top/FnazAQOqIOxqmQ4iHV8xRPsFqLsy" alt="数据来源：Box Office Mojo"></p><p>上面的条形图为了突出漫威电影和非漫威电影，设置了两种颜色。而通常的图形是默认一种颜色，或者通过 cmap 光谱参数各一种颜色。有两种方法可以实现自定义设置柱状颜色。</p><p>一种是通过<code>barh[i].set_color(colors)</code>单独设置，适合要设置的颜色比较少的情况，当数量比较多的时候，可以采取第二种方法就是先设置好颜色。</p><p>这里，使用了第二种方法，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全球票房 </span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">movies = [<span class="string">'阿凡达'</span>,<span class="string">'泰坦尼克号'</span>,<span class="string">'星球大战：原力觉醒'</span>,<span class="string">'复仇者联盟3'</span>,<span class="string">'复仇者联盟4'</span>,<span class="string">'侏罗纪世界'</span>,<span class="string">'复仇者联盟'</span>,<span class="string">'速度与激情7'</span>,<span class="string">'复仇者联盟2'</span>,<span class="string">'黑豹'</span>]</span><br><span class="line"></span><br><span class="line">income = [<span class="number">27.88</span>,<span class="number">21.87</span>,<span class="number">20.68</span>,<span class="number">20.48</span>,<span class="number">19.14</span>,<span class="number">16.72</span>,<span class="number">15.19</span>,<span class="number">15.16</span>,<span class="number">14.05</span>,<span class="number">13.47</span>,]</span><br><span class="line">data_movies = pd.DataFrame(&#123;<span class="string">'movie'</span>:movies,<span class="string">'income'</span>:income&#125;)[::<span class="number">-1</span>]<span class="comment">#倒序</span></span><br><span class="line"><span class="comment"># 绘制票房条形图</span></span><br><span class="line"><span class="comment"># https://stackoverflow.com/questions/37447056/different-colors-for-rows-in-barh-chart-from-pandas-dataframe-python</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">grey = <span class="string">'#969696'</span>  <span class="comment"># 深灰</span></span><br><span class="line">red = <span class="string">'#E24A33'</span>  <span class="comment"># 红色</span></span><br><span class="line"><span class="comment"># 批量设置颜色</span></span><br><span class="line">color = [red, red, grey, red, grey, red, red, grey, grey, grey]</span><br><span class="line">barh = ax.barh(np.arange(<span class="number">10</span>),data_movies[<span class="string">'income'</span>],color=color)</span><br><span class="line"><span class="comment"># 或者单独设置颜色</span></span><br><span class="line"><span class="comment"># barh[2].set_color = [grey]</span></span><br><span class="line"><span class="keyword">for</span> y, x <span class="keyword">in</span> enumerate(data_movies[<span class="string">'income'</span>].values.tolist()):</span><br><span class="line">    ax.text(x<span class="number">-1</span>, y<span class="number">-0.1</span>, <span class="string">'%s'</span> % round(x, <span class="number">1</span>),</span><br><span class="line">            ha=<span class="string">'right'</span>,</span><br><span class="line">            size=<span class="number">15</span>,</span><br><span class="line">            color=<span class="string">'#FFFFFF'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_yticks(np.arange(<span class="number">10</span>)) <span class="comment"># 设置y轴标签数量</span></span><br><span class="line">ax.set_yticklabels(data_movies[<span class="string">'movie'</span>].tolist(), size=<span class="number">15</span>) <span class="comment"># 设置y轴标签</span></span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([<span class="string">'票房(亿美元)'</span>], loc=<span class="string">'best'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">'global.jpg'</span>, dpi=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><p><strong>22 部电影总票房超过 200 亿美元</strong>，这个数字有多牛逼，用国产电影的数据来对比一下就知道了，过去 2015-2018 这四年一共上映了超过 1400 部国产电影，总票房是 1277 亿元。</p><p><img src="http://media.makcyun.top/FnZQ-FFx43bDp2EOhSlFHWnk5tWY" alt="数据来源：国家新闻出版广电*总局电影局"></p><p>条形图：</p><table><thead><tr><th style="text-align:center"><strong>年份</strong></th><th style="text-align:center"><strong>国产电影票房(亿元）</strong></th><th style="text-align:center"><strong>国产电影数量</strong></th></tr></thead><tbody><tr><td style="text-align:center">2015</td><td style="text-align:center">297.0</td><td style="text-align:center">278</td></tr><tr><td style="text-align:center">2016</td><td style="text-align:center">288.4</td><td style="text-align:center">383</td></tr><tr><td style="text-align:center">2017</td><td style="text-align:center">312.1</td><td style="text-align:center">375</td></tr><tr><td style="text-align:center">2018</td><td style="text-align:center">379.3</td><td style="text-align:center">393</td></tr><tr><td style="text-align:center">汇总</td><td style="text-align:center">1276.8</td><td style="text-align:center">1429</td></tr></tbody></table><p>所以，<strong>漫威的 22 部电影票房等于过去四年全部国产电影票房的总和</strong>。</p><p><img src="http://media.makcyun.top/ForKoS3G-udmrZNEoQHNMHAxJMg5" alt=""></p><p>上面条形图设置的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">year = [<span class="string">'2015年'</span>,<span class="string">'2016年'</span>,<span class="string">'2017年'</span>,<span class="string">'2018年'</span>] </span><br><span class="line">income = [<span class="number">297.0</span>,<span class="number">288.4</span>,<span class="number">312.1</span>,<span class="number">379.3</span>]</span><br><span class="line">quantity= [<span class="number">278</span>,<span class="number">383</span>,<span class="number">375</span>,<span class="number">393</span>]</span><br><span class="line">movie_cn = pd.DataFrame(&#123;<span class="string">'year'</span>:year,<span class="string">'income'</span>:income,<span class="string">'quantity'</span>:quantity&#125;)</span><br><span class="line">width = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">barh = ax.barh(np.arange(<span class="number">4</span>),</span><br><span class="line">               movie_cn[<span class="string">'income'</span>],</span><br><span class="line">               width,</span><br><span class="line">               color=grey</span><br><span class="line">               )</span><br><span class="line">barh2 = ax.barh(np.arange(<span class="number">4</span>)+<span class="number">0.3</span>,</span><br><span class="line">                movie_cn[<span class="string">'quantity'</span>],</span><br><span class="line">                width, color=red</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置添加数值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autolabel</span><span class="params">(bars)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> bar <span class="keyword">in</span> bars:</span><br><span class="line">        width = bar.get_width()</span><br><span class="line">        ax.text(width*<span class="number">0.97</span>, bar.get_y() + bar.get_height()/<span class="number">2</span>,</span><br><span class="line">                <span class="string">'%d'</span> % int(width),</span><br><span class="line">                ha=<span class="string">'right'</span>, va=<span class="string">'center'</span>,</span><br><span class="line">                color=<span class="string">'#FFFFFF'</span>,</span><br><span class="line">                size=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">ax.set_yticks(np.arange(<span class="number">4</span>)+<span class="number">0.2</span>)</span><br><span class="line">ax.set_yticklabels(movie_cn[<span class="string">'year'</span>].tolist(), size=<span class="number">15</span>)</span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">430</span>)</span><br><span class="line"></span><br><span class="line">ax.legend([<span class="string">'票房(亿元)'</span>, <span class="string">'影片数量(部)'</span>], loc=<span class="string">'best'</span>)</span><br><span class="line"></span><br><span class="line">autolabel(barh)</span><br><span class="line">autolabel(barh2)</span><br></pre></td></tr></table></figure><p>无数的英雄出现在了这些电影中，钢铁侠、雷神、浩克、惊奇队长、灭霸等人各显神通，个人最喜欢星爵。</p><p>英雄一多就出现了一个很有意思的问题：<strong>这些人中到底谁最厉害？</strong></p><p>答案仁者见仁，智者见智。不过在漫画中每位英雄都给出了详细的战斗力指数，之后会写一篇 Python 分析宇宙英雄数据，对比英雄综合实力。</p><p><img src="http://media.makcyun.top/FjC-dqDPXyXpXv1Ai5RnaCT9SnTy" alt=""></p><p>资料：</p><p><a href="https://www.useit.com.cn/thread-21819-1-1.html" target="_blank" rel="noopener">猫眼 2018 票房分析报告</a></p><p><a href="http://www.chyxx.com/industry/201901/705193.html" target="_blank" rel="noopener">中国电影票房分析</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;漫威电影宇宙已经无敌，霸占全球前十票房中的五席。&lt;/p&gt;
    
    </summary>
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>第一台 Macbook Pro</title>
    <link href="https://www.makcyun.top/2019/04/30/life09.html"/>
    <id>https://www.makcyun.top/2019/04/30/life09.html</id>
    <published>2019-04-30T08:16:24.000Z</published>
    <updated>2019-05-05T12:47:49.626Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Macbook Pro 开箱及使用心得。</p><a id="more"></a><p><strong>摘要：</strong>Macbook Pro 开箱及使用心得。</p><p>转眼间，Macbook Pro 入手已一个月有余，忘了写写购买和使用心得，那就现在开始吧。</p><p>正式介绍 Mac 之前先说说一直用 Windows 的经历。</p><p>2011 年，大一下学期开学买了人生第一台笔记本，不到 4,000 块的华硕。一直不懂电脑，干的最多的事就是用用 Office 、上上网，一直持续到 2017 年。之后，电脑性能越来越差干什么都卡，开个机要两三分钟。后来换上了 SSD 和内存条，老机焕发新春。</p><p>苹果电脑老早就听过，印象中一直是设计师的专属或者拿来装逼用的，加上有贵，从来没想过买一台来用用。直到去年接触编程圈后，几乎无一例外都说过「搞编程，用 Mac」、「用了 Mac 不想再回 Windows」之类的话。这才对苹果电脑产生了兴趣，网上了解到 Mac 和 Windows 使用起来根本不同，行云流水般的操作，顿生好感。</p><p>3 月份决定买一台，上官网看了看，比一般电脑贵多了，越喜欢的越贵，最后想既然打算剁手就干脆多狠点，一次到位省得以后纠结惦记。最后看上了 2018 款、15 英寸、512GB SSD 的配置，正常价格是 21,988 元，可以分 12 期免息。3 月份正值开学季，大学生教育优惠价是 20,388 元，仍然很贵。</p><p>上京东看了下价格一样，会额外送一个售价 2,288 元 的 Beats Solo3 耳机，图案是很丑的迪士尼，二手能卖 1000 算不错。一时头脑发热准备下手，由于已经毕业便找到了还在学校的师弟帮忙，通过京东的学生认证后就可以下单。</p><p>这时又想起港版可能会便宜些，上香港官网看了下，港币价格是一样的 20,388 ，那几天的汇率是 0.85，算下来差不多可以便宜 3,000 块。之后找到了一位在香港上大学的朋友，使用学生证以教育优惠价代买了，实付价 17,500 元。</p><p>想享受香港教育优惠需要注意几点：</p><ul><li>只有在香港读书的大学生才能享受教育优惠，内地的学生证不行。付款可以现金或刷卡，如果是刷卡一定是刷学生本人的卡，不能用你的卡，因为苹果要登记到系统里，付现金就没有事。</li><li>想现金买，最好在银行提前换好港币带过去，不然在香港银联取现金的话，很多银行卡有取现金额限制，一般都是不能超过 10000 港币，而且还有很高的手续费。各银行的手续费见下面。由于我是头天下午突然决定第二天就让朋友买，所以来不及换钱，刚好我用的是浦发卡，手续费最低，让朋友去到香港后取，结果在 ATM 取钱发现 10000 都取不出，十分不方便。最后还是刷了学生朋友本人的卡，之后转账给她才成功。</li></ul><p>买好以后拆了外包装让朋友带到深圳然后顺丰寄到家。担心快递损坏，买了 2 万的报价，保费是千分之一，也就是 200 很贵了，然后在外面又钉了个木框，双保险。</p><p>拿到电脑后，查了下充电次数，只有 1 次，这机也太新了。充电次数能判断这台电脑是否是新机，太多的话就要注意了。还可以通过查询序列号验证是否是正品，查询网址：<a href="https://checkcoverage.apple.com/cn/zh/" target="_blank" rel="noopener">在此</a>。</p><p>有些人会纠结要不要花上千块额外多买一年的 Apple Care ，个人觉得没必要。新机苹果免费全球保修 1 年，香港买的可以在内地保，保存好发票和三包凭据就行了。</p><p>下面就上两张开箱图。</p><p>Mackbook Pro 2018</p><p>内存：16GB</p><p>SSD： 512GB</p><p><img src="http://media.makcyun.top/win/20190505/uNTI14yRG6Q9.jpg?imageslim" alt="mark"></p><p><img src="http://media.makcyun.top/win/20190505/1XgOT1kX5dab.jpg?imageslim" alt="mark"></p><p>Changelog</p><ul><li>新建博文：2019/5/5</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Macbook Pro 开箱及使用心得。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>比百度更好用的软件下载网站</title>
    <link href="https://www.makcyun.top/2019/04/27/weekly_sharing26.html"/>
    <id>https://www.makcyun.top/2019/04/27/weekly_sharing26.html</id>
    <published>2019-04-27T08:16:16.000Z</published>
    <updated>2019-05-11T03:32:28.979Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>下载软件有门道。</p><a id="more"></a><p><strong>摘要</strong>：下载软件有门道。</p><p>这里是「每周分享」的第 26 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>Windows 和 Andriod 软件下载</strong>。</p><p>之前很多期分享推荐了不少软件，却从没分享过有哪些不错的软件下载网站，尤其在听到有人说下载软件很简单，「<strong>电脑上百度，手机上应用市场</strong>」这样的话后，决定这一期来说说。</p><p>百度或者应用市场不是不好，但你值得更好的。下面来推荐几个我常用的软件下载网站。</p><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><h3 id="大眼仔"><a href="#大眼仔" class="headerlink" title="大眼仔"></a>大眼仔</h3><p><a href="http://www.dayanzai.me/" target="_blank" rel="noopener">http://www.dayanzai.me/</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknicGuYezgrqdKMXiaRWKBnEN52a1jwUkgtpEL4JqPOzslQoMrgD32dDxw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>自从 ZD423 站长跑路后，<strong>大眼仔</strong> 就成了我下载软件的首选，一直未变过。这个网站有大量免费的汉化和破解软件，堪称业界仅存良心。</p><p>网站界面干净，没有乱七八糟的广告，博主还是个很有情怀的人。当 ZD423 、殁漂遥这些网站都一一消失时，它还能一直坚挺，且用且珍惜吧。</p><h3 id="异次元"><a href="#异次元" class="headerlink" title="异次元"></a>异次元</h3><p><a href="https://www.iplaysoft.com" target="_blank" rel="noopener">https://www.iplaysoft.com</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknqttwLoI0eW8tUXianSxicQmGDLwIT3nLLUIKFp6r8OIKTGmy5QFOVYeQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>这个网站软件资源非常丰富，从 Win、Mac 到 Linux ，从 Andriod 到 iOS 都有。此外还有详细的软件使用教程，授人以鱼也授人以渔。</p><h3 id="I-Tell-You"><a href="#I-Tell-You" class="headerlink" title="I Tell You"></a>I Tell You</h3><p><a href="https://msdn.itellyou.cn/" target="_blank" rel="noopener">https://msdn.itellyou.cn/</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknZYegDKbRa96Pofl6yJ5H5C962GOxj7icCJI7qlROqJuS48qlcbY7lcA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>很多人常常为不知在哪儿下 Office、操作系统这类软件而发愁，百度上搜了十页八页结果都不行，如果你知道这个网站，那就不会再有这些烦恼。</p><p>在上面能找到很多专业软件，版本齐全，比如从 Office 95 到 Office 2019，从 Windows 98 到 Windows 10 的每一个版本都有。这些软件都是官方版本，提供 SHA1 验证，不用再担心下载的是被人动过手脚的软件。</p><h3 id="吾爱破解"><a href="#吾爱破解" class="headerlink" title="吾爱破解"></a>吾爱破解</h3><p><a href="https://www.52pojie.cn/forum.php" target="_blank" rel="noopener">https://www.52pojie.cn/forum.php</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknoicCqX0edkoZA2XSpr7JibpicVxnKH8iawRHpQDmy9vjDaC1hAW3dTvH6g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>说起软件网站，一定不能没有大名鼎鼎的 52。不要因为这个网站界面看起来有点乱就小瞧它，实际上很多全网一手资源都是在这个网站发布的。当你下次想要找软件资源时，不妨在站内搜搜看，也许就有惊喜发现。</p><p>我收藏夹里还有很多 Windows 软件网站，但觉得这四个基本就够了，多了反而不知道用哪个好。下面来说说下载安卓 App 的好去处。</p><h2 id="安卓软件"><a href="#安卓软件" class="headerlink" title="安卓软件"></a>安卓软件</h2><h3 id="酷安"><a href="#酷安" class="headerlink" title="酷安"></a>酷安</h3><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/tRrxSF9IUIMD4vTRkhayX2IFH40icsokn4HEQr1IYjdHuF9XOiaSFhBxedr5GIA2khBts8qyldCwBlwkydGuEMAQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>下载安卓软件，首选必然是「酷安」，我此前也说过很多次。你如果习惯了到手机自带的各种应用市场下载软件，那使用酷安后一定会觉得好像发现了新世界，感叹原来还有这么多神奇好用的 App。</p><h3 id="手机乐园"><a href="#手机乐园" class="headerlink" title="手机乐园"></a>手机乐园</h3><p><a href="https://soft.shouji.com.cn/" target="_blank" rel="noopener">https://soft.shouji.com.cn/</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknX2YibVzCdVSCMNCbJMWF60icZeag7aRnAoyvxLdErShId2V8lK12ibIHA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>在酷安可以找到各种各样的 App，但是你如果想下载旧版本 App 来用，那「手机乐园」就是你的乐园。这个网站上的软件都提供了非常全历史版本供下载。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknZIUtf5cxia0sTibEQcJYK0icWtibRlZ4V6jo7Oy6HYTic7mIia4vIWzacsvQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>你可能诧异为什么软件不用最新版，毕竟新版本又好看，功能又多。事实上，现在绝大多数软件都是越更新越臃肿，加入很多用不着的功能，甚至还会有广告。</p><p>所以我手机上基本没有最新版的 App，都到这个网站上去找到能用的旧版本。</p><h3 id="艾薇百科"><a href="#艾薇百科" class="headerlink" title="艾薇百科"></a>艾薇百科</h3><p><a href="https://www.aiweibk.com/sjrj/" target="_blank" rel="noopener">https://www.aiweibk.com/sjrj/</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknYNndicjyexjKMVibYoI68uPukCXsMxcy26CJoEwBz1CgudibVmqEETjPw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>看到这个名字不要想歪，是个下载软件的地方。</p><h3 id="手机发烧友"><a href="#手机发烧友" class="headerlink" title="手机发烧友"></a>手机发烧友</h3><p><a href="http://htcui.com/" target="_blank" rel="noopener">http://htcui.com/</a></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/tRrxSF9IUIMD4vTRkhayX2IFH40icsoknicngk8RZoMQT9AaZiaJEOIGccrkdJ3c2o17uEkwxhQJjLUraRTXBzRgA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="mark"></p><p>这个网站也不错。</p><p>好就介绍这么多网站，不多不少，8 个，用好上面网站足以解决软件下载难的问题。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;下载软件有门道。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>最后一把键盘：HHKB Pro2</title>
    <link href="https://www.makcyun.top/2019/04/26/life11.html"/>
    <id>https://www.makcyun.top/2019/04/26/life11.html</id>
    <published>2019-04-26T08:16:24.000Z</published>
    <updated>2019-05-05T12:47:03.509Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>HHKB Pro2 开箱和使用心得。</p><a id="more"></a><p><strong>摘要：</strong>HHKB Pro2 开箱和使用心得。</p><blockquote><p>美西的牛仔会将死去的马留在原地，继续扛着马鞍长途跋涉，穿越一望无垠的沙漠。因为马是消耗品，而马鞍却是与人体融合在一起的「知己」。</p><p>电脑是消耗品，键盘却是传递情感，相伴一生的「挚友」。</p><p>—— 男人也应该享有「剁手」的理 jie 由 kou。</p></blockquote><p>昨天一早，顺丰小哥敲门送上了心心念念的 HHKB Pro2，距离亚马逊下单刚好一周，决定买它也不过一周。发现慢慢养成了一种果断的消费理念，<strong>值得的东西越早买越好，因为：买了，心疼一阵，不买，一直心疼</strong>。</p><p>下面就来写写这款键盘的开箱心得和使用体验吧。</p><p>分别在京东、淘宝和亚马逊上看了价格。京东最贵，自营要 1900，非自营 1700 ；淘宝近 1700 不过送几样配件，官方授权店：<a href="https://keroro-home.taobao.com/" target="_blank" rel="noopener">军曹の宅窝</a></p><p>亚马逊最便宜，1368 加上 130 左右的税费，1500 拿下。本来是有运费的，但使用了免费的 Prime 会员，可以免邮。下单后，霓虹直邮，一周就到速度挺快。</p><p>购买链接：<a href="https://www.amazon.cn/s?k=hhkb+pro2&amp;__mk_zh_CN=%E4%BA%9A%E9%A9%AC%E9%80%8A%E7%BD%91%E7%AB%99&amp;crid=3A48KRSO5HPN6&amp;sprefix=hhkb%2Caps%2C141&amp;ref=nb_sb_ss_i_3_4" target="_blank" rel="noopener">中亚 HHKB Pro2</a></p><p><img src="http://media.makcyun.top/FohLi-1EXqC6AbuwvjxwZuAvwNbj" alt=""></p><p>开箱图：</p><p><img src="http://media.makcyun.top/FpqZIr4j37FbwD685D_G2KSQb-ji" alt=""></p><p><img src="http://media.makcyun.top/FsguJQxGIeui4lKIN1Wl6Rx7DmBA" alt=""></p><p><img src="http://media.makcyun.top/FvIN-XkITR7YTbtiGotEZfr6uMcu" alt=""></p><p>很多人说这是一把只适合程序员玩的键盘。因为只有 60 个键，少了方向键、数字键，Vim 或者 Emacs 党用会觉得很顺手。也有人说适合 Mac 用户，Windows 用户会差很多。</p><p>总之，我目前是属于这些人口中的「不适合的那一类」：不会用 Vim、 Emacs，刚开始 Mac 不久不熟练。相信不少朋友都像属于这一类，如果你不属于那也就没必要看此文了。等后续用熟了再来写感受。</p><p><strong>Changelog</strong></p><ul><li>2019/4/26：晒购买体验</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;HHKB Pro2 开箱和使用心得。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>一个快速搜索筛选豆瓣好书的网站</title>
    <link href="https://www.makcyun.top/2019/04/20/weekly_sharing25.html"/>
    <id>https://www.makcyun.top/2019/04/20/weekly_sharing25.html</id>
    <published>2019-04-20T08:16:16.000Z</published>
    <updated>2019-04-18T11:16:30.639Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>解决找书时间。</p><a id="more"></a><p><strong>摘要</strong>：多看书，看好书。</p><p>这里是「每周分享」的第 25 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>读书和买书</strong>。</p><p>前天给各位争取了「图书日」在当当买书送优惠码的福利，能省点是点，你要打算买还没买可以去瞧瞧。</p><p>传送门：</p><p>一直以为我国人均读书量应该不少，但实际上跟其他国家比起来，差了老远。</p><blockquote><p>中国每人年均读书 <strong>4.66</strong> 本<br>韩国每人年均 <strong>11</strong> 本<br>法国每人年均<strong>14</strong>本<br>日本每人年均 <strong>40</strong> 本<br>德国每人年均 <strong>47</strong> 本<br>俄罗斯每人年均 <strong>55</strong> 本<br>犹太每人年均 <strong>64</strong> 本</p></blockquote><p>以前不爱看书，这两年自学一些东西后才养成主动去找书看的习惯。才知道看书花时间，找书更花时间。国内书评做得比较好的也就豆瓣了，可以根据大家对一本书的评分、打分人数和评论去选择要不要买/看这本书，不过书找起来仍然挺费时间。</p><p>最近发现了一个神网站，一大佬爬了豆瓣上 300 多万本书后，搭建出来这么个搜索网站。可以在上面快速筛选好书，比豆瓣页面简洁、搜索功能更强大。</p><p>网站上有三个搜索选项：书名关键字、分数和打分人数，设置好选项后就可以搜索了，适合去找某一类别下的书。</p><p><img src="http://media.makcyun.top/FqcJ40U0usB0qUPgho4M0cR8K4ln" alt=""></p><p>举个例子，比如想找 Python 方面的好书，可以设置筛选范围在 9 分以上、打分人数超过 100 人，然后就会搜出下面这些书来：</p><p><img src="http://media.makcyun.top/FsnnAnZcc9BNXv4yPTFUPZzx_Edz" alt=""></p><p>质量都不错，要想了解每本书的详情可以点击进入豆瓣的主页查看。</p><p>再比如想看看写作方面有哪些好书，可以设置评分 8.5 分以上，打分人数超过 500 人，就会搜到这些书：</p><p><img src="http://media.makcyun.top/FuEksrr7rj1N8hVGk2h9whrP54CG" alt=""></p><p>怎么样，是不是比豆瓣更好用？书找起来快多了，虽说不绝对权威，但可以做很好的参考。</p><p><strong>网站地址</strong>：<a href="http://sobook.lanbing510.info/" target="_blank" rel="noopener">http://sobook.lanbing510.info/</a></p><p>另外，作者还很 nice 地提供了各类别下的书单文件，方便在 Excel 中搜索筛选。</p><p><img src="http://media.makcyun.top/FjlwF9Axh0J5OzygPDxd4G3-3qGX" alt=""></p><p><img src="http://media.makcyun.top/FiUCfEvSLrdiBYbutlKxEd7MOVZb" alt=""></p><p><strong>GitHub 库地址</strong>：<a href="https://github.com/lanbing510/DouBanSpider" target="_blank" rel="noopener">https://github.com/lanbing510/DouBanSpider</a></p><p>有这两个网址找书再也不用发愁了，感兴趣可以去搜搜看，不错的书可以下单。老规矩，我下载了全部书单文件，如需可以在后台回复：「<strong>豆瓣书</strong>」得到。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;解决找书时间。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Python 一键制作微信好友图片墙</title>
    <link href="https://www.makcyun.top/2019/04/19/Python_learning04.html"/>
    <id>https://www.makcyun.top/2019/04/19/Python_learning04.html</id>
    <published>2019-04-19T08:16:16.000Z</published>
    <updated>2019-05-17T07:34:19.870Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>wxpy、pyinstalller 库的使用。</p><a id="more"></a><p>上午发了张我微信近 2000 位好友的头像拼图，让大伙儿看能不能快速找到自己的头像，没想到反响很强烈，引得阵阵惊呼与膜拜，没有料到。</p><p><img src="http://media.makcyun.top/win/20190420/QHOQuMpm7gum.jpg?imageslim" alt=""></p><p>有没有犯密集恐惧症？这并不震撼，如果你有 5000 位好友的话，做出来的图看着会更刺激些。</p><p>看完了图，你可能想知道这个图咋做出来的，不会是我闲着无聊把把好友头像一个个保存下来再用 PS 拼的吧？</p><p>自然不是了，Python 做的，是不是觉得没有 Python 干不了的事儿。其实，这种图很早就有人玩过了，不过下面还是来说说怎么做出来，这样你也可以做一个自己微信好友的图片墙。</p><p><strong>有两种方法，一种简单的，不用接触 Python 代码，一种稍微复杂点，需要写代码。</strong></p><p>先说简单的方法，只需要两步：运行程序然后扫微信二维码就行了。剩下的交给程序自己蹦跶，泡杯茶在电脑前等待几分钟左右就可以得到图片，具体的等待时间视微信好友数量而不同，我近 2000 好友，用时 10 分钟左右。</p><p>一个简单的操作示意图：</p><p><img src="http://media.makcyun.top/win/20190420/nv3gtyIaU7Pp.png?imageslim" alt=""></p><p>几分钟后就可以得到上面的图片了。</p><p>其实到这儿就完了，是不是很简单。</p><hr><p>你要感兴趣怎么实现的，可以往下看用 Python 代码怎么实现的，代码不长，60 行就可以搞定。</p><p>核心是利用三个个库：</p><ul><li>wxpy 库，用于获取好友头像然后下载</li><li>Pillow 库，用于拼接头像</li><li>Pyinstaller 库，用来打包 Python 程序成 exe 文件</li></ul><p>程序通过三个函数实现，第一个 creat_filepath 函数生成图片下载文件路径，第二个 save_avatar 函数循环获取微信好友头像然后保存到本地，第三个 joint_avatar 函数就是把头像拼接成一张大图。</p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> wxpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建头像存放文件夹</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">creat_filepath</span><span class="params">()</span>:</span></span><br><span class="line">    avatar_dir = os.getcwd() + <span class="string">"\\wechat\\"</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(avatar_dir):</span><br><span class="line">        os.mkdir(avatar_dir)</span><br><span class="line">    <span class="keyword">return</span> avatar_dir</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存好友头像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_avatar</span><span class="params">(avatar_dir)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化机器人，扫码登陆</span></span><br><span class="line">    bot = Bot()</span><br><span class="line">    friends = bot.friends(update=<span class="keyword">True</span>)</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> friend <span class="keyword">in</span> friends:</span><br><span class="line">        friend.get_avatar(avatar_dir + <span class="string">'\\'</span> + str(num) + <span class="string">".jpg"</span>)</span><br><span class="line">        print(<span class="string">'好友昵称:%s'</span> % friend.nick_name)</span><br><span class="line">        num = num + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接头像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">joint_avatar</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="comment"># 获取文件夹内头像个数</span></span><br><span class="line">    length = len(os.listdir(path))</span><br><span class="line">    <span class="comment"># 设置画布大小</span></span><br><span class="line">    image_size = <span class="number">2560</span></span><br><span class="line">    <span class="comment"># 设置每个头像大小</span></span><br><span class="line">    each_size = math.ceil(<span class="number">2560</span> / math.floor(math.sqrt(length)))</span><br><span class="line">    <span class="comment"># 计算所需各行列的头像数量</span></span><br><span class="line">    x_lines = math.ceil(math.sqrt(length))</span><br><span class="line">    y_lines = math.ceil(math.sqrt(length))</span><br><span class="line">    image = Image.new(<span class="string">'RGB'</span>, (each_size * x_lines, each_size * y_lines))</span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    y = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> (root, dirs, files) <span class="keyword">in</span> os.walk(path):</span><br><span class="line">        <span class="keyword">for</span> pic_name <span class="keyword">in</span> files:</span><br><span class="line">            <span class="comment"># 增加头像读取不出来的异常处理</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">with</span> Image.open(path + pic_name) <span class="keyword">as</span> img:</span><br><span class="line">                        img = img.resize((each_size, each_size))</span><br><span class="line">                        image.paste(img, (x * each_size, y * each_size))</span><br><span class="line">                        x += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> x == x_lines:</span><br><span class="line">                            x = <span class="number">0</span></span><br><span class="line">                            y += <span class="number">1</span></span><br><span class="line">                <span class="keyword">except</span> IOError:</span><br><span class="line">                    print(<span class="string">"头像读取失败"</span>)</span><br><span class="line"></span><br><span class="line">    img = image.save(os.getcwd() + <span class="string">"/wechat.png"</span>)</span><br><span class="line">    print(<span class="string">'微信好友头像拼接完成!'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    avatar_dir = creat_filepath()</span><br><span class="line">    save_avatar(avatar_dir)</span><br><span class="line">    joint_avatar(avatar_dir)</span><br></pre></td></tr></table></figure><p>可以直接在运行程序文件，也可以用 Pyinstaller 文件打包后运行。这里额外说一下 pyinstaller 打包的方法和闭坑指南。</p><p><strong>不要直接在系统中用 pyinstaller 打包</strong>，否则打包出来的 exe 文件会很大。建议在虚拟环境中打包，打包出来的 exe 文件会小很多， 10MB 左右。</p><p>虚拟环境创建很简单，简单说一下步骤：</p><p>1 安装 pipenv 和 pyinstaller 包，用于后续创建虚拟环境和打包程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pipenv</span><br><span class="line">pip install pyinstaller <span class="comment"># 已安装就不用安装了</span></span><br></pre></td></tr></table></figure><p>2 选择一个合适的目录作为 Python 虚拟环境，运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipenv install <span class="comment"># 创建虚拟环境</span></span><br><span class="line">pipenv shell <span class="comment"># 创建好后，进入虚拟环境</span></span><br></pre></td></tr></table></figure><p>3 安装程序引用的库，上面程序引用了四个库：wxpy、math、os 和 PIL，一行代码就可以完成安装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipenv install wxpy math os</span><br></pre></td></tr></table></figure><p>4 这里要额外注意 PIL 的安装，现在不用 PIL 库，而是用 Pillow 库取代，所以安装 Pillow 库就行。但不要安装最新的 6.0.0 版本，否则可能会遇到各种错误，例如：PIL 无法识别下载的 jpg 头像文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: cannot identify image file &lt;ImageFieldFile: images</span><br></pre></td></tr></table></figure><p>正确的安装方法是安装低版本，经尝试安装 4.2.1 版本没有问题，安装命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipenv install Pillow==<span class="number">4.2</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><p>5 然后打包程序就可以了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller -F C:\Users\sony\Desktop\wechat_avatar.py </span><br><span class="line"><span class="comment"># 程序路径要改成你电脑上的路径</span></span><br><span class="line"><span class="comment"># -F 表示生成单个 exe 文件，方便运行</span></span><br></pre></td></tr></table></figure><p>运行如下：</p><p><img src="http://media.makcyun.top/FuaHnCvoN-uAV3YX_15g4wC0R57I" alt=""></p><p>运行命令，1 分钟左右若显示 successfully 字样表示程序打包成功：</p><p><img src="http://media.makcyun.top/FpX3sxGov7vDgblpB55KjBoF_2uF" alt=""></p><p>接着在程序目录下找到 <code>wechat_avatar.exe</code> 文件，然后按照第一种方法那样运行就行了。</p><p>以上就是用 Python 制作微信好友图片墙的方法。</p><p>完整代码和 exe 文件可以在我的公众号：<strong>高级农民工，</strong>后台回复「微信好友」得到。</p><hr><p>参考：</p><p><a href="https://github.com/youfou/wxpy" target="_blank" rel="noopener">wxpy 库</a></p><p><a href="https://segmentfault.com/a/1190000015389565" target="_blank" rel="noopener">Pipenv – 超好用的 Python 包管理工具</a></p><p><a href="https://www.zhihu.com/question/268397385/answer/611317903" target="_blank" rel="noopener">pyinstaller打包后程序体积太大，如何解决？</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;wxpy、pyinstalller 库的使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Python学习" scheme="https://www.makcyun.top/categories/Python%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python入门" scheme="https://www.makcyun.top/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>截图、录 GIF、录视频的佳软推荐</title>
    <link href="https://www.makcyun.top/2019/04/13/weekly_sharing24.html"/>
    <id>https://www.makcyun.top/2019/04/13/weekly_sharing24.html</id>
    <published>2019-04-13T08:16:16.000Z</published>
    <updated>2019-04-13T02:27:49.396Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>款款好用。</p><a id="more"></a><p>有了它们，我把所有截图、录 GIF、录屏软件都卸了</p><p>这里是「每周分享」的第 24 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>截图、录 GIF、录视频</strong>。</p><p>平时写公众号，常需要添加个截图作辅助解释，截图不形象就录个 GIF 或者视频。不只是做公众号，日常工作也经常会有这些需求。今天就来跟大家推荐几款非常好用的软件。</p><h3 id="01-截图"><a href="#01-截图" class="headerlink" title="01 截图"></a>01 截图</h3><p><strong>Faststone Capture</strong></p><p><a href="https://www.faststone.org/FSCaptureDetail.htm" target="_blank" rel="noopener">www.faststone.org/FSCaptureDetail.htm</a></p><p><img src="http://media.makcyun.top/Fu8FfBFKjAl2uRAPCyNjwPuguWFT" alt=""></p><p>这是一款我天天都在用的老牌截图软件，体积小巧，平时隐藏在窗口端，需要截图的时候鼠标一点就可以截图，自定义快捷键则可以更快。</p><p>它提供多种截图方式：全屏、窗口截屏、矩形截屏等，还有一个实用功能是「滚动截屏」。图截好可以顺带编辑，比如添加、箭头、文字、马赛克。</p><p>除了截图，它还可以<strong>录制视频</strong>，不过不支持后期编辑，用得比较少。</p><p>我在用了 MAC 之后第一时间想下的软件就是它，遗憾地是，目前只有 Windows 版。如果你想用一款同时支持 Win 和 Mac 的截图软件，那首推下面这款。</p><p><strong>Sinpaste</strong></p><p><a href="https://zh.snipaste.com/" target="_blank" rel="noopener">zh.snipaste.com/</a></p><p><img src="http://media.makcyun.top/Fl7HxP8OzINBKesxqPoLOQBrINPo" alt=""></p><p>Sinpaste 更确切地说是一款「贴图」软件，其次是截图工具。</p><p>贴图的意思就是，截好图后可以置顶在屏幕跟前，这个功能很实用。我们经常需要在不同窗口切换查找输入信息，一般操作就是窗口来回切换，眼睛都晃晕，用了这个贴图功能后，可以直接把多个窗口截图并排放在一起，方便许多。</p><p>软件用起来很简单，F1 键截屏，F3 贴图。除了这两样还有很多其他功能，专业版更强大一些。</p><h3 id="02-录-GIF"><a href="#02-录-GIF" class="headerlink" title="02 录 GIF"></a>02 录 GIF</h3><p><strong>ScreenToGif</strong></p><p><a href="www.screentogif.com/">www.screentogif.com/</a></p><p><img src="http://media.makcyun.top/FojSX7eEg-kEjEm76yKNR7bmDTVC" alt=""></p><p>之前遇到很多人在问怎么录制 GIF，要是只推荐一款 GIF 录制软件的话，那必然是这款 ScreenToGif 。</p><p><img src="https://www.screentogif.com/screenshots/Ribbon.gif" alt=""></p><p><img src="https://www.screentogif.com/screenshots/Face.gif" alt=""></p><p>它不到 1MB 大，功能极简，打开软件就能录制。除了这些优点外，我更满意的是它提供了强大的后期编辑功能。</p><p>很多 GIF 软件只能录而不能编辑，比如不少人推荐的 LICEcap，其实一点都不实用，因为你很难一次性就录好 GIF，要在微信中插入 GIF 的话 ，体积超过 2MB 也插入不了。</p><p>我们知道 GIF 其实就是由一帧帧的图像组成的，编辑 GIF 就是编辑帧 。它提供了删减帧、加速减速帧的功能，以去除不想要的画面减小 GIF 体积。</p><p>还有添加字幕、马赛克、过渡这些功能，可以说只要你想， GIF 就能玩出花来。</p><p>如果想录制更长一点的动画， GIF 就不合适了，需要录制视频。</p><h3 id="03-录视频"><a href="#03-录视频" class="headerlink" title="03 录视频"></a>03 录视频</h3><p><strong>FlashBack</strong></p><p><a href="www.flashbackrecorder.com">www.flashbackrecorder.com</a></p><p><img src="http://media.makcyun.top/FurTigvOCOcCPf3el7i_Mwuw3_ie" alt=""></p><p>说起录视频，可能很多人会推荐大名鼎鼎的 Camtasia，我倒觉得，一般情况下其实用不着体积庞大的 Camtasia，FlashBack 足矣。</p><p>这款软件体积比 Camtasia 小多了，不管是录制还是后期编辑，速度都够快。</p><p>后期编辑功能很多，添加字幕、加速/减速帧、缩放平移这些都有，此外还可以添加音频。视频编辑好，还可以导出各种形式的文件。</p><p><img src="http://media.makcyun.top/Flw7KDfBN0if-qPtdozpCsuybyKM" alt=""></p><p>以上，就介绍这几款我一直在用的佳软。感兴趣，可以搜来试试。你要不想搜，欢迎加入我的知识星球中，或者可以打赏，我看到了也会发给你链接。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;款款好用。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>比百度网盘更好的临时文件传输方法</title>
    <link href="https://www.makcyun.top/2019/04/06/weekly_sharing23.html"/>
    <id>https://www.makcyun.top/2019/04/06/weekly_sharing23.html</id>
    <published>2019-04-06T08:16:16.000Z</published>
    <updated>2019-04-13T00:01:35.809Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>实用技能。</p><a id="more"></a><p>这里是「每周分享」的第 23 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>文件传输</strong>。</p><p>说起文件传输，很多人首先会想到某度网盘，上传好之后给对方发送一个链接就可以了，大部分时候都是可行的。不过，少数情况下就行不通了，因为这种上传方式有几个限制：</p><ul><li>对方也要有网盘账号</li><li>4GB 以内的文件</li><li>非敏感文件，比如不可描述工具</li></ul><p><img src="https://i.loli.net/2019/04/06/5ca7edd84d861.png" alt="2019-04-06_074703.png"></p><p>所以如果出现这样的情况：有人说他没用过网盘、要传输的文件很大或者网盘文件受限制，那就用不了网盘。</p><p>你可能会想到用微信、QQ 也照样可以传啊，但如果对方不是好友怎么办，关系不错倒可以加好友，就是稍微麻烦点，如果关系要一般你可能并不想加，或者你就根本不想多费这档子事。</p><p>另外，还有一种常见需求，要传输的文件很重要或者很隐私，只希望对方一人或者小范围接收。这一点，网盘链接就不合适了，因为发给对方链接后，他可能转手就发给别人了。</p><p>你可能觉得上面说的这些有点扯或者概率很低，这年头谁还没个网盘账号，谁没事会转发扩散资源。但这些确是我经历过的，后来找到了几个神器完美解决了这些痛点，下面来介绍下。</p><h3 id="FireFox-Send"><a href="#FireFox-Send" class="headerlink" title="FireFox Send"></a>FireFox Send</h3><p>网址：<a href="https://send.firefox.com/" target="_blank" rel="noopener">https://send.firefox.com/</a></p><p><img src="http://media.makcyun.top/FrXjsXRQSDu9EWX7aj7dbsapRWRP" alt=""></p><p>Firefox Send 是火狐浏览器一款产品，使用它的网页版可以传送 1GB 以内的文件，上传好后把链接发送给对方就可以。这看起来和网盘分享方式并无不同，但其实大不同。</p><p>首先，<strong>对方接收到这个链接可以直接下载，不需要任何账号</strong>。</p><p>其次，可以选择分享次数和时间限制。比如，<strong>文件只能下载 1 次，那么对方下载文件后链接就失效，别人不能再下载。</strong>适合私人之间的单次分享。如果文件泄露出去了，你就会知道肯定是对方做的。</p><p><img src="http://media.makcyun.top/Foy1YYRbjQ_qiqFKA9h9x3ujbZ7o" alt=""></p><p>分享次数和时间很灵活，<strong>既可以选择 1 到 100 次分享次数，也可以选择 5 分钟到 7 天的时间有效期，过期就作废。</strong></p><p>你可能会觉得在网页上这样传输是否安全，答案是<strong>绝对安全</strong>。</p><p>另外如果觉得网页版不是很方便，想使用软件，那就推荐下面这一款神器。</p><h3 id="SendAnywhere"><a href="#SendAnywhere" class="headerlink" title="SendAnywhere"></a>SendAnywhere</h3><p>网址：<a href="https://send-anywhere.com/" target="_blank" rel="noopener">https://send-anywhere.com/</a></p><p><img src="http://media.makcyun.top/FvUaNm_W8mF_TLuqUNglRFGRqWrT" alt=""></p><p>SendAnywhere 是一款全平台专业分享利器，手机端、PC 端、网页版、Chrome 插件都支持。其传输能力，如它名字一样出色。</p><p>下面以网页版为例介绍一下使用方式。使用起来很简单，可以选择四种传输方式：</p><ul><li>6 位数字</li><li>二维码</li><li>链接</li><li>邮箱</li></ul><p><img src="http://media.makcyun.top/FhrDj7IxZFXu-ikPI5f9NEuEiF5-" alt=""></p><p>使用 6 位数字密码和二维码的直接传输方式，文件只有十分钟有效期，适合紧急或者小文件传输。把 6 位密码或者二维码发送给对方就可以下载。需要注意的是，<strong>下载次数和上面的 FireFox Send 不同，可以多次下载，只要没过期</strong>。</p><p>如果觉得 10 分钟太短，可以注册个账号，文件有效期可以达到 48 小时。</p><p>它还有一个优点是文件上传大小几乎没有限制，因为<strong>一次性最大上传量高达 100G</strong>。相较之下，网盘单个文件体积限制 4GB 就显得小儿科了。虽然很少会传输超过 4G 的文件，但总会有这样的需求存在。</p><p>以上，就是两款临时文件传输神器，可以选择性使用。另外，还忘了说一点上传下载速度比网盘快不少，用来备份照片文件是个不错的方法。</p><p>本文完。</p><p>参考：</p><blockquote><p><a href="https://sspai.com/post/40047" target="_blank" rel="noopener">免费全平台的文件分享利器：SendAnywhere</a></p><p><a href="https://sspai.com/post/40464" target="_blank" rel="noopener">只能用 1 次的网盘</a></p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;实用技能。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>我们说着「中式英语」写着「英式中文」</title>
    <link href="https://www.makcyun.top/2019/04/01/weekly_sharing22.html"/>
    <id>https://www.makcyun.top/2019/04/01/weekly_sharing22.html</id>
    <published>2019-04-01T08:16:16.000Z</published>
    <updated>2019-04-02T00:21:06.067Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>好好写作。</p><a id="more"></a><p>这里是「每周分享」的第 22 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>写作</strong>。<br>最近为了出 Python 专栏课，重新整理了以前写的文章，发现很多地方写得不好，犯了不少错误。一直以为英语没学好，没想到汉语也给带坏了，说着中式英语，写着英式中文。</p><p>上学那阵英语很差，逮住机会就想找老外练口语。有次学校碰到一个老外，他先开的口：</p><blockquote><p>Can you speak english?<br>条件反射地答到：My english is poor.</p></blockquote><p>后面又偶遇了一次，他又主动问道：</p><blockquote><p>How are you？<br>我说：I‘m fine，thank you，and you?</p></blockquote><p><img src="http://media.makcyun.top/FtBIDYlYPFFuBlHU-Jy5HCeQ0Dza" alt=""></p><p>就这样好歹算是认识了，口语进步明显，张口说话的胆子也大了，碰到不会的就问他：</p><blockquote><p>How to say this in English?</p></blockquote><p>被他纠正过许多回，便想请他吃个饭略表谢意。找了家学校附近的餐馆，拿给他菜单让他随意点菜，他看了半天指着一处问我这是什么菜：</p><p><img src="http://media.makcyun.top/FggSaOZJ77LWju5UUoRT3ngcWuga" alt=""></p><p>一看，对饭店老板的英语水平顿生敬意，委婉同老外说不要在意，这道菜不贵：</p><blockquote><p>The price is very suitable.</p></blockquote><p>嗯，以上是我编的，英语说成这样大概率是交不到老外朋友的，不过，相信像我这种水平的应该还大有人在。在网上看到过个段子，讲几个在国外旅行的中国人恰巧遇到一起车祸，这样向警察描述过程：</p><p>「one car come, one car go , two cars pengpeng ,the people die.」</p><p>这些都是一目了然的「中式英语」，想纠正并不难，难的是那些我们并不认为是「中式英语」的地方，比如学校的英文课本、老师出的试卷、学生写的作文。</p><p>拿写英语作文来说，以前最喜欢用一些短语或者同学不知道的单词，既高大上又能凑字数，关键老师给分还高。比如：</p><ul><li>Make great efforts to</li><li>Make an improvement</li><li>Pay attention to</li><li>Try our best to</li><li>accelerate the pace of economic reform</li><li>overwhelming majority</li></ul><p>明明一个词或者更简单的词可以说明，偏要写这些在 The Economist 里几乎见不到的句法。</p><p>不久前，看到一本叫《中式英语之鉴》的书，作者是 Joan Pinkham 女士，她为国内官方机构翻译润稿多年，这本书中她纠正了很多中式英语问题。</p><p><img src="http://media.makcyun.top/FjdEaMG-0IPlPBbqtl2c8_0oGkCl" alt=""></p><p>豆瓣 8.8 分，五六百页，京东上却只要十几块钱，想必不是很火。如果当年学校能给老师和学生发上一本作为辅导书，想必我的英语水平不至于是现在这个样子。</p><p>所幸，大部分人这辈子并不出国，英语使用的场景也不多，所以「中式英语」问题倒也没什么太大影响。然而，多数人地道的英语没学到，却把不地道的英语带到了中文写作中，写出来的都是「英式中文」。</p><p>不信，举一些例子，看看你有没有这样写过。</p><h3 id="名词主语"><a href="#名词主语" class="headerlink" title="名词主语"></a>名词主语</h3><p>习惯说：</p><ul><li><p>流浪汉的爆红，背后有什么秘密？</p></li><li><p>他的收入的减少改变了他的生活方式。</p></li></ul><p>却不会说：</p><ul><li><p>流浪汉爆红，背后有什么秘密？</p></li><li><p>他因为收入减少而改变生活方式。</p></li></ul><p><strong>英文常用名词做主语，而中文是常用一件事 (一个短句)做主语 。</strong></p><h3 id="万恶的「作出」「进行」「造成」"><a href="#万恶的「作出」「进行」「造成」" class="headerlink" title="万恶的「作出」「进行」「造成」"></a>万恶的「作出」「进行」「造成」</h3><p>习惯说：</p><ul><li><p>校友为母校作出了重大贡献。</p></li><li><p>专家们对该问题已经进行了详细的探讨。</p></li><li><p>埃塞俄比亚航空公司客机坠毁，造成一百五十七人死亡。</p></li></ul><p>却不会说：</p><ul><li><p>校友对母校贡献很大。</p></li><li><p>专家们详细探讨了该问题。</p></li><li><p>埃塞俄比亚航空公司客机坠毁，死了一百五十七人。</p></li></ul><p><strong>「作出」「进行」「造成」这些是弱动词，很多人找不到合适的动词就喜欢用这些代替。这正是英文蹩脚的人的特色，把简单明了的动词分解成「弱动词+抽象名词」的组合。</strong></p><h3 id="故作高深的「性」「度」「力」"><a href="#故作高深的「性」「度」「力」" class="headerlink" title="故作高深的「性」「度」「力」"></a>故作高深的「性」「度」「力」</h3><p>习惯说：</p><ul><li><p>这本书可读性高么？</p></li><li><p>他在国内 IT 领域享有很高的知名度。</p></li><li><p>一个人应该有良好的自控力。</p></li></ul><p>却不会说：</p><ul><li><p>这本书值得看。</p></li><li><p>他在国内很有名。</p></li><li><p>一个人应该管好自己。</p></li></ul><p><strong>很多人文章中好用「性」「度」「力」这类抽象名词，说白了就是故作高深，奥威尔将这类词汇称为「语言的义肢」，能砍就砍。</strong></p><h3 id="模糊的…-之一"><a href="#模糊的…-之一" class="headerlink" title="模糊的… 之一"></a>模糊的… 之一</h3><p>习惯说：</p><ul><li><p>这款 App 是最好用的笔记软件之一。</p></li><li><p>他是世界上最有钱的人之一。</p></li><li><p>李白是中国最伟大的诗人之一。</p></li></ul><p>却不会说：</p><ul><li><p>这是一款好用的笔记 App。</p></li><li><p>他是世界顶级富豪。</p></li><li><p>李白是中国的大诗人。</p></li></ul><p><strong>「之一」和「One of ..」一脉相承，不确定范围就不要说「之一」，你觉得精确，其实恰恰相反。</strong></p><h3 id="画蛇添足的「有关」「关于」「由于」"><a href="#画蛇添足的「有关」「关于」「由于」" class="headerlink" title="画蛇添足的「有关」「关于」「由于」"></a>画蛇添足的「有关」「关于」「由于」</h3><p>习惯说：</p><ul><li><p>今天我们讨论有关 Python 的学习方法。</p></li><li><p>关于他的方案，你看过没有？</p></li><li><p>由于他近视，所以看不清东西。</p></li></ul><p>却不会说：</p><ul><li><p>今天我们讨论 Python 的学习方法。</p></li><li><p>你看过他的方案没有？</p></li><li><p>他近视，看不清东西。</p></li></ul><p><strong>英语常用 about、concerning、with regard to 这些词表明逻辑，交代事物因果关系。而多数中文语境自带逻辑关系，不需用这些泛滥的介词</strong>。</p><h3 id="啰嗦的「成功地」"><a href="#啰嗦的「成功地」" class="headerlink" title="啰嗦的「成功地」"></a>啰嗦的「成功地」</h3><p>习惯说：我们成功地把该网页数据爬取下来了。</p><p>却不会说：我们把网页数据爬取下来了。</p><p><strong>爬取下来就是成功，不用再重复，类似这样的副词还有不少。</strong></p><h3 id="形容词必用「的」"><a href="#形容词必用「的」" class="headerlink" title="形容词必用「的」"></a>形容词必用「的」</h3><p>习惯说：</p><ul><li><p>晴朗的天气。</p></li><li><p>这篇文章的阅读量是最高的。</p></li><li><p>我见到一个长得像你兄弟说话也有点像他的陌生男人。</p></li></ul><p>却不会说：</p><ul><li><p>天气晴朗。</p></li><li><p>这篇文章阅读量最高。</p></li><li><p>我见到一个陌生男人，长得像你兄弟，说话也有点像他。</p></li></ul><p><strong>很多人碰到形容词就爱用「的」，其实大多数的「的」都可以删掉。另外，英语习惯把所有形容词都放在名词前面，也就是前置。而中文这样用就会显得冗长，其实更适合后置，短小精练，突出重点。</strong></p><p>以上这些毛病，你有中招么？</p><p>上面的内容参照总结了余光中老先生在 1987 年发表的《论中文的常态与变态》一文。他三十多年前就指出了多数人爱犯的中文写作毛病，今天仍然那么多人没有注意到。</p><p><strong>到底是我们没有学好，还是没有学过？</strong></p><p>网上有很多所谓的爆款写作课，教你怎么写出十万+ 文章，我倒觉得，老师的心思都放在运营技巧上了，哪里还会教你写作。</p><p>想好好学写作，不妨看看余老的<strong>《翻译乃大道》</strong>，让你知道什么是中文。</p><p>最后，缅怀敬爱的余光中先生，他的《乡愁》至今仍记忆犹新。<br><img src="http://media.makcyun.top/FkBV4TMzmhbXdHxD3jLCtiLbJYIE" alt=""></p><h2 id="推荐阅读："><a href="#推荐阅读：" class="headerlink" title="推荐阅读："></a>推荐阅读：</h2><blockquote><p><a href="https://dwz.cn/ZEnsBLKM" target="_blank" rel="noopener">論中文的常態與變態</a></p><p><a href="https://zhuanlan.zhihu.com/p/32030547?utm_source=com.tencent.qqlite&amp;utm_medium=social" target="_blank" rel="noopener">余光中文章精选</a></p><p><a href="https://m.facebook.com/MaYingjeou/posts/1796393143756143" target="_blank" rel="noopener">马英九评论余光中</a></p><p><a href="https://www.yangzhiping.com/column/Pinker-8Principles-writing.html" target="_blank" rel="noopener">阳志平《平克写作八原则》</a></p><p><a href="http://wangshuo.blog.caixin.com/archives/821/5" target="_blank" rel="noopener">王烁《有效写作十三篇》</a></p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;好好写作。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="写作" scheme="https://www.makcyun.top/tags/%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降最直观的理解</title>
    <link href="https://www.makcyun.top/2019/03/28/Machine_learning02.html"/>
    <id>https://www.makcyun.top/2019/03/28/Machine_learning02.html</id>
    <published>2019-03-28T08:16:16.000Z</published>
    <updated>2019-04-02T00:20:57.672Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>通俗易懂。</p><a id="more"></a><p>最近在学习机器学习，入门第一课就是「梯度下降」。看了不少教程都没有很好地理解，直到看到下面这篇文章，算是通俗易懂地理解了。</p><p>作者：六尺帐篷</p><p>链接：<a href="https://www.jianshu.com/p/c7e642877b0e" target="_blank" rel="noopener">https://www.jianshu.com/p/c7e642877b0e</a></p><p>来源：简书</p><p>本文将从一个下山的场景开始，先提出梯度下降算法的基本思想，进而从数学上解释梯度下降算法的原理，最后实现一个简单的梯度下降算法的实例！</p><h1 id="梯度下降的场景假设"><a href="#梯度下降的场景假设" class="headerlink" title="梯度下降的场景假设"></a>梯度下降的场景假设</h1><blockquote><p>梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-6ae594f795406b8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/989/format/webp" alt="img"></p><p>image.png</p><p>我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！</p></blockquote><h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>梯度下降的基本过程就和下山的场景很类似。</p><hr><p>首先，我们有一个可<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDifferentiable_function" target="_blank" rel="noopener"><em>微分</em></a>的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FGradient" target="_blank" rel="noopener"><em>梯度</em></a> ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释)<br>所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？接下来，我们从微分开始讲起</p><h1 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h1><p>看待微分的意义，可以有不同的角度，最常用的两种是：</p><ul><li>函数图像中，某点的切线的斜率</li><li>函数的变化率<br>几个微分的例子：</li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-0eb0f1bfd7de705b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/295/format/webp" alt="img"></p><p>上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-4029977524e3b365.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/498/format/webp" alt="img"></p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度实际上就是多变量微分的一般化。<br>下面这个例子：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-570afdfc6fabf3b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/493/format/webp" alt="img"></p><p>我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用&lt;&gt;包括起来，说明梯度其实一个向量。</p><p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p><ul><li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li><li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</li></ul><p>这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-13d969531284a9f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/652/format/webp" alt="img"></p><h1 id="梯度下降算法的数学解释"><a href="#梯度下降算法的数学解释" class="headerlink" title="梯度下降算法的数学解释"></a>梯度下降算法的数学解释</h1><p>上面我们花了大量的篇幅介绍梯度下降算法的基本思想和场景假设，以及梯度的概念和思想。下面我们就开始从数学上解释梯度下降算法的计算过程和思想！</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" alt="img"></p><p>此公式的意义是：J 是关于Θ的一个函数，我们当前所处的位置为Θ0 点，要从这个点走到 J 的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1 这个点！</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-af8dd9722c762c13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" alt="img"></p><p>下面就这个公式的几个常见的疑问：</p><ul><li>α是什么含义？<br>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-ba3da0b06da97ddb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/827/format/webp" alt="img"></p><ul><li>为什么要梯度要乘以一个负号？<br>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号。</li></ul><h1 id="梯度下降算法的实例"><a href="#梯度下降算法的实例" class="headerlink" title="梯度下降算法的实例"></a>梯度下降算法的实例</h1><p>我们已经基本了解了梯度下降算法的计算过程，那么我们就来看几个梯度下降算法的小实例，首先从单变量的函数开始</p><h2 id="单变量函数的梯度下降"><a href="#单变量函数的梯度下降" class="headerlink" title="单变量函数的梯度下降"></a>单变量函数的梯度下降</h2><p>我们假设有一个单变量的函数</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-abb73822fb6d2a2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/127/format/webp" alt="img"></p><p>函数的微分</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-66ce0cdcef5e2686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/141/format/webp" alt="img"></p><p>初始化，起点为</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-8ee36cc5ce832b17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/86/format/webp" alt="img"></p><p>学习率为</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-798b134107b6593d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/105/format/webp" alt="img"></p><p>根据梯度下降的计算公式</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-f20521a962005299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" alt="img"></p><p>我们开始进行梯度下降的迭代计算过程：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-57538d21dbb34e65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/281/format/webp" alt="img"></p><p>如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-bb7fa36d116fcadc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/601/format/webp" alt="img"></p><h2 id="多变量函数的梯度下降"><a href="#多变量函数的梯度下降" class="headerlink" title="多变量函数的梯度下降"></a>多变量函数的梯度下降</h2><p>我们假设有一个目标函数：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-a56cfde25c688859.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/171/format/webp" alt="img"></p><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-8b1b6f1b200fd7b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/128/format/webp" alt="img"></p><p>初始的学习率为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-ccc1493848871074.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/89/format/webp" alt="img"></p><p>函数的梯度为：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-3d744d9364a4ba40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/221/format/webp" alt="img"></p><p>进行多次迭代：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-b21bf64600c4e32f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/647/format/webp" alt="img"></p><p>我们发现，已经基本靠近函数的最小值点</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-becdcdfdefb4eab7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/590/format/webp" alt="img"></p><h1 id="梯度下降算法的实现"><a href="#梯度下降算法的实现" class="headerlink" title="梯度下降算法的实现"></a>梯度下降算法的实现</h1><p>下面我们将用 python 实现一个简单的梯度下降算法。场景是一个简单的<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLinear_regression" target="_blank" rel="noopener"><em>线性回归</em></a>的例子：假设现在我们有一系列的点，如下图所示</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-333f16d34874c230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/716/format/webp" alt="img"></p><p>我们将用梯度下降法来拟合出这条直线！</p><p>首先，我们需要定义一个代价函数，在此我们选用<a href="https://link.jianshu.com?t=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLeast_squares" target="_blank" rel="noopener"><em>均方误差代价函数</em></a>：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-4e4000e69f05af7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/451/format/webp" alt="img"></p><p>此公示中</p><ul><li><p>m 是数据集中点的个数</p></li><li><p>½是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响</p></li><li><p>y 是数据集中每个点的真实 y 坐标的值</p></li><li><p>h 是我们的预测函数，根据每一个输入 x，根据Θ 计算得到预测的 y 值，即</p></li></ul><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-acea37db1e02004d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/328/format/webp" alt="img"></p><p>我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-bfd1c5136eaaa552.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/485/format/webp" alt="img"></p><p>明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python 中计算矩阵是非常方便的，同时代码也会变得非常的简洁。</p><p>为了转换为矩阵的计算，我们观察到预测函数的形式：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-acea37db1e02004d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/328/format/webp" alt="img"></p><p>我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点 x 增加一维，这一维的值固定为 1，这一维将会乘到Θ0 上。这样就方便我们统一矩阵化的计算：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-a54d53411f945d95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600/format/webp" alt="img"></p><p>然后我们将代价函数和梯度转化为矩阵向量相乘的形式：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-66b04086dd1f8ba9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/516/format/webp" alt="img"></p><h2 id="coding-time"><a href="#coding-time" class="headerlink" title="coding time"></a>coding time</h2><p>首先，我们需要定义数据集和学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of the points dataset.</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Points x-coordinate and dummy value (x0, x1).</span></span><br><span class="line">X0 = np.ones((m, <span class="number">1</span>))</span><br><span class="line">X1 = np.arange(<span class="number">1</span>, m+<span class="number">1</span>).reshape(m, <span class="number">1</span>)</span><br><span class="line">X = np.hstack((X0, X1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Points y-coordinate</span></span><br><span class="line">y = np.array([</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">11</span>, <span class="number">8</span>, <span class="number">12</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span></span><br><span class="line">]).reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Learning Rate alpha.</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br></pre></td></tr></table></figure><p>接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Error function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/<span class="number">2</span>*m) * np.dot(np.transpose(diff), diff)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Gradient of the function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/m) * np.dot(np.transpose(X), diff)</span><br></pre></td></tr></table></figure><p>最后就是算法的核心部分，梯度下降迭代计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, y, alpha)</span>:</span></span><br><span class="line">    <span class="string">'''Perform gradient descent.'''</span></span><br><span class="line">    theta = np.array([<span class="number">1</span>, <span class="number">1</span>]).reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> np.all(np.absolute(gradient) &lt;= <span class="number">1e-5</span>):</span><br><span class="line">        theta = theta - alpha * gradient</span><br><span class="line">        gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><p>当梯度小于 1e-5 时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！</p><p>完整的代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of the points dataset.</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Points x-coordinate and dummy value (x0, x1).</span></span><br><span class="line">X0 = np.ones((m, <span class="number">1</span>))</span><br><span class="line">X1 = np.arange(<span class="number">1</span>, m+<span class="number">1</span>).reshape(m, <span class="number">1</span>)</span><br><span class="line">X = np.hstack((X0, X1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Points y-coordinate</span></span><br><span class="line">y = np.array([</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">11</span>, <span class="number">8</span>, <span class="number">12</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span></span><br><span class="line">]).reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Learning Rate alpha.</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Error function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/<span class="number">2</span>*m) * np.dot(np.transpose(diff), diff)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Gradient of the function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/m) * np.dot(np.transpose(X), diff)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, y, alpha)</span>:</span></span><br><span class="line">    <span class="string">'''Perform gradient descent.'''</span></span><br><span class="line">    theta = np.array([<span class="number">1</span>, <span class="number">1</span>]).reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> np.all(np.absolute(gradient) &lt;= <span class="number">1e-5</span>):</span><br><span class="line">        theta = theta - alpha * gradient</span><br><span class="line">        gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">optimal = gradient_descent(X, y, alpha)</span><br><span class="line">print(<span class="string">'optimal:'</span>, optimal)</span><br><span class="line">print(<span class="string">'error function:'</span>, error_function(optimal, X, y)[<span class="number">0</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>运行代码，计算得到的结果如下：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-af64f7e8e5fb3dfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/416/format/webp" alt="img"></p><p>所拟合出的直线如下：</p><p><img src="https:////upload-images.jianshu.io/upload_images/1234352-27806efbd53ced41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/694/format/webp" alt="img"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>至此，我们就基本介绍完了梯度下降法的基本思想和算法流程，并且用 python 实现了一个简单的梯度下降算法拟合直线的案例！<br>最后，我们回到文章开头所提出的场景假设:<br><strong>这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。</strong><br>可以看到场景假设和梯度下降算法很好的完成了对应！</p><h1 id="Further-reading"><a href="#Further-reading" class="headerlink" title="Further reading"></a>Further reading</h1><ul><li><a href="https://link.jianshu.com?t=https%3A%2F%2Fstorage.googleapis.com%2Fsupplemental_media%2Fudacityu%2F315142919%2FGradient%2520Descent.pdf" target="_blank" rel="noopener">Gradient Descent lecture notes</a> from <a href="https://link.jianshu.com?t=https%3A%2F%2Fwww.udacity.com%2Fcourse%2Fmachine-learning--ud262" target="_blank" rel="noopener">UD262 Udacity Georgia Tech ML Course</a>.</li><li><a href="https://link.jianshu.com?t=https%3A%2F%2Farxiv.org%2Fpdf%2F1609.04747.pdf" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a>.</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;通俗易懂。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://www.makcyun.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.makcyun.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>我的爬虫&amp;数据分析课</title>
    <link href="https://www.makcyun.top/2019/03/26/web_scraping_withpython22.html"/>
    <id>https://www.makcyun.top/2019/03/26/web_scraping_withpython22.html</id>
    <published>2019-03-26T08:16:16.000Z</published>
    <updated>2019-04-02T00:25:40.296Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>7 万字、26 个知识点，13 个爬虫&amp;数据分析实战项目。</p><a id="more"></a><p>我开了一门只要 19.9 元的爬虫课，叫《Python 入门爬虫与数据分析》。</p><h2 id="这是一门够便宜的课"><a href="#这是一门够便宜的课" class="headerlink" title="这是一门够便宜的课"></a><strong>这是一门够便宜的课</strong></h2><p>19.9 元，中间没有误打个「.」号。19.9 元就可以买下我大半年的时光，7 万字、26 个知识点，13 个爬虫&amp;数据分析实战项目，一套精选Python 电子书和 Pyhton 视频参考教程。</p><p><img src="http://media.makcyun.top/Fqej37ZHlkv-iYNNI4i1xhKuJSNM" alt=""></p><h2 id="这是一门友好的零基础入门课"><a href="#这是一门友好的零基础入门课" class="headerlink" title="这是一门友好的零基础入门课"></a><strong>这是一门友好的零基础入门课</strong></h2><p>半年前，我决定转行，从零开始学习 Python。过程很痛苦，常常被很小的一个问题卡很久，全靠一点点 Google 找到答案，再写到教程中去。一路从小白过来，深知在学习过程中可能会遇到的种种问题，所以文章中我都尽可能地写得直白友好、详略得当，让你少走些弯路。</p><h2 id="这是一门可以学到不少东西的课"><a href="#这是一门可以学到不少东西的课" class="headerlink" title="这是一门可以学到不少东西的课"></a><strong>这是一门可以学到不少东西的课</strong></h2><p>你可以学到快速爬取百万行上市公司财务报表：</p><p><img src="http://media.makcyun.top/selenium%E7%88%AC%E5%8F%96%E6%95%88%E6%9E%9C.gif" alt=""></p><p>你可以学到怎么爬取图片：</p><p><img src="http://media.makcyun.top/win/20190325/N8zXpnW58TcW.gif" alt=""></p><p>你可以学到怎么爬取整个网站的 App：</p><p><img src="http://media.makcyun.top/win/20190325/HdeXR7M3Bdvr.png?imageslim" alt=""></p><p>你可以学到如何做一些好看的图：</p><p><img src="http://media.makcyun.top/FnOQKEihSNk2in8GJuR0oGdYD5n8" alt=""></p><p><img src="http://media.makcyun.top/Fm6PEI9Ua91x8ooaOIuyCwFTd0Cv" alt=""></p><p><img src="http://media.makcyun.top/Fvht9nFMhRQZuH_NHAmX7KtVdFsw" alt=""></p><p><img src="http://media.makcyun.top/Fp7vahmHPEbzdKBzdyta_3GrQxqw" alt=""></p><p>还有很多，都在二维码里。</p><p><img src="http://media.makcyun.top/Fn2kGZeKUkJLwcSaNNNLFPRfzKcI" alt=""></p><p>觉得值得，就扫码入手吧。</p><p>另外，说一个福利。加入我知识星球的星友可以免费得到这门课，有任何问题都可以找我，欢迎加入。</p><p><img src="http://media.makcyun.top/FmK9TW8XT7fcwSc1fQnlSixQUUpj" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;7 万字、26 个知识点，13 个爬虫&amp;amp;数据分析实战项目。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>主流网站 Python 爬虫模拟登陆方法汇总</title>
    <link href="https://www.makcyun.top/2019/03/19/web_scraping_withpython21.html"/>
    <id>https://www.makcyun.top/2019/03/19/web_scraping_withpython21.html</id>
    <published>2019-03-19T08:16:16.000Z</published>
    <updated>2019-05-19T10:04:59.896Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>微信、知乎、新浪、京东、淘宝等大型网站的模拟登陆方法。</p><a id="more"></a><p>摘要：介绍微信、知乎、新浪等一众主流网站的模拟登陆爬取方法。</p><p>网络上有形形色色的网站，不同类型的网站爬虫策略不同，难易程度也不一样。从是否需要登陆来说，一些简单网站不需要登陆就可以爬，比如之前爬过的猫眼电影、东方财富网等。有一些网站需要先登陆才能爬，比如知乎、微信等。这类网站在模拟登陆时需要处理验证码、js 加密参数这些问题，爬取难度会大很多。费很大力气登陆进去后才能爬取想要的内容，很花时间。</p><p>是不是一定要自己动手去实现每一个网站的模拟登陆方法呢，从效率上来讲，其实大可不必，已经有前人替我们造好轮子了。</p><p>最近发现一个神库，汇总了数十个主流网站的模拟登陆方法：</p><ul><li><a href="http://zhihu.com/" target="_blank" rel="noopener">知乎</a></li><li><a href="https://wx.qq.com/" target="_blank" rel="noopener">微信网页版登录并获取好友列表</a></li><li><a href="https://www.bilibili.com/" target="_blank" rel="noopener">Bilibili</a></li><li><a href="https://www.facebook.com/" target="_blank" rel="noopener">Facebook</a></li><li><a href="https://twitter.com/" target="_blank" rel="noopener">无需身份验证即可抓取Twitter前端API</a></li><li><a href="http://weibo.com/" target="_blank" rel="noopener">微博网页版</a></li><li><a href="https://qzone.qq.com/" target="_blank" rel="noopener">QQZone</a></li><li><a href="https://www.csdn.net/" target="_blank" rel="noopener">CSDN</a></li><li><a href="https://github.com/CriseLYJ/awesome-python-login-model/blob/master/www.taobao.com" target="_blank" rel="noopener">淘宝</a></li><li><a href="https://github.com/CriseLYJ/awesome-python-login-model/blob/master/www.baidu.com" target="_blank" rel="noopener">Baidu</a></li><li><a href="https://www.guokr.com/" target="_blank" rel="noopener">果壳</a></li><li><a href="https://www.jd.com/" target="_blank" rel="noopener">JingDong 模拟登录</a></li><li><a href="https://mail.163.com/" target="_blank" rel="noopener">163mail</a></li><li><a href="https://www.lagou.com/" target="_blank" rel="noopener">拉钩</a></li><li><a href="https://www.douban.com/" target="_blank" rel="noopener">豆瓣</a></li><li><a href="https://github.com/CriseLYJ/awesome-python-login-model/blob/master/www.baidu.com" target="_blank" rel="noopener">Baidu2</a></li><li><a href="https://www.liepin.com/" target="_blank" rel="noopener">猎聘网</a></li><li><a href="https://github.com/" target="_blank" rel="noopener">Github</a></li><li><a href="https://tuchong.com/" target="_blank" rel="noopener">爬取图虫相应的图片</a></li><li><a href="https://music.163.com/" target="_blank" rel="noopener">网易云音乐</a></li><li><a href="https://www.qiushibaike.com/" target="_blank" rel="noopener">糗事百科</a></li></ul><p>这些网站基本采用是直接登录或者使用 selenium+webdriver 的方式。每一个网站都有完整的模拟登陆代码，拿来就可以用到自己的爬虫中。</p><p>下面我们来测试一下。</p><p>先说说很难爬的「知乎」，假如我们想爬取知乎主页的 HTML 内容，就必须要先登陆才能爬，不然看不到这个界面。下面来简单梳理一下流程。</p><p><img src="http://media.makcyun.top/FtpqTJuPqtURAgdSwt2ent2AOH7V" alt=""></p><p><img src="http://media.makcyun.top/FkqvSBeRrlq04E4g5I_cbzbp8sBS" alt=""></p><p>知乎需要手机号才能注册登陆。为了方便测试，可以随便找个手机号，手机号到哪儿去找呢，我上周写的那篇文章就发挥作用了。文章里介绍了一个免费电话号码网站，用上面的手机号可以成功注册。</p><p>文章传送门：<a href="https://mp.weixin.qq.com/s?__biz=MzA5NDk4NDcwMw==&amp;mid=2651386828&amp;idx=1&amp;sn=b33210dde8e0eea6d06932c0ab70b299&amp;chksm=8bba135cbccd9a4acdd08a4af2536e6dc311ea2bd1845f8c8df9a6fd943aee7f4b7e7dabce4b&amp;token=258555898&amp;lang=zh_CN#rd" target="_blank" rel="noopener">两个神网站保护你的隐私</a></p><p><img src="http://media.makcyun.top/Fh_rim4X-hVVpqOcUDt3lJ0BdKwn" alt=""></p><p><img src="http://media.makcyun.top/FpF3VQg-CG-cg7J45uHmH0uPN5hJ" alt=""></p><p>顺利登录后就可以进入主页了。</p><p>下面，我们用这个库提供的代码来模拟登陆，输出主页 HTML 内容作测试。操作很简单，只需要输入手机号、密码和验证码就可以了。</p><p><img src="https://i.loli.net/2019/03/19/5c9095ba5cc7f.gif" alt=""></p><p>成功登陆后，接下来就可以做一些有意思的事了。比如曾有人爬取所有知乎账号的信息，分析了知乎用户群体画像。</p><p>是不是有点意思。</p><p>再来看看微信。用上面的微信代码可以把全部微信好友信息爬取下来，比如：昵称、性别、地域、个性签名。接着可以分析一下你的朋友圈是什么样的，应该会很有趣。</p><p><img src="https://i.loli.net/2019/03/19/5c909c202df83.gif" alt=""></p><p>还可以爬 B 站：</p><p><img src="https://github.com/CriseLYJ/awesome-python-login-model/raw/master/images/bilibili.gif" alt="img"></p><p>还可以爬链家租房信息：</p><p><img src="https://github.com/CriseLYJ/awesome-python-login-model/raw/master/images/lianjia.gif" alt="img"></p><p>还有很多实用有趣的内容，就不一一罗列了，感兴趣的话可以试试，最后放上 GitHub 库地址：</p><p><a href="https://github.com/CriseLYJ/awesome-python-login-model" target="_blank" rel="noopener">https://github.com/CriseLYJ/awesome-python-login-model</a></p><p>不要闷头造轮子，多抬抬头会发现你在做/想做的东西，别人早已经弄好了，拿来用或者参考学习都是件好事。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;微信、知乎、新浪、京东、淘宝等大型网站的模拟登陆方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>向优秀的人靠近</title>
    <link href="https://www.makcyun.top/2019/03/16/life09.html"/>
    <id>https://www.makcyun.top/2019/03/16/life09.html</id>
    <published>2019-03-16T08:16:24.000Z</published>
    <updated>2019-03-22T23:19:15.530Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>长久的利他必然利己。</p><a id="more"></a><p><strong>摘要：</strong>长久的利他必然利己。</p><p>昨天读者群里的一位朋友告诉我，说刘志军（公众号 Python 之禅作者）在文章里推荐了我的公众号。</p><p>听了很纳闷儿，寻思并没有跟他约公众号互推啊，赶紧去他公众号看了看，还真的是。</p><p><img src="http://media.makcyun.top/FsX2w4XDaAjnVAZ7u7BA0eC3BJ7w" alt=""></p><p>推荐语，有意思、也实在。</p><p>正如他所说，这个推荐纯粹是出于「推荐不错原创号」的初衷。</p><p>感到很意外，有点小激动，有人说是我面子大，我并不认同。</p><p>其实我和军哥并无多少交集，微信也没聊过几句，主要是觉得还不够分量去跟他混熟。</p><p>可是他怎么就推荐我了呢。</p><p>跟背后的几件小事有点关系，如果你想勾搭大佬，不妨看看。</p><p><strong>第一件事。</strong></p><p>你可以看到截图上，打赏最多的人中有我的头像。这就不难理解了吧，我打赏的次数多，他对我有印象了，说明我有一直在关注他的公众号，再顺便翻一下我的公众号，觉得还不错就友情推荐一下，也算是给我的一点福利。当然，我是赚大了，基本上没花什么成本，就收获很多粉丝。意外、惊喜。</p><p>我之前常送书，看到很多人说为什么总中不了奖，你跟上千个人去拼运气，中奖概率当然很低了。要想增大中奖概率，打赏、留言绝对是既划算、赢面又大的方式，每篇推文打赏个一两块，十次也才一二十块，几乎没有人能够打赏这么多次，你要做到了，在我这里的印象分就是独一档，中奖概率自然会高许多。</p><p>这只是第一件事，我并不是平白无故地就打赏他的。很多人也是这样，关注的公众号那么多，要持续打赏一个公众号，必然是有理由的。我是因为第二件事。</p><p><strong>第二件事。</strong></p><p>上两周我在一个几百位大佬的微信群求助了一个问题，最终，只有他私信帮了我。</p><p>就因为这件事，他在我心里的印象分也变得很高了，那之后他发文我基本上都看，顺便打赏。</p><p>你以为完了么，并没有，他私信帮我也是有事铺垫在先的。</p><p><strong>第三件事。</strong></p><p>前阵他做了一个网站，邀请大家去注册体验一下，我看到后就马上微信他，说想注册体验下。我支持他的产品，背后也是有原因的，就是想再次勾搭大佬。为什么说再次呢，这就跟第四件事有关了。</p><p><strong>第四件事</strong></p><p>半年前，我还是一个刚学 Python 的菜鸟，那会儿军哥在我眼里就是高不可攀的大神，当然现在也是。发现他公众号后，果断关注，还加了他微信。有次他发了一条朋友圈，想混个脸熟，就在下面留言评论了下，然而他并没有回复。看吧，我以前也有被大佬忽略的辛酸史。</p><p>再来顺着理一下这几件事背后的逻辑。我刚入门 Python ，想勾搭认识一下大佬，没想评论朋友圈被忽略，之后依然主动去他注册他的产品，继续勾搭，然后被他熟悉，在群里帮我忙，我打赏他文章，他推荐我公众号。</p><p>你看，并不是我有面子，我只是做了少有人做的事情。</p><p>很多人想抱大佬腿，很多人也是玻璃心，如果我也是的话，那他朋友圈不回复我评论，我就会觉得不爽：这人咋这么高高在上，取关。如此的话，也就不会有后面的事。你是个菜鸟，就最好放低姿态，他不回复你，并不是针对你，大佬每天那么忙，不回复也很正常，何况他根本不熟悉你。</p><p>除了放低姿态，就是要主动些。持续关注他动态，公众号留言打赏，能做的就做，你既然想勾搭他，为什么不去做这些简单而少有人做的事情呢。做的多了，他自然就会关注到你，如果没有关注到，说明别人比你做的更多。</p><p>当然，这里并不是教你去一味地做大佬舔狗。先做好自己，你没有拿得出手的东西，大佬就算想推你也推荐不了。我是持续地在公众号输出了一些原创文章，才慢慢混到大佬的圈层，当你跟大佬在一个共同的圈子，勾搭的机会才会多。</p><p>我知道关注我的部分粉丝，会觉得我是大佬，我要谦虚地说我只是菜鸟，没什么意思，对你也没什么用。因为我们站在的高度是不一样的，你说我是大佬，我说军哥是大佬，军哥还会说 stormzhang 是大佬。这么多大佬，你应该多勾搭离你最近的大佬。级别太高的大佬，你费很大劲也难够着，更好的方式是一级一级地让大佬带你。</p><p>想向大佬靠拢，真得用心去付出一点，长久的利他必然利己。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;长久的利他必然利己。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>两个神网站：在线接收短信和邮件</title>
    <link href="https://www.makcyun.top/2019/03/16/weekly_sharing20.html"/>
    <id>https://www.makcyun.top/2019/03/16/weekly_sharing20.html</id>
    <published>2019-03-16T08:16:16.000Z</published>
    <updated>2019-03-16T08:30:55.653Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>日常实用网站，保护隐私。</p><a id="more"></a><p><strong>摘要</strong>：在外不要随便打开 WIFI，另分享两个神网站：在线接收短信和邮件。</p><p>这里是「<strong>每周分享</strong>」的第 20 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：<strong>保护隐私</strong>。</p><p>前天的 315 晚会上，曝光了几家制造机器人骚扰电话的公司，其中一家宣称<strong>一年打出去了 40 亿个骚扰电话</strong>。</p><p>以前，我们接到的骚扰电话基本都是真人给你打的，你可能还会礼貌地听几句再拒绝。而现在，很多电话是都是机器人打给你的，你敢信？真人一天顶多打三五百个电话，而这些机器人一天可以打五千个。</p><p><img src="http://media.makcyun.top/Fgz0uOezftPFDVBrHVHSoxEtxEGx" alt=""></p><p>这么多骚扰电话不算可怕，平均到全中国每个人身上数量也不多。可怕之处在于，这些机器人似乎对你很了解，知道你的性别、年路、收入，有没有购房、贷款需求等等。很纳闷对吧，你好像从来没有告诉过谁。</p><p>其实，他们获取的方法很简单。在商场、便利店放置一种探针盒子，只要你手机打开 Wi-Fi ，它就能识别出你手机的 Mac 地址转换成 IMEI 号，再和他们的大数据库匹配，最终查询到你的手机号和各种信息。所以出门在外，别为了省那点流量钱就到处蹭 WIFI。</p><p>他们的大数据系统是从哪里来的呢，主要就是从你使用的那些 App 中获取到的。很多无良 App 安装时会提示你开启各种不相关的权限，比如通讯录、短信。很多人看都不看就默认同意了，一旦你同意，它就开始收集你信息，存储到数据库中，然后卖给中原地产这些广告商。所以，尽可能地不要用垃圾 App，手机权限能不开就不开。</p><p>这条黑色产业链经这么一曝光，不知道会不会就此销声匿迹，但你能做的就是主动保护自己隐私。</p><p>下面说说怎么保护。</p><p>现在很多网站都需要注册，要么用手机号要么用邮箱，接着会给你发确认的短信验证码或者链接，以保证你用的是自己的。重要的网站，可以用自己的，那些不重要或者很少使用的网站就没有必要用。</p><p>然而，很多人不管什么网站，统统用自己的手机号或邮箱去注册，一是没有隐私保护意识，二是没有空闲手机号和邮箱。今天这个网站注册下，明天那个注册下，长时间下来，自己都不清楚到底注册了多少网站，这是个很大的隐患。</p><p>其实，那些不重要的网站，你可以不用自己的手机号和邮箱，用别人的就可以。这样的话，既能满足一时之需，又不用泄露自己手机号。你可能不信，哪有那么好的人会把自己的拿给你注册。别说，还真有。</p><p>下面，就来说两个神网站，一个专门接收短信验证码，一个专门接收邮件。</p><p><img src="http://media.makcyun.top/Fj7ggWgruMOuO_iDcZ3ubo3oWUOq" alt=""></p><p><img src="http://media.makcyun.top/FkITI5E0Z5LRHKu8yz8qg4h8mp56" alt=""></p><p>这个网站提供很多虚拟手机号码，注册网站时随意填写一个手机号，接收网站发来的短信验证码就能注册。简单实用。</p><p>不过要注意的是，注册那些无关紧要的网站可以，重要的网站别用这个。这些号码是公用的，短信对所有人开放，你能注册登录一个网站，别人也能通过找回短信验证码的方式，登录你的网站看到各种信息。</p><p><strong>接下来说接收邮箱验证信息。</strong>很多网站要你用邮箱注册，它会给你发你一封邮件里面是验证链接，以确认是你的邮箱。如果你不想邮箱塞满垃圾邮件的话，那就可以用下面这个网站，它提供免费临时邮箱。</p><p><img src="http://media.makcyun.top/FhIoi4i5i6sMA7WfQtXbWdemgtbd" alt=""></p><p>这两个网站我用了很久，推荐给你。</p><p>鉴于前不久，写了一篇分享网盘资源网站的文章，不久后该网站就挂掉了，心痛。所以这次不再公开这两个网站，你可以自己搜，或者在我公众号后台回复：「<strong>知识星球</strong>」，我看到了就私发你。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;日常实用网站，保护隐私。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="技能" scheme="https://www.makcyun.top/tags/%E6%8A%80%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>Windows 两款神器：Wox 快速查询启动和 QuickLook 空格预览</title>
    <link href="https://www.makcyun.top/2019/03/16/weekly_sharing20.html"/>
    <id>https://www.makcyun.top/2019/03/16/weekly_sharing20.html</id>
    <published>2019-03-16T08:16:16.000Z</published>
    <updated>2019-05-19T10:10:52.432Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>摆脱鼠标，提升效率。</p><a id="more"></a><p><img src="http://media.makcyun.top/FlyQuwc74BJN0UfwvdH6CqlH2Lnv" alt=""></p><p><strong>摘要</strong>：推荐两款 Windows 神器，远离鼠标，释放右手。</p><p>这里是「每周分享」的第 21 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的话题是：释放右手。</p><p>下面要介绍的内容针对和我一样的 Windows 用户。</p><p>有一阵子，每天都要重复大量的 Ctrl C / V 操作，键盘上的这三个按键都擦出了油渍，锃亮无比。后来受不了剁手入了 G502，不是 502 胶水，是上面那款鼠标。（非广告）</p><p>不得不说，是真好用，复制粘贴、返回关闭这些操作再也不用键盘了。</p><p>鼠标使用多了，新问题也随之而来，一是手指关节不舒服，二是其他操作还是需要「鼠键结合」，比如打开文件/夹、浏览器、切换窗口这些，纯鼠标无法完成，效率仍不高。</p><p>难道只能这样么？</p><p><strong>快捷键操作</strong></p><p>慢慢学会了一些快捷键，利用 Ctrl 、Tab、Win 加几个字母键完成很多快捷操作。比如 Win + Tab 键切换窗口：</p><p><img src="http://media.makcyun.top/Fnt778iGAy-UXCSLOObx0hFdYnWV" alt=""></p><p>觉得眼花缭乱，可以 Alt + Tab。</p><p>Win + D 可以秒回桌面，不用一个个去关或者最小化窗口。</p><p>在浏览器中，不需要用鼠标点击去切换页面，Ctrl + Tab 就可以了，想关闭页面，按下 Ctrl + M 搞定：</p><p><img src="http://media.makcyun.top/Fo5ofU06p1GMWAt1p1ZlxLApweEC" alt=""></p><p>另外，再配合 Vimium 这款插件更是如虎添翼，基本上，浏览器这块儿就能告别鼠标了。</p><p>这些快捷键给我节省了不少时间，不过发现有些地方还是取代不了鼠标，比如打开软件、文件/ 文件夹，还是要在开始菜单或者桌面中去找出来打开。</p><p>难道没有更好的方法了么？</p><h2 id="ALT-空格-搞定一切"><a href="#ALT-空格-搞定一切" class="headerlink" title="ALT + 空格 搞定一切"></a><strong>ALT + 空格 搞定一切</strong></h2><p>这样的操作习惯持续了很久，直到发现了 Wox 这款神器，便再也不用浪时间去找东西。</p><p>Alt + 空格键快速启动 Wox，接着输入想要打开的文件/软件名就 OK，打开、切换如行云流水般丝滑：</p><h3 id="打开软件"><a href="#打开软件" class="headerlink" title="打开软件"></a>打开软件</h3><p><img src="http://media.makcyun.top/FtF3S7Aajx5Uba6jjSvo7mgKFFTH" alt=""></p><p>这款软件可以说是 Everything 的加强版，除了能搜索电脑中的东西，还有一些功能丰富强大的插件可用，下面举几个例子。</p><h3 id="网页搜索"><a href="#网页搜索" class="headerlink" title="网页搜索"></a>网页搜索</h3><p>利用网页搜索插件，可以随便切换 Google 、Bing 、百度搜索引擎，比打开浏览器再去切换、搜索方便多了：</p><p><img src="http://media.makcyun.top/FqqwBa2WofXlA3ZL1WUIQ-zLt_yk" alt=""></p><p>还可以直接 StackOverflow：</p><p><img src="http://media.makcyun.top/FurI1goe800GUITR3WZkjgUM74MR" alt=""></p><h3 id="查天气"><a href="#查天气" class="headerlink" title="查天气"></a>查天气</h3><p>可以查天气：<br><img src="http://media.makcyun.top/FqazaKhxg_0uUrVV7nJTmVItGeld" alt=""></p><h3 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h3><p>有道、谷歌翻译都可以用：<br><img src="http://media.makcyun.top/FtwuvCTw97XS9NBHBDlVkcH1kg1e" alt=""></p><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><p>不用再 Win + R 了：</p><p><img src="http://media.makcyun.top/Fnez7750Z634atytP2PRsIrs_gPn" alt=""></p><p>觉得牛逼么？功能还远不止这些。</p><p>如果你电脑上的软件文件很多或着杂乱无章的话，绝对值得一试。</p><p>不过还有一个痛点没解决，怎么样可以不用打开文件就能查看内容。</p><p>这就要介绍另一款神器。</p><h2 id="完美的「一指禅」"><a href="#完美的「一指禅」" class="headerlink" title="完美的「一指禅」"></a><strong>完美的「一指禅」</strong></h2><p>有时候要找一些文件，只能打开看了才知道是不是要找的，一启动一关闭，时间都浪费了。图片还可以通过设置「大图」预览查看，文件就没办法了。</p><p>不得不说 Mac 用户很幸福， Finder 中有一个「一指禅」功能，只需要按一下空格就可以预览查看文件内容。其实 Windows 也可以实现同样的效果，就是利用这款神器：Quicklook 。</p><p>选中文件按一下空格就可以预览，再按一下关闭预览，找到想要的文件后，敲下「回车」就可以打开，像下面这样：</p><p><img src="http://media.makcyun.top/FoW_vH8XopIXt-aKluV1dLVzR6JJ" alt=""></p><p>图片、PDF、Markdown、代码、GIF 都能完美打开。</p><p>这两款神器节省了我不少时间，大部分时间能够做到手不离开键盘。</p><p>感兴趣的话可以自行搜索下载下来试试，老规矩，为了更方便你可以在公众号后台回复：「wox」得到。</p><p>时间是海绵里的水，只要愿意，挤一挤总是有的。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;摆脱鼠标，提升效率。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="技能" scheme="https://www.makcyun.top/tags/%E6%8A%80%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>快速在中文和英文数字之间添加空格</title>
    <link href="https://www.makcyun.top/2019/03/10/weekly_sharing19.html"/>
    <id>https://www.makcyun.top/2019/03/10/weekly_sharing19.html</id>
    <published>2019-03-10T08:16:16.000Z</published>
    <updated>2019-03-23T05:00:26.543Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>写作排版必备技能：快速在中文和英文数字之间添加空格。</p><a id="more"></a><p><strong>摘要</strong>：快速在中文和英文数字之间添加空格。</p><p>这里是「<strong>每周分享</strong>」的第 19 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期来说说写作排版。</p><p>为什么想起这个主题呢，因为发现一些迷之尴尬的现象，很多人每天都在写文，有的还专门传授写作技巧，却连基本的文案排版都不会，最简单的一点就是「不会在中文和英文数字之间添加空格」。你可以验证一下，看看有多少文章是这样的，我敢说绝大部分都是。</p><p>这么个小细节也算回事儿么？先拿同样一段文字对比一下：</p><p>不会空格的人这样写：</p><blockquote><p>第八代Intel Core处理器将MacBook Pro的运算性能提升到了新的高度。15英寸机型现在可配备六核Intel Core i9处理器，处理速度最快可比上一代提升70%。</p></blockquote><p>会空格的人这样写：</p><blockquote><p>第八代 Intel Core 处理器将 MacBook Pro 的运算性能提升到了新的高度。15 英寸机型现在可配备六核 Intel Core i9 处理器，处理速度最快可比上一代提升 70%。</p></blockquote><p>哪段文字更好看？你要是说前一个或者不知道，那就不用往下看了，这篇文章不是为你准备的。</p><p>明显后者看起来更舒服。事实上，很多优秀的网站或者作者都是这样排版的，比如苹果、少数派、stormzhang、阳志平等。</p><p>我以前从未意识到「适当添加空格」这件事的重要性，直到看到一篇《中文文案排版指北》的教程，里面这么调侃那些不喜欢添加空格的人：</p><blockquote><p>有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。</p></blockquote><p>那之后就刻意遵守这个排版规则，仅这一点小改变，文章观感就明显比以前更舒适。</p><p>不过这样做有一个缺点就是打字速度会降低，因为要常切换添加空格，有时可能还会忘记反而会让文章看起来不统一。于是开始找有没有可以自动添加空格的方法，最近找到了完美解决方案。</p><p>使用「纯纯写作」这款笔记 App，它里面有一个「在中英文之间插入空格」的功能，有了它写文时可以先不管空不空格，写好后一次性添加所有空格，效率更高。</p><p><img src="http://media.makcyun.top/Fgho6Ba9rxPP_5MDI1jRcpycSSyj" alt=""></p><p>不过这款 App 没有电脑版，无法直接实现这个功能，可以曲线借助坚果云网盘，在电脑端和手机端同步使用「拉取」和「上传」功能添加空格，效果像下面这样：</p><p>添加之前：</p><p><img src="http://media.makcyun.top/FjW5bnFglC3B4LsG76M07k_JwomP" alt=""></p><p>添加之后：</p><p><img src="http://media.makcyun.top/FvSyH44S8jBJ5XgciMo9izZ8Jxqj" alt=""></p><p>想要这款 App 可以网上搜，也可以我的公众号后台回复：「<strong>笔记App</strong>」 得到。</p><p>最后想说，<strong>写作好不好是能力，排版好不好是态度。</strong></p><p>觉得不错想看更多的文案排版规则的话，可参见下文：</p><p><a href="https://www.makcyun.top/weekly_sharing5.html">这个 GitHub 库能拯救你的文章排版</a></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;写作排版必备技能：快速在中文和英文数字之间添加空格。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="技能" scheme="https://www.makcyun.top/tags/%E6%8A%80%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>从 Python 官方年度报告学习 PPT 设计</title>
    <link href="https://www.makcyun.top/2019/03/01/weekly_sharing18.html"/>
    <id>https://www.makcyun.top/2019/03/01/weekly_sharing18.html</id>
    <published>2019-03-01T08:16:16.000Z</published>
    <updated>2019-03-03T03:23:02.073Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>分享几点专业 PPT 制作技巧。</p><a id="more"></a><p><strong>摘要</strong>：分享几点专业 PPT 制作技巧，另送一套珍藏模板。</p><p>这里是「<strong>每周分享</strong>」的第 18 期。往期分享内容可以在公众号后台的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>上周的一篇推文分享了一份官方的 Python 2018 年度报告，这份报告做得相当专业，我看了好几遍。</p><p>文章传送门：</p><p><img src="http://media.makcyun.top/Fj00NJEQDNBFZNnvSsCQWr2XnR8k" alt=""></p><p>说它专业，一个是内容本身专业，另一个是报告设计地专业，20 多页的报告仅使用了一些简单的元素组合搭配，却处处体现了设计美感。</p><p>说到「设计」，很多人觉得这是设计师的专业领域，一般人不会也不用会。其实「设计」离我们每个人都很近，你常使用的 PPT 就是设计工具 ，只不过很多人没有把它当作设计工具来用，自然做出的 PPT 也就很一般。其实只要遵循一些简单的设计规则和技巧，我们都可以做出水准不错的 PPT。</p><p>今天就来说说，从这份报告中可以学到哪些 PPT 制作技巧，当学会这些技巧了，相信你的 PPT 水平也会上一个台阶。</p><p>初看这份报告最直观的感觉就是风格统一，简洁专业，看着舒服，我从下面这几点来说说：</p><h3 id="用色统一克制"><a href="#用色统一克制" class="headerlink" title="用色统一克制"></a>用色统一克制</h3><p>整套报告的前后页颜色统一，没有乱入的颜色，且用色很克制，只使用了三种颜色，符合多数专业报告的标准。</p><p>以上这两点看似简单，但很多人却做不到。有的人做 PPT 完全不顾整体，前几张幻灯片用这种颜色，后几张又换成了其他颜色，最后一套 PPT 看起来乱七八糟。其实 PPT 新手通常驾驭不好多种颜色的使用，最安全保险的选择是只使用不超过三种颜色或者使用一类单色，这样的话 PPT 看起来至少风格是统一的，不会乱。</p><p>具体做法很简单，在做 PPT 之前就选好配色方案，这样做的好处就是能够保证只从这一组颜色中去选，不会选到其他颜色，保证了整套 PPT 的颜色统一。</p><p><img src="http://media.makcyun.top/Fs_kgpz4nVvX3jGR-TXa26SCBcrm" alt=""></p><p>通过左侧的图你可以看到，我设置了很多种配色方案，这个好处在于可以随时切换整套 PPT 的色彩搭配，只需要做好一套 PPT 就可以生成很多套不同风格的 PPT 出来。这也是网上卖的那些 PPT 模板，可以给你提供多种配色方案所采用的方法。</p><p><img src="http://media.makcyun.top/FsK5bQYEM5U_6kQzZWenhOQltJI7" alt=""></p><p>你可能会问，这种专业的配色方案是我自己组合搭配出来的么，当然不是，我是从别处「偷」过来的。</p><p>如果你不太懂色彩理论知识，也不会搭搭配颜色，那可以去专业的配色网站上寻找好的配色方案，比如 Color Hunt ，记录色彩方案的 RGB 颜色值再保存到 PPT 中，就可以作为自己的方案了。</p><p><img src="http://media.makcyun.top/FoMybWkZnRG8jmJy4IT8GKFXgA1y" alt=""></p><h3 id="色彩饱和度适中"><a href="#色彩饱和度适中" class="headerlink" title="色彩饱和度适中"></a>色彩饱和度适中</h3><p><img src="http://media.makcyun.top/Fo4eAFBXwZzUY565ZX8gVny1nElE" alt=""></p><p>这份报告使用了两种主色，黄色和蓝色，适中的色彩饱和度让报告看起来舒服不刺眼，而很多新手则爱用 PPT 中默认的高饱和度色彩，这是大忌。</p><p><img src="http://media.makcyun.top/Fl3z62r9UnJnVZttVFDem--x9zSs" alt=""></p><p>饱和度是指色彩纯度，数值范围是在 0-255，常说的大红大紫就是说颜色饱和度数值很高，看起来刺眼。新手不太懂饱和度的概念，只会用会用系统默认的高饱和度色彩，后果就是做出来的 PPT 看着很 LOW。</p><p><img src="http://media.makcyun.top/Fm7szPehsNEJDjlUO9YN5PIgQDDZ" alt=""></p><p>事实上，大多数专业报告和 PPT 使用的都是适中的饱和度色彩。</p><p><img src="http://media.makcyun.top/201901200835_219.jpg" alt=""></p><h3 id="黑金万能搭配色"><a href="#黑金万能搭配色" class="headerlink" title="黑金万能搭配色"></a>黑金万能搭配色</h3><p>报告还使用了一种万能的色彩搭配组合：黑色和金（黄）色，这两种颜色可以说是百搭，好看不俗气，如果你不知道哪两种颜色搭配好，就选这一组吧。</p><p><img src="http://media.makcyun.top/Ft3nTYxxDq1Ys-Q9KgTJufO_UVa8" alt=""></p><p><img src="http://media.makcyun.top/FqG7KHQ_AxZsK5K6JUKIlJmXdrpR" alt=""></p><p>除了这种组合，还有两种万能搭配色，分别是红黑和红金。</p><p><img src="http://media.makcyun.top/FpDr8qPPDmTdk-ZQYCbOKDzT-mVW" alt=""></p><p><img src="http://media.makcyun.top/FrermBQZjKBcLPMD5vKUI8oDZV63" alt=""></p><p>上面说的都是配色方面，下面说一下字体。</p><h3 id="字体选择"><a href="#字体选择" class="headerlink" title="字体选择"></a>字体选择</h3><p>报告的标题字体很好看，没有使用烂大街的宋体、黑体和微软雅黑，使用的是「文悦后现代体」。字体也是影响 PPT 美观的一个重要因素，如果你想让你的 PPT 看起来不一样，那就不要用系统自带的字体，去网上下载一些好看的字体。</p><p><img src="http://media.makcyun.top/FnAcXv-pmNWWkt31Gh-Ar--dd0oW" alt=""></p><p>字体下载网站如果推荐一个的话，我会首推「字客网」，这个网站有很多漂亮的中英文字体，大多都支持下载，唯一需要注意的是字体能否商用。</p><p><img src="http://media.makcyun.top/Ftnm3q7NKfztTQQ1nbMDdZGLh1tg" alt=""></p><h3 id="图表制作"><a href="#图表制作" class="headerlink" title="图表制作"></a>图表制作</h3><p>接下来再来说说这份报告的图表。在 PPT 中绘制图表并不方便，通常是利用 Excel 或者一些形状元素绘制，效率不高。如果能够借助插件的话，图表绘制起来会快很多，比如我使用了一款叫 iSlide 的插件可以快速绘制下面的饼图：</p><p><a href="http://media.makcyun.top/%E9%A5%BC%E5%9B%BE.gif" target="_blank" rel="noopener">http://media.makcyun.top/%E9%A5%BC%E5%9B%BE.gif</a></p><p>这样画图是不是很快？有些人做 PPT 很慢，往往是不会用或者没利用好 PPT 插件。对这款插件感兴趣的话可以下载下来体验下，也可以在我公众号后台回复：<strong>18期</strong> 得到这款插件。</p><p>3 月份，很多人要开始准备做晋升述职类的 PPT 了，想到这个呢，今天就送各位一套珍藏已久的 PPT 模板。个人觉得模板不在于多而在于精，很多公众号动不动送几十上百套烂模板，学不到东西还占地方，我每次只送一套有质量的模板，数量少你不会有压力，也有时间慢慢去学习研究。</p><p>来看看这套模板的部分预览图：</p><p><img src="http://media.makcyun.top/FqVTm-2GDBRMekx2zi7hIuxd4ekE" alt=""></p><p>你如果需要，可以在公众号后台回复 <strong>18 期</strong> 得到。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;分享几点专业 PPT 制作技巧。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="PPT" scheme="https://www.makcyun.top/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>一些不错的 Python 开源电子书</title>
    <link href="https://www.makcyun.top/2019/02/28/python_learning02.html"/>
    <id>https://www.makcyun.top/2019/02/28/python_learning02.html</id>
    <published>2019-02-28T08:16:16.000Z</published>
    <updated>2019-02-28T11:57:58.570Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Python 各个领域的精品。</p><a id="more"></a><p><strong>摘要</strong>：我学习 Python 收藏的一些开源电子书网站。</p><p>昨天给大伙儿送了书，留言区的篇幅占了整篇文章的一半，看来大家都想好（把）好（我）学（掏）习（空）。今天不送纸质书了，分享几个我学 Python 一路以来收藏的几个开源电子书网站。</p><p>说到电子书，你可能会说「我有大把的电子书，还用得着你推荐？」注意，我这里说的不是烂大街的 PDF，是专门放电子书的网站。</p><p>这种网站相比 PDF 电子书有什么好处呢，就是排版更优美。复制粘贴保存到自己的学习笔记中也很方便。</p><p>说到保存学习笔记，昨天偶然发现印象笔记的 Windows 版支持 Markdown 了，你要也是 Windows 重度用户，感觉升级尝试下吧。<br>你要是没用过 Markdown ，可以看我之前的写的这篇文章，保你相见恨晚：</p><p>文章传送门：<a href="https://www.makcyun.top/weekly_sharing4.html">5 分钟内搞定公众号排版</a></p><p><img src="http://media.makcyun.top/FjjgWg5XGhD1MB8As6W5hxu4tPG8" alt=""></p><p>拐回来，下面就来介绍下这些电子书网站，不同的领域都介绍一个：</p><h2 id="Python-入门"><a href="#Python-入门" class="headerlink" title="Python 入门"></a>Python 入门</h2><p>《A Byte of Python（简明 Python 教程）》豆瓣评分：8.7</p><p>一句话介绍：Python 初学者的极佳教材。</p><p>网址：<a href="https://wizardforcel.gitbooks.io/a-byte-of-python/content/" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/a-byte-of-python/content/</a></p><p><img src="http://media.makcyun.top/FuCvUu_lGh7G1TYHxwFgiylPbvWU" alt=""></p><h2 id="Python-进阶"><a href="#Python-进阶" class="headerlink" title="Python 进阶"></a>Python 进阶</h2><p>《Python Cookbook》豆瓣评分：9.3</p><p>一句话介绍：有很多高级技巧，想了解 Python 底层的工作原理就看这本。</p><p>网址：<a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">https://python3-cookbook.readthedocs.io/zh_CN/latest/</a></p><p><img src="http://media.makcyun.top/Fkj848FbY7h1oL6u6Dfhen7zZVp-" alt=""></p><h3 id="Python-数据分析"><a href="#Python-数据分析" class="headerlink" title="Python 数据分析"></a>Python 数据分析</h3><p>《利用 Python 进行数据分析》豆瓣评分：8.6</p><p>一句话介绍：学习 Python 基础库最好的书。</p><p>网址：<a href="https://hand2st.apachecn.org/#/README" target="_blank" rel="noopener">https://seancheney.gitbook.io/python-for-data-analysis-2nd/</a></p><p><img src="http://media.makcyun.top/Fj1pKBrhD_qcPKsdG7YWfzLwygEM" alt=""></p><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p>《Scikit-Learn与TensorFlow机器学习实用指南》豆瓣评分：9.6</p><p>一句话介绍：机器学习书中理论结合实战最好的书。</p><p>网址：<a href="https://hand2st.apachecn.org/#/README" target="_blank" rel="noopener">https://hand2st.apachecn.org/#/README</a></p><p><img src="http://media.makcyun.top/Fnt73izdex7G2h0-80QC6rIFY9Ob" alt=""></p><h3 id="Python-数据结构算法"><a href="#Python-数据结构算法" class="headerlink" title="Python 数据结构算法"></a>Python 数据结构算法</h3><p>《problem-solving-with-algorithms-and-data-structure-using-python》豆瓣评分：9.3</p><p>一句话介绍：Python 数据结构与算法相关的书很少，这本堪称最好。</p><p>网址：<a href="https://facert.gitbooks.io/python-data-structure-cn/" target="_blank" rel="noopener">https://facert.gitbooks.io/python-data-structure-cn/</a></p><p><img src="http://media.makcyun.top/FlIv98HQKMQcdDlUhiYcTc0dYdH9" alt=""></p><p>最后来一个大杂烩，最全的在线手册，各种教程手册汇集到一起，哪个不会点哪个。<br>网址：<a href="https://docs.pythontab.com" target="_blank" rel="noopener">https://docs.pythontab.com</a></p><p><img src="http://media.makcyun.top/FmpHTNvOK7HnheJFwbAaHGdirHMN" alt=""></p><p>以上就是我一直珍藏的几个电子书资源，配合纸质书学习，完美。</p><p>本文完。</p><p>ChangeLog</p><ul><li>2019/2/28 version 1.0</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Python 各个领域的精品。&lt;/p&gt;
    
    </summary>
    
      <category term="Python学习" scheme="https://www.makcyun.top/categories/Python%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python 入门" scheme="https://www.makcyun.top/tags/Python-%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>2018 年大学毕业生薪酬排行榜</title>
    <link href="https://www.makcyun.top/2019/02/25/data_analysis&amp;mining03.html"/>
    <id>https://www.makcyun.top/2019/02/25/data_analysis&amp;mining03.html</id>
    <published>2019-02-25T08:16:16.000Z</published>
    <updated>2019-02-27T08:26:12.179Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>简单分析 2018 年大学毕业生薪酬。</p><a id="more"></a><p><strong>摘要：</strong> 看看你的学校和专业 <strong>钱途</strong> 怎么样，你的工资有没有被平均。</p><p>这两天刷微博看到一个热门话题，叫：<strong>2018 年中国大学毕业生薪酬排行榜 TOP 200</strong>。这份榜单罗列了各高校毕业生的薪资排名。</p><p>名单出来后评论区炸了，有吐槽有调侃。有人说前三名应该是北电、中戏和上戏，有人说自己从毕业到现在赚了两个亿，一个失忆一个回忆，还有的惊叹自己专业薪资那么低（高）。</p><p>我是一谈「钱」就激动，工作后同事之间的薪资是严格保密的，一般很难打听到，越神秘越好奇，总想跟别人比比看自己的工资是高还是低。高了，优越感油然而生；低了，再不跟这个人来往。</p><p>赶紧戳开看了看，看完放心了，都比我高。一起来看看，先是按学校排名：</p><p><img src="http://media.makcyun.top/FkNXzvbVjk1STvX1jCvKA-6qB0L8" alt=""></p><p><img src="http://media.makcyun.top/FmE6gXPQzsVwo-auEwxaGH-Xi4N6" alt=""></p><p><img src="http://media.makcyun.top/FjVypqOoxTlMR3_Y76XWnlgcwIL6" alt=""></p><p><img src="http://media.makcyun.top/FuBjEifs5p4FkDkQbZObwqNbrcyn" alt=""></p><p><img src="http://media.makcyun.top/Fvze--DmHzYmoisZBR4CI5FRoH3l" alt=""></p><p>然后是不同专业的薪资排名：</p><p><img src="http://media.makcyun.top/Fr4PRnO6q0HNrDD8bWJpQCvN3ZnM" alt=""></p><p><img src="http://media.makcyun.top/FhztvzBpwYXmjMyTcX2RA_F64I8w" alt=""></p><p>大概看出这么几点事儿：</p><ul><li><strong>工资最高的是「清北」</strong></li></ul><p>这一点都不意外，2017 届毕业也就是工作一年的大学生，平均薪资在 9000 左右，工作三年后工资达到 1 万 1，五年后则能达到 1 万 3 左右。</p><ul><li><strong>排前 40 的大部分是「985」和「211」高校</strong></li></ul><p>是谁说学校不重要来着，薪资不会撒谎。</p><ul><li><strong>北京外国语、对外经贸、外交学院这些大学薪资很高</strong></li></ul><p>这些学校听得比较少，没想到工资这么高，为什么那么多人说学外语没前途？</p><ul><li><strong>理学和工学类专业薪酬水平较高</strong></li></ul><p>比如软件工程、材料物理、汽车类这些。</p><ul><li><strong>农学、法学和管理学较低，大多数专业平均薪酬不到 3000 元。</strong></li></ul><p>你看完是什么感觉？我是不太信这份榜单的，因为翻了几遍没发现我的学校。</p><p>不过这份榜单说是由中国薪酬网发布的，他们调研了 39 所 「985」和 112 所「211」高校的 280 万学生计算出来的，比较权威。</p><p>暂且不管吧，下面来用 Python 稍微深入地分析一下这份榜单，我分析了这么几点：</p><h2 id="哪些学校薪资有竞争力？"><a href="#哪些学校薪资有竞争力？" class="headerlink" title="哪些学校薪资有竞争力？"></a>哪些学校薪资有竞争力？</h2><p>筛选出来了工作一年和五年后，薪资最高（低）的 10 所高校：</p><table><thead><tr><th>rank</th><th>university</th><th>2017</th><th>university</th><th>2013</th></tr></thead><tbody><tr><td>0</td><td>清华大学</td><td>9065</td><td>北京大学</td><td>13790</td></tr><tr><td>1</td><td>北京大学</td><td>9042</td><td>复旦大学</td><td>13594</td></tr><tr><td>2</td><td>北京外国语大学</td><td>9020</td><td>外交学院</td><td>12669</td></tr><tr><td>3</td><td>上海交通大学</td><td>9010</td><td>清华大学</td><td>12614</td></tr><tr><td>4</td><td>对外经济贸易大学</td><td>8998</td><td>同济大学</td><td>13616</td></tr><tr><td>5</td><td>外交学院</td><td>8956</td><td>国际关系学院</td><td>12786</td></tr><tr><td>6</td><td>复旦大学</td><td>8842</td><td>上海外国语大学</td><td>12587</td></tr><tr><td>7</td><td>浙江大学</td><td>8810</td><td>中国人民大学</td><td>12258</td></tr><tr><td>8</td><td>同济大学</td><td>8784</td><td>对外经济贸易大学</td><td>12316</td></tr><tr><td>9</td><td>中央财经大学</td><td>8771</td><td>广东外语外贸大学</td><td>12229</td></tr></tbody></table><table><thead><tr><th>rank</th><th>university</th><th>2017</th><th>university</th><th>2013</th></tr></thead><tbody><tr><td>190</td><td>兰州理工大学</td><td>3616</td><td>兰州理工大学</td><td>5206</td></tr><tr><td>191</td><td>青岛科技大学</td><td>3584</td><td>延边大学</td><td>5072</td></tr><tr><td>192</td><td>天津师范大学</td><td>3574</td><td>重庆工商大学</td><td>5253</td></tr><tr><td>193</td><td>华北电力大学(保定）</td><td>3542</td><td>青岛科技大学</td><td>5032</td></tr><tr><td>194</td><td>山东中医药大学</td><td>3528</td><td>南京信息工程大学</td><td>4974</td></tr><tr><td>195</td><td>江苏科技大学</td><td>3513</td><td>山东中医药大学</td><td>4871</td></tr><tr><td>196</td><td>武汉工程大学</td><td>3460</td><td>江苏科技大学</td><td>4934</td></tr><tr><td>197</td><td>延边大学</td><td>3437</td><td>华北电力大学(保定）</td><td>4931</td></tr><tr><td>198</td><td>南京信息工程大学</td><td>3426</td><td>武汉工程大学</td><td>4734</td></tr><tr><td>199</td><td>西安建筑科技大学</td><td>3394</td><td>西安建筑科技大学</td><td>4925</td></tr></tbody></table><p>可以看到：</p><ul><li><p>清华毕业一年薪资最高，但五年后就不如北大、复旦甚至外交学院。</p></li><li><p>前十的不少是语言贸易类大学，羡慕。</p></li><li>工程科技类大学薪资很低，有点没想到。</li></ul><h2 id="哪些地方薪资高？"><a href="#哪些地方薪资高？" class="headerlink" title="哪些地方薪资高？"></a>哪些地方薪资高？</h2><p><img src="http://media.makcyun.top/Fjdt5Eisn6x8XBMYRXh0MBxYWU5A" alt=""></p><p>薪资最高的还是上海、北京、广东这些一线城市，看来想拿高工资还是得去一线。另外，西北一片白是因为一所学校都没有统计到，身为一个新疆人很气愤。</p><h2 id="是理工类、综合类还是其他类型学校薪资高？"><a href="#是理工类、综合类还是其他类型学校薪资高？" class="headerlink" title="是理工类、综合类还是其他类型学校薪资高？"></a>是理工类、综合类还是其他类型学校薪资高？</h2><p>国内大学按照学校的类型，可以分为下面十大类。薪资最高的不是理工类，也不是综合型大学，而是语言、艺术类学校，完全没想到啊。看来，今天薪资这么低是因为选错了学校类型。</p><p><img src="http://media.makcyun.top/FmxFiOl9VVFe91j8EdlCVAzKFHTM" alt=""></p><h2 id="学校重不重要？"><a href="#学校重不重要？" class="headerlink" title="学校重不重要？"></a>学校重不重要？</h2><p>出来工作后很多人鼓吹学校不重要，能力才重要。个人觉得这话对一半，要有本事的话，出身好能力又强不更好？</p><p>来看「985 」「211 」和「双非」这三类高校不同工作年限的薪资情况。</p><p>可以看到，不管是工作一年、三年还是五年，「985 」学校的薪资都高于「211 」学校，远高于双非学校。<strong>所以能上好学校还是尽量上。</strong></p><p><img src="http://media.makcyun.top/FiTdml2D8ktKjHRhOSIeRbmgfQcn" alt=""></p><p>以上就是一个的简单的分析，源码在我的知识星球。</p><p><img src="http://media.makcyun.top/FsInbKzdrtxc7GsCbXzTkFB1A27d" alt=""></p><hr><p>参考：</p><p><a href="http://www.sohu.com/a/250611376_508571" target="_blank" rel="noopener">http://www.sohu.com/a/250611376_508571</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;简单分析 2018 年大学毕业生薪酬。&lt;/p&gt;
    
    </summary>
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>珍藏已久的网盘资源搜索网站和下载神器</title>
    <link href="https://www.makcyun.top/2019/02/23/weekly_sharing17.html"/>
    <id>https://www.makcyun.top/2019/02/23/weekly_sharing17.html</id>
    <published>2019-02-23T08:16:16.000Z</published>
    <updated>2019-03-23T05:13:42.366Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>推荐几个精品网盘资源搜索网站和下载软件。</p><a id="more"></a><p><strong>摘要</strong>：分享几个网盘资源搜索网站和下载神器。</p><p>这是「<strong>每周分享</strong>」的第 17 期。最近有不少新朋友关注，所在再介绍一下这个栏目，每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、Ps 等几个方面。往期内容你可以在公众号界面中的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>这一期的主题是网盘资源资源搜索和下载。很多人在网上搜索电影、书籍、课程这些资源的时候，会直接在百度中去搜，往往要花很长时间才能搜到想要的东西。懂点门道的会选择专门的网盘资源网站，比如：去转盘、胖次搜索、网盘007 这些，但这些网站的资源基本差不多，你有的我也有，我没有的你也没有。</p><p>接下来分享两个搜索资源的神级网站，在这两个网站里，我找到了很多其他地方找不到的资源。</p><h2 id="云盘精灵"><a href="#云盘精灵" class="headerlink" title="云盘精灵"></a><a href="www.yunpanjingling.com">云盘精灵</a></h2><p>网址：<a href="http://www.yunpanjingling.com" target="_blank" rel="noopener">www.yunpanjingling.com</a></p><p><img src="http://media.makcyun.top/Fo4wG8c9Blh11rBQ_i-7VuQX99ns" alt=""></p><p>这个网站的特点是能搜到很多付费的东西，比如网课、图书、刚上映的电影等等，具体能搜哪些，自己可以对比其他网站试试，好的话过来留言告诉我。</p><p>不过，该网站的资源是需要充值付费的，下载一个资源 2 分钱，等于不要钱。</p><p>如果你不喜欢付费的，推荐第 2 个网站给你。</p><h2 id="爱搜资源"><a href="#爱搜资源" class="headerlink" title="爱搜资源"></a>爱搜资源</h2><p>网址：<a href="http://www.aisouziyuan.com" target="_blank" rel="noopener">www.aisouziyuan.com</a></p><p><img src="http://media.makcyun.top/Fjgr5SGPEtTRZMSGD60OSA2IUdcY" alt=""></p><p>这个网站可以说是「云盘精灵」的翻版，对比搜索了很多东西后，发现两个网站资源差不多丰富，总体来说，云盘精灵的资源更全面一些。</p><p>有了获取资源的地方，就需要下载工具了。某度的网盘会员既收费还不好用，随便下载个大点的文件，必须得打开网盘才能下，不方便速度也慢。这里推荐几个下载工具，可以加速下载网盘资源。</p><p>先来解决怎么不启动网盘直接下载大文件。</p><p><img src="http://media.makcyun.top/FhiPU36i3unHqg6SSvEeucbrLJEH" alt=""></p><p>你可以看到，我的下载界面多了个插件，可以选择普通下载或者复制链接等模式。点普通下载就可以用浏览器直接下载。这种网盘插件很多，在 Greak Fork 网站能找到很多，安装上就能用。</p><p><img src="http://media.makcyun.top/FhrwbPTko7GF_ML0JJ_T9ooebzp6" alt=""></p><p>普通下载模式速度比较慢，想加快的话可以选择「复制链接」到 IDM 这款下载神器中下载。</p><p><img src="http://media.makcyun.top/FvLbyF1Xkex73ghxFakvYRof13fL" alt=""></p><p>想要这款精巧的神器，可以在公众号后台回复：「<strong>网盘下载</strong>」得到。</p><p>如果你觉得速度还不够快，可以使用下面这款神器。</p><h2 id="速盘"><a href="#速盘" class="headerlink" title="速盘"></a>速盘</h2><p>网址：<a href="http://www.speedpan.com" target="_blank" rel="noopener">www.speedpan.com</a></p><p><img src="http://media.makcyun.top/Fg8ZvjHHuHxNV8Cj5K0bk6nyXEnv" alt=""></p><p>这个一款不限速的网盘下载工具，我没有调到最快，速度还是杠杠的。保险起见，不要用常用的网盘下载，花两三块买个备用网盘，专门用来下载东西，就算被封了也没大碍。要是不知道在哪里买，私信我。</p><p>以上主要是针对 Windows 平台的下载软件，如果你用的是 Mac，可以使用 <strong>BaiduNetdiskPlugin</strong> 这款插件，速度也很快。</p><p>下载地址和使用教程，见下面这个 GitHub 库：</p><p><a href="https://github.com/CodeTips/BaiduNetdiskPlugin-macOS" target="_blank" rel="noopener">https://github.com/CodeTips/BaiduNetdiskPlugin-macOS</a></p><p><img src="http://media.makcyun.top/FoOvGqoWxKPn3kxR9IVc8ZqUf6t3" alt=""></p><p>好，以上就是这一期分享的几个网盘资源搜索和下载的实用工具。<strong>善用佳软，提高生产力。</strong></p><p>本文完。</p><hr><p>/今日留言主题/</p><p>你常用网盘么，还有什么网盘搜索下载的技巧或者网站？</p><p>(留言格式：Day01：blahblah)</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;推荐几个精品网盘资源搜索网站和下载软件。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="网盘搜索下载" scheme="https://www.makcyun.top/tags/%E7%BD%91%E7%9B%98%E6%90%9C%E7%B4%A2%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>美国 IT 大牛建议：2019 年如何学习数据科学</title>
    <link href="https://www.makcyun.top/2019/02/21/translating01.html"/>
    <id>https://www.makcyun.top/2019/02/21/translating01.html</id>
    <published>2019-02-21T08:16:16.000Z</published>
    <updated>2019-02-21T08:41:17.635Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>初学 / 转行 Python 的困惑，这里都有答案。</p><a id="more"></a><p><strong>摘要</strong>：初学、转行 Python 的困惑，这里都有答案。</p><p>作者 | Thomas Nield</p><p>来源 | <a href="https://towardsdatascience.com/how-it-feels-to-learn-data-science-in-2019-6ee688498029" target="_blank" rel="noopener">How it feels to learn data science in 2019</a></p><p>去年我决定从传统水利行业跨行到 Python 领域的时候，满脑子都是困惑与担心，犹豫放弃所学多年的专业知识值不值得，担心万一转行失败怎么办，纠结实际工作比想象中的难怎么办。</p><p>没遇到指点迷津的大佬，只好网上各种搜，众说纷纭，最后在「要不要转行」这个问题上浪费了很长时间。在跨过这个坎之后，回头来看以前那些问题，思路清晰很多。</p><p>其实，在开始阶段，相比具体的专业知识，更重要的是大方向把握。好比，你告诉我旅途上的风景有多么多么美，但我想先知道是哪条路。</p><p>最近看到一篇叫<strong>「2019 年学习数据科学是什么感受」</strong>的文章，深有感触。作者是Thomas Nield，美国西南航空公司的商务顾问，著有《Getting Started with SQL (O’Reilly) 》等书，经验丰富的 IT 大牛。</p><p>文章中他以一问一答的形式，给那些想要踏上数据科学之路的人，提了一些中肯的建议。里面有些观点很有价值，特翻译成文，这里分享给你。</p><p><strong>背景</strong>：假设你是一名「表哥」，平常工作主要使用 Excel，数据透视表、制图表这些。最近了解到未来很多工作岗位会被人工智能会取代，甚至包括你现在的工作。你决定开始学习数据科学、人工智能和机器学习，在 Google 搜索了「如何成为数据科学家」找到了下面这样一份学习路线图，然后你就开始向作者大牛请教了。</p><p><strong>Q：我是否真的必须掌握这个图表中的所有内容，才能成为数据科学家？</strong></p><p><img src="http://media.makcyun.top/Fuf2Oir__fILPJsbDc22DVY3d4MB" alt=""></p><center>成为一名自信数据科学家的必须技能（截至2013年）</center><p>A：简单说，不需要全部。这是 2013 年的路线图，有点过时了，里面连 TensorFlow 都没有，基本没有人再参考。完全可以划掉这个图中的一些路径，前几年「数据科学」划分地过于分散，采用其他方法会更好。</p><p><strong>Q：听你这样说就不那么紧张了，那么我应该回到学校继续深造，然后获得一个数据科学硕士学位吗？ 我看很多数据科学家至少都是硕士。</strong></p><p>A：天哪，你为什么这样做？不要被「数据科学」这些高大上的术语给唬住了，这些术语主要是用来重新定义一些业务分类。事实上，学校教授的东西基本都是过时的技术，不如选择 Coursera 或 Khan Academy 这些在线自学网站。</p><p><strong>Q：那么我该如何开始自学呢？LinkedIn上的人说应该先学习 Linux ，Twitter 的人建议先学习 Scala，而不是 Python 或 R</strong></p><p>A：不要信那些人的话。</p><p><strong>Q：好的，R怎么样？不少人喜欢它。</strong></p><p>A：R 擅长数学建模，但 Python 能做的更多，比如数据处理和搭建 Web 服务，总之比 R 的学习投资回报率高。</p><p><strong>Q：R 在 Tiobe上的排名仍然很高，而且拥有大量的社区和资源，学它有什么不好？</strong></p><p>如果你只是对数学感兴趣，使用 R 完全没问题，配合 Tidyverse 包更是如虎添翼。但数据科学的应用范围远超数学和统计学。所以相信我，Python 在 2019 年更值得学，学它不会让你后悔。</p><p><strong>Q：Python 难学么？</strong></p><p>A：Python 是一种简单的语言，可以帮你可以自动完成许多任务，做一些很酷的事情。不过数据科学不仅仅是脚本和机器学习，甚至不需要依赖 Python 。</p><p><strong>Q：什么意思？</strong></p><p>A：Python 这些只是工具，使用这些工具可以从数据中获取洞察力，这个过程有时会涉及到机器学习，但大部分时间没有。简单地来说，创建图表也可以算是数据科学，所以你甚至不必学习 Python，使用 Tableau 都行，他们宣称使用他们的产品就可以「成为数据科学家」。</p><p><strong>Q：好吧，但数据科学应该不仅仅是制作出漂亮的可视化图表，Excel 中都可以做到，另外学习编程应该很有用，告诉我一些 Python 方面的知识吧</strong></p><p>A：学习 Python，你需要学习一些库，比如用于操作 DataFrame 的 Pandas 、制作图表的 Matplotlib，实际上更好的选择是 Plotly，它用了 d3.js。</p><p><strong>Q：我能懂一些，但什么是 DataFrame？</strong></p><p>A：它是一种有行和列的数据结构，类似 Excel 表，使用它可以实现很酷的转换、透视和聚合等功能。</p><p><strong>Q：那 Python 与 Excel 有什么不同？</strong></p><p>A：大不相同，你可以在 Jupyter Notebook 中完成所有操作，逐步完成每个数据分析阶段并可视化，就像你正在创建一个可以与他人分享的故事。毕竟，沟通和讲故事是数据科学的重要组成部分。</p><p><strong>Q：这听起来和 PowerPoint 没什么区别啊？</strong></p><p>A：当然有区别，Jupyter Notebook 更自动简洁，可以轻松追溯每个分析步骤。有些人不太喜欢它，因为代码不是很实用。如果你想做一款软件产品，更好的方法是使用其他工具模块化封装代码。</p><p><strong>Q：那么数据科学跟软件工程也有关系么？</strong></p><p>A：也可以这么说，但不要走偏，学习数据科学最需要的是数据。初学的最佳方式是网络爬虫，抓取一些网页，使用 Beautiful Soup 解析它生成大量非结构化文本数据下载到电脑上。</p><p><strong>Q：我以为学习数据科学是做表格查询而不是网页抓取的工作，所以我刚学完一本 SQL 的书，SQL 不是访问数据的典型方式吗？</strong></p><p>A：好吧，我们可以使用非结构化文本数据做很多很酷的事情。比如对社交媒体帖子上的情绪进行分类或进行自然语言处理。NoSQL 非常擅长存储这种类型的数据。</p><p><strong>Q：我听说过 NoSQL 这个词，跟 SQL 、大数据有什么关系？</strong></p><p>A：大数据是 2016 年的概念，已经有点过时了，现在大多数人不再使用这个术语。NoSQL 是大数据的产物，今天发展成为了像 MongoDB 一样的平台。</p><p><strong>Q：好的，但为什么称它为 NoSQL？</strong></p><p>A：NoSQL 代表不仅是 SQL，它支持关系表之外的数据结构，不过 NoSQL 数据库通常不使用 SQL，有专门的查询语言，简单对比一下 MongoDB 和 SQL 查询语言：</p><p><img src="http://media.makcyun.top/FtR3i8_rBdOfbhbIxS2H0ZR2iewm" alt=""></p><p><strong>Q：这太可怕了，你意思是每个 NoSQL 平台都有自己的查询语言？SQL 有什么问题？</strong></p><p>A：SQL 没有任何问题，它很有价值。不过这几年非结构化数据是热潮，用它来做分析更容易。需强调的是，尽管 SQL 难学，但它是一种非常通用的语言。</p><p><strong>Q：好的，我可以这样理解么： NoSQL 对数据科学家来说不像 SQL 那么重要，除非工作中需要它？</strong></p><p>A：差不多，除非你想成为一名数据工程师。</p><p><strong>Q：数据工程师？</strong></p><p>A：数据科学家分为两个职业。数据工程师为模型提供可用的数据，机器学习和数学建模涉及比较少，这些工作主要由数据科学家来做。如果你想成为一名数据工程师，建议优先考虑学习 Apache Kafka 而不是 NoSQL，Apache Kafka 现在非常热门。</p><p>如果想成为「数据科学家」，可以看看这张数据科学维恩图。简单来说，数据工程师是一个多领域交叉的岗位，你需要懂数学/统计学、编程以及你专业方面的知识。</p><p><img src="http://media.makcyun.top/FhPe4LdWQpJ4uke26hs7RL4KK2mA" alt=""></p><p><strong>Q：好吧，我不知道我现在是想成为数据科学家还是数据工程师。回过头来，为什么要抓维基百科页面呢？</strong></p><p>A：抓取下来的页面数据，可以作为自然语言处理的输入数据，之后就可以做一些事情，如创建聊天机器人。</p><p><strong>Q：我暂时应该不用接触自然语言处理、聊天机器人、非结构化文本数据这些吧？</strong></p><p>A：不用但值得关注，像 Google 和 Facebook 这些大公司，目前在处理大量非结构化数据（如社交媒体帖子和新闻文章）。除了这些科技巨头，大部分人仍然在使用关系数据库形式的业务运营数据，使用着不是那么前沿的技术，比如 SQL。</p><p><strong>Q：是的，我猜他们还在做挖掘用户帖子、电子邮件以及广告之类的事情。</strong></p><p>A：是的，你会发现 Naive Bayes 有趣也很有用。获取文本正文并预测它所属的类别。先跳过这块，你目前的工作是处理大量表格数据，是想做一些预测或统计分析么？</p><p><strong>Q：对的，我们终于回到正题上了，就是解决实际问题，这是神经网络和深度学习的用武之地吗？</strong></p><p>A：不要着急，如果想学这些，建议从基础开始，比如正态分布、线性回归等。</p><p><strong>Q：明白，但这些我仍然可以在 Excel 中完成，有什么区别？</strong></p><p>A：你可以在 Excel中 做很多事情，但使用程序你可以获得更大的灵活性。</p><p><strong>Q：你说的编程是像 VBA 这样的么？</strong></p><p>A：看来我需要从头说了。Excel 确实有很好的统计运算符和不错的线性回归模型。但如果你需要对每个类别的项目进行单独的正态分布或回归，那么使用 Python 要容易得多，而不是创建一长串的公式，比如下面这样，这会让看公式的人无比痛苦。除此之外，Python 还有功能强大的 scikit-learn 库，可以处理更多的回归和机器学习模型。</p><p><img src="http://media.makcyun.top/FlxDCKtxu19Cp9coHtqS5CQNE-7U" alt=""></p><p><strong>Q：这需要涉及到数学建模领域是吧，我需要学习哪些数学知识？</strong></p><p>A：从线性代数开始吧，它是许多数据科学的基础。你会处理各种矩阵运算、行列式、特征向量这些概念。不得不说，线性代数很抽象，如果你想要得到线性代数的直观解释，3Blue1Brown 是最棒的。</p><p>（这和我之前写的一篇文章观点不谋而合：<a href="https://www.makcyun.top/Machine_learning01.html">最棒的高数和线代入门教程</a>）</p><p><strong>Q：就是作大量的线性代数运算？这听起来毫无意义和无聊，能举个例子么？</strong></p><p>A：好吧，机器学习中会用到大量的线性代数知识，比如：线性回归或构建自己的神经网络时，会使用随机权重值进行大量矩阵乘法和缩放。</p><p><strong>Q：好吧，矩阵与 DataFrame 有什么关系？感觉很相似。</strong></p><p>A：实际上，我需要收回刚才说的话，你可以不用线性代数。</p><p><strong>Q：真的吗？那我还要不要学习线性代数？</strong></p><p>A：就目前而言，你可能不需要学习线性代数，直接使用机器学习库就行，比如 TensorFlow 和 scikit-learn 这些库，它们会帮助你自动完成线性代数部分的工作。不过你需要对这些库的工作原理有所了解。</p><p><strong>Q：说到机器学习，线性回归真的算是机器学习吗？</strong></p><p>A：是的，线性回归是机器学习的敲门砖。</p><p><strong>Q：真棒，我一直在 Excel 中这样做，那我是不是也可以自称「机器学习从业者」？</strong></p><p>A：技术上来说是的，不过你需要扩大知识面。机器学习通常有两个任务：回归或分类。从技术上讲，分类是回归。决策树、神经网络、支持向量机、逻辑回归以及线性回归，这些算法都在做某种形式的曲线拟合，每种算法各有优缺点。</p><p><strong>Q：所以机器学习只是回归？它们都有效地拟合了曲线？</strong></p><p>A：差不多，像线性回归这样的一些模型清晰可解释，而像神经网络这样更先进的模型定义是复杂的，并且难以解释。神经网络实际上只是具有一些非线性函数的多层回归。当你只有 2-3 个变量时，它可能看起来不那么令人印象深刻，但是当你有数百或数千个变量时它就开始变得有趣了。</p><p><strong>Q：那图像识别也只是回归？</strong></p><p>A：是的，每个图像像素基本上变成具有数值的输入变量。你必须警惕维度的诅咒，变量（维度）越多，需要的数据越多，以防变得稀疏。这是机器学习如此不可靠和混乱的众多原因之一，并且需要大量你没有的标记数据。</p><p><strong>Q：机器学习能解决安排员工、交通工具、数独所有这些问题吗？</strong></p><p>A：当你遇到这些类型的问题时，有些人会说这不是数据科学或机器学习而是运筹学。</p><p><strong>Q：这对我来说似乎是实际问题。运营研究与数据科学无关？</strong></p><p>A：实际上，存在相当多的重叠。运筹学已经提供了许多机器学习使用的优化算法。它还为常见的 AI 问题提供了许多解决方案。</p><p><strong>Q：那么我们用什么算法来解决这些问题呢？</strong></p><p>A：绝对不是机器学习算法，很少有人知道这一点。几十年前就有更好的算法，树搜索、元启发式、线性规划和其他运算研究方法已经使用了很长时间，并且比机器学习算法对这些类别的问题做得更好。</p><p><strong>Q：那为什么每个人都在谈论机器学习而不是这些算法呢？</strong></p><p>A：因为很长一段时间里，这些优化算法问题已经有了令人满意的解决方案，但自那时起就一直没有成为头条新闻。几十年前就出现了这些算法的 AI 炒作周期。如今，AI 炒作重新点燃了机器学习及其解决的问题类型：图像识别、自然语言处理、图像生成等。</p><p><strong>Q：所以使用机器学习来解决调度问题，或者像数独一样简单的事情时，这样做是错误的吗？</strong></p><p>A：差不多，机器学习，深度学习这些今天被炒作的任何东西通常都不能解决离散优化问题，至少不是很好，效果非常不理想。</p><p><strong>Q：如果机器学习只是回归，为什么每个人都对机器人和人工智能，这么忧心忡忡，认为会危害我们的工作和社会？我的意思是拟合曲线真的那么危险吗？AI 在进行回归时有多少自我意识？</strong></p><p>A：人们已经找到了一些巧妙的回归应用，例如在给定的转弯上找到最佳的国际象棋移动（离散优化也可以做）或者计算自动驾驶汽车的转向方向。但是大多都是炒作，回归只能干这些事。</p><p><strong>Q：好吧，我要散个步慢慢消化下。我目前的 Excel 工作感觉也算「数据科学」，但数据科学家这个名头有点虚幻。</strong></p><p>A：也许你应该关注一下 IBM。</p><p>本文完。</p><p>参考：</p><p><a href="https://towardsdatascience.com/how-it-feels-to-learn-data-science-in-2019-6ee688498029" target="_blank" rel="noopener">∞ How it feels to learn data science in 2019</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;初学 / 转行 Python 的困惑，这里都有答案。&lt;/p&gt;
    
    </summary>
    
      <category term="Python学习" scheme="https://www.makcyun.top/categories/Python%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数据科学" scheme="https://www.makcyun.top/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>回顾当初是怎么跳出 Excel 转学 Python 的</title>
    <link href="https://www.makcyun.top/2019/02/21/python_learning01.html"/>
    <id>https://www.makcyun.top/2019/02/21/python_learning01.html</id>
    <published>2019-02-21T08:16:16.000Z</published>
    <updated>2019-02-28T07:06:45.962Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>保持兴趣，才能发现更多。</p><a id="more"></a><p><strong>摘要</strong>：回顾当初是怎么跳出 Excel 转学 Python 的 。</p><p>昨天的文章，说了一位用 Excel 做数据分析的「表哥」，在感受到人工智能所带来的行业冲击后，担心未来自己的工作会被取代，决定学一些数据科学和人工智能方面的技能，向 IT 大牛 Thomas Nield 请教之后，大牛告诉他可以学 Python、机器学习这些。</p><p>文章传送门：<a href="https://www.makcyun.top/translating01.html">∞ 2019 年怎么学习数据科学</a></p><p>「表哥」提了一些有点傻的问题，认为 Python 中的数据处理、绘制图表、线性回归这些功能，Excel 也都能做，甚至连编程 Excel 也有 VBA。总之觉得 Excel 不比 Python 差，自己称得上是「数据科学工程师」。</p><p>他提的这些问题，很典型也有意思，没有接触过编程的人会这么天真地对比。很理解他，因为看到了自己的影子，确切地说，他就是前一两年的我。</p><p>以前在学校，做软件模型实验会产生大量的数据，基本都用 Excel 处理，操作基本都是手工粘贴复制、拖拉填拽完成，效率低地可怜，后来跟着同学慢慢学会编公式，尝到了便利的甜头，原来 Excel 还能这样玩。软件用了那么多年，只会那 5% 的基础功能，剩下的基本没点过。</p><p>之后决定好好学习一下 Excel，下载了不少实战教程书看，比如：数据处理分析、函数公式、图表绘制、VBA 这些。如果你现在经常用到 Excel 的话，这里推荐 ExcelHome 论坛，他们出版的一系列书，系统详细，我看完了一整套。</p><p><img src="http://media.makcyun.top/FtzjyjPOqmr_D2cDsHM3Z4-UZtx5" alt=""></p><p>送个福利，公众号后台回复：<strong>「Excel」</strong>可以得到全部电子版 PDF。</p><p>Excel 系统学了一阵之后，思路开阔很多，以前遇到的难题，很快能知道解决方法，顺手拈来，往往还不止一种思路。对数据分析越来越有兴趣，不断尝试 Excel 的各种功能，甚至还用 Power BI 做过一次爬虫数据分析。</p><p><img src="http://media.makcyun.top/Fkf81-L4u9IyPdhyU50mgG_vV7hj" alt=""></p><p>接触网络爬虫后，发现 Excel 的爬虫功能有限，就开始搜爬虫用什么比较好，偶然就了解到了 Python，说是很热门的语言能干很多事，爬虫、数据分析比 Excel 牛逼多了。一时兴起想看看到底怎么个牛逼法，以前没有接触过编程，所以一时半会儿也没搞懂 Python 能干什么，只是觉得比 Excel 厉害。之后决定不能再处在 Excel 舒适区里，得去学 Python。</p><p>虽决定要学 Python，但真学起来的时候发现难度比想象中大，光是思维转换就很困难。一直习惯 Excel 中直观的手动操作，现在却要全部靠编写代码实现。简单的几行代码，报错不断。画一个简单图表、处理一张表格这些用 Excel 几分钟就能搞定的操作，写 Python 用了半个钟，一度怀疑要不要放弃，根本不觉得 Python 比 Excel 效率高。</p><p>强迫自己学了几个月，写了一些爬虫案例后才慢慢找到感觉，体验到学编程的好处。很明显的一点区别就是可复用性，Excel 中的大部分操作是一次性的，操作完一次，下一次只能再手动重复一遍，效率极低，而 Python 写的代码可以无限复用。如果一些操作只需要做一次，那用 Excel 没有什么问题，但重复的操作用 Python 能够一劳永逸了，节省很多时间。</p><p>最直观的一个例子，可以看看之前写的一篇文章：</p><p><a href="https://www.makcyun.top/web_scraping_withpython18.html">∞ 5 行代码入门爬虫</a></p><p>以上是个人过去的一点经验，如果你目前还在大量用 Excel，不妨多尝试写写 Python 代码，一个不错的方法是用 Excel 和 Python 分别实现同样的目标。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;保持兴趣，才能发现更多。&lt;/p&gt;
    
    </summary>
    
      <category term="Python学习" scheme="https://www.makcyun.top/categories/Python%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python 入门" scheme="https://www.makcyun.top/tags/Python-%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>求职必看 | 国内外独角兽公司排行榜</title>
    <link href="https://www.makcyun.top/2019/02/16/hotspot02.html"/>
    <id>https://www.makcyun.top/2019/02/16/hotspot02.html</id>
    <published>2019-02-16T08:16:16.000Z</published>
    <updated>2019-02-17T07:57:50.217Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>世界第 3 大独角兽公司滴滴裁员，求职寒冬有必要了解一下独角兽公司。</p><a id="more"></a><p><strong>摘要</strong>：求职别只盯着 BAT，来看看独角兽公司潜力排行榜。</p><p>年基本过完，寒冬依然没过去。这两天「<strong>滴滴亏损 109亿，裁员 2000 人</strong>」的新闻一爆出，这个冬天瞬间更冷了。</p><p>滴滴是什么公司？你可能说，不就是一家做网约车生意的公司嘛。事实上，滴滴可能比你想象地牛逼。</p><p>根据权威网站<a href="https://www.cbinsights.com/research-unicorn-companies" target="_blank" rel="noopener">「CBInsights」</a>对全世界独角兽公司的最新统计（截止2019 年 1 月），<strong>滴滴是世界第 3 大独角兽公司，估值达到 560 亿美金，近 4000 亿人民币</strong>，比一大堆上市公司都值钱。</p><p>除了第 3 的滴滴，排名第一和第二的分别是中美两国独角兽「扛把子」：今日头条和 Uber。</p><center>全球独角兽公司估值 TOP 20 排名</center><table><thead><tr><th>Rank</th><th>Company</th><th>Country</th><th>Valuation (亿美元)</th></tr></thead><tbody><tr><td>1</td><td>Toutiao (Bytedance)</td><td>China</td><td>750</td></tr><tr><td>2</td><td>Uber</td><td>United States</td><td>720</td></tr><tr><td>3</td><td>Didi Chuxing</td><td>China</td><td>560</td></tr><tr><td>4</td><td>WeWork</td><td>United States</td><td>470</td></tr><tr><td>5</td><td>Airbnb</td><td>United States</td><td>290</td></tr><tr><td>6</td><td>SpaceX</td><td>United States</td><td>220</td></tr><tr><td>7</td><td>Stripe</td><td>United States</td><td>200</td></tr><tr><td>8</td><td>JUUL Labs</td><td>United States</td><td>150</td></tr><tr><td>9</td><td>Epic Games</td><td>United States</td><td>150</td></tr><tr><td>10</td><td>Pinterest</td><td>United States</td><td>120</td></tr><tr><td>11</td><td>Bitmain Technologies</td><td>China</td><td>120</td></tr><tr><td>12</td><td>Samumed</td><td>United States</td><td>120</td></tr><tr><td>13</td><td>Lyft</td><td>United States</td><td>120</td></tr><tr><td>14</td><td>GrabTaxi</td><td>Singapore</td><td>110</td></tr><tr><td>15</td><td>Palantir Technologies</td><td>United States</td><td>110</td></tr><tr><td>16</td><td>Global Switch</td><td>United Kingdom</td><td>110</td></tr><tr><td>17</td><td>Infor</td><td>United States</td><td>100</td></tr><tr><td>18</td><td>DJI Innovations</td><td>China</td><td>100</td></tr><tr><td>19</td><td>One97 Communications (operates Paytm)</td><td>India</td><td>100</td></tr><tr><td>20</td><td>Go-Jek</td><td>Indonesia</td><td>100</td></tr></tbody></table><p><em>完整名单可以在公众号后台回复：「<strong>独角兽</strong>」</em>得到。</p><p>你可能听过独角兽公司，但并不太清楚和一般公司、上市公司有什么区别，简单介绍下：</p><blockquote><p>独角兽公司（Unicorn Companies）是指：成立不到 10 年，估值在 10 亿美元以上，且未在股票市场上市的科技创业公司。超级独角兽公司则是估值在 100 亿美元以上。</p></blockquote><p>独角兽公司在多轮融资后就会谋划上市，公司市值也会随之大涨一番。比如去年上市之前的独角兽公司：小米、美团、拼多多，上市后市值分别增长了 400、1200、1300 亿元。</p><p>回过来说，滴滴这么牛逼的公司都没能抗住这个寒冬，举起了裁员的大刀，看来<strong>这个冬天真的是既冷又长</strong>。实际上，从去年冬天开始，不少公司都被爆出了「裁员缩编」的现象，滴滴只不过是新年里的第一个重磅炸弹而已。求职环境越来越恶劣，以前找工作挑公司，现在能找到都算不错。</p><p>好在马上就要到三月了，难得的「金三银四」黄金求职季。如果你打算找工作，就努力抓紧机会，如果你没找工作计划，那恭喜你不用面临残酷的竞争，至少该认清下形势，把握好时间努力提升自己。</p><p>不少人找工作很被动。有些人只看大厂招聘，有些人则看到有公司招人，岗位匹配就投递简历。前者选择范围有限，后者网撒地又太大，要是好一阵子都找不到工作很容易陷入焦虑。</p><p><strong>其实，找工作之前很重要的一点是锁定目标公司。</strong>别只盯着 BAT，不妨关注一些有潜力的独角兽公司，这样选择面更广，目标也清晰，找工作也能更主动。</p><p>下面来了解下独角兽公司。</p><p>先看看全球情况。下面这张图是全球独角兽公司的分布，目前一共有 326 家独角兽公司，分布在 26 个国家，总估值超过 10,000 亿美元，大部分集中在中美两国。</p><p><img src="http://media.makcyun.top/Fp7vahmHPEbzdKBzdyta_3GrQxqw" alt=""></p><p>从各国具体的数据来看，美国基本占了全球独角兽公司数量和估值的 50%，其次是中国，占了 30% 左右，剩下的二十来个国家只占 20%。</p><p><img src="http://media.makcyun.top/FqC0XkdXPKz4ZPGAUSHM6Mj3gDjv" alt=""></p><p><img src="http://media.makcyun.top/Fl0buLQhFXgFfJCCaGzungbJRJ6t" alt=""></p><p>可以说，<strong>全球只有中美两国在「玩」独角兽公司。</strong></p><p>简单对比一下两国估值排名前十的独角兽公司。可以看到美国除了 Uber，比较熟悉的还有：短期房屋出租网站 Airbnb、伊隆·马斯克的 SpaceX 、图片分享网站 Pinterest 等。</p><p><img src="http://media.makcyun.top/FuMZ-WVCOr8E6lgpXkOTkChulxD-" alt=""></p><p>中国排名前十的则有今日头条、滴滴、大疆等。</p><p><img src="http://media.makcyun.top/FjRbMFdx-8cuJBuQJt3ye2gtR8C8" alt=""></p><p>总体来看，美国的独角兽公司估值比中国均匀一些。另外，上面的公司名单中没有出现「蚂蚁金服」公司，这可是个估值超过 1500 亿美金的「大家伙」，有了蚂蚁金服，中国在账面上至少是不输美国了。</p><p>相较于全球独角兽公司 ，我们可能更关心本土公司的情况。好在，胡润研究院也经常发布独角兽公司报告。</p><p>该机构 1 月 24 日发布了<a href="http://www.hurun.net/CN/Article/Details?num=EF8A8559DC86" target="_blank" rel="noopener">《2018胡润大中华区独角兽指数》</a>报告，将「蚂蚁金服」列入了独角兽企业估值榜单的第一位，估值达到 10,000 亿人民币。</p><p>这份报告中显示，<strong>大中华区一共有 186 家独角兽公司上榜，总估值超过 5 万亿人民币</strong>，体量相当大，平均每个公司估值 270 亿。</p><p>来看看估值前二十名的公司有哪些：</p><p><img src="http://media.makcyun.top/FjQBEVK8BwLEDKyBAQIeF3Moddlu" alt=""></p><p><em>完整名单可以在公众号后台回复：「<strong>独角兽</strong>」</em>得到</p><p><strong>前二十名公司中，有多少你此前是不知道的？</strong>这些公司就是不错的求职目标。</p><p>先来总体看一下 186 家公司都分布在哪些城市：</p><p><img src="http://media.makcyun.top/FkvU9fohVqhTMYEvA1UwQFK7KPWK" alt=""></p><p>独角兽公司主要集中在京津、长江经济带和珠三角三大区域。其中，北京「一枝独秀」，不得不说，北京的科创公司环境氛围是真的好。</p><p>挑选图中比较突出的五个城市作对比，四个一线城市加上杭州。北京的独角兽公司整体估值水平远高于其他四大城市，上海和广州较弱，深圳和杭州旗鼓相当，马老板的「蚂蚁金服」让杭州很突出。</p><p><img src="http://media.makcyun.top/FtpsZtLFOF_m9fzJKsmQAzpSldJ5" alt=""></p><p>放大城市范围，看看大中华区各大城市估值排名前三的独角兽公司有哪些：</p><table><thead><tr><th>City</th><th>Rank</th><th>Company</th><th>Valuation(亿元)</th><th>Industry</th></tr></thead><tbody><tr><td>北京</td><td>1</td><td>今日头条</td><td>5000</td><td>文化娱乐</td></tr><tr><td></td><td>2</td><td>滴滴出行</td><td>3000</td><td>互联网服务</td></tr><tr><td></td><td>3</td><td>京东数字科技</td><td>1000</td><td>互联网服务</td></tr><tr><td>上海</td><td>1</td><td>陆金所</td><td>2500</td><td>互联网金融</td></tr><tr><td></td><td>2</td><td>平安医保科技</td><td>500</td><td>医疗健康</td></tr><tr><td></td><td>3</td><td>金融壹账通</td><td>500</td><td>互联网金融</td></tr><tr><td>广州</td><td>1</td><td>小鹏汽车</td><td>300</td><td>汽车交通</td></tr><tr><td></td><td>2</td><td>云从科技</td><td>200</td><td>人工智能</td></tr><tr><td></td><td>3</td><td>名创优品</td><td>150</td><td>新零售</td></tr><tr><td>深圳</td><td>1</td><td>微众银行</td><td>1500</td><td>互联网金融</td></tr><tr><td></td><td>2</td><td>大疆</td><td>1000</td><td>机器人</td></tr><tr><td></td><td>3</td><td>柔宇科技</td><td>300</td><td>智能硬件</td></tr><tr><td>杭州</td><td>1</td><td>蚂蚁金服</td><td>10000</td><td>互联网金融</td></tr><tr><td></td><td>2</td><td>菜鸟网络</td><td>1000</td><td>物流服务</td></tr><tr><td></td><td>3</td><td>微医</td><td>400</td><td>医疗健康</td></tr><tr><td>南京</td><td>1</td><td>苏宁金服</td><td>500</td><td>互联网金融</td></tr><tr><td></td><td>2</td><td>满帮</td><td>400</td><td>物流服务</td></tr><tr><td></td><td>3</td><td>汇通达</td><td>200</td><td>电子商务</td></tr><tr><td>天津</td><td>1</td><td>神州优车</td><td>400</td><td>互联网服务</td></tr><tr><td></td><td>2</td><td>纳恩博</td><td>100</td><td>智能硬件</td></tr><tr><td>苏州</td><td>1</td><td>信达生物</td><td>100</td><td>医疗健康</td></tr><tr><td></td><td>2</td><td>基石药业</td><td>70</td><td>医疗健康</td></tr><tr><td>香港</td><td>1</td><td>客路旅行</td><td>70</td><td>互联网服务</td></tr><tr><td></td><td>2</td><td>亚洲医疗</td><td>70</td><td>医疗健康</td></tr><tr><td>成都</td><td>1</td><td>1919酒类直供</td><td>70</td><td>新零售</td></tr><tr><td>无锡</td><td>1</td><td>华云数据</td><td>70</td><td>大数据与云计算</td></tr><tr><td>武汉</td><td>1</td><td>斗鱼</td><td>200</td><td>文化娱乐</td></tr><tr><td>绍兴</td><td>1</td><td>电咖汽车</td><td>100</td><td>汽车交通</td></tr><tr><td>重庆</td><td>1</td><td>猪八戒网</td><td>100</td><td>互联网服务</td></tr><tr><td>金华</td><td>1</td><td>零跑汽车</td><td>70</td><td>汽车交通</td></tr><tr><td>长沙</td><td>1</td><td>芒果TV</td><td>100</td><td>文化娱乐</td></tr><tr><td>青岛</td><td>1</td><td>日日顺</td><td>100</td><td>物流服务</td></tr><tr><td>台北</td><td>1</td><td>辉能科技</td><td>70</td><td>新能源</td></tr></tbody></table><p>只有 19 座城市拥有独角兽公司，城市基本是一二线，且一多半的城市只上榜了一家公司，比如成都、重庆。看来，想去独角兽公司工作，得身处一二线城市。</p><p>接着对比一下各个行业的独角兽公司情况：</p><p><img src="http://media.makcyun.top/FjFWSfX9CqEIS3gFtdCtnVsEMms5" alt=""></p><p><strong>互联网金融行业拥有绝对的优势，占了全部公司估值 5 万亿的三分之一。</strong>其次是互联网服务行业，看来互联网行业的工作机会要大很多。</p><p>具体地看看各行业估值前三名的公司是哪些：</p><table><thead><tr><th>Industry</th><th>Rank</th><th>Company</th><th>Valuation(亿元)</th></tr></thead><tbody><tr><td>互联网服务</td><td>1</td><td>滴滴出行</td><td>3000</td></tr><tr><td></td><td>2</td><td>京东数字科技</td><td>1000</td></tr><tr><td></td><td>3</td><td>车好多</td><td>500</td></tr><tr><td>互联网金融</td><td>1</td><td>蚂蚁金服</td><td>10000</td></tr><tr><td></td><td>2</td><td>陆金所</td><td>2500</td></tr><tr><td></td><td>3</td><td>微众银行</td><td>1500</td></tr><tr><td>人工智能</td><td>1</td><td>商汤科技</td><td>400</td></tr><tr><td></td><td>2</td><td>Face++</td><td>200</td></tr><tr><td></td><td>3</td><td>地平线机器人</td><td>200</td></tr><tr><td>区块链</td><td>1</td><td>比特大陆</td><td>500</td></tr><tr><td></td><td>2</td><td>嘉楠耘智</td><td>150</td></tr><tr><td></td><td>3</td><td>亿邦国际</td><td>70</td></tr><tr><td>医疗健康</td><td>1</td><td>平安医保科技</td><td>500</td></tr><tr><td></td><td>2</td><td>微医</td><td>400</td></tr><tr><td></td><td>3</td><td>联影医疗</td><td>300</td></tr><tr><td>大数据与云计算</td><td>1</td><td>金山云</td><td>100</td></tr><tr><td></td><td>2</td><td>盘石股份</td><td>100</td></tr><tr><td></td><td>3</td><td>华云数据</td><td>70</td></tr><tr><td>房产服务</td><td>1</td><td>链家</td><td>400</td></tr><tr><td></td><td>2</td><td>小猪短租</td><td>100</td></tr><tr><td></td><td>3</td><td>V领地</td><td>70</td></tr><tr><td>教育</td><td>1</td><td>VIPKID</td><td>200</td></tr><tr><td></td><td>2</td><td>猿辅导</td><td>200</td></tr><tr><td></td><td>3</td><td>作业帮</td><td>200</td></tr><tr><td>文化娱乐</td><td>1</td><td>今日头条</td><td>5000</td></tr><tr><td></td><td>2</td><td>快手</td><td>1000</td></tr><tr><td></td><td>3</td><td>博纳影业</td><td>200</td></tr><tr><td>新能源</td><td>1</td><td>辉能科技</td><td>70</td></tr><tr><td>新零售</td><td>1</td><td>名创优品</td><td>150</td></tr><tr><td></td><td>2</td><td>瑞幸咖啡</td><td>150</td></tr><tr><td></td><td>3</td><td>1919酒类直供</td><td>70</td></tr><tr><td>智能硬件</td><td>1</td><td>柔宇科技</td><td>300</td></tr><tr><td></td><td>2</td><td>寒武纪科技</td><td>150</td></tr><tr><td></td><td>3</td><td>纳恩博</td><td>100</td></tr><tr><td>机器人</td><td>1</td><td>大疆</td><td>1000</td></tr><tr><td></td><td>2</td><td>优必选</td><td>300</td></tr><tr><td>汽车交通</td><td>1</td><td>威马汽车</td><td>300</td></tr><tr><td></td><td>2</td><td>小鹏汽车</td><td>300</td></tr><tr><td></td><td>3</td><td>奇点汽车</td><td>200</td></tr><tr><td>游戏</td><td>1</td><td>英雄互娱</td><td>100</td></tr><tr><td>物流服务</td><td>1</td><td>菜鸟网络</td><td>1000</td></tr><tr><td></td><td>2</td><td>京东物流</td><td>800</td></tr><tr><td></td><td>3</td><td>满帮</td><td>400</td></tr><tr><td>电子商务</td><td>1</td><td>美菜网</td><td>500</td></tr><tr><td></td><td>2</td><td>每日优鲜</td><td>200</td></tr><tr><td></td><td>3</td><td>小红书</td><td>200</td></tr></tbody></table><p><strong>这里面又有多少公司，你此前是不知道的？</strong> 你不知道意味着很多人也不知道，现在你知道了，就掌握了一定的主动权。</p><p>最后，感兴趣的话可以去了解下，为了更方便你，我已下载好全球 326 家和国内 186 家的独角兽公司数据，在公众号后台回复：「<strong>独角兽</strong>」就可以得到。</p><p>2019 年，撸起袖子加油干。</p><p>本文完。</p><p>参考：</p><p><a href="https://www.cbinsights.com/research-unicorn-companies" target="_blank" rel="noopener">∞ January 2019: research-unicorn-companies</a></p><p><a href="http://www.hurun.net/CN/Article/Details?num=EF8A8559DC86" target="_blank" rel="noopener">∞ 2018胡润大中华区独角兽指数</a></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;世界第 3 大独角兽公司滴滴裁员，求职寒冬有必要了解一下独角兽公司。&lt;/p&gt;
    
    </summary>
    
      <category term="热点分享" scheme="https://www.makcyun.top/categories/%E7%83%AD%E7%82%B9%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="热门事件" scheme="https://www.makcyun.top/tags/%E7%83%AD%E9%97%A8%E4%BA%8B%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>2019 年的第一场雪</title>
    <link href="https://www.makcyun.top/2019/02/14/life08.html"/>
    <id>https://www.makcyun.top/2019/02/14/life08.html</id>
    <published>2019-02-14T08:16:24.000Z</published>
    <updated>2019-02-14T15:05:39.240Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>好些年没看到雪了。</p><a id="more"></a><p><strong>摘要：</strong>流水账一篇。</p><p>自从去年 8 月来到北京这边，就盼望着冬天能下一场雪。从新疆到广东后的的几年里，冬天一直是当夏天过的，一个新疆人快记不清冬天是什么感觉。</p><p>然而你越盼它越不来，十一二月没下，一月也没下，年都快过完了还是没有下。不免遗憾，这个冬天是看雪概率最大的一次，错过不知下次是什么时候，在东莞的那几个冬天，唯一遇到的一次小雪时隔了一百多年。</p><p>多年之前，在新疆念中学，每天骑车去上学。冬天冷下雪更冷，天亮地比内地迟两小时，早上学都是摸黑出门，路灯光线暗。下了一整晚的雪，被来往的车压过之后会变成冰，走在上面稍不留神就会摔跤。经常看到骑车的人骑着骑着就摔到马路中间去了，后面的出租车赶紧急刹或者转方向盘，很险。所以下雪的时候，不再骑车，改为推自行车跑步去四公里之外的学校，持续整个冬天。<strong>那时冬天就是雪，雪就是冬天。</strong></p><p>本来不再抱期望，没想到前天中午突然飘起了小雪，像柳絮满天飞，短短一会儿就停了下来，落到地上还没积起来就化地一干二净，虽不算完整意义上的「下雪」，但好歹是下了。看了看天气预报，说最近几天都会下，很是期待。结果昨天晴了一整天，回头再查天气预报，却显示未来几天都不会再下雪。<strong>不知从什么时候开始，下雪变成了一件奢侈的事情。</strong></p><p>中午在电脑前码字，馍馍大声喊「下雪了」，头歪向窗外，果然。家里人看了看天说估计会下大，不太信，事实证明又想错了。随后，雪越下越大，没有要停下来的意思，到最后如鹅毛大雪一般，小区楼下开始渐渐泛白。雪落在了地上、树枝上、凉亭上，整个小区都变成了白色。</p><p>记不起上一次遇到这么大的雪是哪一年了，可能是 2002 年的第一场雪。</p><p>馍馍兴奋地不行，他在南方出生，这辈子是第一次见到大雪。兴奋地不行，一会儿冲到阳台边，一会儿喊大人去看，生怕他们没看到。透过窗户，小区里的大人和小孩儿纷纷走了出来。问馍馍想不想下去玩，他马上说好，给他「全副武装」好后一起下了楼，一走出来，密密麻麻的雪从天而落，速度如雨滴那般快，比在楼上隔着玻璃看地清楚。馍馍一会儿望着天上问：「雪从哪里来，它们要去哪里」，一会儿看着地下，不停用脚踩来踩去。</p><p><img src="http://media.makcyun.top/Fr9USYwzxXZkdU0MMV-ksK0i6iJJ" alt=""></p><p>在小区逛了一圈，到处都是人，比过年还热闹，有堆雪人的、打雪仗的还有视频直播的。看到那些小朋友带着铲子、手套、雨伞，忽然觉得我们很惨，什么都没带，雪落了一身，反而觉得暖和。给馍馍捏了几个雪球，手就快要冻僵，如《雪国列车》那般冷。他手拿了一会儿后，也冻得不敢再拿，说：「太凉了」。看来忘记了教他「冻」这个词。顺手就把雪球轻轻砸向他的肚子，裂成几瓣掉在了地上，他开心地不行，结果乐极生悲，脚一滑，四脚朝天结结实实地摔了一跤，吓得哇哇大哭。见状，赶紧掏出手机给他拍了几张，他哭得更厉害了。</p><p><img src="http://media.makcyun.top/Fr42DxHf2G91TEJ95alvMJvE1_02" alt=""></p><p>扶他起来哄了两句，他看到别的小朋友在堆雪人，说也想玩，没办法只好尝试着用手把周围的雪堆成一圈，结果几十秒钟不到，手冻得没知觉，最后雪人没堆起来，只滚出一个大雪球，不舍得扔带回了家，给馍馍说是他的「晚餐」，他听罢张嘴就准备咬。</p><p>把馍馍放在家之后，去了附近的一条河边。夏天来过一次，那会儿水面波光粼粼，两岸郁郁葱葱，岸边还有人钓鱼。到了之后，眼前只有茫茫一片白，分不清河跟岸。捡起一块石头用力向远处扔去，听到很闷的回响，确定冰面很厚可以站人。朝河中间走去，一个脚印也没有，伴着大雪纷飞，耳边仿佛响起了《难念的经》，朝远处望去又好似看到《情书》的场景。</p><p><img src="http://media.makcyun.top/Fq4CyJcXGn8yC3NAd5RoFY2vg-vy" alt=""></p><p><img src="http://media.makcyun.top/FgUgPbG3j7Xp8zS1wKQgDKLEN31s" alt=""></p><p>你多久没有近距离地接触过雪了？</p><p>七夕快乐。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;好些年没看到雪了。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>《流浪地球》导演的微博粉丝不到 10 万，却能一打七</title>
    <link href="https://www.makcyun.top/2019/02/11/life07.html"/>
    <id>https://www.makcyun.top/2019/02/11/life07.html</id>
    <published>2019-02-11T08:16:24.000Z</published>
    <updated>2019-02-14T15:08:36.424Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>新年开工第一天，借《流浪地球》打鸡血。</p><a id="more"></a><p><strong>摘要：</strong>新年开工第一天，借《流浪地球》打鸡血。</p><p>一周的春节年假转眼间就过去了，很多朋友今天开始上班了，终于开工了，太好了。</p><p>我在家里祝你们工作顺利，哈哈。</p><p>过了个年，你长了几斤膘？见了多少亲朋好友？耍了多少地方？</p><p>我呢，春节没胖，没见亲朋好友，哪里也没去，除了电影院。不过，这个春节过得挺充实。</p><p>先是年三十的春晚，不得不说，今年的节目水准比前几年好多了，往年春晚没有完整看完过，一个节目也没记住，而今年的语言类节目有不少亮点。到今天年初七，还能记得起几个有意思的节目，比如：佟掌柜闫妮的《办公室故事》、第一次上春晚的葛优带来的《儿子来了》、开心麻花团队的《占位子》、老戏骨林永健的《演戏给你看》、贾玲的《啼笑皆非》以及阔别多年重回春晚舞台的刘谦表演的魔术《魔壶》。</p><p>一台春晚能有这么几个不错的节目，难得。看来一个好导演还是很重要啊，今年的春晚导演叫「刘真」，第一次导演春晚，一位啥都敢说的「真」导演。</p><p><img src="http://media.makcyun.top/FuRI4FDQEqV1xJgJHvV8Vf6vQgYi" alt=""></p><p><strong>就冲他这态度，我希望明年的春晚导演还是他。</strong></p><p>导演虽不像主持人、演员那么为观众所熟知，但他们发挥的作用绝对是最大的，除了上面说的春晚导演刘真，还包括下面要说的电影导演。</p><p>看完春晚也就过了除夕，正式迎来 2019 年，今年的大年初一如果用一个词来形容的话，就是「屌炸天」。为啥？因为这一天有八部电影上映，每一部电影搁到往年都算是大片，堪称「史上最牛逼春节档」，这一天的票房几乎等于 2004 年全年票房。</p><p><img src="http://media.makcyun.top/Fmt8vuZq8qFCX213AJCZPgKLa-KY" alt=""></p><p>2018 年，我没有去过电影院，也基本没有在线上看过电影。春节看到了这些电影，感觉就是「饥不择食」，年初一中午吃了碗泡面，速抢了张电影票就奔去电影院了，平常看电影是挑座位，那一天只能挑还有哪个电影院有票。</p><p>你猜我买的是上面哪部电影？</p><p>你可能猜不到，我买的排名第四的《流浪地球》。今天，你估计会觉得我很会挑，其实不是，是因为前三部火爆地一票难求，所以就买了这部电影，说实话，春节之前根本不知道这部电影。</p><p>两个小时后，看完出了电影院，回答了知乎上的一个问题：</p><p><img src="http://media.makcyun.top/Frgq5_7BIoPzaErvzlH2EYnAYnqk" alt=""></p><p>一个礼拜后的今天来看，这个预言应该不会错，现在票房已经 20 多亿了，保二望一指日可待（第一是 56 亿的《战狼2》）。</p><p>看了《流浪地球》，很容易想起 2015 年看过的《大圣归来》，都是忍不住想再去看一遍，所以没两天就又去了家有 IMAX 版本的电影院。</p><p>去完电影院，开始在知乎、微博、豆瓣、B站上不停地刷这部电影的信息，感觉一年的娱乐时间都提前透支完了。</p><p>了解了电影中很多细节、看了各种好的坏的评论、也看了全部宣传片和MV。</p><p>关于这部电影的评价讨论，经过这些天网络上的各种刷屏，你应该也了解不少了，什么耗时 4 年、拍摄遭遇撤资、空手套《战狼》等等。还不太了解的话，推荐看下面这一篇文章：</p><p><a href="https://mp.weixin.qq.com/s/dn1PPhJ_QogKgWP-ib88vA" target="_blank" rel="noopener">《流浪地球》诞生记：拍了这个电影，我能吹一辈子牛逼</a></p><p>众多评价中，有一个不可忽略的现象，是网上对这部电影的口碑评价呈两极化趋势，尤其是在豆瓣上，出现了大量的一星评论，评分从刚开始的 8.4 分也降到了现在的 8 分以下。</p><p><img src="http://media.makcyun.top/FqH4m4O3d_QjXe5SyyN6qMlC1bKp" alt=""></p><p>那些给这部电影打一星两星的人，说的头头是道，那些看不过去的人看了很不爽，双方就开始互怼，电影里的战场估计都没有这么猛。</p><p><strong>要我给这部电影打分的话，我只想说：「满分是多少就打多少」。</strong>因为，想看到用心的导演拍出更多用心的电影，这些年受够了洋垃圾、流量明星、强行植入中国元素的电影。</p><p>说一点网上没有人说过的，也是偶然发现的，就是电影导演郭帆的微博粉丝数，<strong>只有区区几万！</strong>而且是在这部电影已经火了好几天的情况下，在这之前恐怕只有两三万。</p><p><img src="http://media.makcyun.top/Fsf19CrtG9059wZvH8btTN6OsA7A" alt=""></p><p>虽然不玩微博，但觉得很多草根网红的粉丝数恐怕都不只这么点，更别说那些大碗了，比如下面这几位，<strong>一条微博，留言点赞几十万，转发几百万。</strong></p><p><img src="http://media.makcyun.top/FpxAVFcZRL7kNa8bchdHmQtSHdWD" alt=""></p><p><strong>然而，就是这位只有几万粉丝的导演，拍出了几十亿的电影，票房从第四变成「一打七」，还是吊打。</strong><br><img src="http://media.makcyun.top/Fv4KOIXy7FaqvUbK4d8SjrEfsO15" alt=""></p><p>起先觉得是郭帆导演太低调了，才这么点粉丝，就又随便搜了另外一位导演文牧野的粉丝数，「文牧野」这个名字你可能不熟悉，但说一部他导演的电影你就知道了，就是那部 <strong>票房 31 亿，豆瓣 100 万人评分 9.0</strong> 的《我不是药神》。他的粉丝数是多少呢，23 万。</p><p><img src="http://media.makcyun.top/FocmVmTa4gUHWrr_WO3XDdS4Uz1r" alt=""></p><p>这两组数字对比下来，心痛伴随着开心，心痛就不说了，开心的是：<strong>中国从不缺认认真真做电影的人！</strong></p><p>最后，套用《流浪地球》电影中的一句台词作为结尾：</p><p><strong>无论中国的电影最终走向何方，我选择希望！</strong></p><p>今天新年开工第一天，无论 2019 年会怎么样，愿你今年充满希望。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;新年开工第一天，借《流浪地球》打鸡血。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>推荐几个看最新电影，下无损音乐的网站</title>
    <link href="https://www.makcyun.top/2019/02/02/weekly_sharing15.html"/>
    <id>https://www.makcyun.top/2019/02/02/weekly_sharing15.html</id>
    <published>2019-02-02T08:16:16.000Z</published>
    <updated>2019-02-02T10:53:53.671Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>看最新电影，下无损音乐。</p><a id="more"></a><p><strong>摘要</strong>：看最新电影，下无损音乐，这 5 个网站就够了 。</p><p>这是「<strong>每周分享</strong>」的第 14 期，也是农历 2018 年的最后一期。最近有不少新朋友关注，这里就再介绍一下这个栏目，每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号界面中的 「不务正业」菜单中找到，Python 类的文章在另一个「不误正业」菜单中。</p><p>过年的气息越来越浓，后天就是除夕了，如果你是学生党应该早已开启年假模式，如果你是上班族也差不多到家或者在回家的路上了。一年到头，终于可以好好放松几天，和家人朋友在一起吃吃喝喝，小娱小乐，想想都惬意。</p><p>说起娱乐，很多人会选择看电影听音乐，有空就去电影院，今年春节有很多不错的电影，没空的话，在家看看电影或者听听歌也不错。</p><p>这一期的分享，就来推荐 5 个我珍藏许久的看电影和听音乐的网站，给你的春节添一点乐趣。</p><p>首先是 3 个电影网站。</p><p>能看电影的网站有很多，个人觉得一个网站好不好，主要看新片更新的速度和质量，毕竟一般在网上看电影就是为了看新片。下面推荐的 3 个网站最大特点就是新片更新速度快，甚至快过爱奇艺、优酷。</p><p>这里，选了三部最新的大片作测试。</p><p><img src="http://media.makcyun.top/Fh18UwDzEEUUJeMQaT9LvRR5H3bp" alt=""></p><h3 id="网站-1：新-6V-电影（www-66s-cc-）"><a href="#网站-1：新-6V-电影（www-66s-cc-）" class="headerlink" title="网站 1：新 6V 电影（www.66s.cc/）"></a>网站 1：新 6V 电影（<a href="https://www.66s.cc/" target="_blank" rel="noopener">www.66s.cc/</a>）</h3><p>推荐指数：★★★★★</p><p>这个网站强烈推荐，片源多，新片更新速度快。</p><p><img src="http://media.makcyun.top/Fr5fQxQSJKycjgZ-IcueybeWqkcz" alt=""></p><p>上面三部电影在这个网站上都能搜到，提供了很多资源下载链接，还可以在线观看，良心。</p><p><img src="http://media.makcyun.top/FvGwRsl0Z4KrEWqZ3vxlsFwvs9gZ" alt=""></p><p>网页端：</p><p><img src="http://media.makcyun.top/Fg73i1FQgETARvegChaD6ep87pa9" alt=""></p><p>手机端：</p><p><img src="http://media.makcyun.top/Fg-nf86L9QRVK-ORfP6tgTmYUCFq" alt=""></p><h3 id="网站-2：两个-BT-（www-bttwo-com-）"><a href="#网站-2：两个-BT-（www-bttwo-com-）" class="headerlink" title="网站 2：两个 BT （www.bttwo.com/ ）"></a>网站 2：两个 BT （<a href="www.bttwo.com/">www.bttwo.com/ </a>）</h3><p>推荐指数：★★★★</p><p><img src="http://media.makcyun.top/Fv3fELueaK7p_FXMPZQfFTCJhbeQ" alt=""></p><p>这个网站片源丰富，上面三部大片，搜索到了两部，可以在线播放也可以下载。</p><p><img src="http://media.makcyun.top/Fjvq_eriUStPlOh63AIQ4EfMYhxE" alt=""></p><h3 id="网站-3：疯狂影视搜索-（ifkdy-com-）"><a href="#网站-3：疯狂影视搜索-（ifkdy-com-）" class="headerlink" title="网站 3：疯狂影视搜索 （ifkdy.com/）"></a>网站 3：疯狂影视搜索 （<a href="http://ifkdy.com/" target="_blank" rel="noopener">ifkdy.com/</a>）</h3><p>推荐指数：★★★</p><p>这个网站能直接搜索电影，有片源就可以在线看。不过，上面的三部大片这里只搜到了一部。</p><p><img src="http://media.makcyun.top/FgQFXBmvFbI6amiFEIr90qFt4ClP" alt=""></p><p>说完网站，下面推荐两个听音乐网站。</p><h3 id="网站-1：-音乐搜索神器-（lai-yuweining-cn-music）"><a href="#网站-1：-音乐搜索神器-（lai-yuweining-cn-music）" class="headerlink" title="网站 1： 音乐搜索神器 （lai.yuweining.cn/music）"></a>网站 1： 音乐搜索神器 （<a href="https://lai.yuweining.cn/music" target="_blank" rel="noopener">lai.yuweining.cn/music</a>）</h3><p>推荐指数：★★★★</p><p><img src="http://media.makcyun.top/Fpy3R58cWjefJ0l5oWPTkH-n633T" alt=""></p><p>这个网站可以下载各大付费平台的音乐，以周董的《告白气球》为例，可以在线听也可以下载。</p><p><img src="http://media.makcyun.top/Fj0uZt27xQR7tUFv4sdFeGZZ8qB8" alt=""></p><h3 id="网站-2-：51ape-（http-www-51ape-com-）"><a href="#网站-2-：51ape-（http-www-51ape-com-）" class="headerlink" title="网站 2 ：51ape （http://www.51ape.com/）"></a>网站 2 ：51ape （<a href="http://www.51ape.com/" target="_blank" rel="noopener">http://www.51ape.com/</a>）</h3><p>推荐指数：★★★★★</p><p>这个网站可以下载无损音质的音乐，如果你对音质比较挑剔，这个网站能够满足你，一首歌几十 M 的体积，注意手机内存就是了。</p><p><img src="http://media.makcyun.top/Fr-kNX4gV_lp8NjE0-6_4-Nj3EOR" alt=""></p><p>一个小福利，下载了部分张国荣和周杰伦的歌，你要刚好也喜欢的话，可以后台回复：「<strong>音乐</strong>」 得到。</p><p><img src="http://media.makcyun.top/FqCC2ZEpXHt64MFTXsH5RUca8HG_" alt=""></p><p>最后，这是农历 2018 年的最后一更，好好过个年，就不骚扰你了。</p><p>不过，想到过年期间你玩手机的频率可能会更高，考虑到你可能会有无聊的时候，所以打算推出一个新栏目：<strong>「过年七天乐之年度最佳文章回顾」</strong>，每天转载其他公众号的一篇年度最佳文章，年初一开始，每天晚上 19：00 更新。</p><p>你可能问为什么是这个时间，因为「苏克1900」 啊。</p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://mp.weixin.qq.com/s/Zd1l-HM-wA1Wu3k3flB15g" target="_blank" rel="noopener">Python 告诉你绝不知道的1983-2018 春晚</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;看最新电影，下无损音乐。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="电影音乐网站" scheme="https://www.makcyun.top/tags/%E7%94%B5%E5%BD%B1%E9%9F%B3%E4%B9%90%E7%BD%91%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>机器学习必备 | 最棒的高数和线代入门教程</title>
    <link href="https://www.makcyun.top/2019/01/31/Machine_learning01.html"/>
    <id>https://www.makcyun.top/2019/01/31/Machine_learning01.html</id>
    <published>2019-01-31T08:16:16.000Z</published>
    <updated>2019-02-27T09:04:57.332Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>这门高数和线代教程，绝对刷新你三观。</p><a id="more"></a><p><strong>摘要</strong>：最牛逼的高数和线代教程分享给你 。</p><p>前阵，推荐了一个很棒的 Python 入门视频教程，很多朋友喜欢，觉得是良心推荐，传送门：</p><p><a href="https://www.makcyun.top/web_scraping_withpython19.html">∞ 入门 Python 最好的视频</a></p><p>说实话有点意外，因为这门课一点都不小众，在我推荐之前已经非常火了，我当初也是看了别人的推荐才知道的。尽管如此，从大家的反应来看还是有许多人第一次知道，很开心帮助到了这些朋友，这也让我今后更有动力去分享。</p><p>很多时候，我们不愿意主动去分享的一个重要原因，是自己先否定了自己，觉得「这个大家都知道，没什么稀罕的」，这样想就偏了，张哥说过一句话我觉得有道理：<strong>「在中国，你以为很普通的常识，也至少有一亿人不知道。」</strong>所以，主动分享利他利己，说来就来，今天继续分享干货。</p><p>最近在学习机器学习，涉及到不少高数和线代知识，大一学了这两门课，没怎么学懂，考试只是低空飘过，一直不怎么喜欢这两门课，开心直到现在都没再接触过。近期不得不重新捡回来，很是头大，毕业当废纸论斤卖掉的课本，在网上又高价买了回来。</p><p><img src="http://media.makcyun.top/201901221601_879.jpg" alt=""></p><p>快速过了一遍当初的教材，感觉依然是「知道怎么回事，但不理解为什么」，这一方面归结于自己领悟能力差，另一方面就要吐槽国内的教育理念了，教材不人性，老师教法也有问题。记得当时教线代的是一位刚博士毕业不久的老师，喜欢自 High ，经常在黑板上手推公式，推到最后自己都不会了，站在那儿傻乐，台下早已睡到一片。</p><p>曾一直认为学高数、线代，上面这两本教材是最好的选择，直到后来看了别人推荐的国外课本，才发现原来最好的教材在国外，后悔不已，为什么当初学的时候没有用到国外的教材，这之中的原因就不细说了。<br>下面就推荐我认为入门学习高数、线代最棒的教材和视频。</p><center>教材</center><p><img src="http://media.makcyun.top/FqD9O1FvM0pcZvDdD2g5OSpVZOJD" alt=""></p><p>没有打任何广告，单纯就是觉得这两本书不错，甩国内教材不知道几条街，文末会提供电子书，可以看看。</p><p>下面，是重头戏。推荐一个学习高数、线代最棒的入门视频，如果你有被它们支配过的恐惧，那绝对该看看，因为它会颠覆你的三观。</p><p><center>视频</center><br><img src="http://media.makcyun.top/Fp1ZymREdrlgU8Ive-Y0Yt-_9NGb" alt=""></p><p>这门视频教程是一位斯坦福大学的数学系学生 Gran 制作的，发布在油管的「3BlueBrown」频道上，也搬运到了国内的 B 站上。这门系列课最大的特点就是通过 Python 制作的各种酷炫动画，帮助你理解深奥的数学概念。如果说，国内老师是教你怎么算，Gran 则是教你为什么这么算。</p><p>比如，x 的立方求导，你知道为什么等于 3x 的平方么？</p><p><img src="http://media.makcyun.top/%E5%AF%BC%E6%95%B0.gif" alt=""></p><p>换个角度再来理解下：</p><p><img src="http://media.makcyun.top/%E5%AF%BC%E6%95%B02.gif" alt=""></p><p>Sin(x) 求导为什么等于 Cos(x)？</p><p><img src="http://media.makcyun.top/sinx.gif" alt=""></p><p>除此之外，高数部分还有很多其他内容，微分、积分、泰勒级数这些。</p><p>再来看看线性代数，最头疼的莫过于矩阵、行列式相关的运算和各种性质了，但 Gran 很好地帮你理解这些问题。</p><p>比如，矩阵的运算，最正确地理解是把它当成一种特定的空间变换。</p><p><img src="http://media.makcyun.top/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%952.gif" alt=""></p><p>对于行列式，一直觉得就是一堆数字倒来倒去地计算，其实更好的理解也是放到空间中去。</p><p><img src="http://media.makcyun.top/%E8%A1%8C%E5%88%97%E5%BC%8F.gif" alt=""></p><p><img src="http://media.makcyun.top/%E8%A1%8C%E5%88%97%E5%BC%8F2.gif" alt=""></p><p>怎么样，有没有觉得你大学上的是假的高数和线代？</p><p>以上只是这门系列课的冰山一角，还有很多有意思的内容，可以到 B 站上看看，传送门：</p><p><a href="https://space.bilibili.com/88461692/video" target="_blank" rel="noopener">https://space.bilibili.com/88461692/video</a></p><p>系列课很短，两三个小时就能看完，时间短不代表内容少，实际上内容非常丰富，你可能需要经常暂停下来思考一下，慢慢就会颠覆你此前对高数线代的认知。光看视频可能不够，我在 GitHub 上找到了热心网友做过的笔记，感兴趣的话可以 Star 一下：</p><p><a href="https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/C-%E6%95%B0%E5%AD%A6/B-%E5%BE%AE%E7%A7%AF%E5%88%86%E7%9A%84%E6%9C%AC%E8%B4%A8.md" target="_blank" rel="noopener">3Blue1Brown 教程笔记</a></p><p><img src="http://media.makcyun.top/FpnDUQzbrMW3EjSK95uLLcfrw16R" alt=""></p><p>另外，如果你对视频中酷炫动画感兴趣，想学学看，作者提供了源码，可以试试看，仓库地址：</p><p><a href="https://github.com/3b1b/manim" target="_blank" rel="noopener">https://github.com/3b1b/manim</a></p><p>动画制作教程：</p><p><a href="https://www.bilibili.com/read/mobile/17444" target="_blank" rel="noopener">https://www.bilibili.com/read/mobile/17444</a></p><p>老规矩，为了更方便你，我下载了高数和线代两门课的视频教程以及文章两本教材的电子书，如需，可以在公众号后台回复：<strong>3blue</strong> 得到。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这门高数和线代教程，绝对刷新你三观。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://www.makcyun.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习入门" scheme="https://www.makcyun.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>Python 告诉你所不知道的春晚 1983 -2018</title>
    <link href="https://www.makcyun.top/2019/01/29/web_scraping_withpython20.html"/>
    <id>https://www.makcyun.top/2019/01/29/web_scraping_withpython20.html</id>
    <published>2019-01-29T08:16:16.000Z</published>
    <updated>2019-01-29T11:09:52.712Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>万万没想到。</p><a id="more"></a><p><img src="http://media.makcyun.top/Fo6TVxjP7i8Gf3-l3C3nSnLtSDiq" alt=""></p><p><strong>摘要：用 Python 分析过往 36 年春晚节目数据，发现一些趣事。</strong></p><p>马上就要过年了，距离 2019 己亥猪年的除夕已不足一个礼拜，提起除夕，多数人马上想到「春节联欢晚会」这道丰盛的「年夜大餐」。看过那么多春晚，哪一年、哪些节目、哪些人你还留有深刻印象呢。</p><p>记忆中，只完整地看过 2005 年到 2015 年十年春晚。<strong>05 年之前，还很小，看不懂；15 年之后，长大了，也看不懂了</strong>。</p><p>如今，距离第一届春晚 1983 年，整整过去了 36 年，3 轮的「十二生肖」年。趁今年春晚还没到，来回顾一下过往 36 届春晚的一些有趣数据。</p><h2 id="分析内容"><a href="#分析内容" class="headerlink" title="分析内容"></a>分析内容</h2><p>接下来，通过 Python 数据分析，会回答下面这些问题，在知道答案之前，你可以先猜猜看：</p><ul><li><p>谁导演春晚次数最多？</p></li><li><p>谁主持春晚次数最多？</p></li><li><p>哪两年的除夕刚好是同一天？</p></li><li><p>谁上春晚次数最多，堪称「钉子户」？</p></li><li><p>港台明星上春晚次数对比</p></li><li><p>歌曲、小品、相声类节目数量对比</p></li></ul><h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><p>网上搜了挺久都没有找到齐全的春晚节目数据，连春晚官网也没有，结果在维基百科上找到了。</p><p><img src="http://media.makcyun.top/FtLhRw3W7ga1lpODhSuLviREo8Vy" alt=""></p><p>右侧信息表有导演、主持人、除夕当天日期这几项数据。</p><p><img src="http://media.makcyun.top/FuuQa51ANUOz0GhSIiubMslqZw9E" alt=""></p><p>节目单表是每一年春晚上表演的节目，包括：节目类型、节目名、演员名这几项数据。</p><p>Python 抓取这类表格数据，方法简单，几行代码就能搞定，修改 URL 的 page 参数，可以循环遍历抓取 1983 到 2018 年所有的节目数据。</p><p>数据抓取代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(year)</span>:</span></span><br><span class="line">    keywords = quote(<span class="string">'年中国中央电视台春节联欢晚会'</span>)</span><br><span class="line">    url = <span class="string">'https://zh.wikipedia.org/wiki/&#123;&#125;&#123;&#125;'</span>.format(year,keywords)</span><br><span class="line">    <span class="comment"># 1 节目单； 0 节目信息</span></span><br><span class="line">    <span class="keyword">if</span> year != <span class="number">2014</span>:</span><br><span class="line">        response = pd.read_html(url)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        response = pd.read_html(url)[<span class="number">3</span>]</span><br><span class="line">    response[<span class="string">'year'</span>] = year</span><br><span class="line">    response.drop([<span class="number">0</span>],inplace=<span class="keyword">True</span>) <span class="comment">#删除首行</span></span><br><span class="line">    response.to_csv(<span class="string">'chinese_newyear.csv'</span>,mode=<span class="string">'a'</span>,encoding=<span class="string">'utf_8_sig'</span>,index=<span class="number">0</span>,header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> year <span class="keyword">in</span> range(<span class="number">1983</span>,<span class="number">2018</span>):</span><br><span class="line">        get_content(year)</span><br></pre></td></tr></table></figure><p>抓取下来的节目信息：</p><p><img src="http://media.makcyun.top/FhABd16cEqySy9_5Jwd42rQA6EQX" alt=""></p><p>抓取下来的节目数据：</p><p><img src="http://media.makcyun.top/FqelJCPgzGc1rDcwdHNiRT4Db-2-" alt=""></p><p>抓取下来的数据是脏数据，用 Python 清洗处理一下就可以分析，这些不是重点，所以下面直接进入分析环节，来一探究竟。</p><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h3 id="▌谁导演春晚次数最多？"><a href="#▌谁导演春晚次数最多？" class="headerlink" title="▌谁导演春晚次数最多？"></a>▌谁导演春晚次数最多？</h3><p>导演是春晚的总负责人，好比厨师，厨师决定了春晚大餐好不好吃。36 年间，有很多导演负责过春晚，比如近些年的哈文、朗昆，你可能想知道他们是不是导演次数最多的，下面就来看看导演次数最多的十大导演：</p><p><img src="http://media.makcyun.top/FgJXVXg5IeWQ69_VFegD5ifpsA-5" alt=""></p><p>导演次数最多的是黄一鹤和朗昆导演，两个人都导演了 5 次。</p><p>黄一鹤导演对于 80 后之后的人来说，不算熟悉，因为他导演春晚的时候是在 80 年代，很多人都没有出生。朗坤则相对熟悉些，最近一次导演是 2009 年。那一年的春晚，是印象最深刻的一届，因为诞生了赵本山最棒的小品《不差钱》（个人之见）。</p><p>哈文一共导演了 3 次，都在 2010 年之后，其他的导演就不那么熟悉了，相比于主持人、演员，他们是幕后工作者。</p><p>代码实现如下，关键点在于 For 循环绘制子图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_director</span><span class="params">()</span>:</span></span><br><span class="line">    data = pd.read_csv(<span class="string">'chinese_newyear3.csv'</span>,encoding=<span class="string">'utf_8_sig'</span>)</span><br><span class="line">    <span class="comment"># 筛选导演主持人</span></span><br><span class="line">    data = data[data[<span class="string">'category'</span>] == <span class="string">'导演'</span>]</span><br><span class="line">    <span class="comment"># data = data[data['category'] == '主持']</span></span><br><span class="line">    data2 = data[<span class="string">'content'</span>].str.split(<span class="string">'、'</span>,expand=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计出现次数</span></span><br><span class="line">    data2 = data2.apply(pd.value_counts)</span><br><span class="line">    data2[<span class="string">'col_num'</span>] = data2.sum(axis=<span class="number">1</span>)</span><br><span class="line">    data2.sort_values(by=<span class="string">'col_num'</span>,ascending=<span class="keyword">False</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line">    data2 = data2[<span class="string">'col_num'</span>][:<span class="number">10</span>][::<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> data,data2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis6</span><span class="params">(data,data2)</span>:</span></span><br><span class="line">    data = data.set_index(<span class="string">'year'</span>)</span><br><span class="line">    data2.sort_values(ascending=<span class="keyword">False</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line">    lst = list(data2.index)[:<span class="number">10</span>]</span><br><span class="line">    lst_num = list(data2)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">    colorsall = [color1,color2,color3,color4,color5,color1,color2,color3,color4,color5]</span><br><span class="line">    <span class="keyword">for</span> i,name <span class="keyword">in</span> enumerate(lst):</span><br><span class="line">        data3 = data[<span class="string">'content'</span>].str.contains(name,na=<span class="keyword">False</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">        data3 = pd.DataFrame(data3[data3.values == <span class="number">1</span>])</span><br><span class="line">        data3[<span class="string">'year'</span>] = data3.index</span><br><span class="line"></span><br><span class="line">        axs = fig.add_subplot(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>+i)</span><br><span class="line">        data3.plot(</span><br><span class="line">            ax=axs,</span><br><span class="line">            x=<span class="string">'content'</span>,</span><br><span class="line">            y = <span class="string">'year'</span>,</span><br><span class="line">            kind = <span class="string">'scatter'</span>,</span><br><span class="line">            subplots=<span class="keyword">True</span>,</span><br><span class="line">            sharey=<span class="keyword">True</span>,</span><br><span class="line">            color=colorsall[i],</span><br><span class="line">            )</span><br><span class="line">        new_ticks = np.linspace(<span class="number">1980</span>,<span class="number">2020</span>,<span class="number">41</span>)</span><br><span class="line">        plt.yticks(new_ticks)</span><br><span class="line">        plt.tick_params(direction=<span class="string">'in'</span>)  <span class="comment">#标签朝里</span></span><br><span class="line">        plt.tick_params(which=<span class="string">'major'</span>,length=<span class="number">0</span>)  <span class="comment"># 不显示刻度标签长度</span></span><br><span class="line">        plt.xticks([]) <span class="comment">#去掉坐标标签</span></span><br><span class="line">        plt.xlabel(<span class="string">'%i'</span> %(lst_num[i]),fontsize=<span class="number">10</span>)</span><br><span class="line">        plt.xlim(<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        plt.title(name,color=color5,fontsize=<span class="number">10</span>)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        fig.subplots_adjust(hspace=<span class="number">0</span>,wspace=<span class="number">0</span>) <span class="comment"># 调整子图间距为0</span></span><br><span class="line"></span><br><span class="line">    plt.savefig(<span class="string">'导演次数最多的主持 TOP 10.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="▌谁主持春晚次数最多？"><a href="#▌谁主持春晚次数最多？" class="headerlink" title="▌谁主持春晚次数最多？"></a>▌谁主持春晚次数最多？</h3><p>导演过后就是主持人了，他们堪称春晚的门面，大家也更熟悉些，不管是早年的赵忠祥、倪萍，中生代的朱军、董卿，还是近年新晋的康辉、李思思以及我们新疆大帅锅尼格买提，只要一提名字，你马上就能对号入座。</p><p>可你知道主持界的「钉子户」是谁？「常青树」又是哪些人？ 来看看春晚主持次数最多的 TOP 10 名单：</p><p><img src="http://media.makcyun.top/FgpWEHDt9Z_Ukaxz2rSOGj_P2HsX" alt=""></p><p>一眼望去，十个人每一个都很熟悉。</p><p>排第一的是 <strong>主持了 21 年的朱军</strong>，称得上劳模了，<strong>从 1997 年连续不间断地住持到 2017 年</strong>，远超其他主持人。</p><p>排第二的是周涛，主持了 14 年，2011 年之后退居幕后， 2016 年又复出了一次。对她的印象，莫过于 2003 年和冯巩合作过的一个小品《马路情歌》：</p><p><img src="http://media.makcyun.top/FoeG5CQ_KAOhaJLKsG8ier3zEgIA" alt=""></p><p>排第三的是董卿，从 05 年开始主持，只在 2018 年缺席了一年，大家都喜欢她，所以她缺席也成为去年春晚的一大讨论话题。好消息是，今年的春晚她会继续主持。</p><p>董卿春晚上的亮点太多了，印象最深的是和刘谦三度搭档过的魔术节目。刘谦技术够神，董卿配合地也神。另一个好消息是，自 13 年阔别春晚舞台之后，刘谦今年也会再度登台，二人是否还会继续搭档呢，拭目以待。</p><p><img src="http://media.makcyun.top/Fq41GXs48nnCNDarE-Rt20tKQEbT" alt=""></p><p>再往后是一对著名搭档，赵忠祥和倪萍老师，可惜印象不深，找了找他们曾主持过的春晚照片，满满的历史感。</p><p><img src="http://media.makcyun.top/FvYOIt2sKcC5i5FU28PX6KS9kvXa" alt=""></p><center>（1992 年春晚）</center><p><img src="http://media.makcyun.top/FrTAtCqHfQeR1_5N2qoENIIUpMkL" alt=""></p><center>（1996 年春晚）</center><p>再之后是李咏，刚刚离开不久，喜欢他的风格，希望天堂也有话筒，也有春晚。</p><p><img src="http://media.makcyun.top/Fn2scCwmL-fbiLxq7d3ERq65ulH5" alt=""></p><p>说完主持人，下面说一说除夕的日期。</p><h3 id="▌哪两年的除夕刚好是同一天？"><a href="#▌哪两年的除夕刚好是同一天？" class="headerlink" title="▌哪两年的除夕刚好是同一天？"></a>▌哪两年的除夕刚好是同一天？</h3><p>小时候一直没有搞懂中国的农历，想不明白为什么每年除夕的日子都不一样，为什么过年不是在元旦。后来才知道这是我们老祖宗的智慧，因为农历比阳历更加准确。</p><p>每年除夕的日期都在变化，那你是否好奇过某两年的除夕是在同一天这个问题？</p><p>来看看：</p><p><img src="http://media.makcyun.top/FpXsF9dUESievzOpIKxPn7U5IIN0" alt=""></p><p>以从下往上，从左往右的顺序看上面这张图，会发现几件有意思的事：</p><ul><li><p><strong>除夕最早的一年是 2004 年， 1 月 21 日。</strong></p></li><li><p><strong>除夕最晚的一年是 1985 年， 2 月 19 日，最早最晚差了近一个月。</strong></p></li><li><p><strong>2019 年除夕是 2 月 4 日，和 2000 年是一样的。</strong></p></li><li><p><strong>有不少年份的除夕在同一天。</strong>比如：1993 和 2012 年都在 1 月 22 日，1998 和 2017 年的 1 月 27 日，1987 和 2006 年的 1 月 28 日。很有意思对吧，但至今最多只有两年的除夕是在同一年，没有出现过「三年除夕都在同一天」这种现象，也许以后会。</p></li></ul><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分析除夕日期</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_date</span><span class="params">()</span>:</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">5</span>,<span class="number">8</span>))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    data = pd.read_csv(<span class="string">'chinese_newyear3.csv'</span>,encoding=<span class="string">'utf_8_sig'</span>)</span><br><span class="line">    data = data[data[<span class="string">'category'</span>] == <span class="string">'播出日期'</span>]</span><br><span class="line">    data = data[<span class="string">'content'</span>].str.extract(<span class="string">r'.*?年(.*?)月(.*?)日.*?'</span>)</span><br><span class="line">    data = data.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">    data.columns = [<span class="string">'month'</span>,<span class="string">'day'</span>]</span><br><span class="line">    data[<span class="string">'year'</span>] = np.arange(<span class="number">1983</span>,<span class="number">2019</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># int 转 string</span></span><br><span class="line">    data = data.applymap(str)</span><br><span class="line">    data[<span class="string">'year2'</span>] = <span class="string">'1900'</span></span><br><span class="line">    data[<span class="string">'date'</span>] = data[<span class="string">'year2'</span>].str.cat([data[<span class="string">'month'</span>],data[<span class="string">'day'</span>]],sep=<span class="string">'/'</span>)</span><br><span class="line">    data = data.apply(pd.to_numeric,errors=<span class="string">'ignore'</span>)</span><br><span class="line">    data[<span class="string">'date'</span>] = pd.to_datetime(data[<span class="string">'date'</span>])</span><br><span class="line">    ax.plot(</span><br><span class="line">        data[<span class="string">'date'</span>],</span><br><span class="line">        data[<span class="string">'year'</span>],</span><br><span class="line">        )</span><br><span class="line">    new_yticks = np.linspace(<span class="number">1980</span>,<span class="number">2020</span>,<span class="number">41</span>)</span><br><span class="line">    date_format = mpl.dates.DateFormatter(<span class="string">"%m-%d"</span>)</span><br><span class="line">    ax.xaxis.set_major_formatter(date_format)</span><br><span class="line">    plt.yticks(new_yticks)</span><br><span class="line">    plt.tick_params(direction=<span class="string">'in'</span>)  <span class="comment">#标签朝里</span></span><br><span class="line">    content = list(zip(data[<span class="string">'date'</span>],data[<span class="string">'year'</span>]))</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> content:</span><br><span class="line">        x2 = <span class="string">'%s'</span> %x.strftime(<span class="string">'%m/%d'</span>) <span class="comment"># 只显示月日格式</span></span><br><span class="line">        <span class="comment"># print(x,'\n',y)</span></span><br><span class="line">        plt.text(x, y+<span class="number">0.2</span>,x2, ha=<span class="string">'center'</span>, color=color4)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'历年农历除夕日期变化'</span>,color=color4,fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(<span class="string">'除夕日期变化.png'</span>,dpi=<span class="number">200</span>,)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>下面，来看看更有意思的。</p><h3 id="▌谁上春晚次数最多，堪称「钉子户」？"><a href="#▌谁上春晚次数最多，堪称「钉子户」？" class="headerlink" title="▌谁上春晚次数最多，堪称「钉子户」？"></a>▌谁上春晚次数最多，堪称「钉子户」？</h3><p>很多人梦想着这辈子能上一次春晚，但绝大多数人都只能坐在电视跟前，而有些人则天生是上春晚的，上春晚就像家常便饭，一上就是几十年。<strong>你能猜到这 36 年谁上春晚次数最多么？</strong></p><p>来看看：</p><p><img src="http://media.makcyun.top/FpFZvr6oPLyV_yPQFYhFLvw6VXu1" alt=""></p><p>春晚表演次数最多的是相声演员冯巩大叔，你猜对了么。春晚总共 36 年历史，他就登台了 35 次，十足的「钉子户」。</p><p>前 5 名除了李谷一老师是歌手以外，其他人都是相声或小品演员，也都是你熟悉的面孔：姜昆、蔡明和黄宏。相声和小品为春晚贡献了很多人才。</p><p>5 - 10 名歌唱家占多，有宋祖英，还有一位你知道的，没想到还上了这么多次吧。另外还有两名小品演员本山大叔和黄宏。可惜，不见二人表演已多年。</p><p>10 - 20 名还有很多熟悉的人，比如郭冬临、巩汉林、潘长江等。</p><p>来具体看一下他们都上了哪些年的春晚：</p><p><img src="http://media.makcyun.top/Fs3QZFSbAiJwueq06cfJ7WKs9Zmv" alt=""></p><p>（注：部分演员表演次数和图中点数不一致，是因为某些年 TA 不只表演了一个节目）</p><p><strong>冯巩老师从 86 年至今，雷打不动地从未缺席过春晚。</strong>他上的最多，每年的开场白都是那句「我想死你们了」，对春晚和观众的热爱之情可见一斑。</p><p>他的作品太多了，相声小品通吃，多的让人记不住。</p><p><img src="http://media.makcyun.top/FoJDKf3GUZ5K9XRyCaSM4gUcZIZb" alt=""></p><p><strong>姜昆老师，登台最早，第一年春晚就上台了。</strong> 这些年，消失几年后又出现在春晚舞台，也许是为了以免大家太过于想他。</p><p>最喜欢他 1987 年的《虎口遐想》相声，30 年后的 2017 年又再度带来了《新虎口遐想》。岁月在他的脸上仿佛没有留下痕迹。</p><p><img src="http://media.makcyun.top/FhRUKmnZ8H8VOI9lCDUdL9psqcC2" alt=""></p><center>（1987 年《虎口遐想》）</center><p><img src="http://media.makcyun.top/FmUcVDcVX63SV2fJIBdL41HFIBAF" alt=""></p><center>（2017 年《新虎口遐想》）</center><p><strong>蔡明老师，是女性中上春晚最多的。</strong>她和许多人合作过，和谁合作效果都好。</p><p><img src="http://media.makcyun.top/FmEBl9jB6iACa6oLQeh_fFPEmH9O" alt=""></p><center>（1991 年《陌生人》）</center><p>黄宏老师一连登台 25 年，小品中扮演过很多角色，可惜 2012 年之后就没有上台了。</p><p><strong>李谷一老师，春晚演出时间跨度最大</strong>，1983 年有她，2018 年还有她。对她的印象莫过于每年春晚尾声的那曲《难忘今宵》。实际上，李谷一老师曾一人撑起了早期的春晚。1983 年的春晚，她一人连唱 7 首歌，前无古人，也后无来者。</p><p><img src="http://media.makcyun.top/Fid7gV2T_PTuYsJ-1A5-6xmESPIE" alt=""></p><p>赵本山老师，对他的评价只有一句：<strong>赵本山之后再无春晚。</strong>他在就是压轴，每一部作品都看过很多遍，现在也只有靠怀念，众多作品中，最喜欢两部：</p><p><img src="http://media.makcyun.top/Fn5GCxPsZ6KE-jnv8f30JZhT768R" alt=""></p><center>（2005 年《功夫》）</center><p><img src="http://media.makcyun.top/Fpddpk8HFTZXtO7-gPy7XDO-v5jd" alt=""></p><center>（2009 年《不差钱》）</center><h3 id="▌港台明星上春晚次数对比"><a href="#▌港台明星上春晚次数对比" class="headerlink" title="▌港台明星上春晚次数对比"></a>▌港台明星上春晚次数对比</h3><p>不知你发现没有，前面这些人员都来自内地，港台及海外明星并没有出现，但不可忽略的是，他们的演出让春晚更精彩。</p><p>来看看都有哪些港台明星上过春晚，上了几次：</p><p><img src="http://media.makcyun.top/Fvysmq8qLMLG1VXGyzroz2BUJzcr" alt=""></p><p>（注：这里选的 10 位明星是第一时间想到的，全凭个人主观印象）</p><p><strong>周董上过 5 次春晚，你猜对了么？</strong>最近一次是去年的《告白气球》，当时朋友圈都刷爆了。除此之外，他其他几次登台，都惊艳无比，04 年的《龙拳》，08 年的《青花瓷》，09 年的《本草纲目》以及 11 年的《兰亭集序》。</p><p><img src="http://media.makcyun.top/Fs_wv3pQ91mt5VnP4cEsh4wUGLL0" alt=""></p><p>15 年，偶像换了一波又一波，他却依旧是那个光芒四射的周杰伦。</p><p>成龙大哥同样上了 5 次，一身正气、中国功夫。</p><p>刘德华是四大天王中上春晚最多、最早的，2005 年唱了那首《恭喜发财》之后，每年过年在大街小巷你都能听到。</p><p>王力宏上了 4 次，2012 年龙年，再次唱了那首家喻户晓的《龙的传人》。</p><p>王菲是港台明星中，上春晚次数最多的女星之一，98 年和那英的《相约九八 》影响力空前。其他几次《传奇》、《因为爱》和《岁月》也空灵好听。</p><h3 id="▌歌曲、小品、相声等节目数量对比"><a href="#▌歌曲、小品、相声等节目数量对比" class="headerlink" title="▌歌曲、小品、相声等节目数量对比"></a>▌歌曲、小品、相声等节目数量对比</h3><p>最后，再来看看春晚各类节目构成比例。印象中，最多的节目就是歌曲，其次是小品、相声。可以看到，歌曲类节目基本占了所有节目的一半，小品占了 15%，相声是 9%。其他类型则是一些杂技、戏剧、舞蹈这些，未作统计。</p><p><img src="http://media.makcyun.top/FoLXr3KKWVrBGYmSJBiLqybNxHvV" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 各节目数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis3</span><span class="params">(data)</span>:</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    num_all = data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 歌曲节目数量</span></span><br><span class="line">    num_song = data[data[<span class="string">'category'</span>].str.contains(<span class="string">'歌|尾|开场'</span>)].shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 小品数量</span></span><br><span class="line">    num_sketch = data[data[<span class="string">'category'</span>].str.contains(<span class="string">'小品'</span>)].shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 相声数量</span></span><br><span class="line">    num_crosstalk = data[data[<span class="string">'category'</span>].str.contains(<span class="string">'相声'</span>)].shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 其他节目数量</span></span><br><span class="line">    other = num_all - sum([num_song,num_sketch,num_crosstalk])</span><br><span class="line">    lst = [num_song,num_sketch,num_crosstalk,other]</span><br><span class="line"></span><br><span class="line">    sizes = [num_song,other,num_crosstalk,num_sketch]</span><br><span class="line">    labels = [<span class="string">'歌曲'</span>,<span class="string">'其他'</span>,<span class="string">'相声'</span>,<span class="string">'小品'</span>]</span><br><span class="line">    colors_pie = [color1,color4,color3,color2]</span><br><span class="line">    explode = [<span class="number">0.05</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">    plt.pie(</span><br><span class="line">        sizes,</span><br><span class="line">        autopct=<span class="string">'%.1f%%'</span>,</span><br><span class="line">        labels=labels,</span><br><span class="line">        colors=colors_pie,</span><br><span class="line">        shadow=<span class="keyword">False</span>,</span><br><span class="line">        startangle=<span class="number">270</span>,</span><br><span class="line">        explode=explode,</span><br><span class="line">        textprops=&#123;<span class="string">'fontsize'</span>:<span class="number">14</span>,<span class="string">'color'</span>:colors&#125;</span><br><span class="line">        )</span><br><span class="line">    plt.title(<span class="string">'1983-2018 共 36 年春晚节目类型数量比较'</span>,color=colorstitle,fontsize=fontsize_title)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper right'</span>)</span><br><span class="line">    plt.savefig(<span class="string">'1983-2018 共 36 年间各类节目类型数量比较.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>以上，就是对过去 36 年春晚节目的简单分析。</p><p><strong>你对春晚的了解有增加么？</strong></p><p>如需完整代码，可以加入我的知识星球，都是干货。</p><p><img src="http://media.makcyun.top/19-1-1/31534576.jpg" alt=""></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;万万没想到。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>你根本不会用百度</title>
    <link href="https://www.makcyun.top/2019/01/25/weekly_sharing14.html"/>
    <id>https://www.makcyun.top/2019/01/25/weekly_sharing14.html</id>
    <published>2019-01-25T08:16:16.000Z</published>
    <updated>2019-01-25T02:58:21.075Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>正确使用百度的方法。</p><a id="more"></a><p><strong>摘要</strong>：正确使用百度的方法。</p><p>想必你的朋友圈这两天应该被《搜索引擎百度已死》这篇文章刷屏了吧，随着文章持续发酵，百度股价也大跌 6%，对于很多深受百度搜索之痛的网友来说，这篇文章和这个结果够解气，没办法谁让百度恨铁不成钢。</p><p>一直以来，百度都是中国绝对的门面网站。在 Alexa 世界 TOP 500 网站排名中，<strong>百度位居世界第四，中国第一</strong> 。如此牛逼的网站做出的搜索引擎，和远在大洋那头的谷歌一比，让无数网友都感到汗颜，只能庆幸老外不识中文，不用百度，不然脸更是打地啪啪响。</p><p><img src="http://media.makcyun.top/FoPS_DkF-FaKotCC4HDIz8dw2zpn" alt=""></p><p>于是，懂技术一点的网友纷纷弃百度投谷歌，不懂技术的就只能继续受着了，有人善意支了个招，说用微软的 Bing 搜索吧，于是 Bing 的访问量这两天陡增。万万没想到，这家国内最后一个西方搜索引擎居然很快就无法访问了，你说百度厉不厉害。</p><p>百度这么牛逼，那只能以壮士断腕的勇气去直面它了。当然，正刚不过它，这里就给你支几招怎么 <strong>正确使用百度搜索</strong>。</p><p>上面那篇文章举了多个例子说百度的首页搜索结果，几乎都被广告、百家号及其他自家网站的内容占据了，很难搜到想要的内容。这里，我们也再来测试一下，首先打开手机端浏览器，输入百度网址进入主页面。</p><p>呵，这主页够花里胡哨啊，不能给一个干净清爽点的搜索界面么，这可是你百度的脸啊。</p><p><img src="http://media.makcyun.top/FoW25bszfGWOp478Rz7t127f0_jy" alt=""></p><p>如果你不喜欢看到这些乱七八糟的内容，想干净一点，方法很简单，比如下面这个样子，只需要替换一下网址就行了。把 <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 替换为下面的这一串就行了：</p><p><a href="https://m.baidu.com/?from=1013843a&amp;pu=sz%401321_480&amp;wpo=btmfast" target="_blank" rel="noopener">https://m.baidu.com/?from=1013843a&amp;pu=sz%401321_480&amp;wpo=btmfast</a></p><p>这样再去搜索是不是就爽很多了。</p><p><img src="http://media.makcyun.top/FurRrxaX_FrLgazdgALBkXlP7wRq" alt=""></p><p>接着，随便输入一个关键词，比如「 python学习」看看，<br>嚯，果真清一色的广告啊。</p><p><img src="http://media.makcyun.top/Fr1iYZY4q1vGzeFxFY8QOcPoAz68" alt=""></p><p>看到这些广告你可能就烦死了，有段子说正确使用百度的方式是，搜索结果后直接从第 2 页开始看，因为第一页都是广告或者是百家号的内容。其实，也不用这么麻烦，直接把广告给它干掉不就行了？</p><p>方法很简单，开启强力广告拦截模式就行了。你可能会说：我的浏览器怎么没有这个功能？为了避免打广告，公众号后台回复：<strong>浏览器</strong> 就可以得到我用的这款浏览器。这下看着就舒服多了，虽然第一条还是百家号的内容，不过其他内容还算正常。</p><p><img src="http://media.makcyun.top/FiZdBLLUdFn7ajUYZSHIxmt6ETT7" alt=""></p><p>以上是手机端使用百度的正确方式，下面再看看网页端怎么使用，这里仍然你「Python学习」作为关键词。</p><p><img src="http://media.makcyun.top/FgdDFd3zy2PhqG6lCUHJU8Qr5WUT" alt=""></p><p>首先，在百度搜索就有个很蛋疼的体验，就是它默认会实时展现你搜索的内容，什么意思呢，就是说你还没输完内容它就跳转到搜索页了，不会等你在主页输完再跳转，就像下面这样。如果你想改变，可以在右上角的设置中，关闭实时预览。</p><p><img src="http://media.makcyun.top/FhIh8h9C7tUQASNWZEu4W1h0eJ5R" alt=""></p><p>搜索出来的结果又是满屏的广告，无力吐槽。怎么干掉这些广告呢？很简单，就是使用大名鼎鼎的油猴浏览器插件了（如果你用的是 Chrome 的话）。把这些广告、百家号内容统统干掉，还原出一个干净的搜索引擎界面。</p><p>怎么做呢，分两步。</p><p>第一步，安装油猴（TemperMonkey） 插件。</p><p><img src="http://media.makcyun.top/FjmHeJMJGoDrlQAdmHfUp4ORpXyR" alt=""></p><p>第二步，在 <a href="https://greasyfork.org/zh-CN/scripts" target="_blank" rel="noopener">Greasy Fork</a> 上面搜索百度相关的插件，你可以看到有各种各样的插件，去广告的、移除百家号的，全都安装上就行了。</p><p><img src="http://media.makcyun.top/Fh9FpX-qAuLsuV1__y9CCnCOzO8-" alt=""></p><p>再次重复刚才的搜索，你会看到截然不同的搜索界面，是不是爽很多。</p><p><img src="http://media.makcyun.top/FmyWjUcqC7Uk0Z3WQSKz8-RRxdYM" alt=""></p><p>你可能很兴奋地想居然有这么多开挂的插件，马上安起，不过我觉得既然你都能用这些插件了，其实你一个插件也都不需要了。</p><p>你会用百度了么？</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;正确使用百度的方法。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>今天，公众号达到了一个小里程碑</title>
    <link href="https://www.makcyun.top/2019/01/23/life06.html"/>
    <id>https://www.makcyun.top/2019/01/23/life06.html</id>
    <published>2019-01-23T08:16:24.000Z</published>
    <updated>2019-02-11T11:48:29.816Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>记录我的写作历程。</p><a id="more"></a><p><strong>摘要：</strong>回顾过去 5 个月写公众号的点滴。</p><p>今天，我的公众号迎来了一个小小的里程碑，粉丝数达到了 5000，感谢正在看此文的你，没有你的关注，我现在也达不到这个小目标，最想说的话还是那句：「<strong>你终于来了，这可能是一生中最让我感动的事。</strong>」</p><p>5000 这个数对于很多公众号来说，根本不值一提，但对我来说意味着很多，所以今天给第 5000 位粉丝发了 50 元的红包。</p><p><img src="http://media.makcyun.top/201901231625_244.jpg" alt=""></p><p>发红包不是因为我钱太多，事实上我穷得一批。之所以发红包，一是算做对自己一直在努力坚持的一个正向激励，同时也回馈一下粉丝，我发得开心，收到的人也觉得惊喜意外，何乐而不为。</p><p>事实上，这个惯例从第 1000 位粉丝就开始做了，也希望能够一直做下去，我希望自己的公众号能和别人有一点不同。</p><p><img src="http://media.makcyun.top/201901231626_12.jpg" alt=""></p><p>从去年 8 月底开始再次更新，当时的公众号粉丝数是 200 左右，而这个数字是过去 4 年积累下来的，几乎是冷启动。到现在 5 个月过去了，时间说长不长，说短也不短，这 5 个月堪称我人生的最低谷，不过好在这件事上是坚持下来了，现在越来越理解「触底反弹」是什么意思。</p><p>一直以来，我把公众号当成是自己的一个私人世界，几乎所有的文章都是原创，哪怕一篇文章要写一个礼拜才能写出来，也没有转发过别人的文章，也从没接过广告，唯一的一点收入来自于一些朋友的赞赏。承认自己还是挺倔强的，虽然涨粉速度比别人慢很多，但也没有太着急。</p><p>每一篇文章会自恋地看好几遍，感叹写得真好，排版真漂亮。幻想着发出去后，会有很多人看，会有人点赞。文章发出去之后，会反复看有多少阅读量，新增了多少粉丝。而现实则比较残酷，经常是一篇文章只有几十个阅读量，有时一天都没有一个新粉丝。有那么几次，是想放弃的，但一想自己已经在悬崖边上了，再不逼自己一把，就真的掉下去爬不上来了，所以一次次地又坐回电脑跟前继续写。</p><p>前几个月写的文章只发布在了自己的公众号，没敢向别的公众号投稿，觉得自己还没有什么积累，别人关注了也很可能会取关，一直到在写了十几篇文章后，才慢慢尝试着去投稿。就这样，粉丝数从 200 到 1000，花了 3 个月才达到，很多做公众号的人都挺着急，我想如果他们面对这样的结果，很可能早就放弃了。</p><p>好在，有了前面的积累，状况开始慢慢变好，从粉丝方面来说，从 1000 到 2000 位粉丝用了 5 周时间，2000 到 3000 用了 3 周，3000 到 5000 则只用了 2 周。</p><p>除了得到粉丝认可以外，也结识了不少圈内伙伴，增长了不少见识，这一切是都当初在写公众号前没有预想过的。</p><p>回过头来看，我很庆幸一路坚持了下来，也越来越有自信，现在觉得：不管是小到做公众号，还是大到人生转折，考虑好了就只管去放手一搏，不要抱有太多期待和幻想，这些是阻止你开始的东西。</p><p>新的一年希望我们都能有更大的进步，另外从今天开始，也欢迎你随手帮我点点广告吧，算是对我的支持和肯定。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;记录我的写作历程。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>做好一套 PPT 的核心：注重内容逻辑与视觉化表达</title>
    <link href="https://www.makcyun.top/2019/01/20/weekly_sharing13.html"/>
    <id>https://www.makcyun.top/2019/01/20/weekly_sharing13.html</id>
    <published>2019-01-20T08:16:16.000Z</published>
    <updated>2019-01-21T02:31:56.288Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>GraphicRiver 上销量排名第 2 的 PPT 作品。</p><a id="more"></a><p><strong>摘要</strong>：GraphicRiver 上销量排名第 2 的 PPT 作品 。</p><p>这是「<strong>每周分享</strong>」的第 13 期， 鉴于最近新关注的很多朋友还不太了解这个栏目，所以我这里再介绍一下。顾名思义，就是会在每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、摄影、Ps 等几个方面。你可以在我公众号界面中的 「不务正业」菜单里集中查看，Python 文章则在另一个「不误正业」菜单中。</p><p>上两期，分享了一套 GraphicRiver 上销量第 1 的 PPT 作品，大伙儿反应很是强烈，光公众号后台回复「ppt模板」的消息就有好几百条，完全超出我的预料，看来大家都想做出一手有逼格的 PPT 啊。</p><p>好东西不嫌多，今天我再次拿出压箱底的一套 PPT 作品送给你，这套作品就是 GraphicRiver 上销量排名第 2 的 PPT。</p><p><img src="http://media.makcyun.top/201901200813_854.png" alt=""></p><p>虽然是第 2 名，但这套作品从水准和实用性上，我觉得比排名第 1 的还要棒，随便看几张就知道了。</p><p><img src="http://media.makcyun.top/201901200835_219.jpg" alt=""></p><p>这套作品非常专业，使用了专业的配色、丰富的逻辑关系图、形象的图标、好看的字体，整体视觉效果非常棒，如果你能把这些都学以致用，那你的 PPT 绝对能甩周围的同学同事几条街。</p><p><strong>在送你这套模板之前，我想先和你分享几点经验，能帮助你充分利用它。</strong></p><p>我相信所有人都想把 PPT 做好，但不幸地是，绝大多数人都不得要领，做出的 PPT 离专业水平差老远，归结原因无非这么几点，要么是见得太少没有灵感，要么收藏地太多却不归类总结，要么是过多地关注于奇技淫巧，总之就是走偏了。</p><p>如何不走偏呢，很简单，<strong>注重内容逻辑与视觉化表达</strong>，这就是做出一套好 PPT 的核心。展开一点说就是，把自己的 PPT 文稿，按逻辑关系有条理地组织，配以视觉图形呈现出来就行了。</p><p>文字之间的逻辑关系是什么？很简单，我们从小就学，天天都在用，比如：「因为 Python 很火，所以我要去学」，这就是因果逻辑关系；「学了 Python，可以做爬虫、开发、机器学习」，这就是并列逻辑关系。例子无穷尽，但 <strong>常见的逻辑关系只有 5 种，即：因果关系、递进关系、主次关系、总分关系和并列关系。</strong>把这些逻辑关系运用到 PPT 中，你的 PPT就成功了一半。</p><p>很多人做 PPT 从没思考过处理逻辑关系，猛往幻灯片里面堆文字和数据，殊不知，<strong>做 PPT 最忌讳的就是没有逻辑性和大篇幅文字</strong>（这里主要指演讲型 PPT），你可能会说，我是做 PPT 又不是写作文，搞清这些逻辑关系有什么用？你如果现在真是这么想，那我要恭喜你，因为你终于找到了做 PPT 不得要领的原因。</p><p>下面就以这套模板为例，来说说如何采用「内容逻辑与视觉化表达」的方法来做好一套 PPT。</p><p>先说不内容逻辑与视觉化表达的 PPT 是什么样的，典型的就是这种长篇大段式的 Word 型 PPT，没有做任何内容逻辑和视觉效果处理，不分哪些是需要自己讲的，哪些需要读者看的，全都揉搓在一起丢给读者，效果可想而知。</p><p><img src="http://media.makcyun.top/201901201418_627.jpg" alt=""></p><p>那如何注重内容逻辑和视觉化表达呢，简单，两步走。</p><p>首先，分析文稿中的内容逻辑，提取和重组关键字，删除无用的字句，这些字句口头表述就可以了，不需要出现在幻灯片中。</p><p>然后，确定关键字之间的逻辑关系属于上面 5 种逻辑关系中的哪一种，然后选取适合的视觉图形进行呈现就行了。</p><p>好，下面来看一下这套模板是采用了哪些「内容逻辑和视觉化表达」的制作方法 。</p><h3 id="▌因果关系图"><a href="#▌因果关系图" class="headerlink" title="▌因果关系图"></a>▌因果关系图</h3><p><img src="http://media.makcyun.top/FrfxlEKe5X_2S_4ID5BwFWSKQBHS" alt=""></p><h3 id="▌递进关系图"><a href="#▌递进关系图" class="headerlink" title="▌递进关系图"></a>▌递进关系图</h3><p><img src="http://media.makcyun.top/FgXHonqdFAhUDz3Th4JxCCM6icd7" alt=""></p><p>年终总结中常会用到时间线型的幻灯片，回顾过去一年完成了哪些工作，这种时间线也属于递进关系，可以用下面的这些图。</p><p><img src="http://media.makcyun.top/FtG1cCafWjP0ghWFR8TOf1Rtyuun" alt=""></p><h3 id="▌主次关系图"><a href="#▌主次关系图" class="headerlink" title="▌主次关系图"></a>▌主次关系图</h3><p><img src="http://media.makcyun.top/FnGk-NP5vYvfZCrJrLc0IybTnyzl" alt=""></p><h3 id="▌总分关系图"><a href="#▌总分关系图" class="headerlink" title="▌总分关系图"></a>▌总分关系图</h3><p><img src="http://media.makcyun.top/Fqo93B_pzJmgUcbzvI01wuPA2a3X" alt=""></p><h3 id="▌并列关系图"><a href="#▌并列关系图" class="headerlink" title="▌并列关系图"></a>▌并列关系图</h3><p><img src="http://media.makcyun.top/Fi1votuirufk71Mn3ijtwFteSWBU" alt=""></p><p>上面这些幻灯片，完美使用了「内容逻辑和视觉化表达」制作思路，很多都可以用在我们的 PPT 中，所以，还没有用到的时候，最好就是把它们分门别类地纳入自己的灵感素材库。等需要的时候，就能很快找到，然后套用就行了。</p><p>总之，多看、多积累、多练习就能做好 PPT 了。</p><p>上面，只是这套作品内容的一小部分，还有不少有用的东西可以学习，下面说一下。</p><h3 id="▌专业配色"><a href="#▌专业配色" class="headerlink" title="▌专业配色"></a>▌专业配色</h3><p>很多时候，你的 PPT 看着很 Low，很可能是配色大众，比如下面这张图表，所有元素都一样，一个使用默认配色，一个使用专业配色，这差距就是一个天上一个地下。</p><p><img src="http://media.makcyun.top/Fplp7rpbGDKRHTvv5FmgEZMOLvGs" alt=""></p><p>除了上面的商务蓝配色，模板还提供了多种专业的配色：</p><p><img src="http://media.makcyun.top/FmsK352zFhPzkIzkB0Dag7nus-jX" alt=""></p><h3 id="▌图表"><a href="#▌图表" class="headerlink" title="▌图表"></a>▌图表</h3><p>很多人在 PPT 中插入图表时，直接就把 Excel 中的图表复制过来了，丑到爆，来看看这些养眼的图表。</p><p><img src="http://media.makcyun.top/Fr6YPF-aDCfCnm3703FXsedJjcfo" alt=""></p><h3 id="▌地图"><a href="#▌地图" class="headerlink" title="▌地图"></a>▌地图</h3><p><img src="http://media.makcyun.top/Fszulh53dpwjaK4EKerjDgLtOTnF" alt=""></p><p>好，就罗列这么多，以上仅是 <strong>整套模板 300 张幻灯片中的一小部分，可以说每一张都是精品</strong>。</p><p>知道你等了很久，下面我打算把这套作品分享给你，不过这次想改一下规则，不再采取后台回复信息直接得到这种方式了，为什么呢？因为遇到太多的伸手党，领取后什么表示都没有，有的甚至拿完就取关，说实话，我不太喜欢这样的互动方式。</p><p>所以，今天改一下，方式也很简单，<strong>你如果需要这套 PPT ，可以选取任何一种方式：留言、打赏或者点个好看</strong>（点好看需要留言告诉我一下）来和我互动，我看到了之后就会发给你。</p><p>如果你不想留言、打赏或者点个好看，那看这篇文章就可以，写这篇文章我也是花了不少心思。</p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/weekly_sharing11.html">国外最牛逼的一套 PPT 作品分享给你</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;GraphicRiver 上销量排名第 2 的 PPT 作品。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="PPT" scheme="https://www.makcyun.top/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>由 Python 开发的 7 款世界知名 App</title>
    <link href="https://www.makcyun.top/2019/01/17/translating02.html"/>
    <id>https://www.makcyun.top/2019/01/17/translating02.html</id>
    <published>2019-01-17T08:16:16.000Z</published>
    <updated>2019-03-08T01:54:05.657Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Python 原来这么牛逼。</p><a id="more"></a><p><strong>摘要</strong>：由 Python 开发的 7 款世界知名 App 。</p><p>如今 Python 越来越火，大有登顶编程语言榜首的趋势，很多人开始知道或者学习 Python，今天就介绍几款由 Python 开发的世界大牌 App，让你再次认识 它。</p><p>下面一睹为快，看看有没有你不知道的。</p><h3 id="▌Instagram"><a href="#▌Instagram" class="headerlink" title="▌Instagram"></a>▌Instagram</h3><p><img src="http://media.makcyun.top/201901172114_76.jpg" alt=""></p><p>这款 App 想必你应该很熟悉，不少人也玩过，前几年还可以随便上，现在不行了。</p><p>简单介绍一下它，自 2010 年 10 月诞生之日起，就一直稳坐「在线图片及视频分享社交应用软件」世界老大位置。</p><p>它有多牛逼呢，用两组数据说明一下就知道了。</p><p>其一，根据 Alexa 世界 500 强网站流量排名，Instragram 位居 <strong>美国第 11 位，世界第 17 位</strong>，排它后面的是「微博」。</p><p><img src="http://media.makcyun.top/201901181559_696.png" alt=""></p><p>其二，<strong>日活跃用户达到 3 亿，月活用户达到 10 亿。</strong></p><p>这个数字你可能没概念，拿国内最近风光无限的「抖音」来对比一下就知道了，根据抖音 1 月公布的官方数据，抖音日活用户 <strong>达到 2.5 亿，月活用户有 5 亿</strong>。</p><p>还是比不过 Instagram，不过抖音后劲非常猛，海外版 Tik Tok 已登陆全球 150 个国家，微信都没走出国门，它走出去了。</p><p>话说回来， Instagram 能这么牛逼，主要是因为它有一个更牛逼的爹：脸书「 Facebook 」。</p><h3 id="▌Reddit"><a href="#▌Reddit" class="headerlink" title="▌Reddit"></a>▌Reddit</h3><p><img src="http://media.makcyun.top/201901181620_558.jpg" alt=""></p><p>Reddit 是美国最大的娱乐、社交及新闻网站，由两个维吉尼亚大学的学生在 2005 年创建，最初采用 Common Lisp 语言编写，后面改用 Python 。</p><p>相比 Instagram，它在国内知名度要低一些，而实际上它比 Ins 要牛逼，是 <strong>美国排名第 5 的网站</strong>，排它前面的只有：Google、Youtube、Amazon 和 Facebook 这四大巨头。</p><h3 id="▌Uber"><a href="#▌Uber" class="headerlink" title="▌Uber"></a>▌Uber</h3><p><img src="http://media.makcyun.top/201901181620_98.jpg" alt=""></p><p>Uber 你应该熟悉，前两年和滴滴打得不可开交，目前拥有 1 亿用户，它使用便捷的 Python 来处理大数据。</p><h3 id="▌Dropbox"><a href="#▌Dropbox" class="headerlink" title="▌Dropbox"></a>▌Dropbox</h3><p><img src="http://media.makcyun.top/201901181620_535.jpg" alt=""></p><p>如果你经常使用网盘，那应该会比较熟悉 Dropbox ，它也是用 Python 开发的一款顶级 App。</p><p>提到网盘，就不得不说国内的百度网盘，简单对比一下，在存储空间大小上，百度网盘还是很良心的，免费提提供 2T 存储空间，而 Dropbox 仅提供 2G 免费空间，差了 1000 倍。</p><p>Dropbox 虽然在容量上输给了百度网盘，但在安全性、协同合作等方面，能甩百度网盘好几条街。</p><h3 id="▌Pinterest"><a href="#▌Pinterest" class="headerlink" title="▌Pinterest"></a>▌Pinterest</h3><p><img src="http://media.makcyun.top/201901181620_634.jpg" alt=""></p><p>如果你是一个设计师，那么 Pinterest 你一定不陌生，该网站和 App 也是由 Django 搭建的。</p><p>作为一个图片分享网站，它最大的特点是可以方便地采集和收藏喜欢的图片。比如你喜欢 PPT ，就可以在上面采集 PPT 作品作为灵感储备，类似中国版的「花瓣网」。</p><h3 id="▌Spotify"><a href="#▌Spotify" class="headerlink" title="▌Spotify"></a>▌Spotify</h3><p><img src="http://media.makcyun.top/201901181620_440.jpg" alt=""></p><p>Spotify 是一个起源于瑞典的音乐流服务公司，也是 <strong>当前全球最大的流音乐服务商</strong>，国内众多音乐 App 中，能接近它的也只有网易云音乐。</p><h3 id="▌Disqus"><a href="#▌Disqus" class="headerlink" title="▌Disqus"></a>▌Disqus</h3><p><img src="http://media.makcyun.top/201901181621_19.jpg" alt=""></p><p>Disqus 是一家提供网站留言的公司，超过 75 万个网站使用了它的留言系统功能，它也使用了 Django 的部分功能。</p><p>最好，用一句话总结一下：</p><p><strong>这些 App 太酷了，但一个都上不了。</strong></p><p>你问我有没有办法？有的：</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5NDk4NDcwMw==&amp;mid=2651385551&amp;idx=1&amp;sn=a9b1c43cd6738fa3e543344bcfcf037f&amp;token=1697359516&amp;lang=zh_CN#rd" target="_blank" rel="noopener">∞ 教你科学上网</a></p><p>本文完。</p><p>参考：</p><p><a href="https://djangostars.com/blog/top-seven-apps-built-python/" target="_blank" rel="noopener">∞ Top Seven Apps Built With Python</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython17.html">∞ 分析了 7 万款 App，全是没想到</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">∞ 分析了 6000 款 App，竟然有这么多佳软没用过</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Python 原来这么牛逼。&lt;/p&gt;
    
    </summary>
    
      <category term="Python分享" scheme="https://www.makcyun.top/categories/Python%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="Python入门" scheme="https://www.makcyun.top/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>入门 Python 最好的视频</title>
    <link href="https://www.makcyun.top/2019/01/16/Python_learning03.html"/>
    <id>https://www.makcyun.top/2019/01/16/Python_learning03.html</id>
    <published>2019-01-16T08:16:16.000Z</published>
    <updated>2019-03-07T07:45:37.978Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>入门 Python 最好的视频。</p><a id="more"></a><p><strong>摘要</strong>：Coursera 上最火的 Python 课程送给你，带中英字幕。</p><p>看了标题你可能会质疑，确定是最好的？</p><p>一图以蔽之，连 Python 之父 「龟叔」都在推荐这门课，你说呢？</p><p><img src="http://media.makcyun.top/19-1-14/27623980.jpg" alt=""></p><p>这门课就是 Coursera 上的最火的 Python 入门系列课程： Python for Everybody Specialization ，堪比机器学习领域中，吴恩达的机器学习课程，真正的零基础入门，全是好评。</p><p><img src="http://media.makcyun.top/19-1-14/99419840.jpg" alt=""></p><p><img src="http://media.makcyun.top/19-1-14/3249305.jpg" alt=""></p><p>该系列课一共包含五门课，包括：</p><ul><li>《Python 入门》</li><li>《Python 数据结构》</li><li>《使用 Python 访问网络数据》</li><li>《Python 与数据库》</li><li>《Python 处理与可视化》</li></ul><p>可以说基本是涵盖了 Python 入门的方方面面了。</p><p>课程是由一位有趣、好玩的密歇根大学教授 CharlesSeverance 主讲的，也叫 Dr. Charles 。</p><p><img src="http://media.makcyun.top/19-1-14/61695215.jpg" alt=""></p><p><strong>为什么要把「有趣、好玩」放在最前面呢，因为这正是这门课最大的特点，也可以说是优势。</strong>Dr. Charles 是一位很面善的大叔，随和不故作高深，完全懂我们小白的心思。都说兴趣是最好的老师，但是有这样一位大师来培养你的兴趣，实在不能更好。</p><p><strong>在这门课你可以学到什么？</strong></p><h3 id="▌真正地喜欢上-Python"><a href="#▌真正地喜欢上-Python" class="headerlink" title="▌真正地喜欢上 Python"></a>▌真正地喜欢上 Python</h3><p>很多人在介绍 Python 这门语言的时候，基本都是很枯燥地乏味地这样说：「嗯，这门语言是 20 多年前由一个荷兰人发明的…」而 Dr. Charles 就不一样了，他把 Python 和哈利波特结合起来了，他编了个故事，说想去格兰芬多，结果被安排在了斯莱特林，他很郁闷地问别人为什么，学生告诉他，因为他教 Python （注：Python 另一层含义是蟒蛇）。说完这个笑话，他才开始介绍 Python 历史。</p><p>是不是很有意思，他从你最熟悉的东西开始，很自然地引导你进入一个新领域。</p><p><img src="http://media.makcyun.top/19-1-14/43258560.jpg" alt=" "></p><h3 id="▌计算机系统基础知识"><a href="#▌计算机系统基础知识" class="headerlink" title="▌计算机系统基础知识"></a>▌计算机系统基础知识</h3><p>在学习编程语言之前，他会先带你了解计算机系统的基本组成和运作原理。</p><p><img src="http://media.makcyun.top/19-1-14/81369306.jpg" alt=""></p><p><img src="http://media.makcyun.top/19-1-14/20056609.jpg" alt=""></p><p>是不是很形象生动？</p><h3 id="▌Python-基础"><a href="#▌Python-基础" class="headerlink" title="▌Python 基础"></a>▌Python 基础</h3><p><img src="http://media.makcyun.top/19-1-14/96775347.jpg" alt=""></p><p>Pyhton 相关的基础知识你都可以学到或者了解到，比如数据结构、循环、字符串、正则表达式等等，这些对于入门来说足够了。我就是在看了他讲解的循环语法，才搞懂 if、while、for 循环这些东西。</p><p>其他内容就不过多介绍了，感兴趣的话，可以直接看课程目录：</p><p><img src="http://media.makcyun.top/19-1-14/9784474.jpg" alt=""></p><h3 id="▌福利"><a href="#▌福利" class="headerlink" title="▌福利"></a>▌福利</h3><p>我当时为了学习这门系列课，特地跑到油管下载了全部课程，由于课程是英文字幕，看起来还是略有吃力，没办法英语差，所以费了九牛二虎之力，又找到了中文字幕，前面的视频截图你应该已经看到了。</p><p>如果你想入门 Pyhton 的话，这么课程实在是再适合不过了。现在呢，我就把这门课程的视频和配套电子书都分享给你，公众号后台回复：<strong>Python视频</strong>，就可以得到。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;入门 Python 最好的视频。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="Python入门" scheme="https://www.makcyun.top/tags/Python%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>样机，让你的 PPT 更有逼格</title>
    <link href="https://www.makcyun.top/2019/01/13/weekly_sharing12.html"/>
    <id>https://www.makcyun.top/2019/01/13/weekly_sharing12.html</id>
    <published>2019-01-13T08:16:16.000Z</published>
    <updated>2019-01-13T02:58:47.978Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>用 PS 和 PPT 轻松制作图片样机。</p><a id="more"></a><p><strong>摘要</strong>：用 PS 和 PPT 轻松制作图片样机。</p><p>这是「<strong>每周分享</strong>」的第 12 期， 也是今年的第 2 期。鉴于最近新加了很多朋友，所以我这里再介绍一下这个栏目，顾名思义，就是会在每个周末分享一篇 Python 以外的文章，主题涉及：软件、App、PPT、摄影、Ps 等几个方面。你可以在我公众号界面中的 「不务正业」菜单里集中查看，Python 文章则在另一个「不误正业」菜单中。</p><p>上一期，给大家分享了一套国外销量最牛逼的 PPT 作品，大伙儿反应很是强烈，光公众号后台回复「ppt模板」的消息就有好几百条，完全超出我的预料，看来大家都想做出一手有逼格的 PPT 啊。</p><p>话说回来，一套有逼格的 PPT 往往需要精雕细琢每一张幻灯片，然后再组合起来，形成统一的风格。但其实还有一点非常重要，也可以说是锦上添花的技巧，能够 <strong>让你 PPT 的逼格再上一层楼，那就是制作一张图片样机</strong>。</p><p>你不知道样机是什么？很可能你都见过。</p><p>在我们上一期介绍的 GracphicRiver 网站上，很多 PPT 作品都利用了样机进行展示，比如：</p><p><img src="http://media.makcyun.top/19-1-13/77024788.jpg" alt=""></p><p>样机能够直观地展示出你 PPT 的风格，这种 <strong>立体的展现形式比平面图片有更强的冲击力</strong>，很容易吸引别人注意力，简单做个对比就知道了。</p><p>这是我上一篇文章的样机封面：</p><p><img src="http://media.makcyun.top/19-1-6/76067695.jpg" alt=""></p><p>倘若不使用样机，而只是像下面这样，将 PPT 简单拼接起来，逼格是不是明显就矮了一截？</p><p><img src="http://media.makcyun.top/19-1-12/72834340.jpg" alt=""></p><p>其实，样机远不止上面一种样式，比如可以像下面这样：</p><p><img src="http://media.makcyun.top/19-1-12/62145000.jpg" alt=""></p><p>还可以像下面这样：</p><p><img src="http://media.makcyun.top/19-1-12/54897532.jpg" alt=""></p><p>怎么样，样机的效果不错吧，其实，样机除了酷炫以外还有实际的用途，比如你年终总结汇报的时候，在其他同事面前秀出来，绝对亮瞎老板和同事的眼；再比如有甲方找你做 PPT，做个这样的 DEMO 页发给过去，这单生意很可能就成了。</p><p>到这儿，你是不是也想自己弄个样机出来？</p><p>哈哈，其实制作方法比较简单，可以用 PS 也可以用 PPT 做，下面我就分别介绍一下。</p><p>在 PS 中做，说难也难，说简单也简单，最简单的方法就是直接在网上找一套不错的样机模板，然后替换成你 PPT 中的图片，几分钟就能搞定，我上面做的那张样机就是用 PS 做的。</p><p>比如我这里找了一套不错的样机素材，只需用 PS 打开素材文件，然后用图片填充每个智能对象，填充完成导出为图片就可以了，很 easy 对吧。</p><p><img src="http://media.makcyun.top/ps%E6%A0%B7%E6%9C%BA2.gif" alt=""></p><p><strong>可能你会说，劳资电脑没有 PS 还搞个锤子？</strong></p><p>没有问题，下面就介绍另外一种方法，就是用 PPT 制作，也比较简单，只需要三步。</p><p><strong>首先，复制并排列好多个矩形。</strong></p><p>你可以一个个去复制，但我推荐使用一款叫 iSlide 的 PPT 插件，它的「矩阵布局」功能够帮助你秒绘矩阵，当别人还在吭哧吭哧地复制和对齐时，你样机都做完了。需要注意的是，矩形尺寸比例最好为 16:9，因为现在最常用的的幻灯片尺寸是这个比例，</p><p><img src="http://media.makcyun.top/19-1-13/36468942.jpg" alt=""></p><p><strong>接着，设置三维旋转和立体阴影。</strong></p><p>矩阵复制好以后，选中全部然后按 Ctrl + G 快捷键合并所有矩形，以便后续统一设置三维旋转和阴影格式，这里的旋转和阴影参数，可以随意设置，不同的参数会产生不同的样机形式，所以你可以随意发挥。</p><p><img src="http://media.makcyun.top/19-1-13/50231351.jpg" alt=""></p><p><strong>最后，填充图片，生成样机。</strong></p><p>准备好 PPT 图片，然后复制图片，接着点击矩形进行填充，选择剪贴板就行了，就像下面这样。</p><p><img src="http://media.makcyun.top/ppt%E6%A0%B7%E6%9C%BA1.gif" alt=""></p><p><img src="http://media.makcyun.top/19-1-13/68214426.jpg" alt=""></p><p><strong>最后，完成制作，导出图片即可。</strong></p><p><img src="http://media.makcyun.top/19-1-13/89467734.jpg" alt=""></p><p>怎么样，样机制作是不是挺简单？哈哈</p><p>你可能并不想知道是怎么做的，只想知道怎么才能得到，我很懂你对吧。</p><p>老规矩，上面的 PS 和 PPT 素材为你准备好了，在我的公众号后台回复：<strong>ppt样机</strong>，就可以得到。</p><p><strong>不过，据说有打赏、留言和点好看的才能够得到全部素材。</strong></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/weekly_sharing11.html">国外最牛逼的一套 PPT 作品分享给你</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;用 PS 和 PPT 轻松制作图片样机。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="PPT" scheme="https://www.makcyun.top/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>5 行代码入门 Python 爬虫</title>
    <link href="https://www.makcyun.top/2019/01/10/web_scraping_withpython18.html"/>
    <id>https://www.makcyun.top/2019/01/10/web_scraping_withpython18.html</id>
    <published>2019-01-10T08:16:16.000Z</published>
    <updated>2019-01-10T23:52:48.746Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>5 行代码就能写一个 Python 爬虫。</p><a id="more"></a><p><strong>摘要</strong>：5 行代码就能写一个 Python 爬虫。</p><p>如果你是比较早关注我的话，会发现我此前的大部分文章都是在写 Python 爬虫，前后大概写了十几个爬虫实战案例，一直在埋头往前写，但却没有回到原点过，没有写过为什么要爬虫、爬虫难不难、怎么入门爬虫这些问题。另外，我觉得关注我的朋友中有不少是刚刚入门 Python 或者想学习 Python 的，为了更加友好一些，所以也有必要说一说这几个问题。基于这两点思考，今天就来谈谈 <strong>如何用快速入门爬虫</strong>。</p><p>先说结论：<strong>入门爬虫很容易，几行代码就可以，可以说是学习 Python 最简单的途径</strong>。</p><p>以我纯小白、零基础的背景来说，入门爬虫其实很容易，容易在代码编写很简单，简单的爬虫通常几行就能搞定，而不容易在确定爬虫的目标，也就是说为什么要去写爬虫，有没有必要用到爬虫，是不是手动操作几乎无法完成，互联网上有数以百万千万计的网站，到底以哪一个网站作为入门首选，这些问题才是难点。所以在动手写爬虫前，最好花一些时间想一想这清楚这些问题。</p><p>「Talk is cheap. Show me the code」，下面，就以我写过的一个爬虫为例，说一说如入门 Python 的几个步骤。</p><h3 id="▌确立目标"><a href="#▌确立目标" class="headerlink" title="▌确立目标"></a>▌确立目标</h3><p>第一步，确立目标。</p><p>这里，以我之前写的「爬取国内所有上市公司信息」为例，文章见：</p><p><a href="https://www.makcyun.top/web_scraping_withpython2.html">∞ 10 行代码爬取全国所有A股/港股/新三板上市公司信息</a></p><p>为什么当时想起写这个爬虫呢，是因为这是曾经在工作中想要解决的问题，当时不会爬虫，只能用 Excel 花了数个小时才勉强地把数据爬了下来， 所以在接触到爬虫后，第一个想法就是去实现曾未实现的目标。以这样的方式入门爬虫，好处显而易见，就是有了很明确的动力。<br>很多人学爬虫都是去爬网上教程中的那些网站，网站一样就算了，爬取的方法也一模一样，等于抄一遍，不是说这样无益，但是会容易导致动力不足，因为你没有带着目标去爬，只是为了学爬虫而爬，爬虫虽然是门技术活，但是如果能 <strong>建立在兴趣爱好或者工作任务的前提下，学习的动力就会强很多。</strong></p><p>在确定好爬虫目标后，接着我就在脑中预想了想要得到什么样的结果、如何展示出来、以什么形式展现这些问题。所以，我在爬取网站之前，就预先构想出了想要的一个结果，大致是下面这张图的样子。</p><p><img src="http://media.makcyun.top/19-1-10/9809664.jpg" alt=""></p><p>目标是利用爬下来的数据，尝试从不同维度年份、省份、城市去分析全国的股市信息，然后通过可视化图表呈现出来。</p><p>抛开数据，可能你会觉得这张图在排版布局、色彩搭配、字体文字等方面还挺好看的。这些呢，就跟爬虫没什么关系了，而跟审美有关，提升审美的一种方式是可以通过做 PPT 来实现：</p><p><a href="https://www.makcyun.top/weekly_sharing11.html">∞ 国外最牛逼的一套 PPT 作品分享给你</a></p><p>所以你看，咱们说着说着就从爬虫跳到了 PPT，不得不说我此前发的文章铺垫地很好啊，哈哈。其实，在职场中，你拥有的技能越多越好。</p><h3 id="▌直接开始"><a href="#▌直接开始" class="headerlink" title="▌直接开始"></a>▌直接开始</h3><p>确定了目标后，第二步就可以开始写爬虫了，如果你像我一样，之前没有任何编程基础，那我下面说的思路，可能会有用。</p><p><img src="http://media.makcyun.top/19-1-11/25331188.jpg" alt=""></p><p>刚开始动手写爬虫，我只关注最核心的部分，也就是先成功抓到数据，其他的诸如：下载速度、存储方式、代码条理性等先不管，这样的代码简短易懂、容易上手，能够增强信心。</p><p>所以，我在写第一遍的时候，只用了 5 行代码，就成功抓取了全部所需的信息，当时的感觉就是很爽，觉得爬虫不过如此啊，自信心爆棚。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">178</span>):  <span class="comment"># 爬取全部页</span></span><br><span class="line">tb = pd.read_html(<span class="string">'http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s'</span> % (str(i)))[<span class="number">3</span>] </span><br><span class="line">tb.to_csv(<span class="string">r'1.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="number">1</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>3000+ 上市公司的信息，安安静静地躺在 Excel 中：</p><p><img src="http://media.makcyun.top/18-8-27/96662344.jpg" alt=""></p><h3 id="▌不断完善"><a href="#▌不断完善" class="headerlink" title="▌不断完善"></a>▌不断完善</h3><p>有了上面的信心后，我开始继续完善代码，因为 5 行代码太单薄，功能也太简单，大致从以下几个方面进行了完善：</p><ul><li>增加异常处理</li></ul><p>由于爬取上百页的网页，中途很可能由于各种问题导致爬取失败，所以增加了 try except 、if 等语句，来处理可能出现的异常，让代码更健壮。</p><ul><li>增加代码灵活性</li></ul><p>初版代码由于固定了 URL 参数，所以只能爬取固定的内容，但是人的想法是多变的，一会儿想爬这个一会儿可能又需要那个，所以可以通过修改 URL 请求参数，来增加代码灵活性，从而爬取更灵活的数据。</p><ul><li>修改存储方式</li></ul><p>初版代码我选择了存储到 Excel 这种最为熟悉简单的方式，人是一种惰性动物，很难离开自己的舒适区。但是为了学习新知识，所以我选择将数据存储到 MySQL 中，以便练习 MySQL 的使用。</p><ul><li><p>加快爬取速度</p><p>初版代码使用了最简单的单进程爬取方式，爬取速度比较慢，考虑到网页数量比较大，所以修改为了多进程的爬取方式。</p></li></ul><p>经过以上这几点的完善，代码量从原先的 5 行增加到了下面的几十行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode  <span class="comment"># 编码 URL 字符串</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,</span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">return</span> tbl</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,</span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,</span><br><span class="line">port=<span class="number">3306</span>,</span><br><span class="line">charset = <span class="string">'utf8'</span>,  </span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company2'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># append表示在原有表基础上增加，但该表要有表头</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    generate_mysql()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):  </span><br><span class="line">html = get_one_page(i)</span><br><span class="line">tbl = parse_one_page(html)</span><br><span class="line">write_to_sql(tbl)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)</span><br><span class="line">endtime = time.time()-start_time</span><br><span class="line">print(<span class="string">'程序运行了%.2f秒'</span> %endtime)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"> pool = Pool(<span class="number">4</span>)</span><br><span class="line"> pool.map(main, [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">178</span>)])  <span class="comment">#共有178页</span></span><br><span class="line">endtime = time.time()-start_time</span><br><span class="line">print(<span class="string">'程序运行了%.2f秒'</span> %(time.time()-start_time))</span><br></pre></td></tr></table></figure><p>但是这个过程却觉得很自然，因为每次修改都是针对一个小点，一点点去学，搞懂后添加进来，而如果让我上来就直接写出这几十行的代码，我很可能就放弃了。</p><p>所以，你可以看到，入门爬虫是有套路的，最重要的是给自己信心。</p><p>以上，我从一个小点结合一个实例，介绍了入门学习爬虫的方法，希望对你有用。当然还有其他点，之后再说。</p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython2.html">∞ 10 行代码爬取全国所有 A股/港股/新三板上市公司信息</a></p><p><a href="https://www.makcyun.top/weekly_sharing11.html">∞ 国外最牛逼的一套 PPT 作品分享给你</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;5 行代码就能写一个 Python 爬虫。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>国外最牛逼的一套 PPT 作品分享给你</title>
    <link href="https://www.makcyun.top/2019/01/05/weekly_sharing11.html"/>
    <id>https://www.makcyun.top/2019/01/05/weekly_sharing11.html</id>
    <published>2019-01-05T08:16:16.000Z</published>
    <updated>2019-01-06T08:14:01.255Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>GraphicRiver 上销量排名第一的 PPT 作品送给你 。</p><a id="more"></a><p><strong>摘要</strong>：GraphicRiver 上销量排名第一的 PPT 作品送给你。</p><p>这是「<strong>每周分享</strong>」的第 11 期， 也是今年的第 1 期，新年来点不一样的，聊聊此前都没有说过的主题：「<strong>PPT</strong>」。</p><p>我呢，由于个人兴趣，常混迹于编程圈和 PPT 圈，一段时间观察下来，发现了两点有意思的现象：</p><ul><li><strong>PPT 做得好的人不多</strong></li></ul><p>PPT 几乎人人都会，也是职场中一项非常重要的技能，但是 PPT 做得拿得出手的人却不多。通过一个简单的现象就可以看出来，早在去年我就尝鲜了 PPT 2019 版，但不少人仍然还在用着 2010 版以下的版本，虽然使用 PPT 版本的高低和 PPT 做得好不好没有必然关系，但是使用新的版本至少能够证明一点：<strong>你愿意去尝试，有尝试才有可能</strong>。新版本的 PPT 提供了非常多实用的工具，例如 2016版才有的「变体」功能 。（本文说的都是 PPT，不涉及 Keynote）</p><ul><li><strong>会写代码 PPT 做得又好的人更少</strong></li></ul><p>我看了很多编程方面的文字和视频教程，发现很多 PPT 课件都做得很辣眼睛，不少程序员对 PPT 并不以为然，可能是时间精力都用去写代码了，也可能是觉得 PPT 太 easy。不管怎么说，我认为：<strong>程序员如果能掌握 PPT 技能，会是一种不错的竞争力。</strong></p><p>如果你是学生，答辩的时候 show 出惊艳的 PPT ，说不定就能拿个优秀论文；如果你是职场小白，年终总结的时候亮出漂亮的 PPT ，来年升职加薪不是梦。</p><p>当然，我这里所说的「<strong>做出好 PPT</strong>」，不是说一定要达到专业 PPT 设计师的那种，<strong>只要能够比你周围大多数人做得好那么一点就行</strong>。</p><p>那么问题来了，如果目前自认是 PPT 小白，很想提升 PPT 技能怎么办?</p><p>根据这几年我入门 PPT 以来的经验，觉得对于刚刚入门 PPT 的人来说，最重要的是先学会：<strong>欣赏</strong>。至少能够分辨出哪些是好 PPT ，哪些不是，先提升审美再动手去做，多看才会知道自己做的 PPT 不好在哪里，然后针对性地去改进。</p><p>那么问题又来了，到底哪些算是好 PPT 哪些又不是呢? 我觉得有两点可以参考：</p><ul><li><strong>多看看国外的 PPT 作品</strong></li></ul><p>不是说国外的就一定比国内的好，但国外网站上遇到好 PPT 的概率要比国内的高，<strong>能节省你筛选的时间</strong>。</p><ul><li><strong>那些免费的、动不动送几十 G 资源包的基本是垃圾</strong></li></ul><p>这种现象很常见，不只是 PPT 资料，很多公众号都爱搞：关注公众号免费送一百 G 精华电子书和视频，这样的福利，先不说质量到底怎么样，这么多东西，你收藏了之后也基本不会再去看的，所以没有意义。<strong>真正好的 PPT 必然是花费了很多心血的</strong>，又免费又大量，是不可能的。</p><p>你可能会觉得：「这些我早都知道了，还用得着你说?」咳咳，上面说的那些都是为下面的干货做铺垫。</p><p>我刚才说多上国外的 PPT 设计网站看看，这其中就有一个佼佼者叫 <strong><a href="https://graphicriver.net/presentation-templates/powerpoint-templates" target="_blank" rel="noopener">「GraphicRiver」</a></strong>，该网站汇集了世界上最优秀设计师的 PPT 作品，水准非常高，基本都是收费的，一套十几到几十刀不等。</p><p>随意看看上面的一些作品：</p><p><img src="http://media.makcyun.top/19-1-6/45704306.jpg" alt=""></p><p>是不是开始怀疑人生，都是用 PPT 做，差距咋那么大呢。</p><p>在网站近万套作品中，销量最牛逼的是一套叫<a href="https://graphicriver.net/item/motagua-multipurpose-powerpoint-template/10348960?s_rank=1" target="_blank" rel="noopener">「MOTAGUA」</a>的作品。</p><p><img src="http://media.makcyun.top/19-1-6/1105792.jpg" alt=""></p><p>这套幻灯片销量如此之高，必然是有道理的，我们来看看从这套模板中可以学习到哪些东西：</p><h3 id="▌幻灯片板式"><a href="#▌幻灯片板式" class="headerlink" title="▌幻灯片板式"></a>▌幻灯片板式</h3><p><strong>幻灯片版式是 PPT 的框架，做 PPT 最重要的就是搭框架</strong>，也就是确定每张幻灯片的版式，在这套 PPT 里可以学习到很多版式设计的技巧，比如上下、左右、居中、文字、图片等版式的搭配。</p><p><img src="http://media.makcyun.top/19-1-6/65250476.jpg" alt=""></p><h3 id="▌逻辑关系"><a href="#▌逻辑关系" class="headerlink" title="▌逻辑关系"></a>▌逻辑关系</h3><p><strong>质量上乘的 PPT ，背后是有很强大的内在逻辑关系支撑的</strong>，比如总分、并列、对比、递进等关系。确定了版式后，就可以对幻灯片元素进行逻辑设计，一种不错的设计方式就是使用一些视觉关系管来呈现，模板里面提供了大量的逻辑关系图可以用来参考。</p><p><img src="http://media.makcyun.top/19-1-6/76605904.jpg" alt=""></p><h3 id="▌配色方案"><a href="#▌配色方案" class="headerlink" title="▌配色方案"></a>▌配色方案</h3><p><strong>配色是 PPT设计中非常重要的一环</strong>。很多 PPT 看起来很辣眼睛，往往是配色没搞好，毕竟我们大多数人都没有系统学过色彩理论基础，譬如：同类色、互补色、主色、辅色等等，但没学过不代表搭配不出一套专业的配色，有一种非常简单的方式就是 <strong>提取专业 PPT 中的配色</strong>。这套作品中提供了 多达 60 种配色方案，什么商务风、简约风、欧美风都可以轻松驾驭，PPT 中点点鼠标就能秒变各种配色方案，非常简单。</p><p><img src="http://media.makcyun.top/19-1-6/46589308.jpg" alt=""></p><h3 id="▌图标、地图素材"><a href="#▌图标、地图素材" class="headerlink" title="▌图标、地图素材"></a>▌图标、地图素材</h3><p>有时候我们的 PPT 看起来平淡无奇的另一个重要原因就是 <strong>缺少可视化元素</strong>，全是文字堆砌在那儿。</p><p>解决的方法也很简单，就是尝试把文字替换为图标、地图等可视化素材，让 PPT 看起来更为丰富多彩。作品中也提供了大量的矢量图标和世界上主要国家的地图，支持任意编辑和放大，可以拿来用在我们自己的 PPT 作品中。</p><p><img src="http://media.makcyun.top/19-1-6/15349907.jpg" alt=""></p><p><img src="http://media.makcyun.top/19-1-6/52555349.jpg" alt=""></p><p>总结一下，<strong>拿到一套 PPT 作品，正确的使用思路是学会分解</strong>，可以选择从上面那几个方面去尝试，多关注自己薄弱的地方，比如版式、逻辑、配色、可视化元素这些地方，然后针对性地模仿学习，这样才能充分利用到 PPT 作品的价值，从而快速提升，千万不要想着一口气就可以把 PPT 做得很牛逼。</p><p>很多人不会正确使用 PPT 模板，老想着找一套可以完全进行套用的 PPT 模板，这几乎是不现实的，除非私人定制，因为每个人在做 PPT 时的想法是不一样的。有些人找不到一套可以全套用的，就花很多时间找了几套不错的进行拼凑，结果做出来四不像，不忍直视，最后不得不放弃，自己重新来做，陷入死循环。</p><p>总之，完全套用别人的东西，对于自身 PPT 水平的提高是没有一点益处的，久而久之会发现自己还是原地踏步。</p><p>知道你等了很久，最后，我把这套珍藏了很久的 PPT 作品送给你，在我的公众号后台回复：<strong>ppt模板</strong>，就可以得到。如果想得到全部完整版，可以长按下方二维码加入我的知识星球得到，专享全年所有干货，越早加入越划算。</p><p><img src="http://media.makcyun.top/19-1-1/31534576.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/weekly_sharing10.html">你的手机截图真丑</a></p><p><a href="https://www.makcyun.top/weekly_sharing7.html">盘点那些手机上绝对值得安装的 App</a></p><p><a href="https://www.makcyun.top/weekly_sharing3.html">安卓最强阅读器</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;GraphicRiver 上销量排名第一的 PPT 作品送给你 。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="PPT" scheme="https://www.makcyun.top/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>以豌豆荚为例，用 Scrapy 爬取分类多级页面</title>
    <link href="https://www.makcyun.top/2019/01/02/web_scraping_withpython17.html"/>
    <id>https://www.makcyun.top/2019/01/02/web_scraping_withpython17.html</id>
    <published>2019-01-02T08:16:24.000Z</published>
    <updated>2019-01-03T05:12:23.500Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>使用 Scrapy 爬取豌豆荚全网 70000+ App。</p><a id="more"></a><p><strong>摘要</strong>：使用 Scrapy 爬取豌豆荚全网 70000+ App，并进行探索性分析。</p><p><strong>写在前面</strong>：若对数据抓取部分不感兴趣，可以直接下拉到数据分析部分。</p><h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1 分析背景"></a>1 分析背景</h2><p>之前我们使用了 Scrapy 爬取并分析了酷安网 6000+ App，为什么这篇文章又在讲抓 App 呢?</p><p>因为我喜欢折腾 App，哈哈。当然，主要是因为下面这几点：</p><p><strong>第一、之前抓取的网页很简单</strong></p><p>在抓取酷安网时，我们使用 for 循环，遍历了几百页就完成了所有内容的抓取，非常简单，但现实往往不会这么 easy，有时我们要抓的内容会比较庞大，比如抓取整个网站的数据，为了增强爬虫技能，所以本文选择了「豌豆荚」这个网站。</p><p>目标是： <strong>爬取该网站所有分类下的 App 信息并下载 App 图标</strong>，数量在 <strong>70,000</strong> 左右，比酷安升了一个数量级。</p><p><strong>第二、再次练习使用强大的 Scrapy 框架</strong></p><p>之前只是初步地使用了 Scrapy 进行抓取，还没有充分领会到 Scrapy 有多么牛逼，所以本文尝试深入使用 Scrapy，增加随机 UserAgent、代理 IP 和图片下载等设置。</p><p><strong>第三、对比一下酷安和豌豆荚两个网站</strong></p><p>相信很多人都在使用豌豆荚下载 App，我则使用酷安较多，所以也想比较一下这两个网站的 App 特点。</p><p>话不多说，下面开始抓取流程。</p><h3 id="▌分析目标"><a href="#▌分析目标" class="headerlink" title="▌分析目标"></a>▌分析目标</h3><p>首先，我们先来了解一下要抓取的豌豆荚网页是什么样的，可以看到该网站上的 App 分成了很多类，包括：「应用播放」、「系统工具」等，一共有 14 个大类别，每个大类下又细分了多个小类，例如，影音播放下包括：「视频」、「直播」等。</p><p><img src="http://media.makcyun.top/19-1-3/91274061.jpg" alt=""></p><p>点击「视频」进入第二级子类页面，可以看到每款 App 的部分信息，包括：图标、名称、安装数量、体积、评论等。</p><p><img src="http://media.makcyun.top/19-1-3/97869343.jpg" alt=""></p><p>在之前的一篇文章中（见下方链接），我们分析了这个页面：采用 AJAX 加载，GET 请求，参数很容易构造，但是具体页数不确定，最后分别使用了 For 和 While 循环抓取了所有页数的数据。</p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ Python For 和 While 循环爬取不确定页数的网页</a></p><p>接着，我们可以再进入第三级页面，也就是每款 App 的详情页，可以看到多了下载数、好评率、评论数这几样参数，抓取思路和第二级页面大同小异，同时为了减小网站压力，所以 App 详情页就不抓取了。</p><p><img src="http://media.makcyun.top/19-1-3/93718642.jpg" alt=""></p><p>所以，<strong>这是一个分类多级页面的抓取问题，依次抓取每一个大类下的全部子类数据。</strong></p><p>学会了这种抓取思路，很多网站我们都可以去抓，比如很多人爱爬的「豆瓣电影」也是这样的结构。</p><p><img src="http://media.makcyun.top/19-1-3/68220890.jpg" alt=""></p><h3 id="▌分析内容"><a href="#▌分析内容" class="headerlink" title="▌分析内容"></a>▌分析内容</h3><p>数据抓取完成后，本文主要是对分类型数据的进行简单的探索性分析，包括这么几个方面：</p><ul><li>下载量最多 / 最少的 App 总排名</li><li>下载量最多 / 最少的 App 分类 / 子分类排名</li><li>App 下载量区间分布</li><li>App 名称重名的有多少</li><li>和酷安 App 进行对比</li></ul><h3 id="▌分析工具"><a href="#▌分析工具" class="headerlink" title="▌分析工具"></a>▌分析工具</h3><ul><li>Python</li><li>Scrapy</li><li>MongoDB</li><li>Pyecharts</li><li>Matplotlib</li></ul><h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2 数据抓取"></a>2 数据抓取</h2><h3 id="▌网站分析"><a href="#▌网站分析" class="headerlink" title="▌网站分析"></a>▌网站分析</h3><p>我们刚才已经初步对网站进行了分析，大致思路可以分为两步，首先是提取所有子类的 URL 链接，然后分别抓取每个 URL 下的 App 信息就行了。</p><p><img src="http://media.makcyun.top/19-1-3/4206503.jpg" alt=""></p><p>可以看到，子类的 URL 是由两个数字构成，前面的数字表示分类编号，后面的数字表示子分类编号，得到了这两个编号，就可以抓取该分类下的所有 App 信息，那么怎么获取这两个数值代码呢?</p><p>回到分类页面，定位查看信息，可以看到分类信息都包裹在每个 li 节点中，子分类 URL 则又在子节点 a 的 href 属性中，<strong>大分类一共有 14 个，子分类一共有 88 个</strong>。</p><p><img src="http://media.makcyun.top/19-1-3/90802300.jpg" alt=""></p><p>到这儿，思路就很清晰了，我们可以用 CSS 提取出全部子分类的 URL，然后分别抓取所需信息即可。</p><p>另外还需注意一点，该网站的 <strong>首页信息是静态加载的，从第 2 页开始是采用了 Ajax 动态加载</strong>，URL 不同，需要分别进行解析提取。</p><h3 id="▌Scrapy抓取"><a href="#▌Scrapy抓取" class="headerlink" title="▌Scrapy抓取"></a>▌Scrapy抓取</h3><p>我们要爬取两部分内容，一是 APP 的数据信息，包括前面所说的：名称、安装数量、体积、评论等，二是下载每款 App 的图标，分文件夹进行存放。</p><p>由于该网站有一定的反爬措施，所以我们需要添加随机 UA 和代理 IP，关于这两个知识点，我此前单独写了两篇文章进行铺垫，传送门：</p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ Scrapy 中设置随机 User-Agent 的方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython15.html">∞ Python 爬虫的代理 IP 设置方法汇总</a></p><p>这里随机 UA 使用 <strong>scrapy-fake-useragent </strong>库，一行代码就能搞定，代理 IP 直接上阿布云付费代理，几块钱搞定简单省事。</p><p>下面，就直接上代码了：</p><h4 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a><strong>items.py</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WandoujiaItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    cate_name = scrapy.Field() <span class="comment">#分类名</span></span><br><span class="line">    child_cate_name = scrapy.Field() <span class="comment">#分类编号</span></span><br><span class="line">    app_name = scrapy.Field()   <span class="comment"># 子分类名</span></span><br><span class="line">    install = scrapy.Field()    <span class="comment"># 子分类编号</span></span><br><span class="line">    volume = scrapy.Field()     <span class="comment"># 体积</span></span><br><span class="line">    comment = scrapy.Field()    <span class="comment"># 评论</span></span><br><span class="line">    icon_url = scrapy.Field()   <span class="comment"># 图标url</span></span><br></pre></td></tr></table></figure><h4 id="middles-py"><a href="#middles-py" class="headerlink" title="middles.py"></a><strong>middles.py</strong></h4><p>中间件主要用于设置代理 IP。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line">proxyServer = <span class="string">"http://http-dyn.abuyun.com:9020"</span></span><br><span class="line">proxyUser = <span class="string">"你的信息"</span></span><br><span class="line">proxyPass = <span class="string">"你的信息"</span></span><br><span class="line"></span><br><span class="line">proxyAuth = <span class="string">"Basic "</span> + base64.urlsafe_b64encode(bytes((proxyUser + <span class="string">":"</span> + proxyPass), <span class="string">"ascii"</span>)).decode(<span class="string">"utf8"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbuyunProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = proxyServer</span><br><span class="line">        request.headers[<span class="string">"Proxy-Authorization"</span>] = proxyAuth</span><br><span class="line">        logging.debug(<span class="string">'Using Proxy:%s'</span>%proxyServer)</span><br></pre></td></tr></table></figure><h4 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a><strong>pipelines.py</strong></h4><p>该文件用于存储数据到 MongoDB 和下载图标到分类文件夹中。</p><p>存储到 MongoDB：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MongoDB 存储</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,mongo_url,mongo_db)</span>:</span></span><br><span class="line">        self.mongo_url = mongo_url</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_url = crawler.settings.get(<span class="string">'MONGO_URL'</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_url)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        <span class="comment"># self.db[name].insert(dict(item))</span></span><br><span class="line">        self.db[name].update_one(item, &#123;<span class="string">'$set'</span>: item&#125;, upsert=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure><p>按文件夹下载图标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分文件夹下载</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagedownloadPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self,item,info)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'icon_url'</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(item[<span class="string">'icon_url'</span>],meta=&#123;<span class="string">'item'</span>:item&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">        name = request.meta[<span class="string">'item'</span>][<span class="string">'app_name'</span>]</span><br><span class="line">        cate_name = request.meta[<span class="string">'item'</span>][<span class="string">'cate_name'</span>]</span><br><span class="line">        child_cate_name = request.meta[<span class="string">'item'</span>][<span class="string">'child_cate_name'</span>]</span><br><span class="line">      </span><br><span class="line">        path1 = <span class="string">r'/wandoujia/%s/%s'</span> %(cate_name,child_cate_name)</span><br><span class="line">        path = <span class="string">r'&#123;&#125;\&#123;&#125;.&#123;&#125;'</span>.format(path1, name, <span class="string">'jpg'</span>)</span><br><span class="line">        <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self,results,item,info)</span>:</span></span><br><span class="line">        image_path = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok,x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_path:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'Item contains no images'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h4 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a><strong>settings.py</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">BOT_NAME = <span class="string">'wandoujia'</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">'wandoujia.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'wandoujia.spiders'</span></span><br><span class="line"></span><br><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'wandoujia'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否遵循机器人规则</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br><span class="line"><span class="comment"># 下载设置延迟 由于买的阿布云一秒只能请求5次，所以每个请求设置了 0.2s延迟</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>,</span><br><span class="line">    <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">100</span>, <span class="comment"># 随机UA</span></span><br><span class="line">    <span class="string">'wandoujia.middlewares.AbuyunProxyMiddleware'</span>: <span class="number">200</span> <span class="comment"># 阿布云代理</span></span><br><span class="line">    ）</span><br><span class="line">    </span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'wandoujia.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">'wandoujia.pipelines.ImagedownloadPipeline'</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment"># URL不去重</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy.dupefilters.BaseDupeFilter'</span></span><br></pre></td></tr></table></figure><h4 id="wandou-py"><a href="#wandou-py" class="headerlink" title="wandou.py"></a><strong>wandou.py</strong></h4><p>主程序这里列出关键的部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.cate_url = <span class="string">'https://www.wandoujia.com/category/app'</span></span><br><span class="line">        <span class="comment"># 子分类首页url</span></span><br><span class="line">        self.url = <span class="string">'https://www.wandoujia.com/category/'</span></span><br><span class="line">        <span class="comment"># 子分类 ajax请求页url</span></span><br><span class="line">        self.ajax_url = <span class="string">'https://www.wandoujia.com/wdjweb/api/category/more?'</span></span><br><span class="line">        <span class="comment"># 实例化分类标签</span></span><br><span class="line">        self.wandou_category = Get_category()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.cate_url,callback=self.get_category)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self,response)</span>:</span>    </span><br><span class="line">        cate_content = self.wandou_category.parse_category(response)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>这里，首先定义几个 URL，包括：分类页面、子分类首页、子分类 AJAX 页，也就是第 2 页开始的 URL，然后又定义了一个类 Get_category() 专门用于提取全部的子分类 URL，稍后我们将展开该类的代码。</p><p>程序从 start_requests 开始运行，解析首页获得响应，调用 get_category() 方法，然后使用 Get_category() 类中的 parse_category() 方法提取出所有 URL，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Get_category</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_category</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        category = response.css(<span class="string">'.parent-cate'</span>)</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'cate_name'</span>: item.css(<span class="string">'.cate-link::text'</span>).extract_first(),</span><br><span class="line">            <span class="string">'cate_code'</span>: self.get_category_code(item),</span><br><span class="line">            <span class="string">'child_cate_codes'</span>: self.get_child_category(item),</span><br><span class="line">        &#125; <span class="keyword">for</span> item <span class="keyword">in</span> category]</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取所有主分类标签数值代码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_category_code</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        cate_url = item.css(<span class="string">'.cate-link::attr("href")'</span>).extract_first()</span><br><span class="line">        pattern = re.compile(<span class="string">r'.*/(\d+)'</span>)  <span class="comment"># 提取主类标签代码</span></span><br><span class="line">        cate_code = re.search(pattern, cate_url)</span><br><span class="line">        <span class="keyword">return</span> cate_code.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有子分类名称和编码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_child_category</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        child_cate = item.css(<span class="string">'.child-cate a'</span>)</span><br><span class="line">        child_cate_url = [&#123;</span><br><span class="line">            <span class="string">'child_cate_name'</span>: child.css(<span class="string">'::text'</span>).extract_first(),</span><br><span class="line">            <span class="string">'child_cate_code'</span>: self.get_child_category_code(child)</span><br><span class="line">        &#125; <span class="keyword">for</span> child <span class="keyword">in</span> child_cate]</span><br><span class="line">        <span class="keyword">return</span> child_cate_url</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正则提取子分类编码</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_child_category_code</span><span class="params">(self, child)</span>:</span></span><br><span class="line">        child_cate_url = child.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">        pattern = re.compile(<span class="string">r'.*_(\d+)'</span>)  <span class="comment"># 提取小类标签编号</span></span><br><span class="line">        child_cate_code = re.search(pattern, child_cate_url)</span><br><span class="line">        <span class="keyword">return</span> child_cate_code.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里，除了分类名称 cate_name 可以很方便地直接提取出来，分类编码和子分类的子分类的名称和编码，我们使用了 get_category_code() 等三个方法进行提取。提取方法使用了 CSS 和正则表达式，比较简单。</p><p>最终提取的分类名称和编码结果如下，利用这些编码，我们就可以构造 URL 请求开始提取每个子分类下的 App 信息了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'cate_name'</span>: <span class="string">'影音播放'</span>, <span class="string">'cate_code'</span>: <span class="string">'5029'</span>, <span class="string">'child_cate_codes'</span>: [</span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'视频'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'716'</span>&#125;, </span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'直播'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'1006'</span>&#125;, </span><br><span class="line">    ...</span><br><span class="line">]&#125;, </span><br><span class="line">&#123;<span class="string">'cate_name'</span>: <span class="string">'系统工具'</span>, <span class="string">'cate_code'</span>: <span class="string">'5018'</span>, <span class="string">'child_cate_codes'</span>: [</span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'WiFi'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'895'</span>&#125;, </span><br><span class="line">    &#123;<span class="string">'child_cate_name'</span>: <span class="string">'浏览器'</span>, <span class="string">'child_cate_code'</span>: <span class="string">'599'</span>&#125;, </span><br><span class="line">    ...</span><br><span class="line">]&#125;, </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接着前面的 get_category() 继续往下写，提取 App 的信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self,response)</span>:</span>    </span><br><span class="line">        cate_content = self.wandou_category.parse_category(response)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> cate_content:</span><br><span class="line">            child_cate = item[<span class="string">'child_cate_codes'</span>]</span><br><span class="line">            <span class="keyword">for</span> cate <span class="keyword">in</span> child_cate:</span><br><span class="line">                cate_code = item[<span class="string">'cate_code'</span>]</span><br><span class="line">                cate_name = item[<span class="string">'cate_name'</span>]</span><br><span class="line">                child_cate_code = cate[<span class="string">'child_cate_code'</span>]</span><br><span class="line">                child_cate_name = cate[<span class="string">'child_cate_name'</span>]</span><br><span class="line">                </span><br><span class="line">                page = <span class="number">1</span> <span class="comment"># 设置爬取起始页数</span></span><br><span class="line">                <span class="keyword">if</span> page == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 构造首页url</span></span><br><span class="line">                    category_url = <span class="string">'&#123;&#125;&#123;&#125;_&#123;&#125;'</span> .format(self.url, cate_code, child_cate_code)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    params = &#123;</span><br><span class="line">                        <span class="string">'catId'</span>: cate_code,  <span class="comment"># 类别</span></span><br><span class="line">                        <span class="string">'subCatId'</span>: child_cate_code,  <span class="comment"># 子类别</span></span><br><span class="line">                        <span class="string">'page'</span>: page,</span><br><span class="line">                        &#125;</span><br><span class="line">                    category_url = self.ajax_url + urlencode(params)</span><br><span class="line">                dict = &#123;<span class="string">'page'</span>:page,<span class="string">'cate_name'</span>:cate_name,<span class="string">'cate_code'</span>:cate_code,<span class="string">'child_cate_name'</span>:child_cate_name,<span class="string">'child_cate_code'</span>:child_cate_code&#125;</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(category_url,callback=self.parse,meta=dict)</span><br></pre></td></tr></table></figure><p>这里，依次提取出全部的分类名称和编码，用于构造请求的 URL。由于首页的 URL 和第 2 页开始的 URL 形式不同，所以使用了 if 语句分别进行构造。接下来，请求该 URL 然后调用 self.parse() 方法进行解析，这里使用了 meta 参数用于传递相关参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(response.body) &gt;= <span class="number">100</span>:  <span class="comment"># 判断该页是否爬完，数值定为100是因为无内容时长度是87</span></span><br><span class="line">            page = response.meta[<span class="string">'page'</span>]</span><br><span class="line">            cate_name = response.meta[<span class="string">'cate_name'</span>]</span><br><span class="line">            cate_code = response.meta[<span class="string">'cate_code'</span>]</span><br><span class="line">            child_cate_name = response.meta[<span class="string">'child_cate_name'</span>]</span><br><span class="line">            child_cate_code = response.meta[<span class="string">'child_cate_code'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> page == <span class="number">1</span>:</span><br><span class="line">                contents = response</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                jsonresponse = json.loads(response.body_as_unicode())</span><br><span class="line">                contents = jsonresponse[<span class="string">'data'</span>][<span class="string">'content'</span>]</span><br><span class="line">                <span class="comment"># response 是json,json内容是html，html 为文本不能直接使用.css 提取，要先转换</span></span><br><span class="line">                contents = scrapy.Selector(text=contents, type=<span class="string">"html"</span>)</span><br><span class="line"></span><br><span class="line">            contents = contents.css(<span class="string">'.card'</span>)</span><br><span class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">                <span class="comment"># num += 1</span></span><br><span class="line">                item = WandoujiaItem()</span><br><span class="line">                item[<span class="string">'cate_name'</span>] = cate_name</span><br><span class="line">                item[<span class="string">'child_cate_name'</span>] = child_cate_name</span><br><span class="line">                item[<span class="string">'app_name'</span>] = self.clean_name(content.css(<span class="string">'.name::text'</span>).extract_first())  </span><br><span class="line">                item[<span class="string">'install'</span>] = content.css(<span class="string">'.install-count::text'</span>).extract_first()</span><br><span class="line">                item[<span class="string">'volume'</span>] = content.css(<span class="string">'.meta span:last-child::text'</span>).extract_first()</span><br><span class="line">                item[<span class="string">'comment'</span>] = content.css(<span class="string">'.comment::text'</span>).extract_first().strip()</span><br><span class="line">                item[<span class="string">'icon_url'</span>] = self.get_icon_url(content.css(<span class="string">'.icon-wrap a img'</span>),page)</span><br><span class="line">                <span class="keyword">yield</span> item</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 递归爬下一页</span></span><br><span class="line">            page += <span class="number">1</span></span><br><span class="line">            params = &#123;</span><br><span class="line">                    <span class="string">'catId'</span>: cate_code,  <span class="comment"># 大类别</span></span><br><span class="line">                    <span class="string">'subCatId'</span>: child_cate_code,  <span class="comment"># 小类别</span></span><br><span class="line">                    <span class="string">'page'</span>: page,</span><br><span class="line">                    &#125;</span><br><span class="line">            ajax_url = self.ajax_url + urlencode(params)</span><br><span class="line">            dict = &#123;<span class="string">'page'</span>:page,<span class="string">'cate_name'</span>:cate_name,<span class="string">'cate_code'</span>:cate_code,<span class="string">'child_cate_name'</span>:child_cate_name,<span class="string">'child_cate_code'</span>:child_cate_code&#125;</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(ajax_url,callback=self.parse,meta=dict)</span><br></pre></td></tr></table></figure><p>最后，parse() 方法用来解析提取最终我们需要的 App 名称、安装量等信息，解析完成一页后，page 进行递增，然后重复调用 parse() 方法循环解析，直到解析完全部分类的最后一页。</p><p>最终，几个小时后，我们就可以完成全部 App 信息的抓取，我这里得到 73,755 条信息和 72,150 个图标，两个数值不一样是因为有些 App 只有信息没有图标。</p><p><img src="http://media.makcyun.top/19-1-2/26900812.jpg" alt=""></p><p>图标下载：</p><p><img src="http://media.makcyun.top/18-12-14/2503305.jpg" alt=""></p><p>下面将对提取的信息，进行的数据分析。</p><h2 id="3-数据分析"><a href="#3-数据分析" class="headerlink" title="3 数据分析"></a>3 数据分析</h2><h3 id="▌总体情况"><a href="#▌总体情况" class="headerlink" title="▌总体情况"></a>▌总体情况</h3><p>首先来看一下 App 的安装量情况，毕竟 70000 多款 App，自然很感兴趣 <strong>哪些 App 使用地最多，哪些又使用地最少</strong>。</p><p><img src="http://media.makcyun.top/19-1-3/13256282.jpg" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">colors = <span class="string">'#6D6D6D'</span> <span class="comment">#字体颜色</span></span><br><span class="line">colorline = <span class="string">'#63AB47'</span>  <span class="comment">#红色CC2824  #豌豆荚绿</span></span><br><span class="line">fontsize_title = <span class="number">20</span></span><br><span class="line">fontsize_text = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载量总排名</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis_maxmin</span><span class="params">(data)</span>:</span></span><br><span class="line">    data_max = (data[:<span class="number">10</span>]).sort_values(by=<span class="string">'install_count'</span>)</span><br><span class="line">    data_max[<span class="string">'install_count'</span>] = (data_max[<span class="string">'install_count'</span>] / <span class="number">100000000</span>).round(<span class="number">1</span>)</span><br><span class="line">    data_max.plot.barh(x=<span class="string">'app_name'</span>,y=<span class="string">'install_count'</span>,color=colorline)</span><br><span class="line">    <span class="keyword">for</span> y, x <span class="keyword">in</span> enumerate(list((data_max[<span class="string">'install_count'</span>]))):</span><br><span class="line">        plt.text(x + <span class="number">0.1</span>, y - <span class="number">0.08</span>, <span class="string">'%s'</span> %</span><br><span class="line">                 round(x, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'安装量最多的 10 款 App ?'</span>,color=colors)</span><br><span class="line">    plt.xlabel(<span class="string">'下载量(亿次)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'App'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment"># plt.savefig('安装量最多的App.png',dpi=200)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>看了上图，有两个「<strong>没想到</strong>」：</p><ul><li><p><strong>排名第一的居然是一款手机管理软件</strong></p><p>对豌豆荚网上的这个第一名感到意外，一是，好奇大家都那么爱手机清理或者怕中毒么?毕竟，我自己的手机都「裸奔」了好些年；二是，第一名居然不是鹅厂的其他产品，比入「微信」或者「QQ」。</p></li><li><p><strong>榜单放眼望去，以为会出现的没有出现，没有想到的却出现了</strong></p><p>前十名中，居然出现了书旗小说、印客这些比较少听过的名字，而国民 App 微信、支付宝等，甚至都没有出现在这个榜单中。</p></li></ul><p>带着疑问和好奇，分别找到了「腾讯手机管家」和「微信」两款 App 的主页：</p><p>腾讯手机管家下载和安装量：</p><p><img src="http://media.makcyun.top/19-1-3/48264516.jpg" alt=""></p><p>微信下载和安装量：</p><p><img src="http://media.makcyun.top/19-1-3/7027912.jpg" alt=""></p><p>这是什么情况?</p><p>腾讯管家 3 亿多的下载量等同于安装量，而微信 20 多亿的下载量，只有区区一千多万的安装量，两组数据对比，大致反映了两个问题：</p><ul><li><p>要么是腾讯管家的下载量实际并没有那么多</p></li><li><p>要么是微信的下载量写少了</p></li></ul><p>不管是哪个问题，都反映了一个问题：<strong>该网站做得不够走心啊</strong>。</p><p>为了证明这个观点，将前十名的安装量和下载量都作了对比，发现很多 App 的安装量都和下载量是一样的，也就是说：<strong>这些 App 的实际下载量并没有那么多</strong>，而如果这样的话，那么这份榜单就有很大水分了。</p><p>难道，辛辛苦苦爬了那么久，就得到这样的结果?</p><p>不死心，接着再看看安装量最少的 App 是什么情况，这里找出了其中最少的 10 款：</p><p><img src="http://media.makcyun.top/19-1-3/14911632.jpg" alt=""></p><p>扫了一眼，更加没想到了：</p><p>「QQ 音乐」竟然是倒数第一，竟然只有 3 次安装量！</p><p><strong>确定这和刚刚上市、市值千亿的 QQ 音乐是同一款产品?</strong></p><p>再次核实了一下：</p><p><img src="http://media.makcyun.top/19-1-3/62626697.jpg" alt=""></p><p>没有看错，是写着 <strong>3人安装</strong>！</p><p>这是已经不走心到什么程度了? <strong>这个安装量，鹅厂还能「用心做好音乐」?</strong></p><p>说实话，到这儿已经不想再往下分析下去了，担心爬扒出更多没想到的东西，不过辛苦爬了这么久，还是再往下看看吧。</p><p>看了首尾，我们再看看整体，了解一下全部 App 的安装数量分布，这里去除了有很大水分的前十名 App。</p><p><img src="http://media.makcyun.top/19-1-3/23110694.jpg" alt=""></p><p>很惊讶地发现，竟然有 <strong>多达 67,195 款，占总数的 94% 的 App 的安装量不足 1万！</strong></p><p>如果这个网站的所有数据都是真的话，那么上面排名第一的手机管家，它 <strong>一款就差不多抵得上这 6 万多款 App 的安装量</strong>了！</p><p>对于多数 App 开发者，只能说：<strong>现实很残酷，辛苦开发出来的 App，用户不超过 1万人的可能性高达近 95% </strong>。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis_distribution</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = data.loc[<span class="number">10</span>:,:]</span><br><span class="line">    data[<span class="string">'install_count'</span>] = data[<span class="string">'install_count'</span>].apply(<span class="keyword">lambda</span> x:x/<span class="number">10000</span>)</span><br><span class="line">    bins = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>,<span class="number">1000</span>,<span class="number">10000</span>]</span><br><span class="line">    group_names = [<span class="string">'1万以下'</span>,<span class="string">'1-10万'</span>,<span class="string">'10-100万'</span>,<span class="string">'100-1000万'</span>,<span class="string">'1000万-1亿'</span>]</span><br><span class="line">    cats = pd.cut(data[<span class="string">'install_count'</span>],bins,labels=group_names)</span><br><span class="line">    cats = pd.value_counts(cats)</span><br><span class="line">    bar = Bar(<span class="string">'App 下载数量分布'</span>,<span class="string">'高达 94% 的 App 下载量低于1万'</span>)</span><br><span class="line">    bar.use_theme(<span class="string">'macarons'</span>)</span><br><span class="line">    bar.add(</span><br><span class="line">        <span class="string">'App 数量'</span>,</span><br><span class="line">        list(cats.index),</span><br><span class="line">        list(cats.values),</span><br><span class="line">        is_label_show = <span class="keyword">True</span>,</span><br><span class="line">        xaxis_interval = <span class="number">0</span>,</span><br><span class="line">        is_splitline_show = <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">    bar.render(path=<span class="string">'App下载数量分布.png'</span>,pixel_ration=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="▌分类情况"><a href="#▌分类情况" class="headerlink" title="▌分类情况"></a>▌分类情况</h3><p>下面，我们来看看各分类下 App 情况，不再看安装量，而看数量，以排出干扰。</p><p><img src="http://media.makcyun.top/19-1-3/86279055.jpg" alt=""></p><p>可以看到 14 个大分类中，<strong>每个分类的 App 数量差距都不大</strong>，数量最多的「生活休闲」是「摄影图像」的两倍多一点。</p><p>接着，我们进一步看看 88 个子分类的 App 数量情况，筛选出数量最多和最少的 10 个子类：</p><p><img src="http://media.makcyun.top/19-1-3/63799914.jpg" alt=""></p><p>可以发现两点有意思的现象：</p><ul><li><p><strong>「收音机」类别 App 数量最多，达到 1,300 多款</strong></p><p>这个很意外，当下收音机完全可以说是个老古董了，居然还有那么人去开发。</p></li><li><p><strong>App 子类数量差距较大</strong></p><p>最多的「收音机」是最少的「动态壁纸」近 20 倍，如果我是一个 App 开发者，<strong>那我更愿意去尝试开发些小众类的 App，竞争小一点</strong>，比如：「背单词」、「小儿百科」这些。</p></li></ul><p>看完了总体和分类情况，突然想到一个问题：<strong>这么多 App，有没有重名的呢?</strong></p><p><img src="http://media.makcyun.top/19-1-3/48878130.jpg" alt=""></p><p>惊奇地发现，叫「一键锁屏」的 App 多达 40 款，这个功能 App 很难再想出别的名字了么? 现在很多手机都支持触控锁屏了，比一键锁屏操作更加方便。</p><p>接下来，我们简单对比下豌豆荚和酷安两个网站的 App 情况。</p><h3 id="▌对比酷安"><a href="#▌对比酷安" class="headerlink" title="▌对比酷安"></a>▌对比酷安</h3><p>二者最直观的一个区别是在 App 数量上，豌豆荚拥有绝对的优势，达到了酷安的十倍之多，那么我们自然感兴趣：</p><p><strong>豌豆荚是否包括了酷安上所有的 App ?</strong></p><p>如果是，「你有的我都有，你没有的我也有」，那么酷安就没什么优势了。统计之后，发现豌豆荚 <strong>仅包括了 3,018 款，也就是一半左右</strong>，剩下的另一半则没有包括。</p><p>这里面固然存在两个平台上 App 名称不一致的现象，但更有理由相信 <strong>酷安很多小众的精品 App 是独有的，豌豆荚并没有。</strong></p><p><img src="http://media.makcyun.top/19-1-3/26835054.jpg" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">include = data3.shape[<span class="number">0</span>]</span><br><span class="line">notinclude = data2.shape[<span class="number">0</span>] - data3.shape[<span class="number">0</span>]</span><br><span class="line">sizes= [include,notinclude]</span><br><span class="line">labels = [<span class="string">u'包含'</span>,<span class="string">u'不包含'</span>]</span><br><span class="line">explode = [<span class="number">0</span>,<span class="number">0.05</span>]</span><br><span class="line">plt.pie(</span><br><span class="line">    sizes,</span><br><span class="line">    autopct = <span class="string">'%.1f%%'</span>,</span><br><span class="line">    labels = labels,</span><br><span class="line">    colors = [colorline,<span class="string">'#7FC161'</span>], <span class="comment"># 豌豆荚绿</span></span><br><span class="line">    shadow = <span class="keyword">False</span>,</span><br><span class="line">    startangle = <span class="number">90</span>,</span><br><span class="line">    explode = explode,</span><br><span class="line">    textprops = &#123;<span class="string">'fontsize'</span>:<span class="number">14</span>,<span class="string">'color'</span>:colors&#125;</span><br><span class="line">)</span><br><span class="line">plt.title(<span class="string">'豌豆荚仅包括酷安上一半的 App 数量'</span>,color=colorline,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">'包含不保包含对比.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>接下来，我们看看所包含的 App 当中，在两个平台上的下载量是怎么样的：</p><p><img src="http://media.makcyun.top/19-1-3/22808039.jpg" alt=""></p><p>可以看到，两个平台上 App 下载数量差距还是很明显。</p><p>最后，我面再看看豌豆荚上没有包括哪些APP：</p><p><img src="http://media.makcyun.top/19-1-3/1157763.jpg" alt=""></p><p>可以看到很多神器都没有包括，比如：RE、绿色守护、一个木函等等。豌豆荚和酷安的对比就到这里，如果用一句话来总结，我可能会说：</p><p><strong>豌豆荚太牛逼了， App 数量是酷安的十倍，所以我选酷安。</strong></p><p>以上，就是利用 Scrapy 爬取分类多级页面的抓取和分析的一次实战。</p><p>感兴趣的话可以找类似的网站练练手，如需本文的完整代码，可以加入我的知识星球：「<strong>第2脑袋</strong>」获得，里面有很多干货，可以扫描下方二维码预览下，觉得合适就入圈。</p><p><img src="http://media.makcyun.top/19-1-1/31534576.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ Python For 和 While 循环爬取不确定页数的网页</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython15.html">∞ Python 爬虫的代理 IP 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython14.html">∞ Python爬虫的随机 User-Agent 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">∞ pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;使用 Scrapy 爬取豌豆荚全网 70000+ App。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>原创精华 | 2018 年文章汇总</title>
    <link href="https://www.makcyun.top/2018/12/31/life05.html"/>
    <id>https://www.makcyun.top/2018/12/31/life05.html</id>
    <published>2018-12-31T11:27:22.000Z</published>
    <updated>2019-01-01T02:33:09.675Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>2018 下半年建立博客至今，写了 38 篇文章。</p><a id="more"></a><p>最后一天，对这半年写过的文章进行一下汇总，便于你快速找到想看的文章，文章总体主要包括 5 个方面：Python 基础、Python 爬虫、数据分析、每周分享干货和生活随笔。</p><h3 id="▌Python-基础"><a href="#▌Python-基础" class="headerlink" title="▌Python 基础"></a>▌Python 基础</h3><p><a href="https://www.makcyun.top/weekly_sharing9.html">∞ 入门必看|大佬们推荐的 Python 书单汇总</a></p><p><a href="https://www.makcyun.top/Python_visualization04.html">∞ WordCloud 中英文词云图绘制方法汇总</a></p><p><a href="https://www.makcyun.top/python_data_analysis01.html">∞ Python 日期型数据处理</a></p><p><a href="https://www.makcyun.top/python_data_visualization01.html">∞ Python 折线图绘制技巧</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython11.html">∞ 从函数 def 到类 Class</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">∞ 从 Class 类到 Scrapy</a></p><h3 id="▌Python-爬虫"><a href="#▌Python-爬虫" class="headerlink" title="▌Python 爬虫"></a>▌Python 爬虫</h3><p><a href="https://www.makcyun.top/web_scraping_withpython1.html">∞ 爬虫入门第一课：多种方法爬取猫眼 TOP100 电影</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython2.html">∞ 10 行代码爬取全国所有 A 股/港股/新三板上市公司信息</a></p><p><a href="https://www.makcyun.top/Python_visualization01.html">∞ 20 秒纵览中国大学十年排行榜变迁</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython4.html">∞ 福利 | 单页图片下载，以网易「数独」为例</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython3.html">∞ 福利 | 分析 Ajax 实现多页图片下载，以澎湃网为例</a></p><p><a href="https://www.makcyun.top/Python_visualization03.html">∞ 中国 66 家环保股上市公司市值 TOP20 强</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython5.html">∞ Selenium 自动爬取东方财富网股票财务报表</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython6.html">∞ 50 行代码爬取东方财富网百万行财务报表数据</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython7.html">∞ Selenium + Ajax 爬取IT 桔子网</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython8.html">∞ Python 模拟登录方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython13.html">∞ 如何让 MongoDB 避免存储重复数据</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython14.html">∞ Scrapy 中设置随机 User-Agent 的方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython15.html">∞ Python 爬虫的代理 IP 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython16.html">∞ For 和 While 循环爬取不确定页数的网页</a></p><h3 id="▌数据分析"><a href="#▌数据分析" class="headerlink" title="▌数据分析"></a>▌数据分析</h3><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">∞ pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython11.html">∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（分析篇）</a></p><h3 id="▌每周分享"><a href="#▌每周分享" class="headerlink" title="▌每周分享"></a>▌每周分享</h3><p><a href="https://www.makcyun.top/hexo01.html">∞ 用 GitHub + Hexo 搭建你的个人博客：搭建篇</a></p><p><a href="https://www.makcyun.top/hexo02.html">∞ 用 GitHub + Hexo 搭建你的个人博客：美化篇</a></p><p><a href="https://www.makcyun.top/weekly_sharing4.html">∞ 5 分钟内又快又好搞定公众号排版</a></p><p><a href="https://www.makcyun.top/weekly_sharing5.html">∞ 苹果微软都在使用的一套文案排版</a></p><p><a href="https://www.makcyun.top/weekly_sharing10.html">∞ 提升审美的带壳截图 App</a></p><p><a href="https://www.makcyun.top/weekly_sharing6.html">∞ 不用翻墙，轻松看世界新闻</a></p><p><a href="https://www.makcyun.top/weekly_sharing7.html">∞ 盘点那些手机上绝对值得安装的 App</a></p><p><a href="https://www.makcyun.top/weekly_sharing8.html">∞ 盘点那些手机上绝对值得安装的 App (2)</a></p><p><a href="https://www.makcyun.top/weekly_sharing3.html">∞ 安卓最强阅读器</a></p><p><a href="https://www.makcyun.top/weekly_sharing2.html">∞ 一掏出手机，就暴露了程序猿身份</a></p><p><a href="https://www.makcyun.top/fuli01.html">∞ 关于 PDF 阅读处理软件，你需要的都在这里了</a></p><h3 id="▌生活随笔"><a href="#▌生活随笔" class="headerlink" title="▌生活随笔"></a>▌生活随笔</h3><p><a href="https://www.makcyun.top/life04.html">∞ 年终总结 | 巨大转变的 2018 年</a></p><p><a href="https://www.makcyun.top/life03.html">∞ 27 岁，强烈的中年危机感</a></p><p><a href="https://www.makcyun.top/life02.html">∞ 上了年纪的工程师有哪些出路</a></p><p><a href="https://www.makcyun.top/life01.html">∞ 我的白血病妻子</a></p><p>文章罗列完了，但并没有完，还有两点很重要的需要说明一下：</p><p><strong>第一、里面的很多文章，并不是孤立的，而是有一定的关联顺序。</strong></p><p>我在初学 Python 的时候，看过很多公众号的文章精华汇总，数量是非常多，猛地一看，觉得像挖到了宝，心想看这些就够了。但慢慢发现这样的文章汇总其实是一锅大杂烩，因为每篇文章的知识点都很分散，需要自己一点点地去将孤立的知识点串联起来，非常地花时间，学习效率很低。</p><p>而学习是一个循序渐进的过程，由浅入深，层层递进是比较好的入门方式，所以个人觉得文章不在于多，而在于精，看逻辑清晰、承上启下的文章能够更容易地吸收，更快地入门。</p><p><strong>第二、上面不少文章中的代码或者资源，只提供了一部分，完整地放在了我的「知识星球」。</strong></p><p>不是我耍什么把戏，坦诚地说，<strong>就是想赚点钱。</strong></p><p>写这些原创文章，花了我非常多的时间，很多时候是在医院里完成的，我很乐于分享，但同时也希望能够得到些正向激励，这样在新的一年才会有更大动力去写更多、更值得一看的文章。</p><p>所以，欢迎扫描二维码预览，觉得不错可以加入，早加早划算。<strong>99 块，也不多，但能得到接下来一年我文章中所有的精华干货。</strong></p><p><img src="http://media.makcyun.top/19-1-1/31534576.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;2018 下半年建立博客至今，写了 38 篇文章。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>年终总结 | 巨大转变的 2018 年</title>
    <link href="https://www.makcyun.top/2018/12/31/life04.html"/>
    <id>https://www.makcyun.top/2018/12/31/life04.html</id>
    <published>2018-12-31T08:16:24.000Z</published>
    <updated>2018-12-31T07:49:56.903Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>记录我的 2018 年。</p><a id="more"></a><p>一晃就到了 2018 年的最后一天，今天就东拉西扯，回顾下自己的这一年，若这篇文章，哪怕是某一句话，能对你有一点点用，我觉得算没白写。</p><p>前几年从没想着年尾的时候做个年终总结，所以一年年的时间稀里糊涂地就溜完了，目前还能够记得比较清楚的是 2014 年的时候做过一次总结，以 PPT 视频的形式，名字叫做「我的 1314」，那会儿不过 22、23 岁，我想关注我公众号的不少朋友差不多也是这个年龄，视频里有一点我觉得可能会对你有用，就是 <strong>去计划一个「毕业旅行」</strong>。</p><p>传送门：<a href="https://v.qq.com/x/page/w08212s3lch.html" target="_blank" rel="noopener">https://v.qq.com/x/page/w08212s3lch.html</a></p><p>岁月不饶人，一转眼，刚刚过了 27 岁，我想这个时候如果再不主动去抓住时间，那么它会溜得越来越快，也就意味着会老得更快，竞争力也就越来越弱。</p><p>下面就来说说这一年的自己的经历和收获，最后再设定几个来年的小目标，<strong>有总结有计划，才能做时间的主人</strong>。</p><p>2018 年对我来说，可以用一个词来形容，就是「<strong>巨大转变</strong>」，主要体现在这么几方面：</p><h3 id="▌工作一年，就辞职了"><a href="#▌工作一年，就辞职了" class="headerlink" title="▌工作一年，就辞职了"></a>▌工作一年，就辞职了</h3><p>2017 年夏天毕业后开始工作，到 2018 年 6 月辞职，刚好一年，这里面有客观家庭原因，另外一方面也是想来个彻底转变，顺从自己的内心，果断去放弃不喜欢的本行工作，投身自己的爱好并尝试转行。不过这里要提醒你，我是特殊情况，最好不要辞职自学转行，风险太大。</p><h3 id="▌辗转东莞、深圳、广州和北京四地"><a href="#▌辗转东莞、深圳、广州和北京四地" class="headerlink" title="▌辗转东莞、深圳、广州和北京四地"></a>▌辗转东莞、深圳、广州和北京四地</h3><p>这一年，辗转了东莞、深圳、广州和北京多个地方，每个地方都呆了几个月，这也可能是觉得今年过得特别快的一个原因。第一份工作是在东莞的一家上市公司，主要的工作就是对着 Excel 算这算那，觉得自己的专业一点没用到，所以半年后就辞职了。大年初四，便去深圳开始第二份新工作，做了自己的本专业方面工作，几个月后，最终确定对自己的专业不感兴趣，决定放弃转行，这个过程中偶然接触到了 Python，开始产生了兴趣。</p><h3 id="▌自学-Python"><a href="#▌自学-Python" class="headerlink" title="▌自学 Python"></a>▌自学 Python</h3><p>7 月份开始利用空闲时间，零基础自学 Python，强迫自己在博客和公众号上输出学习总结，现在看来，选择是正确的，半年下来，在以下三方面取得点小小的个人进步：</p><ul><li><strong>更新了公众号</strong></li></ul><p>2014 年夏天，在旅行的途中开通了公众号，却没坚持写下来，一断更就断到了今年，好在，这次坚持了下来。</p><p>统计了下，今年写了 38 篇文章，37 篇原创，平均一个月 6 篇左右，粉丝数上升不多，没有太刻意去采取措施吸粉，相比粉丝数，更加注重的是文章阅读率，达到了 14% 还算不错。</p><ul><li><strong>建立了个人博客</strong></li></ul><p>微信公众号，归根结底还是比较大众的一个东西，一直都想拥有一个独一无二的个人博客，在上面记点东西，写个几十年。<strong>有了强烈的想法后，就会发现浑身有用不完的能量</strong>。</p><p><img src="http://media.makcyun.top/18-12-31/81380555.jpg" alt=""></p><p>7 月份，是最忙的一阵子，晚上在医院守一晚夜，白天回到出租房里开始学习搭建现在的这个博客，前后花了半个月才弄好。到今天，博客底部的统计显示，一共写了 11 万字，从最初的 0 人，达到了今天的 6,000 人访问，17,000 次浏览量，中间一阵子出了点故障没有记录到，真实值可能在两万次左右，这个数值，跟别人比显得很少，<strong>跟半年前的自己比，就是上万倍的提升。</strong></p><ul><li><strong>开通了知识星球</strong></li></ul><p>早在 2017 年 2 月我就加入了几个知识星球，那时候还叫「小密圈」，但从没想过自己去开一个，这一拖就是快两年。</p><p>终于，一个月前，我开通了自己的付费知识星球，开之前犹豫了很久，要不要开？开付费的还是免费的？很多人说，个人影响力还不大的时候就先不要开付费星球。我觉得各有利弊，知识星球和公众号可以很好地互补，也能逼自己做更多的持续输出，虽然目前加入的球友不多，但好歹是上路了，在这里 <strong>感谢最早加入和支持我的几位朋友</strong>。</p><h3 id="▌看了二十本书"><a href="#▌看了二十本书" class="headerlink" title="▌看了二十本书"></a>▌看了二十本书</h3><p>我以前不爱看书，几乎任何书都不喜欢看的那种，印象中没有完整地看过一本书，不是对书没兴趣，是觉得没有外界的推动或者逼迫去让自己要养成阅读这个习惯，<strong>归根结底就是太懒</strong>。</p><p><img src="http://media.makcyun.top/18-12-31/90155994.jpg" alt=""></p><p>好在今年得到了很大的改善，下半年零基础开始学习编程的时候，发现自己一无所知，<strong>甚至比不了一个科班的大一学生</strong>，无数个时刻想锤自己脑袋，前些年怎么不多装些东西。于是开始疯狂看书，先看电子书，不错的就把纸质版买到手，不知不觉就买了许多书，一些算是完整看了，一些只是快速翻了两遍，有的还没来得及看。</p><p><strong>看书绝对是任何阶段都应该保有的一个爱好。</strong></p><h3 id="▌跑了上百公里步"><a href="#▌跑了上百公里步" class="headerlink" title="▌跑了上百公里步"></a>▌跑了上百公里步</h3><p><img src="http://media.makcyun.top/18-12-31/4771916.jpg" alt=""></p><p>跑步这个习惯最早是 10 多年之前上初中的时候养成的，那时候感觉浑身有用不完的力气，后来却因为种种原因中断了，这一停，就长达 10 年之久，没成想那几年就是自己人生精力的最顶峰。上了高中，觉得课业太重，没时间；到了大学，看到少有人跑，就算了；出来工作，上班已然很累，就不要折腾自己了。</p><p>下半年，由于不得不早睡早起，所以渐渐戒掉了熬夜的习惯，作息开始规律起来：<strong>晚十点睡，早六点起，跑 2-3 公里步</strong>。</p><p>大致经历了：从「要人命」到「跑不停」的阶段，以前从不相信别人口中「跑步是会上瘾」这种说法的，总以为他们是在秀优越，之所以有这样的感觉，我发现往往是因为自己只坚持了两三天，而这 <strong>头两三天是开始习惯跑步最痛苦的阶段</strong>，我已经记不清有多少次在这个阶段放弃了。而事实上，第一次跑会难受一个礼拜，第二次跑会难受三天，到了第三第四次，隔天再跑你会惊奇地发现「我竟然可以连续跑了」。</p><p>以上，就是 2018 年的一些回顾，下面简单说说 2019 年设定的几个小目标。</p><h3 id="▌不要停，向前看"><a href="#▌不要停，向前看" class="headerlink" title="▌不要停，向前看"></a>▌不要停，向前看</h3><p>这几个月，我几乎是没有停下脚步，一直逼自己在学，但还有很多东西等着去学，初步确定明年要主攻：机器学习、数据挖掘、数据结构与算法、Linux 系统、数据库等方面的知识，然后继续总结输出。</p><h3 id="▌100-篇原创文章"><a href="#▌100-篇原创文章" class="headerlink" title="▌100 篇原创文章"></a>▌100 篇原创文章</h3><p>参照这小半年写了近 40 篇原创文章来算，我觉得明年实现这个目标，不算好高骛远。</p><h3 id="▌跑步-600-公里"><a href="#▌跑步-600-公里" class="headerlink" title="▌跑步 600 公里"></a>▌跑步 600 公里</h3><p>根据这几个月跑步的距离和频次，定了这个目标，拆开来算，其实不多，一年跑 300 天，每天跑 2公里就行，也就是 5 圈操场跑道。编程界大佬廖雪峰老师，知乎上的个人标签写着「业余马拉松选手」，我想多跑跑步总没坏处，当程序猿没个好身体搞不起的。</p><p>好，东拉西扯了一番，也欢迎你留言说说自己，当立个 Flag，另外说不定后期也会有福利。</p><p>本文完。</p><p>7 月份开始利用空闲时间，零基础自学 Python，强迫自己输出学习总结，现在看来，选择是正确的，半年下来，在以下三方面取得点小小的个人进步：</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;记录我的 2018 年。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>你的手机截图真丑</title>
    <link href="https://www.makcyun.top/2018/12/29/weekly_sharing10.html"/>
    <id>https://www.makcyun.top/2018/12/29/weekly_sharing10.html</id>
    <published>2018-12-29T08:16:16.000Z</published>
    <updated>2018-12-29T03:53:50.752Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>介绍两款炒鸡好用的手机截图 App，提升你的审美。</p><a id="more"></a><p><strong>摘要</strong>：介绍两款炒鸡好用的手机截图 App，提升你的审美。</p><p>这是「<strong>每周分享</strong>」的第 10 期，也是 2018 年的最后一期，最后一期咱们轻松一点，来介绍两款很实用、堪称神器的手机带壳截图 App。</p><p>如果你足够细心的话，会发现我此前的很多文章中，都用到手机带壳截图，也就是在图片外面套了个手机壳，之所以想套个壳，是觉得直接放截图太丑了，尤其是当看到朋友圈中那些摆满九宫格的手机截图，我就很受不了，普通的截图图片用了之后逼格立马就上来了。</p><p><img src="http://media.makcyun.top/18-12-8/52805793.jpg" alt=""></p><p>这里，我找了三个场景下的截图：聊天界面、朋友圈、公众号文章，来对比一下无手机壳和带手机壳的截图效果：</p><p>无手机壳：</p><p><img src="http://media.makcyun.top/18-12-28/80823869.jpg" alt=""></p><p>带手机壳：</p><p><img src="http://media.makcyun.top/18-12-28/29158243.jpg" alt=""></p><p>所以当你发朋友圈或者给别人发截图时，想要有逼格一点，可以尝试使用带壳截图。</p><p><strong>那么，问题就来了，怎么给手机带壳截图？</strong></p><p>咳咳，下面就到了神器亮相的时候了，介绍一款由「少数派」开发的非常不错的带壳截图 App，页面长这个样子：</p><p><img src="http://media.makcyun.top/18-12-28/65116285.jpg" alt=""></p><p>这款 App 支持常见的 11 款手机品牌，共包含 47 个机型，比如 HTC、华为、Sony 等，只要点击某款机型然后插入图片即可完成带壳截图，小学生都会的操作，比如使用 Nexus 可以得到这样的效果：</p><p><img src="http://media.makcyun.top/18-12-28/26348746.jpg" alt=""></p><p>手机壳很好看对吧，500px 上搜索「<a href="https://500px.com/seanarcher" target="_blank" rel="noopener">Sean Archer</a>」还有更多，只能帮你到这儿了，注意身体。</p><p>接着说，你会发现，里面可能没有你使用的 iPhone、OPPO、VIVO 等品牌或者有品牌但没有相应的机型，对的，我也是在换了手机后发现没有适应的机型，套别的机型会有什么问题呢？</p><p>问题大了，由于手机尺寸大小不一样，所以手机截图会变形，这样就不好看。</p><p>于是，我又开始物色有没有更好、支持更多机型的 App，还真给我让我找到了，就是下面将要介绍的这款终极截图 App ，名字叫「带壳截图 PRO」，页面做得很清爽：</p><p><img src="http://media.makcyun.top/18-12-28/93137147.jpg" alt=""></p><p>我大致数了下，这款 App 支持 21 款手机，共计上百款机型，可以说你使用的手机基本上都能够在这里找到模型：</p><p><img src="http://media.makcyun.top/18-12-28/61113793.jpg" alt=""></p><p>这里，随便挑了几款机型试试看截图效果，今天很冷，就放几张冬天的高清图吧：</p><p><img src="http://media.makcyun.top/18-12-28/22880853.jpg" alt=""></p><p>再比如：</p><p><img src="http://media.makcyun.top/18-12-28/15042090.jpg" alt=""></p><p>还比如：</p><p><img src="http://media.makcyun.top/18-12-28/97449212.jpg" alt=""></p><p>除了上述常规截图玩法，你还可以做成手持效果：</p><p><img src="http://media.makcyun.top/18-12-28/3653986.jpg" alt=""></p><p>还可以来个双面特写：</p><p><img src="http://media.makcyun.top/18-12-28/20788757.jpg" alt=""></p><p>逼格还不够高，就再来个「千手观音」：</p><p><img src="http://media.makcyun.top/18-12-28/32440381.jpg" alt=""></p><p>以上就是本期的分享，如果对里面的 App 感兴趣的话可以去找来试试。</p><p>还是老规矩，为了更方便你，我这里准备已下载好了，公众号后台回复：<strong>「App截图」就可以得到。</strong> 另外若喜欢里面的几张高清图片，可以扫描下方二维码加入我的知识星球得到，里面有很多干货，越早加入越划算。</p><p>本文完。</p><p><img src="http://media.makcyun.top/18-12-26/16445406.jpg" alt=""></p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/weekly_sharing7.html">盘点那些手机上绝对值得安装的 App</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/weekly_sharing3.html">安卓最强阅读器</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;介绍两款炒鸡好用的手机截图 App，提升你的审美。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Python For 和 While 循环爬取不确定页数的网页</title>
    <link href="https://www.makcyun.top/2018/12/26/web_scraping_withpython16.html"/>
    <id>https://www.makcyun.top/2018/12/26/web_scraping_withpython16.html</id>
    <published>2018-12-26T08:16:24.000Z</published>
    <updated>2018-12-27T12:17:27.180Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Requests 和 Scrapy 中分别用 For 循环和 While 循环爬取不确定页数的网页。</p><a id="more"></a><p><strong>摘要</strong>：Requests 和 Scrapy 中分别用 For 循环和 While 循环爬取不确定页数的网页。</p><p>我们通常遇到的网站页数展现形式有这么几种：</p><p>第一种是直观地显示所有页数，比如此前爬过的酷安、东方财富网，<br>文章见：</p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython6.html">∞ 50 行代码爬取东方财富网百万行财务报表数据</a></p><p><img src="http://media.makcyun.top/18-12-26/49266613.jpg" alt=""></p><p>第二种是不直观显示网页总页数，需要在后台才可以查看到，比如之前爬过的虎嗅网，文章见：</p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">∞ pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><p><img src="http://media.makcyun.top/18-12-26/4137026.jpg" alt=""></p><p>第三种是今天要说的，不知道具体有多少页的网页，比如豌豆荚：</p><p><img src="http://media.makcyun.top/18-12-26/21074762.jpg" alt=""></p><p>对于，前两种形式的网页，爬取方法非常简单，使用 For 循环从首页爬到尾页就行了，第三种形式则不适用，因为不知道尾页的页数，所以循环到哪一页结束无法判断。</p><p>那如何解决呢？有两种方法。</p><p>第一种方式 <strong>使用 For 循环配合 break 语句</strong>，尾页的页数设置一个较大的参数，足够循环爬完所有页面，爬取完成时，break 跳出循环，结束爬取。</p><p>第二种方法 <strong>使用 While 循环，可以结合 break 语句，也可以设起始循环判断条件为 True</strong>，从头开始循环爬取直到爬完最后一页，然后更改判断条件为 False 跳出循环，结束爬取。</p><h2 id="实际案例"><a href="#实际案例" class="headerlink" title="实际案例"></a>实际案例</h2><p>下面，我们以 <a href="https://www.wandoujia.com/category/5029_716" target="_blank" rel="noopener">豌豆荚</a> 网站中「视频」类别下的 App 信息为例，使用上面两种方法抓取该分类下的所有 App 信息，包括 App 名称、评论、安装数量和体积。</p><p>首先，简要分析下网站，可以看到页面是通过 Ajax 加载的，GET 请求附带一些参数，可以使用 params 参数构造 URL 请求，但不知道一共有多少页，为了确保下载完所有页，设置较大的页数，比如 100页 甚至 1000 页都行。</p><p>下面我们尝试使用 For 和 While 循环爬取 。</p><p><img src="http://media.makcyun.top/18-12-26/57399.jpg" alt=""></p><h2 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h2><h3 id="▌For-循环"><a href="#▌For-循环" class="headerlink" title="▌For 循环"></a>▌For 循环</h3><p>主要代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Get_page</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># ajax 请求url</span></span><br><span class="line">        self.ajax_url = <span class="string">'https://www.wandoujia.com/wdjweb/api/category/more'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(self,page,cate_code,child_cate_code)</span>:</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'catId'</span>: cate_code,</span><br><span class="line">            <span class="string">'subCatId'</span>: child_cate_code,</span><br><span class="line">            <span class="string">'page'</span>: page,</span><br><span class="line">        &#125;</span><br><span class="line">        response = requests.get(self.ajax_url, headers=headers, params=params)</span><br><span class="line">        content = response.json()[<span class="string">'data'</span>][<span class="string">'content'</span>] <span class="comment">#提取json中的html页面数据</span></span><br><span class="line">        <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, content)</span>:</span></span><br><span class="line">        <span class="comment"># 解析网页内容</span></span><br><span class="line">        contents = pq(content)(<span class="string">'.card'</span>).items()</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">            data1 = &#123;</span><br><span class="line">                <span class="string">'app_name'</span>: content(<span class="string">'.name'</span>).text(),</span><br><span class="line">                <span class="string">'install'</span>: content(<span class="string">'.install-count'</span>).text(),</span><br><span class="line">                <span class="string">'volume'</span>: content(<span class="string">'.meta span:last-child'</span>).text(),</span><br><span class="line">                <span class="string">'comment'</span>: content(<span class="string">'.comment'</span>).text(),</span><br><span class="line">            &#125;</span><br><span class="line">            data.append(data1)</span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            <span class="comment"># 写入MongoDB</span></span><br><span class="line">            self.write_to_mongodb(data)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 实例化数据提取类</span></span><br><span class="line">    wandou_page = Get_page()</span><br><span class="line">    cate_code = <span class="number">5029</span> <span class="comment"># 影音播放大类别编号</span></span><br><span class="line">    child_cate_code = <span class="number">716</span> <span class="comment"># 视频小类别编号</span></span><br><span class="line">     <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">100</span>):</span><br><span class="line">        print(<span class="string">'*'</span> * <span class="number">50</span>)</span><br><span class="line">        print(<span class="string">'正在爬取：第 %s 页'</span> % page)</span><br><span class="line">        content = wandou_page.get_page(page,cate_code,child_cate_code)</span><br><span class="line">        <span class="comment"># 添加循环判断，如果content 为空表示此页已经下载完成了,break 跳出循环</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> content == <span class="string">''</span>:</span><br><span class="line">            wandou_page.parse_page(content)</span><br><span class="line">            sleep = np.random.randint(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">            time.sleep(sleep)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'该类别已下载完最后一页'</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>这里，首先创建了一个 Get_page 类，get_page 方法用于获取 Response 返回的 json 数据，通过 <a href="https://www.json.cn/" target="_blank" rel="noopener">json.cn</a> 网站解析 json 解析后发现需要提取的内容是一段包裹在 data 字段下 content 键中的 html 文本，可以使用 parse_page 方法中的 pyquery 函数进行解析，最后提取出 App 名称、评论、安装数量和体积四项信息，完成抓取。</p><p>在主函数中，使用了 if 函数进行条件判断，若 content 不为空，表示该页有内容，则循环爬下去，若为空则表示此页面已完成了爬取，执行 else 分支下的 break 语句结束循环，完成爬取。</p><p><img src="http://media.makcyun.top/18-12-27/15411909.jpg" alt=""></p><p>爬取结果如下，可以看到该分类下一共完成了全部 41 页的信息抓取。</p><p><img src="http://media.makcyun.top/18-12-27/12704438.jpg" alt=""></p><h3 id="▌While-循环"><a href="#▌While-循环" class="headerlink" title="▌While 循环"></a>▌While 循环</h3><p>While 循环和 For 循环思路大致相同，不过有两种写法，一种仍然是结合 break 语句，一种则是更改判断条件。</p><p>总体代码不变，只需修改 For 循环部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">page = <span class="number">2</span> <span class="comment"># 设置爬取起始页数</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">50</span>)</span><br><span class="line">    print(<span class="string">'正在爬取：第 %s 页'</span> %page)</span><br><span class="line">    content = wandou_page.get_page(page,cate_code,child_cate_code)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> content == <span class="string">''</span>:</span><br><span class="line">        wandou_page.parse_page(content)</span><br><span class="line">        page += <span class="number">1</span></span><br><span class="line">        sleep = np.random.randint(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">        time.sleep(sleep)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'该类别已下载完最后一页'</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>或者：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">page = <span class="number">2</span> <span class="comment"># 设置爬取起始页数</span></span><br><span class="line">page_last = <span class="keyword">False</span> <span class="comment"># while 循环初始条件</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> page_last:</span><br><span class="line">   <span class="comment">#...</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># break</span></span><br><span class="line">        page_last = <span class="keyword">True</span> <span class="comment"># 更改page_last 为 True 跳出循环</span></span><br></pre></td></tr></table></figure><p>结果如下，可以看到和 For 循环的结果是一样的。</p><p><img src="http://media.makcyun.top/18-12-27/38514291.jpg" alt=""></p><p>我们可以再测试一下其他类别下的网页，比如选择「K歌」类别，编码为：718，然后只需要对应修改主函数中的child_cate_code 即可，再次运行程序，可以看到该类别下一共爬取了 32 页。</p><p><img src="http://media.makcyun.top/18-12-27/91221430.jpg" alt=""></p><p>由于 Scrapy 中的写法和 Requests 稍有不同，所以接下来，我们在 Scrapy 中再次实现两种循环的爬取方式 。</p><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><h3 id="▌For-循环-1"><a href="#▌For-循环-1" class="headerlink" title="▌For 循环"></a>▌For 循环</h3><p>Scrapy 中使用 For 循环递归爬取的思路非常简单，即先批量生成所有请求的 URL，包括最后无效的 URL，后续在 parse 方法中添加 if 判断过滤无效请求，然后爬取所有页面。<strong>由于 Scrapy 依赖于Twisted框架，采用的是异步请求处理方式，也就是说 Scrapy 边发送请求边解析内容，所以这会发送很多无用请求。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    pages=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>):</span><br><span class="line">        url=<span class="string">'http://www.example.com/?page=%s'</span>%i</span><br><span class="line">        page = scrapy.Request(url,callback==self.pare)</span><br><span class="line">        pages.append(page)</span><br><span class="line">    <span class="keyword">return</span> pages</span><br></pre></td></tr></table></figure><p>下面，我们选取豌豆荚「新闻阅读」分类下的「电子书」类 App 页面信息，使用 For 循环尝试爬取，主要代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    cate_code = <span class="number">5019</span> <span class="comment"># 新闻阅读</span></span><br><span class="line">    child_cate_code = <span class="number">940</span> <span class="comment"># 电子书</span></span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">50</span>)</span><br><span class="line">    pages = []</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">50</span>):</span><br><span class="line">        print(<span class="string">'正在爬取：第 %s 页 '</span> %page)</span><br><span class="line">        params = &#123;</span><br><span class="line">        <span class="string">'catId'</span>: cate_code,</span><br><span class="line">        <span class="string">'subCatId'</span>: child_cate_code,</span><br><span class="line">        <span class="string">'page'</span>: page,</span><br><span class="line">        &#125;</span><br><span class="line">        category_url = self.ajax_url + urlencode(params)</span><br><span class="line">        pa = <span class="keyword">yield</span> scrapy.Request(category_url,callback=self.parse)</span><br><span class="line">        pages.append(pa)</span><br><span class="line">    <span class="keyword">return</span> pages</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(response.body) &gt;= <span class="number">100</span>:  <span class="comment"># 判断该页是否爬完，数值定为100是因为response无内容时的长度是87</span></span><br><span class="line">        jsonresponse = json.loads(response.body_as_unicode())</span><br><span class="line">        contents = jsonresponse[<span class="string">'data'</span>][<span class="string">'content'</span>]</span><br><span class="line">        <span class="comment"># response 是json,json内容是html，html 为文本不能直接使用.css 提取，要先转换</span></span><br><span class="line">        contents = scrapy.Selector(text=contents, type=<span class="string">"html"</span>)</span><br><span class="line">        contents = contents.css(<span class="string">'.card'</span>)</span><br><span class="line">        <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">            item = WandoujiaItem()</span><br><span class="line">            item[<span class="string">'app_name'</span>] = content.css(<span class="string">'.name::text'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'install'</span>] = content.css(<span class="string">'.install-count::text'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'volume'</span>] = content.css(<span class="string">'.meta span:last-child::text'</span>).extract_first()</span><br><span class="line">            item[<span class="string">'comment'</span>] = content.css(<span class="string">'.comment::text'</span>).extract_first().strip()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><p>上面代码很好理解，简要说明几点：</p><p>第一、判断当前页是否爬取完成的判断条件改为了 response.body 的长度大于 100。</p><p>因为请求已爬取完成的页面，返回的 Response 结果是不为空的，而是有长度的 json 内容（长度为 87），其中 content 键值内容才为空，所以这里判断条件选择比 87 大的数值即可，比如 100，即大于 100 的表示此页有内容，小于 100 表示此页已爬取完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"state"</span>:&#123;<span class="string">"code"</span>:<span class="number">2000000</span>,<span class="string">"msg"</span>:<span class="string">"Ok"</span>,<span class="string">"tips"</span>:<span class="string">""</span>&#125;,<span class="string">"data"</span>:&#123;<span class="string">"currPage"</span>:<span class="number">-1</span>,<span class="string">"content"</span>:<span class="string">""</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>第二、<strong>当需要从文本中解析内容时，不能直接解析，需要先转换。</strong></p><p>通常情况下，我们在解析内容时是直接对返回的 response 进行解析，比如使用 response.css() 方法，但此处，我们的解析对象不是 response，而是 response 返回的 json 内容中的 html 文本，文本是不能直接使用 .css() 方法解析的，所以在对 html 进行解析之前，需要添加下面一行代码转换后才能解析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">contents = scrapy.Selector(text=contents, type=<span class="string">"html"</span>)</span><br></pre></td></tr></table></figure><p>结果如下，可以看到发送了全部 48 个请求，实际上该分类只有 22 页内容，即多发送了无用的 26 个请求。</p><p><img src="http://media.makcyun.top/18-12-27/96177340.jpg" alt=""></p><h3 id="▌While-循环-1"><a href="#▌While-循环-1" class="headerlink" title="▌While 循环"></a>▌While 循环</h3><p>接下来，我们使用 While 循环再次尝试抓取，代码省略了和 For 循环中相同的部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        page = <span class="number">2</span> <span class="comment"># 设置爬取起始页数</span></span><br><span class="line">        dict = &#123;<span class="string">'page'</span>:page,<span class="string">'cate_code'</span>:cate_code,<span class="string">'child_cate_code'</span>:child_cate_code&#125; <span class="comment"># meta传递参数</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(category_url,callback=self.parse,meta=dict)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(response.body) &gt;= <span class="number">100</span>:  <span class="comment"># 判断该页是否爬完，数值定为100是因为无内容时长度是87</span></span><br><span class="line">        page = response.meta[<span class="string">'page'</span>]</span><br><span class="line">        cate_code = response.meta[<span class="string">'cate_code'</span>]</span><br><span class="line">        child_cate_code = response.meta[<span class="string">'child_cate_code'</span>]</span><br><span class="line">   <span class="comment">#...</span></span><br><span class="line">       <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># while循环构造url递归爬下一页</span></span><br><span class="line">        page += <span class="number">1</span></span><br><span class="line">        params = &#123;</span><br><span class="line">                <span class="string">'catId'</span>: cate_code,</span><br><span class="line">                <span class="string">'subCatId'</span>: child_cate_code,</span><br><span class="line">                <span class="string">'page'</span>: page,</span><br><span class="line">                &#125;</span><br><span class="line">        ajax_url = self.ajax_url + urlencode(params)</span><br><span class="line">        dict = &#123;<span class="string">'page'</span>:page,<span class="string">'cate_code'</span>:cate_code,<span class="string">'child_cate_code'</span>:child_cate_code&#125;</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(ajax_url,callback=self.parse,meta=dict)</span><br></pre></td></tr></table></figure><p>这里，简要说明几点：</p><p>第一、While 循环的思路是先从头开始爬取，使用 parse() 方法进行解析，然后递增页数构造下一页的 URL 请求，再循环解析，直到爬取完最后一页即可，这样 <strong>不会像 For 循环那样发送无用的请求</strong>。</p><p>第二、parse() 方法构造下一页请求时需要利用 start_requests() 方法中的参数，可以 <strong>使用 meta 方法来传递参数</strong>。</p><p>运行结果如下，可以看到请求数量刚好是 22 个，也就完成了所有页面的 App 信息爬取。</p><p><img src="http://media.makcyun.top/18-12-27/98316582.jpg" alt=""></p><p>以上，就是本文的所有内容，小结一下：</p><ul><li>在爬取不确定页数的网页时，可以采取 For 循环和 While 循环两种思路，方法大致相同。</li><li>在 Requests 和 Scrapy 中使用 For 循环和 While 循环的方法稍有不同，因此本文以豌豆荚网站为例，详细介绍了循环构造方法。</li><li>之所以写本文内容和之前的几篇文章（设置随机 UA、代理 IP），<strong>是为了下一篇文章「分析豌豆荚全网 70000+ App 信息」做铺垫，敬请期待。</strong></li></ul><h2 id="完整案例代码"><a href="#完整案例代码" class="headerlink" title="完整案例代码"></a>完整案例代码</h2><p>如需本文完整的案例代码，可以扫描下方图片二维码加入我的知识星球：「<strong>第2脑袋</strong>」，里面有很多干货，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-26/16445406.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython15.html">∞ Python 爬虫的代理 IP 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython14.html">∞ Python爬虫的随机 User-Agent 设置方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython13.html">∞ 爬虫断了？一招搞定 MongoDB 重复数据</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">∞ Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython6.html">∞ 50 行代码爬取东方财富网百万行财务报表数据</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">∞ pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Requests 和 Scrapy 中分别用 For 循环和 While 循环爬取不确定页数的网页。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python 爬虫的代理 IP 设置方法汇总</title>
    <link href="https://www.makcyun.top/2018/12/25/web_scraping_withpython15.html"/>
    <id>https://www.makcyun.top/2018/12/25/web_scraping_withpython15.html</id>
    <published>2018-12-25T08:16:24.000Z</published>
    <updated>2018-12-26T11:44:48.283Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Requests 和 Scrapy 中的代理 IP 设置。</p><a id="more"></a><p><strong>摘要：</strong>对于采取了比较强的反爬措施网站来说，要想顺利爬取网站数据，设置随机 User-Agent 和代理 IP 是非常有效的两个方法，继上一篇文章介绍了随机 UserAgent 的设置方法之后，本文接着介绍如何在 Requests 和 Scrapy 中设置代理 IP。</p><p>上一篇文章见：</p><p><a href="https://www.makcyun.top/web_scraping_withpython14.html">∞ Scrapy 中设置随机 User-Agent 的方法汇总</a></p><p>本文的目标测试网页选择下面这个 URL，请求该网页可以返回当前的 IP 地址：</p><p><a href="http://icanhazip.com" target="_blank" rel="noopener">∞ http://icanhazip.com</a></p><p>下面，我们就先来说说 Requests 中如何设置代理 IP。</p><h2 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h2><h3 id="▌不使用代理"><a href="#▌不使用代理" class="headerlink" title="▌不使用代理"></a>▌不使用代理</h3><p>首先，先来看一下不使用代理 IP 的情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://icanhazip.com'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(url) <span class="comment">#不使用代理</span></span><br><span class="line">    print(response.status_code)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.args)</span><br></pre></td></tr></table></figure><p>运行上面的程序，会返回我们电脑本机的 IP，可以通过百度查询 IP 地址对比一下就知道了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">200</span></span><br><span class="line"><span class="number">124.238</span><span class="number">.223</span>.xxx <span class="comment"># 后三位隐去了</span></span><br><span class="line"></span><br><span class="line">[Finished <span class="keyword">in</span> <span class="number">0.8</span>s]</span><br></pre></td></tr></table></figure><p></p><p><img src="http://media.makcyun.top/18-12-26/70746468.jpg" alt=""></p><h3 id="▌使用代理"><a href="#▌使用代理" class="headerlink" title="▌使用代理"></a>▌使用代理</h3><p>然后，我们测试一下使用代理后的情况。</p><p>常见的代理包括 HTTP 代理和 SOCKS5 代理，前者可以找一些免费代理 IP 进行测试，由于我电脑上使用的是 Shadowsocks，所以就介绍一下 SOCKS5 代理的设置。</p><p>首先，电脑上要安装有 Shadowsocks ，如果你还没听过或者使用过这个神器，可以参考下我之前写的篇文章：</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5NDk4NDcwMw==&amp;mid=2651385551&amp;idx=1&amp;sn=a9b1c43cd6738fa3e543344bcfcf037f&amp;scene=19&amp;ascene=0&amp;devicetype=android-25&amp;version=2607033c&amp;nettype=WIFI&amp;abtest_cookie=BAABAAoACwATABQABAAjlx4AV5keAJuZHgCcmR4AAAA%3D&amp;lang=zh_CN&amp;pass_ticket=hsEDDoXYSx3CI9%2BZQ8%2BsNNO8E8Ipn3zFIw6XokYPmRuhbrhXYZ9jTZkt6cGMZdKm&amp;wx_header=1" target="_blank" rel="noopener">∞ 如何正确地科学上网</a></p><p>启动该软件后默认会在 1080 端口下创建 SOCKS5 代理服务，代理为：<code>127.0.0.1:1080</code>，然后我们在 Requests 中使用该代理，方法很简单只需要添加一项 proxies 参数即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">proxies = [</span><br><span class="line">    &#123;<span class="string">'http'</span>:<span class="string">'socks5://127.0.0.1:1080'</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'https'</span>:<span class="string">'socks5://127.0.0.1:1080'</span>&#125;</span><br><span class="line">]</span><br><span class="line">proxies = random.choice(proxies)</span><br><span class="line">print(proxies)</span><br><span class="line">url = <span class="string">'http://icanhazip.com'</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(url,proxies=proxies) <span class="comment">#使用代理</span></span><br><span class="line">    print(response.status_code)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        print(response.text)</span><br><span class="line"><span class="keyword">except</span> requests.ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.args)</span><br></pre></td></tr></table></figure><p>这里，proxies 参数是字典类型，键名<code>&#39;http&#39;</code> 表示协议类型，键值 <code>&#39;socks5://127.0.0.1:1080&#39;</code>表示代理，这里添加了 http 和 https 两个代理，这样写是因为有些网页采用 http 协议，有的则是采用 https 协议，为了在这两类网页上都能顺利使用代理，所以一般都同时写上，当然，如果确定了某网页的请求类型，可以只写一种，比如这里我们请求的 url 使用的是 http 协议，那么使用 http 代理就可以，random 函数用来随机选择一个代理，我们来看一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:1080'</span>&#125;</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="number">45.78</span><span class="number">.42</span>.xxx <span class="comment">#xxx表示隐去了部分信息</span></span><br></pre></td></tr></table></figure><p>可以看到，这里随机选择了 http 协议的代理后，返回的 IP 就是我真实的 IP 代理地址，成功代理后就可以爬一些墙外的网页了。</p><p>延伸一下，假如随机选择的是 https 代理，那么返回的 IP 结果还一样么？我们尝试重复运行一下上面的程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:1080'</span>&#125;</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="number">124.238</span><span class="number">.223</span>.xxx</span><br></pre></td></tr></table></figure><p>可以看到这次使用了 https 代理，返回的 IP 却是本机的真实 IP，也就是说代理没有起作用。</p><p>进一步地，我们将 url 改为 https 协议 <code>&#39;https://icanhazip.com&#39;</code>，然后再尝试分别用 http 和 https 代理请求，查看一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#http 请求</span></span><br><span class="line">&#123;<span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:1080'</span>&#125;</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="number">124.238</span><span class="number">.223</span>.xxx</span><br><span class="line"></span><br><span class="line"><span class="comment">#https 请求</span></span><br><span class="line">&#123;<span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:1080'</span>&#125;</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="number">45.78</span><span class="number">.42</span>.xxx</span><br></pre></td></tr></table></figure><p>可以看到，两种请求的结果和之前的刚好相反了，由于 url 采用了 https 协议，则起作用的是 https 代理，而 http 代理则不起作用了，所以显示的是本机 IP。</p><p>因此，可以得到这样的一个结论：</p><p><strong>HTTP 代理，只代理 HTTP 网站，对于 HTTPS 的网站不起作用，也就是说，用的是本机 IP</strong>。</p><p>HTTPS 代理则同理。</p><h3 id="▌使用付费代理"><a href="#▌使用付费代理" class="headerlink" title="▌使用付费代理"></a>▌使用付费代理</h3><p>上面，我们只使用了一个代理，而在爬虫中往往需要使用多个代理，那有如何构造呢，这里主要两种方法，一种是使用免费的多个 IP，一种是使用付费的 IP 代理，免费的 IP 往往效果不好，那么可以搭建 IP 代理池，但对新手来说搞一个 IP 代理池成本太高，如果只是个人平时玩玩爬虫，完全可以考虑付费 IP，几块钱买个几小时动态 IP，多数情况下都足够爬一个网站了。</p><p>这里推荐一个付费代理「<a href="https://www.abuyun.com" target="_blank" rel="noopener">阿布云代理</a>」，最近使用了一下，效果非常不错，5 块钱买了 5个小时，爬完了一个网站，所以没有必要为了省 5 块钱，而费劲地去搞 IP 代理池。</p><p><img src="http://media.makcyun.top/18-12-26/96609111.jpg" alt=""></p><p>首次使用的话，可以选择购买一个小时的动态版试用下，点击生成隧道代理信息作为凭证加入到代码中。</p><p><img src="http://media.makcyun.top/18-12-26/97892605.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-12-26/92017323.jpg" alt=""></p><p>将信息复制到官方提供的 <a href="https://www.abuyun.com/http-proxy/dyn-manual-python.html" target="_blank" rel="noopener">Requests</a> 代码中，运行来查看一下代理 IP 的效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment"># 待测试目标网页</span></span><br><span class="line">targetUrl = <span class="string">"http://icanhazip.com"</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxies</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 代理服务器</span></span><br><span class="line">    proxyHost = <span class="string">"http-dyn.abuyun.com"</span></span><br><span class="line">    proxyPort = <span class="string">"9020"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 代理隧道验证信息</span></span><br><span class="line">    proxyUser = <span class="string">"HS77K12Q77V4G9MD"</span></span><br><span class="line">    proxyPass = <span class="string">"4131FFDFCE27F104"</span></span><br><span class="line"></span><br><span class="line">    proxyMeta = <span class="string">"http://%(user)s:%(pass)s@%(host)s:%(port)s"</span> % &#123;</span><br><span class="line">      <span class="string">"host"</span> : proxyHost,</span><br><span class="line">      <span class="string">"port"</span> : proxyPort,</span><br><span class="line">      <span class="string">"user"</span> : proxyUser,</span><br><span class="line">      <span class="string">"pass"</span> : proxyPass,</span><br><span class="line">    &#125;</span><br><span class="line">    proxies = &#123;</span><br><span class="line">        <span class="string">"http"</span>  : proxyMeta,</span><br><span class="line">        <span class="string">"https"</span> : proxyMeta,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">        resp = requests.get(targetUrl, proxies=proxies)</span><br><span class="line">        <span class="comment"># print(resp.status_code)</span></span><br><span class="line">        print(<span class="string">'第%s次请求的IP为：%s'</span>%(i,resp.text))</span><br><span class="line"> </span><br><span class="line">get_proxies()</span><br></pre></td></tr></table></figure><p>可以看到每次请求都会使用不同的 IP，是不是很简单？比搞 IP 代理池省事多了。</p><p><img src="http://media.makcyun.top/18-12-26/76840723.jpg" alt=""></p><p>以上，介绍了 Requests 中设置代理 IP 的方法，下面我们接着介绍在 Scrapy 中如何设置。</p><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><h3 id="▌middlewares-py-中设置"><a href="#▌middlewares-py-中设置" class="headerlink" title="▌middlewares.py 中设置"></a>▌middlewares.py 中设置</h3><p>这种方法需要先在 middlewares.py 中设置代理 IP 中间件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ip)</span>:</span></span><br><span class="line">        self.ip = ip</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(ip=crawler.settings.get(<span class="string">'PROXIES'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        ip = random.choice(self.ip)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = ip</span><br><span class="line">        logging.debug(<span class="string">'Using Proxy:%s'</span>%ip)</span><br></pre></td></tr></table></figure><p>接着，需要在 settings.py 添加几个在西刺上找的代理 IP，格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PROXIES = [</span><br><span class="line">    <span class="string">'https://127.0.0.1:8112'</span>, </span><br><span class="line">    <span class="string">'https://119.101.112.176:9999'</span>,</span><br><span class="line">    <span class="string">'https://119.101.115.53:9999'</span>,</span><br><span class="line">    <span class="string">'https://119.101.117.226:9999'</span>]</span><br></pre></td></tr></table></figure><p>然后，我们仍然以 “<a href="http://icanhazip.com&quot;" target="_blank" rel="noopener">http://icanhazip.com&quot;</a> 为目标网页，运行 Scrapy 项目重复请求 5 次，查看一下每次返回的 IP 情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        items = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">            item = <span class="keyword">yield</span> scrapy.Request(self.cate_url,callback=self.get_category)</span><br><span class="line">            items.append(item)</span><br><span class="line">        <span class="keyword">return</span> items</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(response.text)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="http://media.makcyun.top/18-12-26/64392950.jpg" alt=""></p><p>可以看到部分 IP 成功请求得到了相应，部分 IP 则无效请求失败，因为这几个 IP 是免费的 IP，所有失效很正常。</p><h3 id="▌使用付费代理-1"><a href="#▌使用付费代理-1" class="headerlink" title="▌使用付费代理"></a>▌使用付费代理</h3><p>接下来我们使用阿布云付费代理，继续尝试一下，在 middlewares.py 中添加下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 阿布云ip代理配置，包括账号密码 """</span></span><br><span class="line"><span class="comment"># 阿布云scrapy 写法</span></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line">proxyServer = <span class="string">"http://http-dyn.abuyun.com:9020"</span></span><br><span class="line">proxyUser = <span class="string">"HS77K12Q77V4G9MD"</span>  <span class="comment"># 购买后点击生成获得</span></span><br><span class="line">proxyPass = <span class="string">"4131FFDFCE27F104"</span><span class="comment"># 购买后点击生成获得</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for Python3</span></span><br><span class="line">proxyAuth = <span class="string">"Basic "</span> + base64.urlsafe_b64encode(bytes((proxyUser + <span class="string">":"</span> + proxyPass), <span class="string">"ascii"</span>)).decode(<span class="string">"utf8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbuyunProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" 阿布云ip代理配置 """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = proxyServer</span><br><span class="line">        request.headers[<span class="string">"Proxy-Authorization"</span>] = proxyAuth</span><br><span class="line">        logging.debug(<span class="string">'Using Proxy:%s'</span>%proxyServer)</span><br></pre></td></tr></table></figure><p>由于，在阿布云购买的是最基础的代理，即每秒 5 个请求，因为 Scrapy 默认的并发数是 16 个，所以需要对 Scrapy 请求数量进行一下限制，可以设置每个请求的延迟时间为 0.2s ，这样一秒就刚好请求 5 个，最后启用上面的代理中间件类即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 启用限速设置 """</span></span><br><span class="line">AUTOTHROTTLE_ENABLED = <span class="keyword">True</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">0.2</span>  <span class="comment"># 每次请求间隔时间</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line"> <span class="comment">#'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, #启用随机UA</span></span><br><span class="line"> <span class="comment">#'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 100,  #启用随机UA</span></span><br><span class="line"> <span class="string">'wandoujia.middlewares.AbuyunProxyMiddleware'</span>: <span class="number">200</span>, <span class="comment"># 启用阿布云代理</span></span><br><span class="line"> <span class="comment">#value值越小优先级越高</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后同样地请求 5 次，查看每次请求返回的 IP ：</p><p><img src="http://media.makcyun.top/18-12-26/82480128.jpg" alt=""></p><p>可以看到，每个 IP 都顺利请求成功了，所以说付费地效果还是好。</p><h3 id="▌使用-scrapy-proxies-库代理"><a href="#▌使用-scrapy-proxies-库代理" class="headerlink" title="▌使用 scrapy-proxies 库代理"></a>▌使用 scrapy-proxies 库代理</h3><p>除了上述两种方法，我们还可以使用 GitHub 上的一个 IP 代理库：<a href="https://github.com/aivarsk/scrapy-proxies" target="_blank" rel="noopener">scrapy-proxies</a>，库的使用方法很简单， 三个步骤就可以开启代理 IP。</p><p>首先，运行下面命令安装好这个库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy_proxies</span><br></pre></td></tr></table></figure><p>然后，在 Scrapy 项目中的 settings.py 文件中，添加下面一段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">RETRY_TIMES = <span class="number">3</span> <span class="comment"># 自定义请求失败重试次数</span></span><br><span class="line"><span class="comment">#重试的包含的错误请求代码</span></span><br><span class="line">RETRY_HTTP_CODES = [<span class="number">500</span>, <span class="number">503</span>, <span class="number">504</span>, <span class="number">400</span>, <span class="number">403</span>, <span class="number">404</span>, <span class="number">408</span>]</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>: <span class="number">90</span>,</span><br><span class="line">    <span class="string">'scrapy_proxies.RandomProxy'</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>: <span class="number">110</span>,</span><br><span class="line">&#125;</span><br><span class="line">PROXY_LIST = <span class="string">r'/proxies.txt'</span>  <span class="comment"># proxy 代理文件存放位置，此处为程序所在磁盘根目录下</span></span><br><span class="line">PROXY_MODE = <span class="number">0</span> <span class="comment"># 每次请求都使用不同的代理</span></span><br></pre></td></tr></table></figure><p>最后，需要提供多个代理 IP，我们在西刺上随便找几个 IP，然后存放在 PROXY_LIST 指定的 txt 文件中即可，格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://<span class="number">119.101</span><span class="number">.112</span><span class="number">.176</span>:<span class="number">9999</span></span><br><span class="line">https://<span class="number">119.101</span><span class="number">.115</span><span class="number">.53</span>:<span class="number">9999</span></span><br><span class="line">https://<span class="number">119.101</span><span class="number">.117</span><span class="number">.53</span>:<span class="number">999</span></span><br></pre></td></tr></table></figure><p>然后重复之前的操作，查看代理 IP 的设置效果。</p><p>我在使用该库的过程中，发现有一些问题，不知道是配置不对还是怎么回事，效果不是太好，所以推荐使用前两种方法。</p><p>好，以上就是在 Requests 和 Scrapy 中使用代理 IP 的方法总结，如果爬虫项目不大、追求稳定且不差钱的话，建议直接上付费代理。</p><p>如需完整实例代码，可以扫描下方图片二维码加入我的知识星球：「<strong>第2脑袋</strong>」，里面有很多干货，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-26/16445406.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython14.html">∞ Scrapy 中设置随机 User-Agent 的方法汇总</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython13.html">∞ 爬虫断了？一招搞定 MongoDB 重复数据</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">∞ 从函数 def 到类 Class</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">∞ 从 类 Class 到 Scrapy</a></p><p>参考：</p><p><a href="https://www.hitoy.org/difference-between-http-and-https-proxy.html" target="_blank" rel="noopener">∞ HTTP 代理和 HTTPS 代理的区别</a></p><p><a href="https://lilywei739.github.io/2017/01/25/principle_for_http_https.html" target="_blank" rel="noopener">∞ HTTP、HTTPS代理分析及原理</a></p><p><a href="https://vimcaw.github.io/blog/2017/08/13/ShadowsocksR%E4%BB%A3%E7%90%86%E6%96%B9%E5%BC%8F/" target="_blank" rel="noopener">∞ Shadowsocks 代理方式</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Requests 和 Scrapy 中的代理 IP 设置。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>入门必看|大佬们推荐的 Python 书单汇总</title>
    <link href="https://www.makcyun.top/2018/12/22/weekly_sharing9.html"/>
    <id>https://www.makcyun.top/2018/12/22/weekly_sharing9.html</id>
    <published>2018-12-22T08:16:16.000Z</published>
    <updated>2018-12-26T12:00:54.844Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>介绍 Python 大佬推荐的学习书，以及我入门 Python 的学习路径。</p><a id="more"></a><p><strong>摘要</strong>：此文主要针对想入门 Python 但不知道看什么书好和有选择纠结症的童鞋，大佬们可绕道。</p><p>写在前面：本文有很多超链接，建议点击底部「阅读原文」进行跳转查看。</p><p>这是「每周分享」的第 9 期，前 8 期基本都在分享一些我珍藏推荐的好软件、App，（前期文章汇总见底部推荐阅读处）数量不算少，够诸位消化一阵了。</p><p>转眼也到了年终，这一期换个话题，围绕这几个问题：「<strong>学习 Python 该看哪些书？不同的书该怎么看？按照什么样的顺序看？」</strong>，来聊一聊如何入门 Python，为了更有说服性一些，这里我把入门时看过的一些大佬推荐的书单进行了汇总，最后结合我的学习路径谈谈怎么读书。</p><p>半年前，Python 对我来说就是谜一样的东西，根本不知道如何下手、从何处下手，整天像无头苍蝇一样到处找资源，个把月过去了还没找到 Python 大门在哪儿，主要是花了很多的时间在纠结「<strong>该学习 Python 还是 R、学习 Python3 还是 Python 2 、看什么入门书最合适？」</strong>这些问题。知乎、豆瓣、CSDN、各大佬的公众号搜罗逛了一圈下来，只明确了前两个问题，就是要学习 Python，而且是 Python3，但对于看什么书，陷入了纠结迟迟下不了手。</p><p>现在看来，这应该是属于必经的过程，当涉足一个陌生的学习领域，对什么都不了解，即使别人给的建议再对，也会掂量犹豫几下。慢慢地，我开始进行总结，把一些大佬推荐的入门书籍文章进行汇总对比，然后就发现有些书是都在推荐的，于是决定重点就看这些书，这样才算慢慢摸到 Python 的大门。</p><p>话不多说，下面就分享 5 位大佬推荐的书单，除了入门书，还包括数据分析、数据挖掘、机器学习等方面，可以说是非常全面。</p><h3 id="▌刘志军-Python-之禅-作者"><a href="#▌刘志军-Python-之禅-作者" class="headerlink" title="▌刘志军 (Python 之禅 作者)"></a>▌<a href="https://mp.weixin.qq.com/s/kzQmj2adP-m0GdLFrVZYdA" target="_blank" rel="noopener">刘志军 (Python 之禅 作者)</a></h3><p>刘志军是位不折不扣的 Python 大佬，他博客中的 Python 文章最早可以追溯到 2013 年。</p><p><img src="http://media.makcyun.top/18-12-22/34020177.jpg" alt=""></p><h3 id="▌leoxin-菜鸟学-Python-作者"><a href="#▌leoxin-菜鸟学-Python-作者" class="headerlink" title="▌leoxin (菜鸟学 Python 作者)"></a>▌<a href="https://mp.weixin.qq.com/s/eUMcXAAnTFy73FWdWrQ1-g" target="_blank" rel="noopener">leoxin (菜鸟学 Python 作者)</a></h3><p>辛哥爬取分析了豆瓣 Python 相关的 1000 多本书籍，从各个角度找到了最受欢迎的书目，然后给出了自己的推荐。</p><p><img src="http://media.makcyun.top/18-12-22/40060223.jpg" alt=""></p><h3 id="▌刘顺祥-数据分析-1480-作者"><a href="#▌刘顺祥-数据分析-1480-作者" class="headerlink" title="▌刘顺祥 (数据分析 1480 作者)"></a>▌<a href="https://mp.weixin.qq.com/s/BX7PS2DO7OhgWYscX-G3HQ" target="_blank" rel="noopener">刘顺祥 (数据分析 1480 作者)</a></h3><p>刘顺祥大佬的公众号干货很多，入门时学习到很多。</p><p><img src="http://media.makcyun.top/18-12-22/64045072.jpg" alt=""></p><h3 id="▌秦路-七周成为数据分析师课程作者"><a href="#▌秦路-七周成为数据分析师课程作者" class="headerlink" title="▌秦路 (七周成为数据分析师课程作者)"></a>▌<a href="https://mp.weixin.qq.com/s/_NW-6TT1GdFE5VEDA_MRrQ" target="_blank" rel="noopener">秦路 (七周成为数据分析师课程作者)</a></h3><p>秦路大佬在天善智能开设的《七周成为数据分析师》课程非常全面，他的推荐非常值得参考。</p><p><img src="http://media.makcyun.top/18-12-22/74476124.jpg" alt=""></p><h3 id="▌王大伟-Python爱好者作者"><a href="#▌王大伟-Python爱好者作者" class="headerlink" title="▌王大伟 (Python爱好者作者)"></a>▌<a href="https://mobile.hellobi.com/#/blogs/detail/11325" target="_blank" rel="noopener">王大伟 (Python爱好者作者)</a></h3><p>王大伟大佬写的文章非常有趣，我看了他的几篇关于类（Class） 的文章后才彻底搞懂类是怎么回事。</p><p><img src="http://media.makcyun.top/18-12-22/69975073.jpg" alt=""></p><p>以上就是 5 位大佬的推荐，想必你心里大概有个谱了，下面再说说我看过的一些书，然后分享一下我的入门路径。</p><h3 id="▌我都看了哪些书"><a href="#▌我都看了哪些书" class="headerlink" title="▌我都看了哪些书"></a>▌我都看了哪些书</h3><p><img src="http://media.makcyun.top/18-12-22/1548248.jpg" alt=""></p><p>你可能注意到了，以上推荐了少说也有好几十本书，范围还是有点大，就算都是值得看的书，也没么多时间精力都去看，所以上面只是入门 Python 的第一个步骤，即筛选书的范围，还有更为重要的两个步骤。</p><p><strong>第一，首先要确先定你学你 Python 的目的</strong>。也就是你想学了去干嘛，是做爬虫、数据分析挖掘、机器学习、web 开发还是什么其他的，虽说不同的方向都需要有 Python 基础，但对 Python 的基础也是有所侧重，只有确定一个方向才可以进一步筛选书和书中章节的范围。</p><p><strong>第二，确定了书的范围后，要琢磨好怎么去看每一本书、以什么样的顺序去看书。</strong>不然，同时看好几本书，每一本都从头开始看，坚持不了几天就会放弃。</p><p>下面以我入门的过程来具体说一下。</p><p>由于我此前是零编程基础，helloworld 都不会打的那种，首先在知乎上看了几个 Python 入门的回答后，觉得用 Python 做数据分析这个方向不错，加上我此前学 Excel 时就对数据分析比较感兴趣，所以就确定了这个方向，但很快就发现行不通，因为我连基本的 Python 操作都不会，处处卡壳，时间都花在抠一个个的小问题上去了，折腾到最后也没太大兴趣去分析了，而且数据分析本身是有一套理论方法的，我更不会。然后我就想如果同时学 Python 操作和分析方法，比较耗费精力，所以就放弃直接学数据分析这个想法。</p><p>然后我选了另外一条路，就是爬虫，因为基础的爬虫比数据分析简单，学习曲线不陡，而且爬虫比较有意思，写出来别人也更愿意看，进一步了解到初步的爬虫学习主要学几个爬虫类库、网页解析提取库、框架这几块就行了，这样一下就缩小了书的选择范围和内容范围。</p><p>至此，我就选择了<strong>「Python 基础——爬虫——数据分析」</strong>这样一条路线。</p><p>首先，我选择了《深入浅出 Python 》这本书作为入门的第一本书，这本书浅显易懂，注释详尽，对新手很友好。接着，我又大致过了一遍《Python 编程从入门到实践》，前面几章写得非常实用，这样对 Python 就有了一个大致了解。</p><p>接着，便开始上手爬虫，但爬虫类的书非常少，起先只找到两本，一本是国外的《Python 网络数据采集》，书不厚，看了后大致了解了：爬虫是怎么一回事、爬虫能做什么、要会哪些东西等这几个问题，另一本是韦玮老师的《精通 Python 网络爬虫》，这本书当时觉得还不错，有很多实操案例，但是理论部分欠缺一些。<br>后来偶然搜到了崔庆才大佬的爬虫文章，很赞果断就买了他刚出的《Python3 网络爬虫实战》这本书，由此算是找到了爬虫方向。</p><p>之后通过爬虫把数据爬下来后就开始尝试一些简单的分析，但发现很多操作根本不熟练，于是采取了两种方法去学习，首先是谷歌解决实际问题，然后闲的时候翻看了《利用 Python 进行数据分析》、《流畅的 python》、《 Python Cookbook》这几本书，算是系统地巩固了一下相关知识。</p><p>就这样，几个月下来，练习了 10 个左右的爬虫，自认为算是入门了 Python 爬虫和数据分析。</p><p>以上就是本期的推荐，如果对里面的书感兴趣的话可以去找来看看，老规矩，为了更方便你，我这里准备好了部分电子书，<strong>公众号回复「Python书」就可以得到。</strong> 下载下来结合「静读天下」 App 看，会让手机阅读学习变得更简单。</p><p><img src="http://media2.makcyun.top/%E9%9D%99%E8%AF%BB%E5%A4%A9%E4%B8%8B.gif" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/weekly_sharing7.html">盘点那些手机上绝对值得安装的 App</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/weekly_sharing3.html">安卓最强阅读器</a></p><p><a href="https://www.makcyun.top/fuli01.html">PDF 阅读处理软件，你需要的都在这里了</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;介绍 Python 大佬推荐的学习书，以及我入门 Python 的学习路径。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="App" scheme="https://www.makcyun.top/tags/App/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy 中设置随机 User-Agent 的方法汇总</title>
    <link href="https://www.makcyun.top/2018/12/21/web_scraping_withpython14.html"/>
    <id>https://www.makcyun.top/2018/12/21/web_scraping_withpython14.html</id>
    <published>2018-12-21T08:16:24.000Z</published>
    <updated>2018-12-21T12:05:35.378Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>一行代码搞定 Scrapy 中的随机 UA 设置。</p><a id="more"></a><p><strong>摘要：</strong>爬虫过程中的反爬措施非常重要，其中设置随机 User-Agent 是一项重要的反爬措施，Scrapy 中设置随机 UA 的方式有很多种，有的复杂有的简单，本文就对这些方法进行汇总，提供一种只需要一行代码的设置方式。</p><p>最近使用 Scrapy 爬一个网站，遇到了网站反爬的情况，于是开始搜索一些反爬措施，了解到设置随机 UA 来伪装请求头是一种常用的方式，这能够做到一定程度上避免网站直接识别出你是一个爬虫从而封掉你。设置随机 UA 的方法有挺多种，有的需要好多行代码，有的却只需要一行代码就搞定了，接下来就来介绍下。</p><h3 id="▌常规设置-UA"><a href="#▌常规设置-UA" class="headerlink" title="▌常规设置 UA"></a>▌常规设置 UA</h3><p>首先，说一下常规情况不使用 Scrapy 时的用法，比较方便的方法是利用 <code>fake_useragent</code>包，这个包内置大量的 UA 可以随机替换，这比自己去搜集罗列要方便很多，下面来看一下如何操作。</p><p>首先，安装好<code>fake_useragent</code>包，一行代码搞定：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fake-useragent</span><br></pre></td></tr></table></figure><p>然后，就可以测试了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(ua.random)</span><br></pre></td></tr></table></figure><p>这里，使用了 ua.random 方法，可以随机生成各种浏览器的 UA，见下图：</p><p><img src="http://media.makcyun.top/18-12-21/34714572.jpg" alt=""></p><p>如果只想要某一个浏览器的，比如 Chrome ，那可以改成 <code>ua.chrome</code>，再次生成随机 UA 查看一下：</p><p><img src="http://media.makcyun.top/18-12-21/15257325.jpg" alt=""></p><p>以上就是常规设置随机 UA 的一种方法，非常方便。</p><p>下面，我们来介绍在 Scrapy 中设置随机 UA 的几种方法。</p><p>先新建一个 Project，命名为 <code>wanojia</code>，测试的网站选择为：<code>http://httpbin.org/get</code>。</p><p>首先，我们来看一下，如果不添加 UA 会得到什么结果，可以看到显示了<code>scrapy</code>，这样就暴露了我们的爬虫，很容易被封。</p><p><img src="http://media.makcyun.top/18-12-21/68168721.jpg" alt=""></p><p>下面，我们添加上 UA 。</p><h3 id="▌直接设置-UA"><a href="#▌直接设置-UA" class="headerlink" title="▌直接设置 UA"></a>▌直接设置 UA</h3><p><img src="http://media.makcyun.top/18-12-21/3222551.jpg" alt=""></p><p>第一种方法是和上面程序一样，直接在主程序中设置 UA，然后运行程序，通过下面这句命令可以输出该网站的 UA，见上图箭头处所示，每次请求都会随机生成 UA，这种方法比较简单，但是每个 requests 下的请求都需要设置，不是很方便，既然使用了 Scrapy，它提供了专门设置 UA 的地方，所以接下来我们看一下如何单独设置 UA。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.request.headers[<span class="string">'User-Agent'</span>]</span><br></pre></td></tr></table></figure><h3 id="▌手动添加-UA"><a href="#▌手动添加-UA" class="headerlink" title="▌手动添加 UA"></a>▌手动添加 UA</h3><p><img src="http://media.makcyun.top/18-12-21/52073035.jpg" alt=""></p><p>第二种方法，是在 settings.py 文件中手动添加一些 UA，然后通过 <code>random.choise</code> 方法随机调用，即可生成 UA，这种方便比较麻烦的就是需要自己去找 UA，而且增加了代码行数量。</p><h3 id="▌middlewares-py-中设置-UA"><a href="#▌middlewares-py-中设置-UA" class="headerlink" title="▌middlewares.py 中设置 UA"></a>▌middlewares.py 中设置 UA</h3><p>第三种方法，是使用 fake-useragent 包，在 middlewares.py 中间件中改写 process_request() 方法，添加以下几行代码即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgent</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        ua = UserAgent()</span><br><span class="line">        request.headers[<span class="string">'User-Agent'</span>] = ua.random</span><br></pre></td></tr></table></figure><p>然后，我们回到 <code>settings.py</code> 文件中调用自定义的 UserAgent，注意这里要先关闭默认的 UA 设置方法才行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>, </span><br><span class="line">    <span class="string">'wandoujia.middlewares.RandomUserAgent'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，我们成功得到了随机 UA。</p><p><img src="http://media.makcyun.top/18-12-21/43758922.jpg" alt=""></p><h3 id="▌一行代码设置-UA"><a href="#▌一行代码设置-UA" class="headerlink" title="▌一行代码设置 UA"></a>▌一行代码设置 UA</h3><p>可以看到，上面几种方法其实都不太方便，代码量也比较多，有没有更简单的设置方法呢？</p><p>有的，<strong>只需要一行代码就搞定，利用一款名为 <code>scrapy-fake-useragent</code> 的包。</strong></p><p>先贴一下该包的官方网址：<a href="https://pypi.org/project/scrapy-fake-useragent/" target="_blank" rel="noopener">https://pypi.org/project/scrapy-fake-useragent/</a>，使用方法非常简单，安装好然后使用就行了。</p><p>执行下面的命令进行安装，然后在 settings.py 中启用随机 UA 设置命令就可以了，非常简单省事。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy-fake-useragent</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: <span class="keyword">None</span>, <span class="comment"># 关闭默认方法</span></span><br><span class="line">    <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">400</span>, <span class="comment"># 开启</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们输出一下 UA 和网页 Response，可以看到成功输出了结果。</p><p><img src="http://media.makcyun.top/18-12-21/60286497.jpg" alt=""></p><p>以上就是 Scrapy 中设置随机 UA 的几种方法，推荐最后一种方法，即安装 <code>scrapy-fake-useragent</code> 库，然后在 settings 中添加下面这一行代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">400</span>,</span><br></pre></td></tr></table></figure><p>另外，反爬措施除了设置随机 UA 以外，还有一种非常重要的措施是设置随机 IP，我们后续再进行介绍。</p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython13.html">爬虫断了？一招搞定 MongoDB 重复数据</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">从函数 def 到类 Class</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">从 类 Class 到 Scrapy</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;一行代码搞定 Scrapy 中的随机 UA 设置。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫断了？一招搞定 MongoDB 重复数据</title>
    <link href="https://www.makcyun.top/2018/12/19/web_scraping_withpython13.html"/>
    <id>https://www.makcyun.top/2018/12/19/web_scraping_withpython13.html</id>
    <published>2018-12-19T08:16:24.000Z</published>
    <updated>2018-12-19T12:58:36.818Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>MongoDB 避免插入重复数据。</p><a id="more"></a><p><strong>摘要：</strong>尽量使用 update_one() 方法而不是 insert_one() 插入数据。</p><p>相信你一定有过这样的经历：大晚上好不容易写好一个爬虫，添加了种种可能出现的异常处理，测试了很多遍都没有问题，点击了 RUN 开始正式运行 ，然后美滋滋地准备钻被窝睡觉，睡前还特意检查了下确认没有问题，合上眼后期待着第二天起来，数据都乖乖地躺在 MongoDB 中。第二天早上一睁眼就满心欢喜地冲到电脑前，结果发现爬虫半夜断了，你气得想要砸电脑，然后你看了一下 MongoDB 中爬了一半的数据，在想是删掉重新爬，还是保留下来接着爬。</p><p>到这儿问题就来了，删掉太可惜，接着爬很可能会爬到重复数据，虽然后期可以去重，但你有强迫症，就是不想爬到重复数据，怎么办呢？</p><p>这就遇到了「爬虫断点续传」问题，关于这个问题的解决方法有很多种，不过本文主要介绍<strong>数据存储到 MongoDB 时如何做到只插入新数据，而重复数据自动过滤不插入。</strong></p><p>先来个简单例子，比如现在有两个 list ，data2 中的第一条数据和 data 列表中的第一条数据是重复的，我们想将这两个 list 依次插入 MnogoDB 中去， 通常我们会使用 insert_one() 或者 insert_many() 方法插入，这里我们使用 insert_one() 插入，看一下效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data = [</span><br><span class="line">&#123;<span class="string">'index'</span>:<span class="string">'A'</span>,<span class="string">'name'</span>:<span class="string">'James'</span>,<span class="string">'rank'</span>:<span class="string">'1'</span> &#125;,</span><br><span class="line">&#123;<span class="string">'index'</span>:<span class="string">'B'</span>,<span class="string">'name'</span>:<span class="string">'Wade'</span>,<span class="string">'rank'</span>:<span class="string">'2'</span> &#125;,</span><br><span class="line">&#123;<span class="string">'index'</span>:<span class="string">'C'</span>,<span class="string">'name'</span>:<span class="string">'Paul'</span>,<span class="string">'rank'</span>:<span class="string">'3'</span> &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">data2 = [</span><br><span class="line">&#123;<span class="string">'index'</span>:<span class="string">'A'</span>,<span class="string">'name'</span>:<span class="string">'James'</span>,<span class="string">'rank'</span>:<span class="string">'1'</span> &#125;,</span><br><span class="line">&#123;<span class="string">'index'</span>:<span class="string">'D'</span>,<span class="string">'name'</span>:<span class="string">'Anthony'</span>,<span class="string">'rank'</span>:<span class="string">'4'</span> &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>,<span class="number">27017</span>)</span><br><span class="line">db = client.Douban</span><br><span class="line">mongo_collection = db.douban</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">    mongo_collection.insert_one(i)</span><br></pre></td></tr></table></figure><p>插入第一个 list ：</p><p><img src="http://media.makcyun.top/18-12-19/73105799.jpg" alt=""></p><p>插入第二个 list ：</p><p><img src="http://media.makcyun.top/18-12-19/64313840.jpg" alt=""></p><p>你会发现，重复的数据 A 被插入进去了，那么怎么只插入 D，而不插入 A 呢，这里就要用到 update_one() 方法了，改写一下插入方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data2:</span><br><span class="line">    mongo_collection.update_one(i,&#123;<span class="string">'$set'</span>:i&#125;,upsert=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-12-19/5158090.jpg" alt=""></p><p>这里用到了 <code>$set</code> 运算符，该运算符作用是将字段的值替换为指定的值，upsert 为 True 表示插入。这里也可以用 update() 方法，但是这个方法比较老了，不建议使用。另外尝试使用 update_many() 方法发现不能更新多个相同的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data2:</span><br><span class="line">mongo_collection.update(i, i, upsert=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>下面举一个豆瓣电影 TOP250 的实例，假设我们先获取 10 个电影的信息，然后再获取前 20 个电影，分别用 insert_one() 和 update_one() 方法对比一下结果。</p><p>insert_one() 方法会重复爬取 前 10 个电影的数据：</p><p><img src="http://media.makcyun.top/18-12-19/34180814.jpg" alt=""></p><p>update_one() 方法则只会插入新的 10 个电影的数据：</p><p><img src="http://media.makcyun.top/18-12-19/19347856.jpg" alt=""></p><p>这就很好了对吧，所以当我们去爬那些需要分页的网站，最好在爬取之前使用 update_one() 方法，这样就算爬虫中断了，也不用担心会爬取重复数据。</p><p>​</p><p>​</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client.Douban</span><br><span class="line">mongo_collection = db.douban</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Douban</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.url = <span class="string">'https://api.douban.com/v2/movie/top250?'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(self, start_page)</span>:</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'start'</span>: start_page,</span><br><span class="line">            <span class="string">'count'</span>: <span class="number">10</span></span><br><span class="line">        &#125;</span><br><span class="line">        response = requests.get(self.url, params=params).json()</span><br><span class="line">        movies = response[<span class="string">'subjects'</span>]</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'rating'</span>: item[<span class="string">'rating'</span>][<span class="string">'average'</span>],</span><br><span class="line">            <span class="string">'genres'</span>:item[<span class="string">'genres'</span>],</span><br><span class="line">            <span class="string">'name'</span>:item[<span class="string">'title'</span>],</span><br><span class="line">            <span class="string">'actor'</span>:self.get_actor(item[<span class="string">'casts'</span>]),</span><br><span class="line">            <span class="string">'original_title'</span>:item[<span class="string">'original_title'</span>],</span><br><span class="line">            <span class="string">'year'</span>:item[<span class="string">'year'</span>],</span><br><span class="line">        &#125; <span class="keyword">for</span> item <span class="keyword">in</span> movies]</span><br><span class="line"></span><br><span class="line">        self.write_to_mongodb(data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_actor</span><span class="params">(self, actors)</span>:</span></span><br><span class="line">        actor = [i[<span class="string">'name'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> actors]</span><br><span class="line">        <span class="keyword">return</span> actor</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write_to_mongodb</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">if</span> mongo_collection.update_one(item, &#123;<span class="string">'$set'</span>: item&#125;, upsert=<span class="keyword">True</span>):</span><br><span class="line">                <span class="comment"># if mongo_collection.insert_one(item):</span></span><br><span class="line">                print(<span class="string">'存储成功'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'存储失败'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_douban</span><span class="params">(self, total_movie)</span>:</span></span><br><span class="line">        <span class="comment"># 每页10条，start_page循环1次</span></span><br><span class="line">        <span class="keyword">for</span> start_page <span class="keyword">in</span> range(<span class="number">0</span>, total_movie, <span class="number">10</span>):</span><br><span class="line">            self.get_content(start_page)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    douban = Douban()</span><br><span class="line">    douban.get_douban(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">从函数 def 到类 Class</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">从 类 Class 到 Scrapy</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;MongoDB 避免插入重复数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>盘点那些手机上绝对值得安装的 App（2）</title>
    <link href="https://www.makcyun.top/2018/12/15/weekly_sharing8.html"/>
    <id>https://www.makcyun.top/2018/12/15/weekly_sharing8.html</id>
    <published>2018-12-15T08:16:24.000Z</published>
    <updated>2019-01-08T08:17:08.322Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>这些良心 App 值得安装。</p><a id="more"></a><p>这是「每周分享」的第 8 期，这一期的主题接着上一期：「<strong>介绍一些值得安装的手机 App</strong> 」。</p><p>上一期介绍了前 3 排 共12 款 App，文章参见：</p><p><a href="https://www.makcyun.top/weekly_sharing7.html">盘点那些手机上绝对值得安装的 App</a></p><p>这一期介绍下面图中的后 12 款。</p><p><img src="http://media.makcyun.top/18-12-8/52805793.jpg" alt=""></p><h3 id="1-QQ-轻聊版"><a href="#1-QQ-轻聊版" class="headerlink" title="1. QQ 轻聊版"></a>1. QQ 轻聊版</h3><p>QQ 现在大家用地越来越少了，平常主要用来接收一些群消息，但还是属于必安的 App。</p><p><img src="http://media.makcyun.top/18-12-15/6834640.jpg" alt=""></p><p>不过普通的 QQ 版本做得让人很不爽，各种乱七八糟的订阅号会不停地弹出消息，底部提供的选项卡也都没什么用，一个新闻选项卡充斥着无聊的新闻，如果稍微没点自制力，就会「中毒」，从此一发而不可收拾，另一个动态选项卡，给你一堆诸如空间动态、兴趣部落、微视、直播这些功能，我不知道还有多少人常使用这些，至少我是觉得很鸡肋，完全没有必要出现在一个聊天通讯软件中。</p><p><img src="http://media.makcyun.top/18-12-15/33095857.jpg" alt=""></p><p>其实，我们需要的是一个功能纯粹简单的 QQ ，能收发消息就行，其他统统不要。<br>你可能会觉得我在说笑，你还别说，真的有这样的 QQ，下面就推荐这款我用了几年的「QQ 轻聊版」。顾名思义，就是很轻便的 QQ，<strong>没有乱七八糟的功能，专注于沟通聊天</strong>，上图直接对比两款 QQ 版本的界面就知道了。</p><h3 id="2-share-微博客户端"><a href="#2-share-微博客户端" class="headerlink" title="2. share 微博客户端"></a>2. share 微博客户端</h3><p><img src="http://media.makcyun.top/18-12-15/47402378.jpg" alt=""></p><p>很久没有刷微博的习惯了，为了这个 App 特地翻看了一下以前发的些微博，都是 2014 年的事了，那会儿还想当摄影师。</p><p><img src="http://media.makcyun.top/18-12-15/35092494.jpg" alt=""></p><p>如果你爱刷微博，那不妨试用下这款，和官方的微博版本相比这款第三方客户端轻便很多，颜值也高，<strong>用了它，能抵消一些微博海量信息带来的浮躁。</strong></p><h3 id="3-酷安"><a href="#3-酷安" class="headerlink" title="3. 酷安"></a>3. 酷安</h3><p><img src="http://media.makcyun.top/18-12-15/12138557.jpg" alt=""></p><p>这个 App 就不多说了，搞机爱好者的天堂，众多精品 App 都出自这个社区。</p><h3 id="4-纯纯写作"><a href="#4-纯纯写作" class="headerlink" title="4. 纯纯写作"></a>4. 纯纯写作</h3><p><img src="http://media.makcyun.top/18-12-15/28537303.jpg" alt=""></p><p>自从习惯使用 Markdown 以后， 电脑上的 Word 很少再打开，一直也希望在手机端也能够使用 Markdown ，找了很多 App 后，发现这款「纯纯写作」非常不错，除了 <strong>完美支持 Markdown</strong> 以外，还能够快速实现电脑和手机端的同步，如果你喜欢写作，那么用它会非常方便。</p><h3 id="5-快图浏览"><a href="#5-快图浏览" class="headerlink" title="5. 快图浏览"></a>5. 快图浏览</h3><p><img src="http://media.makcyun.top/18-12-15/99431294.jpg" alt=""></p><p>如果要给相册类 App 排个名的话，<strong>「快图浏览」说第二，那没有 App 敢称自己是第一了</strong>。 只有 3M 大小的它，能够瞬间发现和加载上万张图片，如果你是拍照狂魔，用它打开再多的照片也能秒开，另外还拥有隐藏私密照片、自动备份百度网盘等功能，非常的实用。</p><h3 id="6-静读天下"><a href="#6-静读天下" class="headerlink" title="6. 静读天下"></a>6. 静读天下</h3><p><img src="http://media.makcyun.top/18-12-16/91707523.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-12-16/66429540.jpg" alt=""></p><p>「静读天下」这款 App 可以说是 <strong>离线文档阅读类 App 中最牛逼的了</strong>，如果你爱手机阅读电子书，那么一定不要错过它，我之前专门写过一篇文章来介绍它的一些实用功能：</p><p><a href="https://www.makcyun.top/weekly_sharing3.html">安卓最强阅读器</a></p><p>如果想使用全部功能，需要付费购买专业版，也不贵，我这里直接是专业版，你懂的，如有条件尽量支持正版。</p><h3 id="7-WPS-阅读器"><a href="#7-WPS-阅读器" class="headerlink" title="7. WPS 阅读器"></a>7. WPS 阅读器</h3><p><img src="http://media.makcyun.top/18-12-15/56734553.jpg" alt=""></p><p>我们平常在手机中接触到的很多文件都是 PDF 形式，除了阅读以外，有时还会有这些需求：转换为 Word、提取 PDF、合并 PDF、转为图片、拍照扫描等，那么，这款 「WPS」能够完美实现这些功能。如果你想了解更多 PDF 处理的软件，可以看我之前写的这篇文章：</p><p><a href="https://www.makcyun.top/fuli01.html">PDF 阅读处理软件，你需要的都在这里了</a></p><h3 id="8-搜书大师"><a href="#8-搜书大师" class="headerlink" title="8. 搜书大师"></a>8. 搜书大师</h3><p><img src="http://media.makcyun.top/18-12-16/67124612.jpg" alt=""></p><p>如果你还在百度搜索下载电子书的话，那么已经非常 OUT 了，这款专门搜索下载电子书的 App 拥有海量的电子书资源，资源直接是百度资源，保存下载就能打开看了。</p><h3 id="9-海贝音乐"><a href="#9-海贝音乐" class="headerlink" title="9. 海贝音乐"></a>9. 海贝音乐</h3><p><img src="http://media.makcyun.top/18-12-15/27094486.jpg" alt=""></p><p>音乐类 App 网易云已经做得非常好了，但这里提这款「海贝音乐」是因为发现它有一个强大的功能是：<strong>能够自动识别不同文件夹下的音频文件然后播放</strong>，这有什么用呢，提示一下，如果你买了些网课，是分不同文件夹存在百度网盘里的，那么用它就对了。</p><h3 id="10-绿色守护"><a href="#10-绿色守护" class="headerlink" title="10. 绿色守护"></a>10. 绿色守护</h3><p><img src="http://media.makcyun.top/18-12-15/73086012.jpg" alt=""></p><p>这款 App 是 这期介绍的唯一一款系统类软件，它的作用就是能够强制休眠 App，如果你发现的手机一天耗电很快，那很可能是因为太多垃圾 App 在后台自启动了，使用它能够强制休眠这些 App ，增强你手机的的续航能力。</p><h3 id="11-Poweramp"><a href="#11-Poweramp" class="headerlink" title="11. Poweramp"></a>11. Poweramp</h3><p><img src="http://media.makcyun.top/18-12-15/88658946.jpg" alt=""></p><p>如果你爱听歌，且对音质很讲究，那么你很可能会喜欢上这款「Poweramp」，使用它的 <strong>自定义均衡器设置</strong>，你完全可以调出各种音质。</p><h3 id="12-咳咳"><a href="#12-咳咳" class="headerlink" title="12. 咳咳"></a>12. 咳咳</h3><p><img src="http://media.makcyun.top/18-12-15/22289595.jpg" alt=""></p><p>这是最后一款 App，听名字你就应该很熟悉了，可以看到外面丰富多彩的世界。</p><p>好，以上就是这一期介绍的 12 款佳软，如需可以在公众号后台回复「<strong>佳软2</strong>」得到部分，如果想获得全部，可以长按下方图片二维码加入我的知识星球：「<strong>第2脑袋</strong>」，里面有很多干货，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><center>欢迎点赞、评论和分享，利他最终一定利己.</center><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/weekly_sharing7.html">盘点那些手机上绝对值得安装的 App</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p><a href="https://www.makcyun.top/weekly_sharing3.html">安卓最强阅读器</a></p><p><a href="https://www.makcyun.top/fuli01.html">PDF 阅读处理软件，你需要的都在这里了</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这些良心 App 值得安装。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="App" scheme="https://www.makcyun.top/tags/App/"/>
    
  </entry>
  
  <entry>
    <title>从 Class 类到 Scrapy</title>
    <link href="https://www.makcyun.top/2018/12/14/web_scraping_withpython12.html"/>
    <id>https://www.makcyun.top/2018/12/14/web_scraping_withpython12.html</id>
    <published>2018-12-14T08:16:24.000Z</published>
    <updated>2019-02-27T08:23:36.077Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>对于 Python 初学者来说，习惯使用函数写代码后，开始学 Scrapy 会感到很复杂，不知如何下手写代码，本文通过实际案例，对比普通函数（类）和 Scrapy 中代码的写法，助你快速入门 Scrapy。</p><a id="more"></a><p><strong>摘要：</strong>通过实际爬虫案例，分别用普通函数（类）和 Scrapy 进行实现，通过代码，助你快速入门 Scrapy。</p><p>上一篇文章，我们通过 3 个实际爬虫案例，分别用函数（def）和 类（Class） 两种方法进行了实现，相信能够帮助你加深对类（Class）概念和用法的理解。在该文的第 3 个例子中，我们从类的写法延伸到了 Pyspider 中类代码的写法，本文进一步补充，通过实际爬虫案例分别用普通类的写法和 Scrapy 中类代码的写法进行实现。</p><p><a href="https://www.makcyun.top/web_scraping_withpython11.html">从函数 def 到类 Class</a></p><p>Scrapy 爬虫框架非常强大，但是初学起来会觉得有点复杂，因为完整的一段代码需要拆分放在不同的模块下，比如写一个爬虫，原先我们只需要用函数或者类从头写到尾即可，一目了然，但是在 Scrapy 中则不同，我们首先要在 <code>items.py</code> 中定义爬取的字段内容，在主程序模块中编写爬虫主程序，在 <code>pipeline.py</code>模块中实现数据处理、存储，在 <code>middlewares.py</code> 模块中定义代理 IP、UA 等。</p><p>总之代码的写法会发生一些变化，我在没适应用 Scrapy 之前，习惯在 Sublime 中完整地用函数实现一遍，然后再迁移到 Scrapy 框架中，虽然慢，但是写多几次后就适应了Scrapy 的写法，这比一上来就直接在 Scrapy 中写过渡地要顺利一些。</p><p>好，下面我们就以之前一篇爬取酷安 App 的文章为例进行说明，这篇文章用了 Scrapy 来实现，下面再用普通的函数写法实现一遍，并对关键的地方进行一下对比。</p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><h3 id="▌爬取思路分析"><a href="#▌爬取思路分析" class="headerlink" title="▌爬取思路分析"></a>▌爬取思路分析</h3><p>在上面这篇文章里，我面已经对 <a href="https://www.coolapk.com/apk?p=1" target="_blank" rel="noopener">目标网站</a> 进行了分析，这里简单回顾一下，便于把握后续的抓取思路。</p><p>首先，网页请求是 GET 形式，URL 只有一个页数递增参数，构造翻页非常简单。每页显示了 10 条 App 信息，通过点击尾页，发现一共有 610 页，也就是说一共有 6100 款左右的 App 。</p><p><img src="http://media.makcyun.top/18-12-14/90161707.jpg" alt=""></p><p>接下来，我们需要进入每一个 App 的主页，抓取 App 相关字段信息，确定了 8 个关键字段，分别是：App 名称、下载量、评分、评分人数、评论数、关注人数、体积、App 分类标签。</p><p><img src="http://media.makcyun.top/18-11-29/57226641.jpg" alt=""></p><p>然后，打开网页后台，利用正则表达式、CSS分别提取每个字段的信息即可。</p><p><img src="http://media.makcyun.top/18-11-30/61796640.jpg" alt=""></p><p>如果你还不熟悉正则、CSS、Xpath 这几种网页内容提取方法，可以参考我早先总结的这篇文章：</p><p><a href="https://www.makcyun.top/web_scraping_withpython1.html">四种方法爬取猫眼 TOP100 电影</a></p><p>通过上述分析，就可以确定爬取思路了：首先可以通过两种方式构造分页循环，一种是利用 for 循环直接构造 610 页 URL 链接，另外一种是获取下一页的节点，不断递增直到最后一页。第一种方式简单但只适合总页数确定的形式，第二种方式稍微复杂一点，但不管知不知道总页数都可以循环。</p><p>接着，每页抓取 10 款 App URL，进入 App 详情页后，利用 CSS 语法抓取每个 App 的 8 个字段信息，最后保存到 MongoDB中，结果形式如下：</p><p><img src="http://media.makcyun.top/18-11-30/67473511.jpg" alt=""></p><p>下面我们就来实操对比一下。</p><h3 id="▌获取网页-Response"><a href="#▌获取网页-Response" class="headerlink" title="▌获取网页 Response"></a>▌获取网页 Response</h3><p>首先，遍历每页的 URL 请求获得响应 Response，提取每款 App 主页的 URL 请求，以便下一步解析提取字段内容。</p><p>def 写法：</p><p>两次 for 循环，提取所有的 URL 链接，供下一步解析内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    url = <span class="string">'https://www.coolapk.com/apk?p=%s'</span> %page</span><br><span class="line">    response = requests.get(url,headers=headers).text</span><br><span class="line">    content = pq(response)(<span class="string">'.app_left_list&gt;a'</span>).items()</span><br><span class="line">    urls = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> content:</span><br><span class="line">        url = urljoin(<span class="string">'https://www.coolapk.com'</span>,item.attr(<span class="string">'href'</span>))</span><br><span class="line">        urls.append(url)</span><br><span class="line">    <span class="keyword">return</span> urls</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">610</span>): </span><br><span class="line">        get_page(page)</span><br></pre></td></tr></table></figure><p>Scrapy 写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KuspiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'kuspider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.coolapk.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.coolapk.com/apk/'</span>]</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        pages = []</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">610</span>):  </span><br><span class="line">            url = <span class="string">'https://www.coolapk.com/apk/?page=%s'</span> % page</span><br><span class="line">            page = scrapy.Request(</span><br><span class="line">                url, callback=self.parse, headers=self.headers)</span><br><span class="line">            pages.append(page)</span><br><span class="line">        <span class="keyword">return</span> pages</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">        <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">            url = content.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">            url = response.urljoin(url)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_url)</span><br></pre></td></tr></table></figure><p>这里有几点不同的地方，简单进行说明：</p><ul><li><p>循环构造方式不同</p><p>普通函数用两个 for 循环就可以，Scrapy 中是构造最外层的循环，实现方法是先构造一个空列表，存放 page，URL 构造好之后通过 scrapy.Request () 方法进行请求，获得响应 response ，传递给 callback 参数指定的 parse() 方法，再进一步进行第二个 for 循环。</p></li><li><p>内容提取形式不同</p><p>以 CSS 语法提取为例，普通函数和 Scrapy 中内容提取的方法稍有不同， 下面以提取提取单个节点文本、提取属性、提取多个节点，这三种最为常见的提取形式为例，将普通函数和 Scrapy 的写法进行对比：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#提取单个节点文本</span></span><br><span class="line">name = item(<span class="string">'.list_app_title'</span>).text()</span><br><span class="line">name = item.css(<span class="string">'.detail_app_title::text'</span>).extract_first()</span><br><span class="line"><span class="comment">#提取属性</span></span><br><span class="line">url = item(<span class="string">'.app_left_list&gt;a'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">url = item.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line"><span class="comment">#提取多个节点</span></span><br><span class="line">content = pq(response)(<span class="string">'.app_left_list&gt;a'</span>).items() </span><br><span class="line">contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br></pre></td></tr></table></figure></li></ul><p>这里顺便再说一下 Scrapy 遍历分页的第二种方式。</p><p>如果不通过构造 for 循环的方式遍历，可以先请求第一页获得 response 进行解析，然后再获取下一页 url 重复调用解析方法，直到解析完最后一页为止，这种方法 start_requests 构造就很简单，直接传递 url 到下一个 parse() 方法即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">   url = <span class="string">'https://www.coolapk.com/apk/?page=1'</span></span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">               url, callback=self.parse, headers=self.headers)</span><br><span class="line">           pages.append(page)</span><br><span class="line">       <span class="keyword">return</span> pages</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">   contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">   <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">       url = content.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">       url = response.urljoin(url)</span><br><span class="line">       <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_url)</span><br><span class="line">   </span><br><span class="line">   next_page = response.css(<span class="string">'.pagination li:nth-child(8) a::attr("href")'</span>).extract_first()</span><br><span class="line">   url = response.urljoin(next_page)</span><br><span class="line">   <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse )</span><br></pre></td></tr></table></figure><h3 id="▌解析网页提取字段"><a href="#▌解析网页提取字段" class="headerlink" title="▌解析网页提取字段"></a>▌解析网页提取字段</h3><p>接下来，我们就要提取App 名称、下载量、评分这些字段信息了。</p><p>def 写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_content</span><span class="params">(urls)</span>:</span></span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        response = requests.get(url,headers=headers).text</span><br><span class="line">        doc = pq(response) <span class="comment"># pyquery解析</span></span><br><span class="line">        name = doc(<span class="string">'.detail_app_title'</span>).remove(<span class="string">'span'</span>).text()</span><br><span class="line">        <span class="comment"># 若不想要子节点文本则先去除掉</span></span><br><span class="line">        results = get_comment(doc)</span><br><span class="line">        tags = get_tags(doc)</span><br><span class="line">        score = doc(<span class="string">'.rank_num'</span>).text()</span><br><span class="line">        <span class="comment"># 评论数</span></span><br><span class="line">        num_score = doc(<span class="string">'.apk_rank_p1'</span>).text()</span><br><span class="line">        num_score = re.search(<span class="string">'共(.*?)个评分'</span>,num_score).group(<span class="number">1</span>)</span><br><span class="line">        data =&#123;</span><br><span class="line">            <span class="string">'name'</span>:name,</span><br><span class="line">            <span class="string">'volume'</span>:results[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'download'</span>:results[<span class="number">1</span>],</span><br><span class="line">            <span class="string">'follow'</span>:results[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'comment'</span>:results[<span class="number">3</span>],</span><br><span class="line">            <span class="string">'tags'</span>:str(tags),</span><br><span class="line">            <span class="string">'score'</span>:score,</span><br><span class="line">            <span class="string">'num_score'</span>:num_score</span><br><span class="line">        &#125;</span><br><span class="line">        lst.append(data)</span><br><span class="line">        data = pd.DataFrame(lst)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>这里，值得注意一点：</p><p>pyquery 提取文本的时候，默认会提取节点内所有的文本内容，如果你只想要其中某个节点的，那么最好先删除掉不需要的节点，再提取文本。</p><p><img src="http://media.makcyun.top/18-11-30/61796640.jpg" alt=""></p><p>比如这里，我们在提取 app 名称的时候，如果直接用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name = doc(<span class="string">'.detail_app_title'</span>)text()</span><br></pre></td></tr></table></figure><p>提取出来的则是「酷安 8.8.3」，如果只想要「酷安」，不想要下面的版本信息：8.8.3，需要删除子节点 span 后再提取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name = doc(<span class="string">'.detail_app_title'</span>).remove(<span class="string">'span'</span>).text()</span><br></pre></td></tr></table></figure><p>Scrapy 写法：</p><p>获取字段信息，我们需要现在 settings.py 中设置，然后才能提取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KuanItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line"><span class="comment"># define the fields for your item here like:</span></span><br><span class="line">name = scrapy.Field()</span><br><span class="line">volume = scrapy.Field()</span><br><span class="line">download = scrapy.Field()</span><br><span class="line">follow = scrapy.Field()</span><br><span class="line">comment = scrapy.Field()</span><br><span class="line">tags = scrapy.Field()</span><br><span class="line">score = scrapy.Field()</span><br><span class="line">num_score = scrapy.Field()</span><br></pre></td></tr></table></figure><p>回到主程序中，通过 <code>item = Kuan2Item()</code> 来调用上面定义的字段信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        url = content.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">        url = response.urljoin(url)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_url)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    item = Kuan2Item()</span><br><span class="line">    item[<span class="string">'name'</span>] = response.css(<span class="string">'.detail_app_title::text'</span>).extract_first()</span><br><span class="line">    results = self.get_comment(response)</span><br><span class="line">    item[<span class="string">'volume'</span>] = results[<span class="number">0</span>]</span><br><span class="line">    item[<span class="string">'download'</span>] = results[<span class="number">1</span>]</span><br><span class="line">    item[<span class="string">'follow'</span>] = results[<span class="number">2</span>]</span><br><span class="line">    item[<span class="string">'comment'</span>] = results[<span class="number">3</span>]</span><br><span class="line">    item[<span class="string">'tags'</span>] = self.get_tags(response)</span><br><span class="line">    item[<span class="string">'score'</span>] = response.css(<span class="string">'.rank_num::text'</span>).extract_first()</span><br><span class="line">    num_score = response.css(<span class="string">'.apk_rank_p1::text'</span>).extract_first()</span><br><span class="line">    item[<span class="string">'num_score'</span>] = re.search(<span class="string">'共(.*?)个评分'</span>, num_score).group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h3 id="▌存储到-MongoDB"><a href="#▌存储到-MongoDB" class="headerlink" title="▌存储到 MongoDB"></a>▌存储到 MongoDB</h3><p>提取完信息以后，我们便可以选择将数据存储到 MongoDB 中。</p><p>通过上面的方法，我们提取出了字段内容 data，然后转换为了 DataFrame，DataFrame 存储到 MongoDB 非常简单，几行代码就能搞定。</p><p>def 写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>,<span class="number">27017</span>)</span><br><span class="line">db = client.KuAn</span><br><span class="line">mongo_collection = db.kuan</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_file</span><span class="params">(data)</span>:</span></span><br><span class="line">content = json.loads(data.T.to_json()).values()</span><br><span class="line">    <span class="keyword">if</span> mongo_collection.insert_many(content):</span><br><span class="line">        print(<span class="string">'存储到 mongondb 成功'</span>)</span><br></pre></td></tr></table></figure><p>这里用了 inset_many () 方法来插入数据，但其实不太建议，因为一旦出现爬虫中断，我们再接着爬的时候，它会插入重复数据，虽然我们可以再后续处理时去除重复数据，但有更好的方法，那就是用 update_one() 方法，该方法能够保证直插入新数据，重复数据不插入，下面我们在 Scrapy 中使用：</p><p>Scrapy 写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,mongo_url,mongo_db)</span>:</span></span><br><span class="line">        self.mongo_url = mongo_url</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_url = crawler.settings.get(<span class="string">'MONGO_URL'</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_url)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        <span class="comment"># update_one 方法可以不插入重复内容</span></span><br><span class="line">        self.db[name].update_one(item, &#123;<span class="string">'$set'</span>: item&#125;, upsert=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure><p>简单说明几点：</p><p>from crawler() 是一个类方法，用 ＠class method 标识，这个方法的作用主要是用来获取我们在 settings.py 中设置的这几项参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'KuAn'</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'kuan.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>open_spider() 方法主要进行一些初始化操作 ，在 Spider 开启时，这个方法就会被调用 。</p><p>process_item() 方法是最重要的方法，实现插入数据到 MongoDB 中。</p><p>Scrapy 字段提取后，通过 yield 返回的是生成器，内容是单个字典信息，此时，我们可以下面这句代码，实现只插入新数据，忽略重复数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.db[name].update_one(item, &#123;<span class="string">'$set'</span>: item&#125;, upsert=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>以上，我们从获取网页 Response、解析内容、MongoDB 存储三个方面，对比了普通函数和 Scrapy 代码的写法，这三部分内容是多数爬虫的主要部分。当然，还有其他的内容比如：下载图片、反爬措施等，我们留在后续的 Scrapy 文章中继续介绍。</p><p>如需完整代码，可以加入我的知识星球「<strong>第2脑袋</strong>」获取，里面有很多干货，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython12.html">从函数 def 到类 Class</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;对于 Python 初学者来说，习惯使用函数写代码后，开始学 Scrapy 会感到很复杂，不知如何下手写代码，本文通过实际案例，对比普通函数（类）和 Scrapy 中代码的写法，助你快速入门 Scrapy。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>盘点那些手机上绝对值得安装的 App</title>
    <link href="https://www.makcyun.top/2018/12/08/weekly_sharing7.html"/>
    <id>https://www.makcyun.top/2018/12/08/weekly_sharing7.html</id>
    <published>2018-12-08T08:16:24.000Z</published>
    <updated>2019-01-06T12:57:59.468Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>这些良心 App 值得安装。</p><a id="more"></a><p>这是「每周分享」的第 7 期，这一期的主题是「<strong>介绍手机上一些值得安装的 App</strong> 」。</p><p>最近写了一篇爬酷安 App 的文章：</p><p><a href="https://www.makcyun.top/web_scraping_withpython10.html">Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软</a></p><p>文章里面总结了一些非常良心好用的 App，堪称「神器」，我从里面优中选优，挑选了 24 款佳软（见下面这张图），好东西就要拿出来分享，所以今天就和你来介绍一下它们各自的功能特点，希望里面刚好有你需要的。这一期先介绍一半，也就是前 3 排 共12 款 App，剩下的留到下一期，可能有些你正在用或者用过，或者你有用到更好的 App，不妨留言告诉我。</p><p><img src="http://media.makcyun.top/18-12-8/52805793.jpg" alt=""></p><h3 id="1-存储空间清理"><a href="#1-存储空间清理" class="headerlink" title="1. 存储空间清理"></a>1. 存储空间清理</h3><p>先来第 1 款系统空间清理软件，这款 App 做得很精致，文件扫描和分析功能很强大，能够快速帮你到手机里的垃圾文件、大型文件，然后轻松有效释放手机空间，3.5 元就可以升级解锁所有功能，良心作者。</p><p><img src="http://media.makcyun.top/18-12-8/49781140.jpg" alt=""></p><h3 id="2-百度网盘"><a href="#2-百度网盘" class="headerlink" title="2. 百度网盘"></a>2. 百度网盘</h3><p>第 2 款属于重磅 App，不用过多介绍。以前的网盘还算良心，现在，呵呵。不是会员的话，下载速度低地可怜，这就催生一波不可描述版本，比如这款 SVIP 版，下载速度杠杠地，100K/s 和 3M/s 的差距。有人可能会说不安全会限速封号，反正我用了几个月至今木有问题。</p><p><img src="http://media.makcyun.top/18-12-8/27555347.jpg" alt=""></p><h3 id="3-山寨云"><a href="#3-山寨云" class="headerlink" title="3. 山寨云"></a>3. 山寨云</h3><p>第 3 款 App 也是和百度网盘有关，看来树大招风啊，尤其是歪的树，它的名字够爽快直白「就山寨你」，但功能可一点不含糊，最犀利的就是下载速度快，轻松达到 4M/s。另外一个牛逼的功能就是提供了多个网盘搜索源，可以直接搜索任何网盘资源，太良心了有木有。</p><p><img src="http://media.makcyun.top/18-12-8/63551641.jpg" alt=""></p><h3 id="4-强力监测"><a href="#4-强力监测" class="headerlink" title="4. 强力监测"></a>4. 强力监测</h3><p>这一款超高颜值的 App 主要用来监测你的手机运行状况，可视化查看手机温度、CPU占用、内存空间、电量等信息，一目了然。</p><p><img src="http://media.makcyun.top/18-12-8/15526497.jpg" alt=""></p><h3 id="5-ES-文件浏览器"><a href="#5-ES-文件浏览器" class="headerlink" title="5. ES 文件浏览器"></a>5. ES 文件浏览器</h3><p><img src="http://media.makcyun.top/18-12-8/88228268.jpg" alt=""></p><p>接下来这款「ES 文件浏览器」是手机文件管理浏览类 App，功能非常强大，随着手机使用越久，内存空间就被各种文档、垃圾文件占满了，用它能快速清理手机空间。另外它能将手机内的文档进行归档，比如图片、音乐、视频、文件。这就很方便，比如我最近看了「霸王别姬」后，被张国荣圈粉，于是下了很多他的音乐，很方便地就能在这个 App 中查看到。</p><h3 id="6-Root-Explorer"><a href="#6-Root-Explorer" class="headerlink" title="6. Root Explorer"></a>6. Root Explorer</h3><p><img src="http://media.makcyun.top/18-12-8/61526288.jpg" alt=""></p><p>这款 Root Explorer 我们在之前的文章分析中就曾多次提及，最牛逼的文件管理类 App 之一，能够卸载手机内置的 App，好东西不嫌多，强烈推荐，可以配合 ES 文件浏览器使用。</p><h3 id="7-一个木函"><a href="#7-一个木函" class="headerlink" title="7. 一个木函"></a>7. 一个木函</h3><p><img src="http://media.makcyun.top/18-12-8/57242201.jpg" alt=""></p><p>这款叫做 「一个木函」名字有点怪，但堪称「真神器」，不到 3M 大小的它拥有几十种实用强大的黑科技功能。</p><p>随便介绍两个功能，「电量伪装」有什么用呢，比如有时你想找借口停止聊天或者挂人电话，就可以「手机没电」为由，修改手机电量，然后发个图给他看，表明没有「忽悠」他。</p><p>「网速测试」功能很简单了，轻轻一点就能测出当前手机的网速。其他诸如：查快递、查 WIFI 密码、制作微信表情包、带壳截图这些功能都太实用了有木有。</p><p><img src="http://media.makcyun.top/18-12-8/86934596.jpg" alt=""></p><h3 id="8-MX-播放器"><a href="#8-MX-播放器" class="headerlink" title="8. MX 播放器"></a>8. MX 播放器</h3><p>下面这款「MX 播放器」可以说是最好用的视频播放器了，堪称手机中里的 potplayer，简洁地不能再简洁了，各种操作都可以通过手势来完成。觉得最好用的一项功能是结合前面的「ES 文件浏览器」然后加速播放百度网盘里的视频，也许你可能不太明白什么意思，简单来说就是百度网盘看视频不支持加速播放，很蛋疼对吧，但这款播放器就弥补了这个缺点，它最高支持 4 倍播放速度，原本 1 个小时的视频，用它只需要 15 分钟就能看完，时间就是金钱啊。</p><p><img src="http://media.makcyun.top/18-12-8/67449777.jpg" alt=""></p><h3 id="9-Via"><a href="#9-Via" class="headerlink" title="9. Via"></a>9. Via</h3><p>下面要强烈介绍的是两款浏览器 App，第一款 Via 堪称众多浏览器中的佼佼者，只有 500K 大小的它，简洁清爽，功能却一点不含糊，甚至比很多体积大得多的浏览器功能都要强大。</p><p>介绍下它的特点，就是提供了很多插件，插件就是外挂啊，比如知乎去网页限制、各大平台音乐下载、抖音下载等，以知乎这个插件来解释下什么意思，我们知道一般浏览器查看知乎内容时，如果想看全文，它会强制你下载知乎 App 才能看，很麻烦对不对，但是用这个浏览器，你就不用下载 App 了，能够直接看全文内容，很爽对吧。其他平台的 App 都类似，也就是说 <strong>用了这个浏览器，能让你少安装很多 App</strong>。</p><p><img src="http://media.makcyun.top/18-12-8/95207665.jpg" alt=""></p><h3 id="10-X-浏览器"><a href="#10-X-浏览器" class="headerlink" title="10. X 浏览器"></a>10. X 浏览器</h3><p>第 2 款浏览器「X 浏览器」，是真牛 X，同样的走简洁风，提供的功能很多包括：电脑桌面模式、护眼模式、无痕模式等，说说「无痕模式」，<strong>它可以隐藏你的网页浏览痕迹</strong>，假如你想上点不可描述的网站，也不会怕别人翻手机看到。</p><p><img src="http://media.makcyun.top/18-12-8/19507692.jpg" alt=""></p><p>再说一个「拦截广告」功能，它可以智能拦截一些广告，比如百度搜索「python 书」，普通模式就会弹出很多广告，而使用了拦截广告功能，就看不到那些烦人的广告了，很爽对吧。</p><p><img src="http://media.makcyun.top/18-12-8/12381698.jpg" alt=""></p><h3 id="11-幸运破解器"><a href="#11-幸运破解器" class="headerlink" title="11. 幸运破解器"></a>11. 幸运破解器</h3><p>接下来推荐一款很有意思的 App「幸运破解器」，名字起得很贴切，它可以对一些 App 进行破解，比如去除广告、破解付费内容等，很强大，你可以去一个个去尝试想要破解的 App，运气好的话就能成功破解。</p><p><img src="http://media.makcyun.top/18-12-8/37225881.jpg" alt=""></p><h3 id="12-TickTick"><a href="#12-TickTick" class="headerlink" title="12. TickTick"></a>12. TickTick</h3><p>最后一款 「TickTick」 是时间任务管理类 App，中文版叫「滴答清单」，很好用，提供的功能非常多。如果你想成为一个时间管理高手，那这款 App 能帮到你，但是它的很多功能都是付费的，价格还不菲。</p><p>如果你细心的话，可以看到图中这款我用的 App 截图箭头所指的地方，还剩 17000 多天的试用期，折算就是差不多 50 年，嘿嘿。</p><p><img src="http://media.makcyun.top/18-12-8/93851101.jpg" alt=""></p><p>以上就是这一期介绍的 12 款佳软，如需可以在公众号后台回复「<strong>佳软</strong>」得到部分，如果想获得全部，可以长按下方图片二维码加入我的知识星球：「<strong>第2脑袋</strong>」，里面有很多干货，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><center>欢迎点赞、评论和分享，利他最终一定利己.</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;这些良心 App 值得安装。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="App" scheme="https://www.makcyun.top/tags/App/"/>
    
  </entry>
  
  <entry>
    <title>从函数 def 到类 Class</title>
    <link href="https://www.makcyun.top/2018/12/07/web_scraping_withpython11.html"/>
    <id>https://www.makcyun.top/2018/12/07/web_scraping_withpython11.html</id>
    <published>2018-12-07T08:16:24.000Z</published>
    <updated>2018-12-14T09:41:16.112Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>实例对比，快速上手 Python 类（Class）和 Pyspider 代码的写法。</p><a id="more"></a><p><strong>摘要：</strong>初学 Python 过程中，我们可能习惯了使用函数（def），在开始学习类（Class）的用法时，可能会觉得它的写法别扭，类的代码写法也不像函数那么简单直接，也会产生「有了函数为什么还需要类」的疑问。然而面向对象编程是 Python 最重要的思想，类（Class）又是面向对象最重要的概念之一，所以要想精通 Python ，则必须得会使用类（Class）来编写代码，而且 Pyspider 和 Scrapy 两大框架都使用了类的写法，基于此，本文将介绍如何从函数的写法顺利过渡到类的编写习惯。</p><p>关于类（Class）的教程，网上主要有两类，一类是廖雪峰大佬的，另一类是不加说明默认你已经会这种写法，而直接使用的。</p><p>廖雪峰的教程非常棒，但是更适合入门 Python 有一段时间或者看过一些更基础的教程之后再回过头来看，否则可能会觉得他的教程理论性重于实用性。我第一次看了他的教程中关于类的相关知识后，觉得理解了，但一尝试自己写时就不太会了。</p><p>第二类教程，网上有很多案例，这类教程存在的问题就是，你能看懂意思，但还是不太会运用到自己的案例中。</p><p>总结一下这两类教程对新手不友好的地方就是 <strong>没有同时给出两种写法的实例</strong>，这就没办法对比，而「对比学习」是一种学习新知识非常快的途径，简单来说就是学新知识的时候，先从我们已经掌握的知识出发，和新知识进行对比，快速找到共同点和不同点，共同点我们能很快掌握，针对不同点通过对比去感悟领会，从而快速学会新知识。</p><p>接下来，就举几个同时使用了函数写法和类的写法的案例，希望能够帮助你快速完成从函数到类的编程思想的过渡转换。</p><h3 id="▌爬取豆瓣电影-TOP250"><a href="#▌爬取豆瓣电影-TOP250" class="headerlink" title="▌爬取豆瓣电影 TOP250"></a>▌爬取豆瓣电影 TOP250</h3><p>第一个案例：爬取豆瓣电影 TOP250，我们的目标是通过调用豆瓣 API 接口，获取电影名称、评分、演员等信息，然后存储到 CSV 文件中，部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(start_page)</span>:</span></span><br><span class="line">    url = <span class="string">'https://api.douban.com/v2/movie/top250?'</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'start'</span>:start_page,</span><br><span class="line">        <span class="string">'count'</span>:<span class="number">50</span></span><br><span class="line">        &#125;</span><br><span class="line">    response = requests.get(url,params=params).json()</span><br><span class="line">    movies = response[<span class="string">'subjects'</span>]</span><br><span class="line">    data = [&#123;</span><br><span class="line">        <span class="string">'rating'</span>:item[<span class="string">'rating'</span>][<span class="string">'average'</span>],</span><br><span class="line">        <span class="string">'genres'</span>:item[<span class="string">'genres'</span>],</span><br><span class="line">        <span class="string">'name'</span>:item[<span class="string">'title'</span>],</span><br><span class="line">        <span class="string">'actor'</span>:get_actor(item[<span class="string">'casts'</span>]),</span><br><span class="line">        <span class="string">'original_title'</span>:item[<span class="string">'original_title'</span>],</span><br><span class="line">        <span class="string">'year'</span>:item[<span class="string">'year'</span>],</span><br><span class="line">    &#125; <span class="keyword">for</span> item <span class="keyword">in</span> movies]</span><br><span class="line">    write_to_file(data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_actor</span><span class="params">(actors)</span>:</span></span><br><span class="line">    actor = [i[<span class="string">'name'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> actors]</span><br><span class="line">    <span class="keyword">return</span> actor</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'douban_def.csv'</span>,<span class="string">'a'</span>,encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        w = csv.writer(f)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">            w.writerow(item.values())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_douban</span><span class="params">(total_movie)</span>:</span></span><br><span class="line">    <span class="comment"># 每页50条，start_page循环5次</span></span><br><span class="line">    <span class="keyword">for</span> start_page <span class="keyword">in</span> range(<span class="number">0</span>,total_movie,<span class="number">50</span>):</span><br><span class="line">        get_content(start_page)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    get_douban(<span class="number">250</span>)</span><br></pre></td></tr></table></figure><p>打开 CSV 文件查看输出的结果：</p><p><img src="http://media.makcyun.top/18-12-6/61971916.jpg" alt=""></p><p>以上，我们通过四个函数就完成了数据的爬取和存储，逻辑很清晰，下面我们使用类的写法实现同样的功能，部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Douban</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.url = <span class="string">'https://api.douban.com/v2/movie/top250?'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(self,start_page)</span>:</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'start'</span>:start_page,</span><br><span class="line">            <span class="string">'count'</span>:<span class="number">50</span></span><br><span class="line">            &#125;</span><br><span class="line">        response = requests.get(self.url,params=params).json()</span><br><span class="line">        movies = response[<span class="string">'subjects'</span>]</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'rating'</span>:item[<span class="string">'rating'</span>][<span class="string">'average'</span>],</span><br><span class="line">            <span class="string">'genres'</span>:item[<span class="string">'genres'</span>],</span><br><span class="line">            <span class="string">'name'</span>:item[<span class="string">'title'</span>],</span><br><span class="line">            <span class="string">'actor'</span>:self.get_actor(item[<span class="string">'casts'</span>]),</span><br><span class="line">            <span class="string">'original_title'</span>:item[<span class="string">'original_title'</span>],</span><br><span class="line">            <span class="string">'year'</span>:item[<span class="string">'year'</span>],</span><br><span class="line">        &#125; <span class="keyword">for</span> item <span class="keyword">in</span> movies]</span><br><span class="line">        self.write_to_file(data)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_actor</span><span class="params">(self,actors)</span>:</span></span><br><span class="line">        actor = [i[<span class="string">'name'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> actors]</span><br><span class="line">        <span class="keyword">return</span> actor</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write_to_file</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'douban_class.csv'</span>,<span class="string">'a'</span>,encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">            w = csv.writer(f)</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">                w.writerow(item.values())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_douban</span><span class="params">(self,total_movie)</span>:</span></span><br><span class="line">        <span class="comment"># 每页50条，start_page循环5次</span></span><br><span class="line">        <span class="keyword">for</span> start_page <span class="keyword">in</span> range(<span class="number">0</span>,total_movie,<span class="number">50</span>):</span><br><span class="line">            self.get_content(start_page)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    douban = Douban()</span><br><span class="line">    douban.get_douban(<span class="number">250</span>)</span><br></pre></td></tr></table></figure><p>可以看到，上面的案例中，类的写法和函数的写法大部分都是一样的，仅少数部分存在差异。主要有这么几点差异：</p><ul><li><p>增加了一个 <code>__init__</code>函数。</p><p>这是一个特殊的函数，它的作用主要是事先把一些重要的属性填写进来，它的特点是第一个参数永远是<code>self</code>，表示创建的实例本身，这里的实例就是最下面的 <code>douban</code>（实例通过类名+() 创建）。</p></li><li><p>类中的函数和普通的函数相比，只有一点不同。</p><p>类中的函数（也称为方法）的第一个参数永远是实例变量<code>self</code>，并且调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别。</p></li><li><p>在执行类的时候需要先实例化</p><p>这里我们定义了一个类，类名是 Douban（首字母要大写），在运行类的时候，需要先实例化，这里实例化为 <code>douban</code>，然后调用 get_douban（）方法完成数据的爬取和存储。</p></li></ul><p>下面，我们再来看看第二个例子。</p><h3 id="▌模拟登陆-IT-桔子"><a href="#▌模拟登陆-IT-桔子" class="headerlink" title="▌模拟登陆 IT 桔子"></a>▌模拟登陆 IT 桔子</h3><p><img src="http://media.makcyun.top/18-12-6/12211046.jpg" alt=""></p><p>我们使用 Selenium 模拟登陆 IT 桔子网并输出网页源码，使用函数的部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    browser = webdriver.Chrome()</span><br><span class="line">    browser.get(<span class="string">'https://www.itjuzi.com/user/login'</span>)</span><br><span class="line">    account = browser.find_element(By.ID, <span class="string">"create_account_email"</span>)</span><br><span class="line">    password = browser.find_element(By.ID, <span class="string">"create_account_password"</span>)</span><br><span class="line">    account.send_keys(<span class="string">'irw27812@awsoo.com'</span>) <span class="comment"># 输入账号和密码</span></span><br><span class="line">    password.send_keys(<span class="string">'test2018'</span>) <span class="comment"># 输入账号密码</span></span><br><span class="line">    submit = browser.find_element(By.ID,<span class="string">"login_btn"</span>)</span><br><span class="line">    submit.click() <span class="comment"># 点击登录按钮</span></span><br><span class="line">    get_content()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>)</span><br><span class="line">    print(browser.page_source)  <span class="comment"># 输出网页源码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">login()</span><br></pre></td></tr></table></figure><p>以上代码实现了自动输入账号密码然后进入 IT 桔子网，下面我们再来看看类的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,account,password)</span>:</span></span><br><span class="line">        self.login_url = <span class="string">'https://www.itjuzi.com/user/login'</span></span><br><span class="line">        self.get_url = <span class="string">'http://radar.itjuzi.com/investevent'</span></span><br><span class="line">        self.account = account</span><br><span class="line">        self.password = password</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">        browser.get(self.login_url)</span><br><span class="line">        account = browser.find_element(By.ID, <span class="string">"create_account_email"</span>)</span><br><span class="line">        password = browser.find_element(By.ID, <span class="string">"create_account_password"</span>)</span><br><span class="line">        account.send_keys(self.account) <span class="comment"># 输入账号和密码</span></span><br><span class="line">        password.send_keys(self.password)</span><br><span class="line">        submit = browser.find_element(By.ID,<span class="string">"login_btn"</span>)</span><br><span class="line">        submit.click() <span class="comment"># 点击登录按钮</span></span><br><span class="line">        self.get_content()   <span class="comment"># 调用下面的方法</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(self)</span>:</span></span><br><span class="line">        browser.get(self.get_url)</span><br><span class="line">        print(browser.page_source)  <span class="comment"># 输出网页源码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = Spider(account=<span class="string">'irw27812@awsoo.com'</span>,password=<span class="string">'test2018'</span>)</span><br><span class="line">    <span class="comment"># 当有其他账号时，在这里更改即可,很方便</span></span><br><span class="line">    <span class="comment"># spider = Spider('fru68354@nbzmr.com','test2018')</span></span><br><span class="line">    spider.login()</span><br></pre></td></tr></table></figure><p>这里，我们将一些固定的参数，比如 URL、Headers 都放在 <code>__init__</code> 方法中，需要的时候在各个函数中进行调用，这样的写法逻辑更加清晰。</p><p>下面，我们再看看第三个例子爬取虎嗅文章，从普通类的写法过渡到 pyspider 框架中类的写法，这样有助于快速上手 pyspider 框架。</p><h3 id="▌爬取虎嗅文章"><a href="#▌爬取虎嗅文章" class="headerlink" title="▌爬取虎嗅文章"></a>▌爬取虎嗅文章</h3><p><img src="http://media.makcyun.top/18-12-6/73018539.jpg" alt=""></p><p>我们目标是通过分析 AJAX 请求，遍历爬取虎嗅网的文章信息，先来看看普通类的写法，部分代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>,<span class="number">27017</span>)</span><br><span class="line">db = client.Huxiu</span><br><span class="line">mongo_collection = db.huxiu_news</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Huxiu</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span></span><br><span class="line">            &#125;</span><br><span class="line">        self.url = <span class="string">'https://www.huxiu.com/v2_action/article_list'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(self,page)</span>:</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'page'</span>: page,</span><br><span class="line">            &#125;</span><br><span class="line">        response = requests.post(self.url,data=data,headers=self.headers)</span><br><span class="line">        self.parse_content(response)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_content</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        content = response.json()[<span class="string">'data'</span>]</span><br><span class="line">        doc = pq(content)</span><br><span class="line">        lis = doc(<span class="string">'.mod-art'</span>).items()</span><br><span class="line">        data =[&#123;</span><br><span class="line">        <span class="string">'title'</span>: item(<span class="string">'.msubstr-row2'</span>).text(),</span><br><span class="line">        <span class="string">'url'</span>:<span class="string">'https://www.huxiu.com'</span>+ str(item(<span class="string">'.msubstr-row2'</span>).attr(<span class="string">'href'</span>)),</span><br><span class="line">        <span class="string">'name'</span>: item(<span class="string">'.author-name'</span>).text(),</span><br><span class="line">        <span class="string">'write_time'</span>:item(<span class="string">'.time'</span>).text(),</span><br><span class="line">        <span class="string">'comment'</span>:item(<span class="string">'.icon-cmt+ em'</span>).text(),</span><br><span class="line">        <span class="string">'favorites'</span>:item(<span class="string">'.icon-fvr+ em'</span>).text(),</span><br><span class="line">        <span class="string">'abstract'</span>:item(<span class="string">'.mob-sub'</span>).text()</span><br><span class="line">        &#125; <span class="keyword">for</span> item <span class="keyword">in</span> lis] </span><br><span class="line">        self.save_to_file(data)</span><br><span class="line">    <span class="comment"># 存储到 mongodb</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_to_file</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        df = pd.DataFrame(data)</span><br><span class="line">        content = json.loads(df.T.to_json()).values()</span><br><span class="line">        <span class="keyword">if</span> mongo_collection.insert_many(content):</span><br><span class="line">            print(<span class="string">'存储到 mongondb 成功'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'存储失败'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_huxiu</span><span class="params">(self,start_page,end_page)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page,end_page) :</span><br><span class="line">            print(<span class="string">'正在爬取第 %s 页'</span> % page)</span><br><span class="line">            self.get_content(page)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    huxiu = Huxiu()</span><br><span class="line">    huxiu.get_huxiu(<span class="number">1</span>,<span class="number">2000</span>)</span><br></pre></td></tr></table></figure><p>然后再看看在 Pyspider 中的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config:&#123;</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span></span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 修改taskid，避免只下载一个post请求</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self,task)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> md5string(task[<span class="string">'url'</span>]+json.dumps(task[<span class="string">'fetch'</span>].get(<span class="string">'data'</span>,<span class="string">''</span>)))</span><br><span class="line">     </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">2000</span>):</span><br><span class="line">            print(<span class="string">'正在爬取第 %s 页'</span> % page)</span><br><span class="line">            self.crawl(<span class="string">'https://www.huxiu.com/v2_action/article_list'</span>,method=<span class="string">'POST'</span>,data=&#123;<span class="string">'page'</span>:page&#125;, callback=self.index_page)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        content = response.json[<span class="string">'data'</span>]</span><br><span class="line">        <span class="comment"># 注意，在上面的class写法中，json后面需要添加()，pyspider中则不用</span></span><br><span class="line">        doc = pq(content)</span><br><span class="line">        lis = doc(<span class="string">'.mod-art'</span>).items()</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'title'</span>: item(<span class="string">'.msubstr-row2'</span>).text(),</span><br><span class="line">            <span class="string">'url'</span>:<span class="string">'https://www.huxiu.com'</span>+ str(item(<span class="string">'.msubstr-row2'</span>).attr(<span class="string">'href'</span>)),</span><br><span class="line">            <span class="string">'name'</span>: item(<span class="string">'.author-name'</span>).text(),</span><br><span class="line">            <span class="string">'write_time'</span>:item(<span class="string">'.time'</span>).text(),</span><br><span class="line">            <span class="string">'comment'</span>:item(<span class="string">'.icon-cmt+ em'</span>).text(),</span><br><span class="line">            <span class="string">'favorites'</span>:item(<span class="string">'.icon-fvr+ em'</span>).text(),</span><br><span class="line">            <span class="string">'abstract'</span>:item(<span class="string">'.mob-sub'</span>).text()</span><br><span class="line">            &#125; <span class="keyword">for</span> item <span class="keyword">in</span> lis ] </span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_result</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            self.save_to_mongo(result)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        df = pd.DataFrame(result)</span><br><span class="line">        content = json.loads(df.T.to_json()).values()</span><br><span class="line">        <span class="keyword">if</span> mongo_collection.insert_many(content):</span><br><span class="line">            print(<span class="string">'存储到 mongondb 成功'</span>)</span><br></pre></td></tr></table></figure><p>可以看到，pyspider 中主体部分和普通类的写法差不多，不同的地方在于 pyspider 中有一些固定的语法，这可以通过参考 pyspider 教程快速掌握。</p><p>通过以上三个例子的对比，我们可以感受到函数（def）、 类（Class）和 pyspider 三种代码写法的异同点，采取这样对比式的学习能够快速掌握新的知识。</p><p>如需完整代码，可以加入我的知识星球「<strong>第2脑袋</strong>」获取，里面有很多干货，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython7.html">Selenium 爬取 IT 桔子创业公司信息</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;实例对比，快速上手 Python 类（Class）和 Pyspider 代码的写法。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>生活随笔：27 岁，强烈的中年危机感</title>
    <link href="https://www.makcyun.top/2018/12/03/life03.html"/>
    <id>https://www.makcyun.top/2018/12/03/life03.html</id>
    <published>2018-12-03T08:16:24.000Z</published>
    <updated>2018-12-03T14:29:40.993Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>加油吧，骚年。</p><a id="more"></a><p>今天，刚刚好 27 岁。</p><p>今年之前，一直都还觉得自己很年轻，恍恍惚惚地觉得才刚刚跨出校门不久，加上时常被人问起「你是 90 后吧」心中还不免一喜，又被提醒了一次「在别人眼中，自己还很年轻」。</p><p>今年之前，每年的生日，都是在 20 岁的基础上往上加 1 岁，21 岁的时候觉得仅仅是刚刚过了 20 岁，还小；22 岁的时候想着昨天还只是 21 岁，还早。如此往复，渐渐就有了一种错觉，以至于 26 岁的时候觉得自己好像还只是 22、23 岁。</p><p>到现在，也仅仅只毕业了一年半而已，一年半之前，却还时常厌倦 7 年的校园生活，迫不及待地想要进入社会、进入职场。</p><p>17 岁的时候，想快点 20 岁，27 岁的时候，却害怕 30 岁。</p><h3 id="▌以前的日子太顺风顺水了"><a href="#▌以前的日子太顺风顺水了" class="headerlink" title="▌以前的日子太顺风顺水了"></a>▌以前的日子太顺风顺水了</h3><p>到了今年，到了 27 岁，才慢慢意识到，原来之所以以前会有「自己很年轻」的错觉是因为 <strong>日子过得太顺风顺水了</strong>，因为 <strong>对自己太随便了</strong>，因为 <strong>对自己一点都不狠</strong>。</p><p>7 年的校园生活，搬过两次校区、做过家教、进入了卓越工程师班、保了研、穷游了 70 天、加入过校学生会、给老板打工做过项目。</p><p>这一年半却不知不觉辗转了 4 个地方，成都、东莞、深圳和北京，换了两份工作，经历了 <strong>职场新人到无业游民</strong>。</p><h3 id="▌以前对自己太随便了"><a href="#▌以前对自己太随便了" class="headerlink" title="▌以前对自己太随便了"></a>▌以前对自己太随便了</h3><p>2010 年高中毕业，填报高考志愿时仅仅因为不知道对什么感兴趣，因为周边人说「不要学计算机、软件，是烂大街的专业」，所以就随便在志愿上填报了「水利工程」专业，结果一上大一就后悔了。</p><p>到大一下可以转专业的时候，心想转到其他学院，意味着要补上落了一年的课，大二就要比别人辛苦两倍，还是算了，不要那么辛苦，好好摆正心态，<strong>此前对本专业不感兴趣肯定是自己不够努力</strong>，于是后面进入了卓越工程师班。</p><p>大三想跨专业考研，后面有了保研机会，心想能保研干嘛还考研，研究生本硕同专业肯定比跨专业轻松，于是就保了研，后面的半年无心学术，做家教攒够了钱，毕业出国穷游了 70 天。</p><p>研一，导师让学 Fortran 编程，当时觉得编程太枯燥没什么用，跟老板扭了一阵后，改学使用现成的商业模型，直到毕业，才发现就只会用几个软件模型。</p><p><strong>而这一年半的收获，我觉得比前 7 年都大。</strong>前一年的两份工作中，有了大量使用 Excel 的机会，就此慢慢产生了兴趣，开始学函数、数据透视表、VBA，兴趣愈发浓烈，便又接触使用了 POWER BI，尝试做了酷炫的可视化报表，后期利用简单的数据抓取功能实现了一个股票数据的爬虫，很是兴奋，再往后很快就发现它的功能不够强大，通过搜索知道了 R 语言可以做爬虫，便上手开始学习，这才是第一次主动接触编程，之后发现 Python 比 R 更强大，便又开始学 Python，直到现在。</p><h3 id="▌-以前对自己一点都不狠"><a href="#▌-以前对自己一点都不狠" class="headerlink" title="▌ 以前对自己一点都不狠"></a>▌ 以前对自己一点都不狠</h3><p>回想大学期间，上课基本上都是在玩手机，不喜欢的课甚至干脆不去，没课的时候都去兼职，总之就是怎么舒服、爽快怎么来。对未来做什么工作没有过规划，甚至连校招找工作时都没有。那时候总以为 <strong>时间是用不完的，船到桥头自然会直的。</strong></p><p>这半年，辞了职后下定决心放手一搏，准备跨行。于是便从零开始学编程、购买书和网课、加入付费社群，接触到了一群 95 后。开始学爬虫、建博客、更新公众号，直到现在。</p><p>「<strong>只要出发了，一切都不算晚。</strong>」</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;加油吧，骚年。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（分析篇）</title>
    <link href="https://www.makcyun.top/2018/12/02/data_analysis&amp;mining02.html"/>
    <id>https://www.makcyun.top/2018/12/02/data_analysis&amp;mining02.html</id>
    <published>2018-12-02T02:16:24.000Z</published>
    <updated>2019-03-07T03:42:53.176Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。</p><a id="more"></a><p><strong>摘要：</strong> 如今移动互联网越来越发达，我们每个人的手机上至少都安装了好几十款 App，随着各式各样的 App 层出不穷，也就产生了优劣之分，而我们肯定愿意去使用那些良心佳软，而如何去发现这些 App 呢，本文使用 Scrapy 框架爬取了著名应用下载市场「酷安网」上的 6000 余款 App，通过分析，发现了各个类别领域下的佼佼者，这些 App 堪称真正的良心心之作，使用它们将会给你带来全新的手机使用体验。</p><p>上一篇文章我们完成了 App 数据的抓取工作，这一篇文章我们将对这些数据进行探索性分析。</p><h2 id="1-总体情况"><a href="#1-总体情况" class="headerlink" title="1. 总体情况"></a>1. 总体情况</h2><p>我们主要从总体和分类两个维度对 App 下载量、评分、体积等指标进行分析。</p><h3 id="1-1-下载量排名"><a href="#1-1-下载量排名" class="headerlink" title="1.1. 下载量排名"></a>1.1. 下载量排名</h3><p>首先来看一下 App 的下载量情况，很多时候我们下载一个 App ，下载量是一个非常重要的参考指标，由于绝大多数 App 的下载量都相对较少，直方图无法看出趋势，所以我们择将数据进行分段，离散化为柱状图，绘图工具采用的是 Pyecharts。</p><p><img src="http://media.makcyun.top/FqnL1UWAUYVHCugOzBPwDgUOkDQA" alt=""></p><p>可以看到多达 5517 款（占总数 84%）App 的下载量不到 10 万， 而下载量超过 500 万的仅有 20 款，开发一个要想盈利的 App ，用户下载量尤为重要，从这一点来看，<strong>大部分 App 的处境都比较尴尬，至少是在酷安平台上。</strong></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Bar</span><br><span class="line"><span class="comment"># 下载量分布</span></span><br><span class="line">bins = [<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>,<span class="number">500</span>,<span class="number">10000</span>]</span><br><span class="line">group_names = [<span class="string">'&lt;=10万'</span>,<span class="string">'10-100万'</span>,<span class="string">'100-500万'</span>,<span class="string">'&gt;500万'</span>]</span><br><span class="line">cats = pd.cut(df[<span class="string">'download'</span>],bins,labels=group_names) <span class="comment">#  用 pd.cut() 方法进行分段</span></span><br><span class="line">cats = pd.value_counts(cats)</span><br><span class="line">bar = Bar(<span class="string">'App 下载数量区间分布'</span>,<span class="string">'绝大部分 App 下载量低于 10 万'</span>)</span><br><span class="line"><span class="comment"># bar.use_theme('macarons')</span></span><br><span class="line">bar.add(</span><br><span class="line">    <span class="string">'App 数量 (个)'</span>,</span><br><span class="line">    list(cats.index),</span><br><span class="line">    list(cats.values),</span><br><span class="line">    is_label_show = <span class="keyword">True</span>,</span><br><span class="line">    is_splitline_show = <span class="keyword">False</span>,</span><br><span class="line">)</span><br><span class="line">bar.render(path=<span class="string">'download_interval.png'</span>,pixel_ration=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>接下来，我们看看 <strong>下载量最多的 20 款 App </strong>是哪些：</p><p><img src="http://media.makcyun.top/18-11-29/36948560.jpg" alt=""></p><p>可以看到，这里「酷安」App 以 5000 万+ 次的下载量遥遥领先，是第二名微信 2700 万下载量的近两倍，这么巨大的优势也很容易理解，毕竟是自家的 App，如果你手机上没有「酷安」，说明你还不算是一个真正的「<strong>搞机爱好者</strong>」，从图中我们还可以看出以下几点信息：</p><ul><li><p>TOP 20 款 App 中，很多都是 <strong>装机必备</strong>，算是比较大众型的 App。</p></li><li><p>右侧 App 评分图中可以看到仅有 5 款 App 评分超过了 4 分（5 分制），绝大多数的评分都不到 3 分，甚至到不到 2 分，<strong>到底是因为这些 App 开发者做不出好 App 还是根本不想做出来？</strong></p></li><li><p>相较于其他 App，<strong>RE 管理器</strong>、<strong>绿色守护</strong> 这几款非常突出，其中 RE 管理器在如此高的下载量下，仍然能够得到 4.8 分（最高分）并且体积只有几 M，实属难得，<strong>什么是「良心 App」，这类就是</strong>。</p></li></ul><p>作为对比，我们再看一下 <strong>下载量最少的 20 个 App</strong>。</p><p><img src="http://media.makcyun.top/18-11-30/50145986.jpg" alt=""></p><p>可以看到，与上面的那些下载量多的 App 相比，这些就相形见绌了，下载量最少的 「广州限行通」更是只有 <strong>63 次下载</strong>。</p><p>这也不奇怪，可能是 App 没有宣传、也可能是刚开发出来，这么少的下载量评分还不错，也还能继续更新，为这些开发者点赞。</p><p>其实，这类 App 不算囧，真正囧的应该是那些 <strong>下载量很多、评分却低到不能再低</strong> 的 App，给人的感觉是：「<strong>我就这么烂，爱咋咋地有本事别用</strong>」。</p><h3 id="1-2-评分排名"><a href="#1-2-评分排名" class="headerlink" title="1.2. 评分排名"></a>1.2. 评分排名</h3><p>接下来，我们看看 App 的总体得分情况。这里，将得分分为了以下 4 个区间段，并且为不同分数定义了相应的等级。</p><p><img src="http://media.makcyun.top/18-11-30/92654379.jpg" alt=""></p><p>可以发现这么几点有意思的现象：</p><ul><li><strong>3 分以下的软件非常少，只占不到 10%</strong>，而之前下载量最多的 20 款 APP 中，微信、QQ、淘宝、支付宝等大多数软件的得分都不到 3 分，<strong>这就有点尴尬了。</strong></li><li>中品也就是中等得分的 App 数量最多。</li><li>4 分以上的 高分 APP 数量占了近一半（46%），可能是这些 App 的确还不错，也可能是由于评分数量过少，为了优中选优，后续有必要设置一定筛选门槛。</li></ul><p>接下来，我们看看评分最高的 20 款 App 有哪些，很多时候我们下载 App 都是跟着「<strong>哪个评分高，下载哪个</strong>」这种感觉走。</p><p><img src="http://media.makcyun.top/18-11-30/61307481.jpg" alt=""></p><p>可以看到，评分最高的 20 个 App，它们都得到了 4.8 分 ，包括：RE 管理器（再次出现）、Pure 轻雨图标包等，还有一些不太常见，可能这些都是不错的 App，不过我们还需要结合看一下下载量，它们的下载量都在 1 万以上，有了一定的下载量，评分才算比较可靠，我们就能放心的下载下来体验一下了。</p><p>经过上面的总体分析，我们大致发现了一些不错的 App ，但还不够，所以接下来将进行细分并设置一定筛选条件。</p><h2 id="2-分类情况"><a href="#2-分类情况" class="headerlink" title="2. 分类情况"></a>2. 分类情况</h2><p>按照 App 功能和日常使用场景，将 App 分为以下 9 大类别，然后 <strong>从每个类别中筛选出 20 款最棒的 App</strong>。</p><p><img src="http://media.makcyun.top/18-12-1/63941977.jpg" alt=""></p><p>为了尽可能找出最好的 App，这里不妨设置 3 个条件：</p><ul><li>评分不低于 4 分</li><li>下载量不低于 1 万</li><li>设置一个总分评价指标（总分 = 下载量 * 评分），再标准化为满分 1000 分，作为 App 的排名参照指标。</li></ul><p>经过评选之后，我们依次得到了各个类别下分数最高的 20 款 App，<strong>这些 App 大部分的确是良心软件</strong>。</p><h3 id="2-1-系统工具"><a href="#2-1-系统工具" class="headerlink" title="2.1. 系统工具"></a>2.1. 系统工具</h3><p>系统工具包括了：输入法、文件管理 、系统清理、桌面、插件、锁屏等。</p><p><img src="http://media.makcyun.top/18-11-30/9653670.jpg" alt=""></p><p>可以看到，第一名是大名鼎鼎的老牌文件管理器「<strong>RE 管理器</strong>」，仅有 5 M 大小的它除了具备普通文件管理器的各项功能以外，最大的特点是能够卸载手机自带的 App，不过需要 Root。</p><p>「<strong>ES 文件浏览器</strong>」的文件分析器功能非常强大，能够有效清理臃肿的手机空间。</p><p>「<strong>一个木函</strong>」这款 App 就比较牛逼了，正如它的软件介绍「拥有很多，不如有我」所说，打开它你能发现它提供了好几十项实用功能，比如：翻译、以图搜图、快递查询、制作表情包等等。</p><p>再往下的「<strong>Super SU</strong>」、「<strong>存储空间清理</strong>」、「<strong>镧</strong>」、「<strong>MT 管理器</strong>」、「<strong>My Android Tools</strong>」都力荐，总之，这份榜单上的 App 可以说都值得进入你的手机 App 使用名单。</p><h3 id="2-2-社交聊天"><a href="#2-2-社交聊天" class="headerlink" title="2.2. 社交聊天"></a>2.2. 社交聊天</h3><p><img src="http://media.makcyun.top/18-11-30/22554865.jpg" alt=""></p><p>社交聊天类中， 「<strong>Share 微博客户端</strong>」位居第一，作为一款第三方客户端 App，它自然有比官方版本好的地方，比如相比正版 70M 的体积，它只有其十分之一大小，也几乎没有广告，还有额外强大的诸多功能，如果你爱刷微博，那么不妨尝试下这款「Share」。</p><p>「即刻」这款 App 也相当不错，再往下还能看到前阵子很火的「子弹短信」，宣称将要取代微信，看来短期内应该是做不到了。</p><p>你可能会发现，这份社交榜单上没有出现「知乎」、「豆瓣」、「简书」这类常见的 App，是因为它们的评分都比较低，分别只有 2.9分、3.5分和 2.9 分，自然进入不了这份名单，如果你一定想用它们，推荐去使用它们的第三方客户端或者历史版本。</p><h3 id="2-3-资讯阅读"><a href="#2-3-资讯阅读" class="headerlink" title="2.3. 资讯阅读"></a>2.3. 资讯阅读</h3><p><img src="http://media.makcyun.top/18-11-30/51456686.jpg" alt=""></p><p>可以看到，在资讯阅读类中，「<strong>静读天下</strong>」牢牢占据了第一名，我之前专门写过一篇文章介绍它：<a href="https://www.makcyun.top/weekly_sharing3.html">安卓最强阅读器</a>。</p><p>同类别中的「多看阅读」、「追书神器」、「微信读书」也都进入了榜单。</p><p>另外，如果你经常为不知道去哪里下载电子书而头疼，那不妨试一下「<strong>搜书大师</strong>」、「<strong>老子搜书</strong>」。</p><h3 id="2-4-影音娱乐"><a href="#2-4-影音娱乐" class="headerlink" title="2.4. 影音娱乐"></a>2.4. 影音娱乐</h3><p><img src="http://media.makcyun.top/18-11-30/12201900.jpg" alt=""></p><p>接下来是影音娱乐版块，网易家的「<strong>网易云音乐</strong>」毫无压力地占据头名，难得的大厂精品。</p><p>如果你爱玩游戏，那么 「Adobe AIR」应该尝试一下。</p><p>如果你很文艺，那么应该会喜欢「VUE」这款短视频拍摄 App，创作好以后发到朋友圈绝对能装逼。</p><p>最后一位的「<strong>海贝音乐</strong>」很赞，最近发现它有一个强大的功能是结合百度网盘使用，它能够自动识别音频文件然后播放。</p><h3 id="2-5-通讯网络"><a href="#2-5-通讯网络" class="headerlink" title="2.5. 通讯网络"></a>2.5. 通讯网络</h3><p>下面到了通讯网络类别，这个类别主要包括：浏览器、通讯录、通知、邮箱等小类。</p><p><img src="http://media.makcyun.top/18-11-30/81440821.jpg" alt=""></p><p>浏览器，我们每个人手机上都有，用的也五花八门，有些人就用手机自带的浏览器，有些人用 Chrome、火狐这类大牌浏览器。</p><p>不过你会发现榜单上的前三位你可能听都没听过，但是它们真的很牛逼，用「<strong>极简高效、清爽极速</strong>」来形容再适合不过，其中 「<strong>Via</strong> 」和 「<strong>X 浏览器</strong>」 体积不到 1M ，真正的「麻雀虽小、五脏俱全」，强烈推荐。</p><h3 id="2-6-摄影图片"><a href="#2-6-摄影图片" class="headerlink" title="2.6. 摄影图片"></a>2.6. 摄影图片</h3><p>拍照修图也是我们常用的功能。</p><p>也许你有自己的图片管理软件，但是这里要强烈推荐第一名「<strong>快图浏览</strong>」这款 App，只有 3M 大小的它，能够瞬间发现和加载上万张图片，如果你是拍照狂魔，用它打开再多的照片也能秒开，另外还拥有隐藏私密照片、自动备份百度网盘等功能。它是我使用时间最久的 App 之一。</p><p><img src="http://media.makcyun.top/18-11-30/82403429.jpg" alt=""></p><h3 id="2-7-文档写作"><a href="#2-7-文档写作" class="headerlink" title="2.7. 文档写作"></a>2.7. 文档写作</h3><p>我们时常需要在手机上写作、做备忘录，那么自然需要好的文档写作类 App。</p><p><img src="http://media.makcyun.top/18-11-30/70855830.jpg" alt=""></p><p>「<strong>印象笔记</strong>」就不用多说了，我觉得最好用的学习总结类 App，免费版一般也够用，但是推荐订阅会员，遇到双十一、周年庆这种日子，会有 6折优惠，一年不到 100 块还是很划算了。</p><p>如果你喜欢使用 Markdown 写作，那么「<strong>纯纯写作</strong>」这款精巧的 App 应该会很适合你。</p><p>体积不到 3M 却拥有云备份、生成长图、中英文自动空格等数十项功能，即使这样，仍然保持了蕴繁于简的设计风格，这大概就是两三个月之内，下载量就从两三万飙升了十倍的原因，而这款 App 的背后是一位 <strong>牺牲了几年的业余时间不断开发和更新的大佬</strong>，值得敬佩。</p><h3 id="2-8-出行交通购物"><a href="#2-8-出行交通购物" class="headerlink" title="2.8. 出行交通购物"></a>2.8. 出行交通购物</h3><p>这个类别中，排名第一的居然是 12306，一提起它，就会想起那一张张奇葩的验证码。不过这里的 App 不是官网的 ，而是第三方开发的。最牛逼的功能应该就是「抢票了」，如果你还在靠发朋友圈来抢票的话，那不妨试一下它。</p><p><img src="http://media.makcyun.top/18-11-30/97982037.jpg" alt=""></p><h3 id="2-9-Xposed-插件"><a href="#2-9-Xposed-插件" class="headerlink" title="2.9. Xposed 插件"></a>2.9. Xposed 插件</h3><p>最后一个类别是 Xposed，很多人应该不太熟悉，但是一提微信上的抢红包、防撤回功能，应该很多人就知道了。这些牛逼又不同寻常的功能就用到了 Xposed 框架里的各种模块功能。这个框架由国外著名的 <a href="https://forum.xda-developers.com/" target="_blank" rel="noopener">XDA 手机论坛</a>，你经常听到的一些所谓由 XDA 大神破解的软件，就是来自这个论坛。</p><p>简单地说就是，安装了 Xposed 这个框架之后，就可以在里面安装一些好玩有趣的插件，有了这些插件，你的手机就能实现更多更大的功能。比如：能够去除广告、破解 App 付费功能、杀死耗电的自启动进程、虚拟手机定位等功能。</p><p>不过使用这个框架和这些插件需要刷机、ROOT，门槛有点高。</p><p><img src="http://media.makcyun.top/18-11-30/67777533.jpg" alt=""></p><h2 id="3-小结"><a href="#3-小结" class="headerlink" title="3. 小结"></a>3. 小结</h2><ul><li>本文使用 Scrapy 框架爬取分析了酷安网的 6000 款 App，初学 Scrapy 可能会觉得程序写起来比较散乱，所以可以尝试先使用普通的函数方法，把程序完整地写在一起，再分块拆分到 Scrapy 项目中，这样也有助于从单一程序到框架写法的思维转变，之后会写单独写一篇文章。</li><li>由于网页版的 App 数量比 App 中的少，所以还有很多好用的 App 没有包括进来，比如 Chrome 、MX player、Snapseed 等，建议使用酷安 App，那里有更多好玩的东西。</li></ul><p>通过这两篇文章，我们完成一个项目从抓取到分析的过程，文中涉及了很多精品佳软，如有兴趣可以去尝试下载体验一下，为了更方便你，我这里也收集好了 <strong>24 款精品 App</strong>。</p><p><img src="http://media.makcyun.top/18-12-1/55888908.jpg" alt=""></p><h2 id="4-资源获取"><a href="#4-资源获取" class="headerlink" title="4. 资源获取"></a>4. 资源获取</h2><p>如需完整代码和上图中的 App 可以加入我的「<strong>知识星球：第2脑袋</strong>」获取，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。&lt;/p&gt;
    
    </summary>
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy 爬取并分析酷安 6000 款 App，找到良心佳软（抓取篇）</title>
    <link href="https://www.makcyun.top/2018/11/29/web_scraping_withpython10.html"/>
    <id>https://www.makcyun.top/2018/11/29/web_scraping_withpython10.html</id>
    <published>2018-11-29T08:16:24.000Z</published>
    <updated>2018-12-14T11:11:14.070Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。</p><a id="more"></a><p><strong>摘要：</strong> 如今移动互联网越来越发达，我们每个人的手机上至少都安装了好几十款 App，随着各式各样的 App 层出不穷，也就产生了优劣之分，而我们肯定愿意去使用那些良心佳软，而如何去发现这些 App 呢，本文使用 Scrapy 框架爬取了著名应用下载市场「酷安网」上的 6000 余款 App，通过分析，发现了各个类别领域下的佼佼者，这些 App 堪称真正的良心心之作，使用它们将会给你带来全新的手机使用体验。</p><h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1. 分析背景"></a>1. 分析背景</h2><h3 id="1-1-为什么选择酷安"><a href="#1-1-为什么选择酷安" class="headerlink" title="1.1. 为什么选择酷安"></a>1.1. 为什么选择酷安</h3><p>如果说 GitHub 是程序员的天堂，那么 <strong>酷安</strong> 则是手机 App 爱好者们（别称「搞机」爱好者）的天堂，相比于那些传统的手机应用下载市场，酷安有三点特别之处：</p><p>第一、可以搜索下载到各种 <strong>神器、佳软</strong>，其他应用下载市场几乎很难找得到。</p><p>比如之前的文章中说过的终端桌面「Aris」、安卓最强阅读器「静读天下」、RSS 阅读器 「Feedme」 等。</p><p>第二、可以找到很多 App 的破解版。</p><p>我们提倡「为好东西付费」，但是有些 App 很蛋疼，比如「百度网盘」，在这里面就可以找到很多 App 的破解版。</p><p>第三、可以找到 App 的历史版本。</p><p>很多人喜欢用最新版本的 App，一有更新就马上升级，但是现在很多 App 越来越功利、越更新越臃肿、广告满天飞，倒不如回归本源，使用体积小巧、功能精简、无广告的早期版本。</p><p>作为一名 App 爱好者，我在酷安上发现了很多不错的 App，越用越感觉自己知道的仅仅是冰山一角，便想扒一扒这个网站上到底有多少好东西，手动一个个去找肯定是不现实了，自然想到最好的方法——用爬虫来解决，为了实现此目的，最近就学习了一下 Scrapy 爬虫框架，爬取了该网 6000 款左右的 App，通过分析，找到了不同领域下的精品 App，下面我们就来一探究竟。</p><h3 id="1-2-分析内容"><a href="#1-2-分析内容" class="headerlink" title="1.2. 分析内容"></a>1.2. 分析内容</h3><ul><li>总体分析 6000 款 App 的评分、下载量、体积等指标。</li><li>根据日常使用功能场景，将 App 划分为：系统工具、资讯阅读、社交娱乐等 10 大类别，筛选出每个类别下的精品 App。</li></ul><h3 id="1-3-分析工具"><a href="#1-3-分析工具" class="headerlink" title="1.3. 分析工具"></a>1.3. 分析工具</h3><ul><li>Python</li><li>Scrapy</li><li>MongoDB</li><li>Pyecharts</li><li>Matplotlib</li></ul><h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2. 数据抓取"></a>2. 数据抓取</h2><p>由于酷安手机端 App 设置了反扒措施，使用 Charles 尝试后发现无法抓包， 暂退而求其次，使用 Scrapy 抓取网页端的 App 信息。抓取时期截止到 2018 年 11 月 23日，共计 6086 款 App，共抓取 了 8 个字段信息：App 名称、下载量、评分、评分人数、评论数、关注人数、体积、App 分类标签。</p><h3 id="2-1-目标网站分析"><a href="#2-1-目标网站分析" class="headerlink" title="2.1. 目标网站分析"></a>2.1. 目标网站分析</h3><p>这是我们要抓取的 <a href="https://www.coolapk.com/apk?p=1" target="_blank" rel="noopener">目标网页</a>，点击翻页可以发现两点有用的信息：</p><ul><li>每页显示了 10 条 App 信息，一共有610页，也就是 6100 个左右的 App 。</li><li>网页请求是 GET 形式，URL 只有一个页数递增参数，构造翻页非常简单。</li></ul><p><img src="http://media.makcyun.top/18-11-29/13499186.jpg" alt=""></p><p>接下来，我们来看看选择抓取哪些信息，可以看到，主页面内显示了 App 名称、下载量、评分等信息，我们再点击 App 图标进入详情页，可以看到提供了更齐全的信息，包括：分类标签、评分人数、关注人数等。由于，我们后续需要对 App 进行分类筛选，故分类标签很有用，所以这里我们选择进入每个 App 主页抓取所需信息指标。</p><p><img src="http://media.makcyun.top/18-11-29/57226641.jpg" alt=""></p><p><img src="C:\Users\sony\AppData\Roaming\Typora\typora-user-images\1543460157799.png" alt=""></p><p>通过上述分析，我们就可以确定抓取流程了，首先遍历主页面 ，抓取 10 个 App 的详情页 URL，然后详情页再抓取每个 App 的指标，如此遍历下来，我们需要抓取 6000 个左右网页内容，抓取工作量不算小，所以，我们接下来尝试使用 Scrapy 框架进行抓取。</p><h3 id="2-2-Scrapy-框架介绍"><a href="#2-2-Scrapy-框架介绍" class="headerlink" title="2.2. Scrapy 框架介绍"></a>2.2. Scrapy 框架介绍</h3><p>介绍 Scrapy 框架之前，我们先回忆一下 Pyspider 框架，之前一篇爬取分析 <a href="https://www.makcyun.top/web_scraping_withpython9.html">虎嗅网</a> 的文章，我们使用了它，它是由国内大神编写的一个爬虫利器， Github Star 超过 10K，但是它的整体功能还是相对单薄一些，还有比它更强大的框架么？有的，就是这里要说的 Scrapy 框架，Github Star 超过 30K，是 Python 爬虫界使用最广泛的爬虫框架，玩爬虫这个框架必须得会。</p><p>网上关于 Scrapy 的官方文档和教程很多，这里罗列几个。</p><blockquote><p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html" target="_blank" rel="noopener">Scrapy 中文文档</a></p><p><a href="https://cloud.tencent.com/developer/column/1108/tag-10756" target="_blank" rel="noopener">崔庆才的 Scrapy 专栏</a></p><p><a href="https://blog.csdn.net/hk2291976/article/details/51405052" target="_blank" rel="noopener">Scrapy 爬拉勾</a></p><p><a href="https://zhuanlan.zhihu.com/p/24769534" target="_blank" rel="noopener">Scrapy 爬豆瓣电影</a></p></blockquote><p>Scrapy 框架相对于 Pyspider 相对要复杂一些，有不同的处理模块，项目文件也由好几个程序组成，不同的爬虫模块需要放在不同的程序中去，所以刚开始入门会觉得程序七零八散，容易把人搞晕，建议采取以下思路快速入门 Scrapy：</p><ul><li><p>首先，快速过一下上面的参考教程，了解 Scrapy 的爬虫逻辑和各程序的用途与配合。</p></li><li><p>接着，看上面两个实操案例，熟悉在 Scrapy 中怎么写爬虫。</p></li><li><p>最后，找个自己感兴趣的网站作为爬虫项目，遇到不懂的就看教程或者 Google。</p></li></ul><p>这样的学习路径是比较快速而有效的，比一直抠教程不动手要好很多。下面，我们就以酷安网为例，用 Scrapy 来爬取一下。</p><h3 id="2-3-抓取数据"><a href="#2-3-抓取数据" class="headerlink" title="2.3. 抓取数据"></a>2.3. 抓取数据</h3><p>首先要安装好 Scrapy 框架，如果是 Windwos 系统，且已经安装了 Anaconda，那么安装 Scrapy 框架就非常简单，只需打开 Anaconda Prompt 命令窗口，输入下面一句命令即可，会自动帮我们安装好 Scrapy 所有需要安装和依赖的库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda pip scrapy</span><br></pre></td></tr></table></figure><h4 id="2-3-1-创建项目"><a href="#2-3-1-创建项目" class="headerlink" title="2.3.1. 创建项目"></a>2.3.1. 创建项目</h4><p>接着，我们需要创建一个爬虫项目，所以我们先从根目录切换到需要放置项目的工作路径，比如我这里设置的存放路径为：E:\my_Python\training\kuan，接着继续输入下面一行代码即可创建 kuan 爬虫项目：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换工作路径</span></span><br><span class="line">e:</span><br><span class="line">cd E:\my_Python\training\kuan</span><br><span class="line"><span class="comment"># 生成项目</span></span><br><span class="line">scrapy startproject kuspider</span><br></pre></td></tr></table></figure><p>执行上面的命令后，就会生成一个名为 kuan 的 scrapy 爬虫项目，包含以下几个文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrapy. cfg <span class="comment"># Scrapy 部署时的配置文件</span></span><br><span class="line">kuan <span class="comment"># 项目的模块，需要从这里引入</span></span><br><span class="line">_init__.py</span><br><span class="line">items.py <span class="comment"># 定义爬取的数据结构</span></span><br><span class="line">middlewares.py <span class="comment"># Middlewares 中间件</span></span><br><span class="line">pipelines.py <span class="comment"># 数据管道文件，可用于后续存储</span></span><br><span class="line">settings.py <span class="comment"># 配置文件</span></span><br><span class="line">spiders <span class="comment"># 爬取主程序文件夹</span></span><br><span class="line">_init_.py</span><br></pre></td></tr></table></figure><p>下面，我们需要再 spiders 文件夹中创建一个爬取主程序：kuan.py，接着运行下面两行命令即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd kuan <span class="comment"># 进入刚才生成的 kuan 项目文件夹</span></span><br><span class="line">scrapy genspider kuan www.coolapk.com  <span class="comment"># 生成爬虫主程序文件 kuan.py</span></span><br></pre></td></tr></table></figure><h4 id="2-3-2-声明-item"><a href="#2-3-2-声明-item" class="headerlink" title="2.3.2. 声明 item"></a>2.3.2. 声明 item</h4><p>项目文件创建好以后，我们就可以开始写爬虫程序了。</p><p>首先，需要在 items.py 文件中，预先定义好要爬取的字段信息名称，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KuanItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line"><span class="comment"># define the fields for your item here like:</span></span><br><span class="line">name = scrapy.Field()</span><br><span class="line">volume = scrapy.Field()</span><br><span class="line">download = scrapy.Field()</span><br><span class="line">follow = scrapy.Field()</span><br><span class="line">comment = scrapy.Field()</span><br><span class="line">tags = scrapy.Field()</span><br><span class="line">score = scrapy.Field()</span><br><span class="line">num_score = scrapy.Field()</span><br></pre></td></tr></table></figure><p>这里的字段信息就是我们前面在网页中定位的 8 个字段信息，包括：name 表示 App 名称、volume 表示体积、download 表示 下载数量。在这里定义好之后，我们在后续的爬取主程序中会利用到这些字段信息。</p><h4 id="2-3-3-爬取主程序"><a href="#2-3-3-爬取主程序" class="headerlink" title="2.3.3. 爬取主程序"></a>2.3.3. 爬取主程序</h4><p>创建好 kuan 项目后，Scrapy 框架会自动生成爬取的部分代码，我们接下来就需要在 parse 方法中增加网页抓取的字段解析内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KuspiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'kuan'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.coolapk.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.coolapk.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>打开主页 Dev Tools，找到每项抓取指标的节点位置，然后可以采用 CSS、Xpath、正则等方法进行提取解析，这些方法 Scrapy 都支持，可随意选择，这里我们选用 CSS 语法来定位节点，不过需要注意的是，Scrapy 的 CSS 语法和之前我们利用 pyquery 使用的 CSS 语法稍有不同，举几个例子，对比说明一下。</p><p><img src="http://media.makcyun.top/18-11-30/64130730.jpg" alt=""></p><p>首先，我们定位到第一个 APP 的主页 URL 节点，可以看到 URL 节点位于 class 属性为 <code>app_left_list</code> 的 div 节点下的 a 节点中，其 href 属性就是我们需要的 URL 信息，这里是相对地址，拼接后就是完整的 URL ：<a href="https://www.coolapk.com/apk/com.coolapk.market" target="_blank" rel="noopener">www.coolapk.com/apk/com.coolapk.market</a>。</p><p>接着我们进入酷安详情页，选择 App 名称并进行定位，可以看到 App 名称节点位于 class 属性为 <code>.detail_app_title</code> 的 p 节点的文本中。</p><p><img src="http://media.makcyun.top/18-11-30/61796640.jpg" alt=""></p><p>定位到这两个节点之后，我们就可以使用 CSS 提取字段信息了，这里对比一下常规写法和 Scrapy 中的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常规写法</span></span><br><span class="line">url = item(<span class="string">'.app_left_list&gt;a'</span>).attr(<span class="string">'href'</span>)</span><br><span class="line">name = item(<span class="string">'.list_app_title'</span>).text()</span><br><span class="line"><span class="comment"># Scrapy 写法</span></span><br><span class="line">url = item.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">name = item.css(<span class="string">'.detail_app_title::text'</span>).extract_first()</span><br></pre></td></tr></table></figure><p>可以看到，要获取 href 或者 text 属性，需要用 :: 表示，比如获取 text，则用 ::text。extract_first() 表示提取第一个元素，如果有多个元素，则用 extract() 。接着，我们就可以参照写出 8 个字段信息的解析代码。</p><p>首先，我们需要在主页提取 App 的 URL 列表，然后再进入每个 App 的详情页进一步提取 8 个字段信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        url = content.css(<span class="string">'::attr("href")'</span>).extract_first()</span><br><span class="line">        url = response.urljoin(url)  <span class="comment"># 拼接相对 url 为绝对 url</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse_url)</span><br></pre></td></tr></table></figure><p>这里，利用 response.urljoin() 方法将提取出的相对 URL 拼接为完整的 URL，然后利用 scrapy.Request() 方法构造每个 App 详情页的请求，这里我们传递两个参数：url 和 callback，url 为详情页 URL，callback 是回调函数，它将主页 URL 请求返回的响应 response 传给专门用来解析字段内容的 parse_url() 方法，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_url</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    item = KuanItem()</span><br><span class="line">    item[<span class="string">'name'</span>] = response.css(<span class="string">'.detail_app_title::text'</span>).extract_first()</span><br><span class="line">    results = self.get_comment(response)</span><br><span class="line">    item[<span class="string">'volume'</span>] = results[<span class="number">0</span>]</span><br><span class="line">    item[<span class="string">'download'</span>] = results[<span class="number">1</span>]</span><br><span class="line">    item[<span class="string">'follow'</span>] = results[<span class="number">2</span>]</span><br><span class="line">    item[<span class="string">'comment'</span>] = results[<span class="number">3</span>]</span><br><span class="line">    item[<span class="string">'tags'</span>] = self.get_tags(response)</span><br><span class="line">    item[<span class="string">'score'</span>] = response.css(<span class="string">'.rank_num::text'</span>).extract_first()</span><br><span class="line">    num_score = response.css(<span class="string">'.apk_rank_p1::text'</span>).extract_first()</span><br><span class="line">    item[<span class="string">'num_score'</span>] = re.search(<span class="string">'共(.*?)个评分'</span>,num_score).group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">yield</span> item</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_comment</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    messages = response.css(<span class="string">'.apk_topba_message::text'</span>).extract_first()</span><br><span class="line">    result = re.findall(<span class="string">r'\s+(.*?)\s+/\s+(.*?)下载\s+/\s+(.*?)人关注\s+/\s+(.*?)个评论.*?'</span>,messages)  <span class="comment"># \s+ 表示匹配任意空白字符一次以上</span></span><br><span class="line">    <span class="keyword">if</span> result: <span class="comment"># 不为空</span></span><br><span class="line">        results = list(result[<span class="number">0</span>]) <span class="comment"># 提取出list 中第一个元素</span></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tags</span><span class="params">(self,response)</span>:</span></span><br><span class="line">    data = response.css(<span class="string">'.apk_left_span2'</span>)</span><br><span class="line">    tags = [item.css(<span class="string">'::text'</span>).extract_first() <span class="keyword">for</span> item <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">return</span> tags</span><br></pre></td></tr></table></figure><p>这里，单独定义了 get_comment() 和 get_tags() 两个方法.</p><p>get_comment() 方法通过正则匹配提取 volume、download、follow、comment 四个字段信息，正则匹配结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">result = re.findall(<span class="string">r'\s+(.*?)\s+/\s+(.*?)下载\s+/\s+(.*?)人关注\s+/\s+(.*?)个评论.*?'</span>,messages)</span><br><span class="line">print(result) <span class="comment"># 输出第一页的结果信息</span></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">[(<span class="string">'21.74M'</span>, <span class="string">'5218万'</span>, <span class="string">'2.4万'</span>, <span class="string">'5.4万'</span>)]</span><br><span class="line">[(<span class="string">'75.53M'</span>, <span class="string">'2768万'</span>, <span class="string">'2.3万'</span>, <span class="string">'3.0万'</span>)]</span><br><span class="line">[(<span class="string">'46.21M'</span>, <span class="string">'1686万'</span>, <span class="string">'2.3万'</span>, <span class="string">'3.4万'</span>)]</span><br><span class="line">[(<span class="string">'54.77M'</span>, <span class="string">'1603万'</span>, <span class="string">'3.8万'</span>, <span class="string">'4.9万'</span>)]</span><br><span class="line">[(<span class="string">'3.32M'</span>, <span class="string">'1530万'</span>, <span class="string">'1.5万'</span>, <span class="string">'3343'</span>)]</span><br><span class="line">[(<span class="string">'75.07M'</span>, <span class="string">'1127万'</span>, <span class="string">'1.6万'</span>, <span class="string">'2.2万'</span>)]</span><br><span class="line">[(<span class="string">'92.70M'</span>, <span class="string">'1108万'</span>, <span class="string">'9167'</span>, <span class="string">'1.3万'</span>)]</span><br><span class="line">[(<span class="string">'68.94M'</span>, <span class="string">'1072万'</span>, <span class="string">'5718'</span>, <span class="string">'9869'</span>)]</span><br><span class="line">[(<span class="string">'61.45M'</span>, <span class="string">'935万'</span>, <span class="string">'1.1万'</span>, <span class="string">'1.6万'</span>)]</span><br><span class="line">[(<span class="string">'23.96M'</span>, <span class="string">'925万'</span>, <span class="string">'4157'</span>, <span class="string">'1956'</span>)]</span><br></pre></td></tr></table></figure><p>然后利用 result[0]、result[1] 等分别提取出四项信息，以 volume 为例，输出第一页的提取结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">item[<span class="string">'volume'</span>] = results[<span class="number">0</span>]</span><br><span class="line">print(item[<span class="string">'volume'</span>])</span><br><span class="line"><span class="number">21.74</span>M</span><br><span class="line"><span class="number">75.53</span>M</span><br><span class="line"><span class="number">46.21</span>M</span><br><span class="line"><span class="number">54.77</span>M</span><br><span class="line"><span class="number">3.32</span>M</span><br><span class="line"><span class="number">75.07</span>M</span><br><span class="line"><span class="number">92.70</span>M</span><br><span class="line"><span class="number">68.94</span>M</span><br><span class="line"><span class="number">61.45</span>M</span><br><span class="line"><span class="number">23.96</span>M</span><br></pre></td></tr></table></figure><p>这样一来，第一页 10 款 App 的所有字段信息都被成功提取出来，然后返回到 yied item 生成器中，我们输出一下它的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'酷安'</span>, <span class="string">'volume'</span>: <span class="string">'21.74M'</span>, <span class="string">'download'</span>: <span class="string">'5218万'</span>, <span class="string">'follow'</span>: <span class="string">'2.4万'</span>, <span class="string">'comment'</span>: <span class="string">'5.4万'</span>, <span class="string">'tags'</span>: <span class="string">"['酷市场', '酷安', '市场', 'coolapk', '装机必备']"</span>, <span class="string">'score'</span>: <span class="string">'4.4'</span>, <span class="string">'num_score'</span>: <span class="string">'1.4万'</span>&#125;, </span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'微信'</span>, <span class="string">'volume'</span>: <span class="string">'75.53M'</span>, <span class="string">'download'</span>: <span class="string">'2768万'</span>, <span class="string">'follow'</span>: <span class="string">'2.3万'</span>, <span class="string">'comment'</span>: <span class="string">'3.0万'</span>, <span class="string">'tags'</span>: <span class="string">"['微信', 'qq', '腾讯', 'tencent', '即时聊天', '装机必备']"</span>,<span class="string">'score'</span>: <span class="string">'2.3'</span>, <span class="string">'num_score'</span>: <span class="string">'1.1万'</span>&#125;,</span><br><span class="line">...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="2-3-4-分页爬取"><a href="#2-3-4-分页爬取" class="headerlink" title="2.3.4. 分页爬取"></a>2.3.4. 分页爬取</h4><p>以上，我们爬取了第一页内容，接下去需要遍历爬取全部 610 页的内容，这里有两种思路：</p><ul><li>第一种是提取翻页的节点信息，然后构造出下一页的请求，然后重复调用 parse 方法进行解析，如此循环往复，直到解析完最后一页。</li><li>第二种是先直接构造出 610 页的 URL 地址，然后批量调用 parse 方法进行解析。</li></ul><p>这里，我们分别写出两种方法的解析代码。</p><p>第一种方法很简单，直接接着 parse 方法继续添加以下几行代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    contents = response.css(<span class="string">'.app_left_list&gt;a'</span>)</span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        ...</span><br><span class="line">    </span><br><span class="line">    next_page = response.css(<span class="string">'.pagination li:nth-child(8) a::attr(href)'</span>).extract_first()</span><br><span class="line">    url = response.urljoin(next_page)</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse )</span><br></pre></td></tr></table></figure><p>第二种方法，我们在最开头的 parse() 方法前，定义一个 start_requests() 方法，用来批量生成 610 页的 URL，然后通过 scrapy.Request() 方法中的 callback 参数，传递给下面的 parse() 方法进行解析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        pages = []</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">610</span>):  <span class="comment"># 一共有610页</span></span><br><span class="line">            url = <span class="string">'https://www.coolapk.com/apk/?page=%s'</span>%page</span><br><span class="line">            page =  scrapy.Request(url,callback=self.parse)</span><br><span class="line">            pages.append(page)</span><br><span class="line">        <span class="keyword">return</span> pages</span><br></pre></td></tr></table></figure><p>以上就是全部页面的爬取思路，爬取成功后，我们需要存储下来。这里，我面选择存储到 MongoDB 中，不得不说，相比 MySQL，MongoDB 要方便省事很多。</p><h4 id="2-3-5-存储结果"><a href="#2-3-5-存储结果" class="headerlink" title="2.3.5. 存储结果"></a>2.3.5. 存储结果</h4><p>我们在 pipelines.py 程序中，定义数据存储方法，MongoDB 的一些参数，比如地址和数据库名称，需单独存放在 settings.py 设置文件中去，然后在 pipelines 程序中进行调用即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,mongo_url,mongo_db)</span>:</span></span><br><span class="line">        self.mongo_url = mongo_url</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_url = crawler.settings.get(<span class="string">'MONGO_URL'</span>),</span><br><span class="line">            mongo_db = crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_url)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        self.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure><p>首先，我们定义一个 MongoPipeline(）存储类，里面定义了几个方法，简单进行一下说明：</p><p>from crawler() 是一个类方法，用 ＠class method 标识，这个方法的作用主要是用来获取我们在 settings.py 中设置的这几项参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'KuAn'</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'kuan.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>open_spider() 方法主要进行一些初始化操作 ，在 Spider 开启时，这个方法就会被调用 。</p><p>process_item() 方法是最重要的方法，实现插入数据到 MongoDB 中。</p><p><img src="http://media.makcyun.top/18-11-30/67473511.jpg" alt=""></p><p>完成上述代码以后，输入下面一行命令就可以开始整个爬虫的抓取和存储过程了，单机跑的话，6000 个网页需要不少时间才能完成，保持耐心。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl kuan</span><br></pre></td></tr></table></figure><p>这里，还有两点补充：</p><p>第一，<strong>为了减轻网站压力，我们最好在每个请求之间设置几秒延时</strong>，可以在 KuspiderSpider() 方法开头出，加入以下几行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"DOWNLOAD_DELAY"</span>: <span class="number">3</span>, <span class="comment"># 延迟3s,默认是0，即不延迟</span></span><br><span class="line">        <span class="string">"CONCURRENT_REQUESTS_PER_DOMAIN"</span>: <span class="number">8</span> <span class="comment"># 每秒默认并发8次，可适当降低</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>第二，为了更好监控爬虫程序运行，有必要 <strong>设置输出日志文件</strong>，可以通过 Python 自带的 logging 包实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(filename=<span class="string">'kuan.log'</span>,filemode=<span class="string">'w'</span>,level=logging.WARNING,format=<span class="string">'%(asctime)s %(message)s'</span>,datefmt=<span class="string">'%Y/%m/%d %I:%M:%S %p'</span>)</span><br><span class="line">logging.warning(<span class="string">"warn message"</span>)</span><br><span class="line">logging.error(<span class="string">"error message"</span>)</span><br></pre></td></tr></table></figure><p>这里的 level 参数表示警告级别，严重程度从低到高分别是：DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL，如果想日志文件不要记录太多内容，可以设置高一点的级别，这里设置为 WARNING，意味着只有 WARNING 级别以上的信息才会输出到日志中去。</p><p>添加 datefmt 参数是为了在每条日志前面加具体的时间，这点很有用处。</p><p><img src="http://media.makcyun.top/18-11-30/67506230.jpg" alt=""></p><p>以上，我们就完成了整个数据的抓取，有了数据我们就可以着手进行分析，不过这之前还需简单地对数据做一下清洗和处理。</p><h2 id="3-数据清洗处理"><a href="#3-数据清洗处理" class="headerlink" title="3. 数据清洗处理"></a>3. 数据清洗处理</h2><p>首先，我们从 MongoDB 中读取数据并转化为 DataFrame，然后查看一下数据的基本情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_kuan</span><span class="params">()</span>:</span></span><br><span class="line">    client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line">    db = client[<span class="string">'KuAn'</span>]</span><br><span class="line">    collection = db[<span class="string">'KuAnItem'</span>]</span><br><span class="line">    <span class="comment"># 将数据库数据转为DataFrame</span></span><br><span class="line">    data = pd.DataFrame(list(collection.find()))</span><br><span class="line">    print(data.head())</span><br><span class="line">    print(df.shape)</span><br><span class="line">    print(df.info())</span><br><span class="line">    print(df.describe())</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-11-30/16708510.jpg" alt=""></p><p>从 data.head() 输出的前 5 行数据中可以看到，除了 score 列是 float 格式以外，其他列都是 object 文本类型。</p><p>comment、download、follow、num_score 这 5 列数据中部分行带有「万」字后缀，需要将字符去掉再转换为数值型；volume 体积列，则分别带有「M」和「K」后缀，为了统一大小，则需将「K」除以 1024，转换为 「M」体积。</p><p>整个数据一共有 6086 行 x 8 列，每列均没有缺失值。</p><p>df.describe() 方法对 score 列做了基本统计，可以看到，所有 App 的平均得分是 3.9 分（5 分制），最低得分 1.6 分，最高得分 4.8 分。</p><p>下面，我们将以上几列文本型数据转换为数值型数据，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_processing</span><span class="params">(df)</span>:</span></span><br><span class="line"><span class="comment">#处理'comment','download','follow','num_score','volume' 5列数据，将单位万转换为单位1，再转换为数值型</span></span><br><span class="line">    str = <span class="string">'_ori'</span></span><br><span class="line">    cols = [<span class="string">'comment'</span>,<span class="string">'download'</span>,<span class="string">'follow'</span>,<span class="string">'num_score'</span>,<span class="string">'volume'</span>]</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        colori = col+str</span><br><span class="line">        df[colori] = df[col] <span class="comment"># 复制保留原始列</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (col == <span class="string">'volume'</span>):</span><br><span class="line">            df[col] = clean_symbol(df,col)<span class="comment"># 处理原始列生成新列</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = clean_symbol2(df,col)<span class="comment"># 处理原始列生成新列</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将download单独转换为万单位</span></span><br><span class="line">    df[<span class="string">'download'</span>] = df[<span class="string">'download'</span>].apply(<span class="keyword">lambda</span> x:x/<span class="number">10000</span>)</span><br><span class="line">    <span class="comment"># 批量转为数值型</span></span><br><span class="line">    df = df.apply(pd.to_numeric,errors=<span class="string">'ignore'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_symbol</span><span class="params">(df,col)</span>:</span></span><br><span class="line">    <span class="comment"># 将字符“万”替换为空</span></span><br><span class="line">    con = df[col].str.contains(<span class="string">'万$'</span>)</span><br><span class="line">    df.loc[con,col] = pd.to_numeric(df.loc[con,col].str.replace(<span class="string">'万'</span>,<span class="string">''</span>)) * <span class="number">10000</span></span><br><span class="line">    df[col] = pd.to_numeric(df[col])</span><br><span class="line">    <span class="keyword">return</span> df[col]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_symbol2</span><span class="params">(df,col)</span>:</span></span><br><span class="line">    <span class="comment"># 字符M替换为空</span></span><br><span class="line">    df[col] = df[col].str.replace(<span class="string">'M$'</span>,<span class="string">''</span>)</span><br><span class="line">    <span class="comment"># 体积为K的除以 1024 转换为M</span></span><br><span class="line">    con = df[col].str.contains(<span class="string">'K$'</span>)</span><br><span class="line">    df.loc[con,col] = pd.to_numeric(df.loc[con,col].str.replace(<span class="string">'K$'</span>,<span class="string">''</span>))/<span class="number">1024</span></span><br><span class="line">    df[col] = pd.to_numeric(df[col])</span><br><span class="line">    <span class="keyword">return</span> df[col]</span><br></pre></td></tr></table></figure><p>以上，就完成了几列文本型数据的转换，我们再来查看一下基本情况：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">comment</th><th style="text-align:center">download</th><th style="text-align:center">follow</th><th style="text-align:center">num_score</th><th style="text-align:center">score</th><th style="text-align:center">volume</th></tr></thead><tbody><tr><td style="text-align:center">count</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td><td style="text-align:center">6086</td></tr><tr><td style="text-align:center">mean</td><td style="text-align:center">255.5</td><td style="text-align:center">13.7</td><td style="text-align:center">729.3</td><td style="text-align:center">133.1</td><td style="text-align:center">3.9</td><td style="text-align:center">17.7</td></tr><tr><td style="text-align:center">std</td><td style="text-align:center">1437.3</td><td style="text-align:center">98</td><td style="text-align:center">1893.7</td><td style="text-align:center">595.4</td><td style="text-align:center">0.6</td><td style="text-align:center">20.6</td></tr><tr><td style="text-align:center">min</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1.6</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">25%</td><td style="text-align:center">16</td><td style="text-align:center">0.2</td><td style="text-align:center">65</td><td style="text-align:center">5.2</td><td style="text-align:center">3.7</td><td style="text-align:center">3.5</td></tr><tr><td style="text-align:center">50%</td><td style="text-align:center">38</td><td style="text-align:center">0.8</td><td style="text-align:center">180</td><td style="text-align:center">17</td><td style="text-align:center">4</td><td style="text-align:center">10.8</td></tr><tr><td style="text-align:center">75%</td><td style="text-align:center">119</td><td style="text-align:center">4.5</td><td style="text-align:center">573.8</td><td style="text-align:center">68</td><td style="text-align:center">4.3</td><td style="text-align:center">25.3</td></tr><tr><td style="text-align:center">max</td><td style="text-align:center">53000</td><td style="text-align:center">5190</td><td style="text-align:center">38000</td><td style="text-align:center">17000</td><td style="text-align:center">4.8</td><td style="text-align:center">294.2</td></tr></tbody></table><p>从中可以看出以下几点信息：</p><ul><li>download 列为 App 下载数量，下载量最多的 App 有 5190 万次，最少的为 0 (很少很少)，平均下载次数为 14 万次；</li><li>volume 列为 App 体积，体积最大的 App 达到近 300M，体积最小的几乎为 0，平均体积在 18M 左右。</li><li>comment 列为 App 评分，评分数最多的达到了 5 万多条，平均有 200 多条。</li></ul><p>以上，就完成了基本的数据清洗处理过程，下面一篇文章我们将对数据进行探索性分析。</p><h2 id="4-资源获取"><a href="#4-资源获取" class="headerlink" title="4. 资源获取"></a>4. 资源获取</h2><p>如需完整代码可以搜索加入我的「<strong>知识星球：第2脑袋</strong>」获取，期待你的到来。</p><p><img src="http://media.makcyun.top/18-12-1/95375420.jpg" alt=""></p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython9.html">pyspider 爬取并分析虎嗅网 5 万篇文章</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;发现了RE 管理器、一个木函、VIA 浏览器、快图浏览等一众好软件。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>有了它，你手机上的新闻资讯类 App 都可以卸了</title>
    <link href="https://www.makcyun.top/2018/11/24/weekly_sharing6.html"/>
    <id>https://www.makcyun.top/2018/11/24/weekly_sharing6.html</id>
    <published>2018-11-24T08:16:24.000Z</published>
    <updated>2019-01-06T11:03:03.790Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>InoReader + Feedme 订阅一切，大概是不用翻墙就能看世界各地新闻最简单的方式。</p><a id="more"></a><p>这是「每周分享」的第 6 期。</p><p>这一期的主题是「改善手机阅读新闻资讯的体验」。</p><p>我们多数人的手机上会装不只一个新闻和资讯方面的 App，因为一个 App 无法满足我们广泛的资讯需求。比如有时想无脑娱乐下，那就想上「今日头条」；有时想看点深度文章，「虎嗅」和「澎湃」就比较适合；有时文艺病犯了，打开「简书」、「豆瓣」就没错；如果你是程序员的话还会常逛「GitHub」、「Medium」这些。</p><p><img src="http://media.makcyun.top/19-1-6/9691055.jpg" alt=""></p><p><img src="https://i.loli.net/2018/11/24/5bf8daeb7dabe.jpg" alt=""></p><p>这样一来，手机上就不知不觉安装了大量的 App，我也经历了这样的一个时期。但现在我只留下了上面这一个可以「<strong>匹敌上百款资讯软件</strong>」的 App。</p><p>下面说一说，为什么我会卸载掉那么多的 App 而只保留一个。</p><p>在使用 App 的过程中，我发现虽然可以靠下载很多的 App 来获取多样的信息，但随之而来的是各种不便，比如有这么几点：</p><ul><li><p><strong>占用了大量内存空间</strong></p><p>现在一个 App 动辄几十上百 M，十几个 App 装下来，几百 M 甚至上 G 的空间就被馋蚀了，你可能觉得这点不算什么，毕竟，现在的手机基本都有 64G、128G甚至更多多的存储空间。但这仅仅只是开始 ，随着使用时长增加，软件升级、文章图片下载、内容缓存带来的空间占用才是大头。最直观的就是微信了，微信安装包才几十 M，但用久了它会占用几个 G 的空间。</p></li><li><p><strong>增加了 App 切换时间成本</strong></p><p>App 一多，就增加了寻找和切换的时间成本，可能你会说：我用的 Nova 桌面对 App 做了精细的文件夹分类管理，但我仍然觉得那不还足够方便。</p></li><li><p><strong>提供了很多实际不需要的功能</strong></p><p>很多情况下，我们选择使用一个 App 是因为它里面的一项或者两三项功能，其他大部分功能是不需要的。比如拿豆瓣举例，我只喜欢里面的「图书」功能，其他的电影、广播、小组这些功能并不需要；再比如我喜欢看「腾讯新闻」的 NBA，其他的板块的内容并不感兴趣，但你不得不先下载这个 App，然后点击多次才能进入 NBA 板块页面。</p><p>所以常常仅为了一个功能，我们就要去下载个 App。你可能会说：「为什么要下载 App ，使用网页版不好么？」 ，我想说在手机上使用 Chrome 浏览器网页版的「书签」功能，仍然是非常不方便的，何况很多人并不太会使用「书签」功能。</p></li><li><p><strong>带来了大量诱惑信息</strong></p><p>我们处于一个信息爆炸的时代，手机碎片化阅读变得越来越普遍。有时候，只想花 10 分钟简单纯粹地看下热点信息或者感兴趣的话题内容，但是一打开 App 就会出现太多具有诱惑力的信息，忍不住要点进去看，看完一篇又一篇，一看就是半个钟。<strong>还有一个普遍的现象是文章底部评论板块的沦陷</strong>，不知道什么时候开始，很多 App 的评论区变成了脑残、杠精和键盘侠的战场，比如 NBA 板块。虽然我们可能并不会参与到这些评论中去，但就是忍不住想去看看那些逗比说的话，久而久之就形成了「<strong>不看文章，直奔评论区</strong>」的焦虑心态，而这显然不是我们的初衷。</p></li><li><p><strong>设置了烦人的广告</strong></p><p>这一点应该是最让人不爽的，很多 App 会在启动界面设置好几秒的广告，如果不想看那么就得去购买高级付费版。有的 App 甚至还会在文章中插入很多广告，干扰我们的阅读。</p></li><li><p><strong>保存收藏起来比较麻烦</strong></p><p>当我们在 App 中看到一篇不错的文章，想要保存下来时就比较麻烦了。通常只能是通过分享保存到印象笔记、微信公众号中。而这种无法分类存档、只是堆积的文章收藏形式，其实没什么用处，以为收藏了自己回头会看，其实再也不会看。所以这种形式的存储，作用仅限于抚慰对知识获取的焦虑，过不多久就会忘掉，久而久之，仍然会有一种：读了那么多文章，脑中依然空无一物的失落。</p></li></ul><p><strong>不知从什么时候开始，想要获得一种「读书看报」式的简单体验，已经变成了一种奢求。</strong></p><p>那么有没有什么好的方法能够解决以上痛点？</p><p>当然是有的，而且还是最好的方法，那就是：「<strong>RSS 订阅</strong>」。</p><p>RSS 是英文 Really Simple Syndication 的简称，也就是「简易信息聚合」。它可以让我们根据自己的阅读喜好，选择感兴趣的网站、博客、栏目中的资讯信息内容，然后将这些聚合汇总供我们集中阅读。这项技术其实早在 1995 年就出现了，经过前些年的辉煌期后，近些年反而变得小众了，<strong>但事实上它才是最棒的阅读方式</strong>。</p><p>为什么这么说呢，因为它与上面那些 APP 之间有一个根本的区别。普通的新闻 APP 为了尽可能地足所有人需求和口味，恨不得把所有的内容都装进来，而你只能被动地去接受。但是，使用 「RSS 订阅」则由你做主，你可以自主选择你想要的信息内容，然后借助 RSS 阅读器去集中查看就可以了。所以，这意味着：</p><ul><li>你可以只要腾讯新闻 APP 的 NBA 板块内容，而不要娱乐、军事等内容。</li><li>你可以把虎嗅、澎湃等很多个 App 或者网页的内容都装在一起，<strong>最终在在一个 RSS 阅读器中查看 。</strong></li><li><strong>你甚至无需「不可描述工具」，就能看到墙外面的新闻！</strong></li></ul><p>用一句话总结就是：<strong>如果你喜欢使用 RSS 阅读，那么你将会拥有一片新大陆。</strong></p><p>这大概是不用翻墙就能看世界各地新闻最简单的方式了。</p><p><img src="https://i.loli.net/2018/11/24/5bf8fb0dd4e71.jpg" alt=""></p><p>到这儿你可能有点激动，摩拳擦掌，跃跃欲试了。</p><p>先介绍一下怎么使用 RSS 订阅。</p><p>阅读最重要的就是找到 RSS 订阅来源，其实很好找，很多网站、博客都提供。它的形式通常是一个由 XML 文档构成的URL，只要将这个 URL 复制到一个 RSS 阅读器中进行搜索，搜索到之后订阅即可，以后这个网页只要有更新，那么就会出现在你的 RSS 阅读器中，无需再去打开这个网页查看内容了，非常地方便。</p><p>这里，比如以我的博客为例，简单说明下：</p><p><img src="https://i.loli.net/2018/11/24/5bf8f0c096964.png" alt=""></p><p>可以看到我的博客主页：<a href="https://www.makcyun.top/">https://www.makcyun.top/</a> 提供了 RSS 订阅的标志，点击打开该网页会出现一个 XML 文档，不用管文档的具体内容，只需要复制网页的 URL：<a href="https://www.makcyun.top/atom.xml">https://www.makcyun.top/atom.xml</a> 到 RSS 阅读器中，就可以订阅博客内容了，从此当博客有更新，就能及时查看最新内容。</p><p><img src="https://i.loli.net/2018/11/24/5bf8f2d3165c2.png" alt=""></p><p><img src="https://i.loli.net/2018/11/24/5bf8fc7a63dae.jpg" alt=""></p><p>看到这儿，你可能会有两个感兴趣的地方：</p><ul><li><strong>有没有集中的资讯 RSS 订阅源？？</strong></li><li><strong>用什么 App 看这些信息？</strong></li></ul><p>下面就是这期的干货了。</p><p>推荐两个非常棒的 RSS 来源，首先是 <strong><a href="https://docs.rsshub.app/" target="_blank" rel="noopener">RSSHub</a></strong> 。</p><p>这个 <a href="https://github.com/DIYgod/RSSHub" target="_blank" rel="noopener">GitHub</a> 库提供了多种类型的订阅源，多达上百个网站，不同网站下又可以订阅不同板块的内容如果上面找不到你喜欢的网站，你还可以自己动手制作 RSS 订阅源，可以说是「<strong>万网皆可订阅了</strong>」。</p><p><img src="https://i.loli.net/2018/11/24/5bf8ff6baf217.png" alt=""></p><p>第二个是 <strong><a href="https://github.com/Gracker/Rss-IT.git" target="_blank" rel="noopener">RSS - IT 人</a></strong> ，这个库提供了了一些 IT 大佬的博客，比如阮一峰 （GitHub 上排名第一的中国人）。</p><p>有了这两个库，订阅源已经不是问题了，剩下的问题在于：<strong>用什么 App 去阅读？</strong></p><p>市面上的 RSS 阅读器多如牛毛，要想挑选出一个功能强大、阅读体验好、颜值高又的 App 很不容易。比较知名的 InoReader、Feedly、NewsBlur 等等。但用了之后，发现它们多多少少都有些不如意的地方，比如订阅不方便、界面不美观等。</p><p><strong>这里推荐一种更好的阅读方式：在电脑上使用 InoReader ，在手机上使用一款小而精的佳软：Feedme</strong>。</p><p>以阮一峰老师的博客为例，先对比一下普通网页版、InoReader 和 Feddme 版本的排版，可以看到 Feedme 要好看很多。</p><p><img src="https://i.loli.net/2018/11/24/5bf8daf03d299.jpg" alt=""></p><p>其次，它的订阅功能很强大，支持中文关键字搜索。这样，当我们没有一个明确的 RSS 订阅源时，可以通过关键字搜索来订阅感兴趣的资讯。</p><p><img src="https://i.loli.net/2018/11/24/5bf909491a002.jpg" alt=""></p><p>以上就是在 App 中使用 RSS 阅读的正确打开方式了。但这仍然不足以体现 RSS 订阅功能的强大，<strong>更牛逼的地方是在电脑上使用。</strong></p><p><img src="https://i.loli.net/2018/11/24/5bf90956ce962.png" alt=""></p><p>在电脑上打开 InoReader ，可以看到它为我们打造了一套个人专属、功能丰富的资讯信息面板。在这个面板中，可以实现这么几个功能：</p><ul><li><p>查看过往阅读历史</p><p>很多时候，我们看过的新闻看过就看过了，但是在这上面你可以查到你的文章阅读记录。</p></li><li><p>利用文件夹和标签功能对收藏的文章进行分类</p><p>这个功能类似于印象笔记，但是比印象笔记的剪藏功能强大多了。文章分门别类之后，我们再回头进行总结和记录就很方便。</p></li><li><p>将文章保存为 PDF 永久保存</p></li><li><p>专注阅读</p><p>在这里面阅读几乎没有广告，提供了更好的阅读体验。</p></li></ul><p><img src="https://i.loli.net/2018/11/24/5bf90d78cfd7c.png" alt=""></p><p>好，以上就是这期的内容。文中所说的阅读 App，可以在公众号后台回复「<strong>RSS</strong>」获得。</p><p>本文完。</p><p><img src="http://pbscl931v.bkt.clouddn.com/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;InoReader + Feedme 订阅一切，大概是不用翻墙就能看世界各地新闻最简单的方式。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="App" scheme="https://www.makcyun.top/tags/App/"/>
    
  </entry>
  
  <entry>
    <title>上了年纪的工程师有哪些出路</title>
    <link href="https://www.makcyun.top/2018/11/20/life02.html"/>
    <id>https://www.makcyun.top/2018/11/20/life02.html</id>
    <published>2018-11-20T08:16:24.000Z</published>
    <updated>2018-12-19T03:12:32.973Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>吴军《谷歌方法论》专栏课程记录。</p><a id="more"></a><p>一直很喜欢吴军老师在得到开的「谷歌方法论」专栏课程，收获非常多。惊叹于吴老师不仅对 IT 领域有着非常深厚和独到的见解，而且在其他众多领域也有着丰富的知识储备。他能够把晦涩、深奥的专业知识以通俗易懂的方式和道理讲述出来，仅从这方面来说，他无疑是一个真正的大师与专家。除此之外，针对读者留言提出的一些关于工作瓶颈、职场困惑、人生抉择等方面的具体问题，他也会给出非常值得借鉴和参考的建议。</p><p>不久前，他在一封来信中，针对一位读者提出的<strong>「大龄工程师有哪些出路」</strong>这个问题给了几点建议，对于我这个还不算是 IT 工程师就已经是大龄的人来说，深有感触并且得到了启发。如果你目前或者不远的将来也担心面临这样的处境，不妨参考一下他说的这几点建议。下面，我将分享与你。</p><p>问题大意是说：<strong>国外的一线大龄（近 50 岁）工程师，能够很轻松地选择跳槽，而对于国内的一线工程师（年近 40 岁）来说，竞争力就在慢慢下降也基本不会再换工作，而且很多公司基本不要超过 35 岁的工程师</strong>。是什么原因导致了这种巨大差距，对于大龄工程师又有些什么建议？</p><p><strong>原文：</strong></p><p>这个问题很有代表性。<br>我们在讨论这个问题之前，首先要明确两个概念。</p><p><strong>第一个是关于一线工程师的定义。</strong></p><p>一般人会理解为做具体工作的工程师，而且比较倾向于看成是甶领导分配任务，自己独立完成任务的人。但这些人其实只是我所说的第五级的工程师。一线工程师可以是四级、三级，甚至更高。如果一个人到了四十岁，还是五级工程师，就怨不得别人不要他了，因为这说明他的学习能力太差，没有发展潜力。</p><p>事实上，中国还真是很缺四级的工程师，在 Google 和微软这样的公司，只要是所谓的高级工程师（勉强达到四级的要求），目前中国各大企业都抢着要，开出的薪水可比一个总监高得多，这些人在 BAT 这样的企业里，我还没见到谁的收入在百万以下。</p><p>Google 中国在历史上最好的工程师（没有之一）当属郄（qiè）小虎，今天 20 多岁的年轻人写代码还真写不过他。用腾讯原来主管投资的副总裁彭志坚的话讲，他是中国互联网企业从 Google 全世界华裔工程师中最想挖的三个工程师之一。今天全中国能做高管的人多如牛毛，但郄小虎这样的人，一个巴掌就数过来了，因此后来他想去哪家公司就去哪家，而且只接受 CTO 的头衔。就这样，排着队想挖他的大公司至少有两位数。</p><p><strong>第二个要明确的概念是，什么叫做竞争力下降。</strong></p><p>如果拼熬夜，40 岁的人确实拼不过 20 岁的，更何况 40 岁的人还有一大堆家庭负担，工作的时间和强度远不如 20 岁的人。但是，今天 40 岁的人其实智力远没有衰退，如果 40 岁的人能够解决一些 20 岁的人解决不了的问题，那么就不存在所谓的竞争力不足的问题了。如果 40 岁的人做不到这一点，除了知识没有及时更新，变得老化之外，很重要的原因是 <strong>心态上不愿意像 20 岁的人那样踏踏实实做具体工作。</strong>今天一些年轻人愿意死磕一个星期，找到计算机程序里的 bug ，这样他们可以证明自己的水平。但是 40 岁的人常常不愿意再这样工作，当然竞争力就会下降了。相比之下，美国和德国很多 40 岁的工程师，依然在像 20 岁那样工作，当然就没有被淘汰的问题。而一旦有一些 40 岁的人开始以慢节奏工作时，作为企业的主管，出于对招聘安全的考虑，干脆所有年纪大一点的人都不招了。</p><p>对于年纪大一点的工程师，他给出了三点简单的建议：</p><p><strong>1. 自省，看看自己的本事是否随着年龄的增长在增长。</strong></p><p>如果是，其实不用担心。就像郄小虎，永远不用担心没人要他一样。</p><p><strong>2.如果发现自己的本事和 20 岁的时候没有变化，那也怪不得别人，因为现在的果，源于过去的因。</strong></p><p>如果自己现在还在单位里，没有换工作的打算，赶快甩掉身上的懒肉，补上这十多年来应该具有的进步；如果自己正在换工作，也没有关系，总结一下自己过去完成的不超过三件最拿得出手的工作，好好包装一下自己。虽然中国的企业不喜欢要年纪大的人，但是它们也不愿意培养新人，通常希望来了人，就能马上上手干活。这就给有工作经验的人提供了机会，虽然包装出来的本事未必是真本事。</p><p>为什么不要写三个以上的成就呢？坦率地来讲，今天绝大部分人没有那么多亮眼的成就，能有三个就很不错了，如果列举多了，一定是凑数，把平庸的工作也列举出来了。即使是郄小虎，在我印象中，他最拿得出手的成果有两样，首先是把Google 整个广告收入提高了 10% 左右，这可是不得了的贡献，他也因此获得了 Google 的最高奖——创始人奖。其次，他成功地将 Google 花了 30 多亿美元收购的双击公司 (Doubleclick) 的广告系统换成了 Google 的，完成了 Google 对双击公司在工程和产品上的整合，这也是一件了不得的事情。当然，他还做了很多其他的工作，肯定水平不差，但是讲不讲都不重要了。</p><p><strong>3.如果工作中使用的工程工具已经老化，或者你所从事的工作是没有前途的，趁早换新的，不要等到自己手上没有武器时再发愁。</strong></p><p>以上是他给出的几点建议，希望对你也能有所启发。</p><p>本文完。</p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;吴军《谷歌方法论》专栏课程记录。&lt;/p&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>这个 GitHub 库能拯救你的文章排版</title>
    <link href="https://www.makcyun.top/2018/11/16/weekly_sharing5.html"/>
    <id>https://www.makcyun.top/2018/11/16/weekly_sharing5.html</id>
    <published>2018-11-16T08:16:24.000Z</published>
    <updated>2018-12-19T03:12:32.972Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Apple、Microsoft、少数派都在使用这一套文案排版标准。</p><a id="more"></a><p>这是「每周分享」的第 5 期。</p><p>上一期文章 「<a href="https://www.makcyun.top/weekly_sharing4.html">程序员是如何在 5 分钟内搞定公众号排版的</a>」，介绍了如何快速对公众号文章进行排版，但是没有介绍文案排版中的另一个重点，即 「<strong>如何让文章看起来更美观</strong>」。</p><p><img src="http://media.makcyun.top/18-11-15/82118168.jpg" alt=""></p><p><strong>你能看出上面两个排版哪个更好以及在好哪里么</strong>？</p><p>我相信，不少人在对公众号文章进行排版时，主要是凭个人感觉或者喜好来排的，而这会导致两个问题：</p><ul><li><p>无论怎么排版，总感觉和网上优秀的文章相比，自己的文章看起来不是那么优雅。</p></li><li><p>这种凭感觉的排版会造成排版风格不够固定，因为感觉是经常在变的，所以排版风格也就跟着会变化，而 <strong>频繁的风格变化会给读者增加阅读成本。</strong></p></li></ul><p>我自己是一直深受上面两个问题困扰，所以一直在网上尝试寻找解决方法，直到最近知道了 GitHub 上一个库的存在：「<a href="https://github.com/sparanoid/chinese-copywriting-guidelines" target="_blank" rel="noopener">中文文案排版指北</a>」，让我终于找到了「北」。</p><p>看了这篇文案排版说明之后，我发现原来 <strong>优秀的文章排版，都遵循了一套潜在的业内标准和规范</strong>。</p><p>当我尝试着将这套标准应用到了自己的文章中之后，发现文章变得好看多了。下面就来介绍一下这个库规定了哪些文案排版标准。</p><h2 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h2><p>我以前从来没有意识到「文字之间需要适当地添加空格」这件事的重要性：</p><blockquote><p>「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。」</p></blockquote><h3 id="中英文之间需要增加空格"><a href="#中英文之间需要增加空格" class="headerlink" title="中英文之间需要增加空格"></a>中英文之间需要增加空格</h3><p>正确：你可以搜索 makcyun 关注我的博客。</p><p>错误：你可以搜索makcyun关注我的博客。</p><p>例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。</p><h3 id="中文与数字之间需要增加空格"><a href="#中文与数字之间需要增加空格" class="headerlink" title="中文与数字之间需要增加空格"></a>中文与数字之间需要增加空格</h3><p>正确：拿钢琴来说，键盘有始也有终，它有 88 个键，并不是无限的。</p><p>错误：拿钢琴来说，键盘有始也有终，它有88个键，并不是无限的。</p><h3 id="数字与单位之间需要增加空格"><a href="#数字与单位之间需要增加空格" class="headerlink" title="数字与单位之间需要增加空格"></a>数字与单位之间需要增加空格</h3><p>正确：双十一我买了一副 2000 元的耳机。</p><p>错误： 双十一我买了一副 2000元的耳机。</p><p>例外1：<a href="https://www.apple.com/macbook-pro/specs/" target="_blank" rel="noopener">当表示规格时，不需要添加空格</a>：</p><p>正确：我的手机有 64GB 内存</p><p>错误：我的手机有 64 GB 内存</p><p>例外2：度／百分比与数字之间不需要增加空格。</p><p>正确：今天很冷，只有 1°，所以我几乎 90% 的时间都是在被窝里。</p><p>错误：今天很冷，只有 1 °，所以我几乎 90 % 的时间都是在被窝里。</p><h3 id="全角标点与其他字符之间不加空格"><a href="#全角标点与其他字符之间不加空格" class="headerlink" title="全角标点与其他字符之间不加空格"></a>全角标点与其他字符之间不加空格</h3><p>正确：我刚刚买了一部 iPhone，好开心！</p><p>错误：我刚刚买了一部 iPhone ，好开心！</p><h2 id="标点符号"><a href="#标点符号" class="headerlink" title="标点符号"></a>标点符号</h2><p>说起标点符号，就要先说一下「全角」和「半角」这两种标点符号类型。首先，简单来说，中文字符是全角字符，拉丁字母和数字则是半角字符。全角标点占 2 个字节，宽一些；半角标点占 1 个字节，窄一些。比如表示中英文的逗号分别是「，」和「,」。</p><h3 id="使用全角中文标点"><a href="#使用全角中文标点" class="headerlink" title="使用全角中文标点"></a>使用全角中文标点</h3><p>正确：嗨！你今天去 Python 技术主题大会（PyCon）现场了么？</p><p>错误：嗨!你今天去 Python 技术主题大会(PyCon)现场了么?</p><h3 id="遇到完整的英文整句、特殊名词，其內容使用半角标点"><a href="#遇到完整的英文整句、特殊名词，其內容使用半角标点" class="headerlink" title="遇到完整的英文整句、特殊名词，其內容使用半角标点"></a>遇到完整的英文整句、特殊名词，其內容使用半角标点</h3><p>正确：我发现很多人都喜欢用乔布说的那句话：「Stay hungry, stay foolish.」</p><p>错误：我发现很多人都喜欢用乔布说的那句话：「Stay hungry，stay foolish。」</p><h2 id="名词"><a href="#名词" class="headerlink" title="名词"></a>名词</h2><h3 id="专有名词使用正确的大小写"><a href="#专有名词使用正确的大小写" class="headerlink" title="专有名词使用正确的大小写"></a>专有名词使用正确的大小写</h3><p>正确：</p><p>我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。</p><p>错误：</p><p>我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。</p><h3 id="不要使用不地道的缩写"><a href="#不要使用不地道的缩写" class="headerlink" title="不要使用不地道的缩写"></a>不要使用不地道的缩写</h3><p>正确：我们需要一位熟悉 JavaScript、HTML5 的前端开发者。</p><p>错误：我们需要一位熟悉 Js、h5 的前端开发者。</p><h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><h3 id="简体中文使用直角引号"><a href="#简体中文使用直角引号" class="headerlink" title="简体中文使用直角引号"></a>简体中文使用直角引号</h3><p>直角引号就是「」、『』，他们才是真正的中文引号，常见的 “” 是弯角引号，是当时我国参考了英文引号之后，而制定出的简体中文引号标准。实际上，在文章中使用直角引号要比弯角引号更好看。</p><p>对比一下：</p><p>「老师，『有条不紊』的『紊』是什么意思？」</p><p>“老师，‘有条不紊’的‘紊’是什么意思？”</p><h3 id="中文尽量不要使用斜体"><a href="#中文尽量不要使用斜体" class="headerlink" title="中文尽量不要使用斜体"></a>中文尽量不要使用斜体</h3><p><em>中文不太适合用斜体字</em>，斜体之后效果糟糕，字形会扭曲。</p><h3 id="链接之间增加空格"><a href="#链接之间增加空格" class="headerlink" title="链接之间增加空格"></a>链接之间增加空格</h3><p>如果想浏览我的博客，请点击底部 <a href="#">阅读原文</a> 查看。</p><p>如果想浏览我的博客，请点击底部<a href="#">阅读原文</a>查看。</p><h3 id="首行顶格写不用缩进"><a href="#首行顶格写不用缩进" class="headerlink" title="首行顶格写不用缩进"></a>首行顶格写不用缩进</h3><p>这个你可能会不同意，不过这样做，效果的确更好。先来解释一下「首行缩进」的目的是为了什么：</p><p>「 <strong>每段之前空两格</strong>」 是我们从小学写作文时养成的习惯，也是正式文体的格式要求，目的是为了区分自然段。</p><p>但是像我们现在接触的阅读，都是没有固定的格式要求的，如微信公众号、电子文档等，所以大家一般都采用「 空出一行」 进行自然段与自然段之间的区分。有了段落区分之后，也就没有必要再再去空格了。而且这种写作方式非常省事，<br>看起来也很整齐 。</p><p>以上就是一套优秀的文案排版所遵循的标准。你可能会质疑它到底对不对，那么你可以看看 <a href="https://www.apple.com/cn/" target="_blank" rel="noopener">Apple 中国</a>，<br><a href="https://www.microsoft.com/zh-cn/" target="_blank" rel="noopener">Microsoft 中国官网</a>、<a href="https://sspai.com/" target="_blank" rel="noopener">少数派网站</a> 的排版，它们基本上都是采用了上面的排版标准。</p><p>本文完。</p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Apple、Microsoft、少数派都在使用这一套文案排版标准。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="文章排版" scheme="https://www.makcyun.top/tags/%E6%96%87%E7%AB%A0%E6%8E%92%E7%89%88/"/>
    
  </entry>
  
  <entry>
    <title>Python 可视化(4)：WordCloud 中英文词云图绘制方法汇总</title>
    <link href="https://www.makcyun.top/2018/11/14/Python_visualization04.html"/>
    <id>https://www.makcyun.top/2018/11/14/Python_visualization04.html</id>
    <published>2018-11-14T08:16:24.000Z</published>
    <updated>2019-03-07T00:49:45.114Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>英文和中文词云图绘制总结。</p><a id="more"></a><p><strong>摘要：</strong> 当我们手中有一篇文档，比如书籍、小说、电影剧本，若想快速了解其主要内容是什么，那么可以通过绘制WordCloud 词云图，通过关键词（高频词）就可视化直观地展示出来，非常方便。本文主要介绍常见的英文和中文文本的词云图绘制，以及通过 DataFrame 数据框绘制 Frequency 频率词云图。</p><p>在上一篇文章「<a href="https://www.makcyun.top/web_scraping_withpython9.html">pyspider 爬取并分析虎嗅网 5 万篇文章</a> 」中的文本可视化部分，我们通过 WordCloud 和 jieba 两个包绘制了中文词云图，当时只是罗列出了代码，并没有详细介绍。接下来，将详细说明词云图绘制步骤，并且对词云图的种类进行拓展。</p><p><img src="http://media.makcyun.top/18-11-7/78186420.jpg" alt=""></p><h2 id="1-英文词云"><a href="#1-英文词云" class="headerlink" title="1. 英文词云"></a>1. 英文词云</h2><p>这里，我们先绘制英文文本的词云图，因为它相对简单一些，这里，以《海上钢琴师》这部电影的剧本为例。</p><p>首先，需要准备好电影剧本的文本文件（如下图）：</p><p><img src="http://media.makcyun.top/18-11-13/56930265.jpg" alt=""></p><p>接下来，我们绘制一个最简单的矩形词云图，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 获取当前文件路径</span></span><br><span class="line">d = path.dirname(__file__) <span class="keyword">if</span> <span class="string">"__file__"</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> os.getcwd()</span><br><span class="line"><span class="comment"># 获取文本text</span></span><br><span class="line">text = open(path.join(d,<span class="string">'legend1900.txt'</span>)).read()</span><br><span class="line"><span class="comment"># 生成词云</span></span><br><span class="line">wc = WordCloud(scale=<span class="number">2</span>,max_font_size = <span class="number">100</span>)</span><br><span class="line">wc.generate_from_text(text)</span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">plt.imshow(wc,interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line"><span class="comment">#存储图像</span></span><br><span class="line">wc.to_file(<span class="string">'1900_basic.png'</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># plt.savefig('1900_basic.png',dpi=200)</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先，通过 open() 方法读取文本文件，然后 WordCloud 方法设置了词云参数，generate_from_text() 生成该文本词云，然后显示和保存词云图，十几行代码就可以生成最简单的词云图。</p><p><img src="http://media.makcyun.top/18-11-13/4871189.jpg" alt=""></p><p>通过上面的词云图，你可能会发现有几点问题：</p><ul><li>可不可以随便更换背景，比如白色？</li><li>词云图是矩形，能不能换成其他形状或者自定义图片样式？</li><li>词云中最显眼的词汇 「ONE」，并没有实际含义，能不能去掉？</li></ul><p>以上这些都是可以更改的，如果你想实现以上想法，那么需要先了解一下 WordCloud 的API 参数及它的一些方法。</p><p>这里，我们列出它的各项参数，并注释重要的几项：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">wordcloud.WordCloud(</span><br><span class="line">    font_path=<span class="keyword">None</span>,  <span class="comment"># 字体路径，英文不用设置路径，中文需要，否则无法正确显示图形</span></span><br><span class="line">    width=<span class="number">400</span>, <span class="comment"># 默认宽度</span></span><br><span class="line">    height=<span class="number">200</span>, <span class="comment"># 默认高度</span></span><br><span class="line">    margin=<span class="number">2</span>, <span class="comment"># 边缘</span></span><br><span class="line">    ranks_only=<span class="keyword">None</span>, </span><br><span class="line">    prefer_horizontal=<span class="number">0.9</span>, </span><br><span class="line">    mask=<span class="keyword">None</span>, <span class="comment"># 背景图形，如果想根据图片绘制，则需要设置</span></span><br><span class="line">    scale=<span class="number">1</span>, </span><br><span class="line">    color_func=<span class="keyword">None</span>, </span><br><span class="line">    max_words=<span class="number">200</span>, <span class="comment"># 最多显示的词汇量</span></span><br><span class="line">    min_font_size=<span class="number">4</span>, <span class="comment"># 最小字号</span></span><br><span class="line">    stopwords=<span class="keyword">None</span>, <span class="comment"># 停止词设置，修正词云图时需要设置</span></span><br><span class="line">    random_state=<span class="keyword">None</span>, </span><br><span class="line">    background_color=<span class="string">'black'</span>, <span class="comment"># 背景颜色设置，可以为具体颜色,比如white或者16进制数值</span></span><br><span class="line">    max_font_size=<span class="keyword">None</span>, <span class="comment"># 最大字号</span></span><br><span class="line">    font_step=<span class="number">1</span>, </span><br><span class="line">    mode=<span class="string">'RGB'</span>, </span><br><span class="line">    relative_scaling=<span class="string">'auto'</span>, </span><br><span class="line">    regexp=<span class="keyword">None</span>, </span><br><span class="line">    collocations=<span class="keyword">True</span>, </span><br><span class="line">    colormap=<span class="string">'viridis'</span>, <span class="comment"># matplotlib 色图，可更改名称进而更改整体风格</span></span><br><span class="line">    normalize_plurals=<span class="keyword">True</span>, </span><br><span class="line">    contour_width=<span class="number">0</span>, </span><br><span class="line">    contour_color=<span class="string">'black'</span>, </span><br><span class="line">    repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>了解各项参数后，我们就可以自定义想要的词云图了。比如更换一下背景颜色和整体风格，就可以修改以下几项：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wc = WordCloud(</span><br><span class="line">    scale=<span class="number">2</span>,<span class="comment"># 缩放2倍</span></span><br><span class="line">    max_font_size = <span class="number">100</span>,</span><br><span class="line">    background_color = <span class="string">'#383838'</span>,<span class="comment"># 灰色</span></span><br><span class="line">    colormap = <span class="string">'Blues'</span>) </span><br><span class="line"><span class="comment"># colormap名称 https://matplotlib.org/examples/color/colormaps_reference.html</span></span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-11-13/7484409.jpg" alt=""></p><p>接下来，我们提升一点难度，通过设置 StopWords 去掉没有实际意义的「ONE」，然后将词云图绘制在我们自定义的一张图片上。</p><p><img src="http://media.makcyun.top/18-11-13/96391276.jpg" alt=""></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud,STOPWORDS,ImageColorGenerator</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wc_english</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 获取当前文件路径</span></span><br><span class="line">    d = path.dirname(__file__) <span class="keyword">if</span> <span class="string">"__file__"</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> os.getcwd()</span><br><span class="line">    <span class="comment"># 获取文本text</span></span><br><span class="line">    text = open(path.join(d,<span class="string">'legend1900.txt'</span>)).read()</span><br><span class="line">    <span class="comment"># 读取背景图片</span></span><br><span class="line">    background_Image = np.array(Image.open(path.join(d, <span class="string">"mask1900.jpg"</span>)))</span><br><span class="line">    <span class="comment"># or</span></span><br><span class="line">    <span class="comment"># background_Image = imread(path.join(d, "mask1900.jpg"))</span></span><br><span class="line">    <span class="comment"># 提取背景图片颜色</span></span><br><span class="line">    img_colors = ImageColorGenerator(background_Image)</span><br><span class="line">    <span class="comment"># 设置英文停止词</span></span><br><span class="line">    stopwords = set(STOPWORDS)</span><br><span class="line">    wc = WordCloud(</span><br><span class="line">        margin = <span class="number">2</span>, <span class="comment"># 设置页面边缘</span></span><br><span class="line">        mask = background_Image,</span><br><span class="line">        scale = <span class="number">2</span>,</span><br><span class="line">        max_words = <span class="number">200</span>, <span class="comment"># 最多词个数</span></span><br><span class="line">        min_font_size = <span class="number">4</span>, <span class="comment"># 最小字体大小</span></span><br><span class="line">        stopwords = stopwords,</span><br><span class="line">        random_state = <span class="number">42</span>,</span><br><span class="line">        background_color = <span class="string">'white'</span>, <span class="comment"># 背景颜色</span></span><br><span class="line">        max_font_size = <span class="number">150</span>, <span class="comment"># 最大字体大小</span></span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 生成词云</span></span><br><span class="line">    wc.generate_from_text(text)</span><br><span class="line">    <span class="comment"># 等价于</span></span><br><span class="line">    <span class="comment"># wc.generate(text)</span></span><br><span class="line">    <span class="comment"># 根据图片色设置背景色</span></span><br><span class="line">    wc.recolor(color_func=img_colors)</span><br><span class="line">    <span class="comment">#存储图像</span></span><br><span class="line">    wc.to_file(<span class="string">'1900pro1.png'</span>)</span><br><span class="line">    <span class="comment"># 显示图像</span></span><br><span class="line">    plt.imshow(wc,interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>首先，通过 open() 方法读取文本文件，Image.open() 方法读取了背景图片，np.array 方法将图片转换为矩阵。接着设置了词云自带的英文 StopWords 停止词，用来分割筛除文本中不需要的词汇，比如：a、an、the 这些。然后，在 wordcloud 方法中，设置词云的具体参数。generate_from_text() 方法生成该词云，recolor() 则是根据图片色彩绘制词云颜色，绘制效果如下：</p><p><img src="http://media.makcyun.top/18-11-13/67270011.jpg" alt=""></p><p>接着，我们还是看到了显眼的「ONE」，下面我们将它去除掉，方法也很简单，几行代码搞定：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取文本词排序，可调整 stopwords</span></span><br><span class="line">process_word = WordCloud.process_text(wc,text)</span><br><span class="line">sort = sorted(process_word.items(),key=<span class="keyword">lambda</span> e:e[<span class="number">1</span>],reverse=<span class="keyword">True</span>)</span><br><span class="line">print(sort[:<span class="number">50</span>]) <span class="comment"># 获取文本词频最高的前50个词</span></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[(<span class="string">'one'</span>, <span class="number">60</span>), (<span class="string">'ship'</span>, <span class="number">47</span>), (<span class="string">'Nineteen Hundred'</span>, <span class="number">43</span>), (<span class="string">'know'</span>, <span class="number">38</span>), (<span class="string">'music'</span>, <span class="number">36</span>), ...]</span><br><span class="line"></span><br><span class="line">stopwords = set(STOPWORDS)</span><br><span class="line">stopwords.add(<span class="string">'one'</span>)</span><br></pre></td></tr></table></figure><p>这里，我们可以对文本词频进行排序，通过输出看到 「ONE」词频最高，然后添加进 stopwords 中，就可以屏蔽该词从而不再显示。这种手动添加停止词的方法适用于词数量比较少的情况。</p><p><img src="http://media.makcyun.top/18-11-13/21724138.jpg" alt=""></p><p>另外，我们还可以将词云图颜色显示为黑白渐变色，也只需修改几行代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grey_color_func</span><span class="params">(word, font_size, position, orientation, random_state=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"hsl(0, 0%%, %d%%)"</span> % random.randint(<span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 随机设置hsl色值</span></span><br><span class="line">wc.recolor(color_func=grey_color_func)</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-11-13/72250527.jpg" alt=""></p><p>以上，就是英文词云图绘制的几种方法，下面我们介绍中文词云图的绘制。</p><h2 id="2-中文词云"><a href="#2-中文词云" class="headerlink" title="2. 中文词云"></a>2. 中文词云</h2><p>相比于英文词云，中文在绘制词云图前，需要先切割词汇，这里推荐使用 jieba 包来切割分词。因为它可以说是最好的中文分词包了，GitHub 上拥有 160 K 的 Star 数。</p><p>安装好 jieba 包后，我们就可以对文本进行分词然后生成词云了。这里，选取吴军老师的著作《浪潮之巅》作为中文文本的案例，仍然采用图片形式的词云图。素材准备好后，接下来就可以开始中文词云图绘制。</p><p><img src="http://media.makcyun.top/18-11-13/11957799.jpg" alt=""></p><p>首先，需要读取文本文件，相比于英文，这里要添加文本编码格式，否则会报错，添加几行检测代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">text = open(path.join(d,<span class="string">'langchao.txt'</span>),<span class="string">'rb'</span>).read()</span><br><span class="line">text_charInfo = chardet.detect(text)</span><br><span class="line">print(text_charInfo)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">&#123;<span class="string">'encoding'</span>: <span class="string">'UTF-8-SIG'</span>, <span class="string">'confidence'</span>: <span class="number">1.0</span>, <span class="string">'language'</span>: <span class="string">''</span>&#125;</span><br><span class="line">text = open(path.join(d,<span class="string">r'langchao.txt'</span>),encoding=<span class="string">'UTF-8-SIG'</span>).read()</span><br></pre></td></tr></table></figure><p>接着，对文本进行分词。jieba 分词有 3 种方式：精确模式、全模式和搜索引擎模式，它们之间的差别，可以用一个例子来体现。</p><p>比如，有这样的一句话：「”我来到北京清华大学”」，用 3 种模式进行分词，结果分别如下：</p><ul><li><p>全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</p></li><li><p>精确模式: 我/ 来到/ 北京/ 清华大学</p></li><li><p>搜索引擎模式： 我/ 来/ 来到/ 北京/ 清华/ 大学/ 清华大学/</p></li></ul><p>根据结果可知，我们应该选择「精确模式」来分词。关于 jieba 包的详细用法，可以参考 GitHub 仓库链接：</p><p><a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">https://github.com/fxsjy/jieba</a></p><p>分词完成后，我们还需要设置 stopwords 停止词，由于 WordCloud 没有中文停止词，所以我们需要自行构造。这里可以采取两种方式来构造：</p><ul><li>通过 stopwords.update() 方法手动添加</li><li>根据已有 stopwords 词库遍历文本筛除停止词</li></ul><h3 id="2-1-stopwords-update-手动添加"><a href="#2-1-stopwords-update-手动添加" class="headerlink" title="2.1. stopwords.update() 手动添加"></a>2.1. stopwords.update() 手动添加</h3><p>这种方法和前面的英文停止词构造的方法是一样的，目的是在词云图中不显示 stopwords 就行了 。先不设置 stopwords，而是先对文本词频进行排序，然后将不需要的词语添加为 stopwords 即可。下面用代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取文本词排序，可调整 stopwords</span></span><br><span class="line">process_word = WordCloud.process_text(wc,text)</span><br><span class="line">sort = sorted(process_word.items(),key=<span class="keyword">lambda</span> e:e[<span class="number">1</span>],reverse=<span class="keyword">True</span>)</span><br><span class="line">print(sort[:<span class="number">50</span>]) <span class="comment"># # 获取文本词频最高的前50个词</span></span><br><span class="line"></span><br><span class="line">[(<span class="string">'公司'</span>, <span class="number">1273</span>), (<span class="string">'但是'</span>, <span class="number">769</span>), (<span class="string">'IBM'</span>, <span class="number">668</span>), (<span class="string">'一个'</span>, <span class="number">616</span>), (<span class="string">'Google'</span>, <span class="number">429</span>), (<span class="string">'自己'</span>, <span class="number">396</span>), (<span class="string">'因此'</span>, <span class="number">363</span>), (<span class="string">'微软'</span>, <span class="number">358</span>), (<span class="string">'美国'</span>, <span class="number">344</span>), (<span class="string">'没有'</span>, <span class="number">334</span>)...]</span><br></pre></td></tr></table></figure><p>可以看到，我们先输出文本词频最高的一些词汇后，发现：但是、一个、因此、没有等这些词都是不需要显示在词云图中的。因此，可以把这些词用列表的形式添加到 stopwords 中，然后再次绘制词云图就能得出比较理想的效果，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chardet</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">text+=<span class="string">' '</span>.join(jieba.cut(text,cut_all=<span class="keyword">False</span>)) <span class="comment"># cut_all=False 表示采用精确模式</span></span><br><span class="line"><span class="comment"># 设置中文字体</span></span><br><span class="line">font_path = <span class="string">'C:\Windows\Fonts\SourceHanSansCN-Regular.otf'</span>  <span class="comment"># 思源黑体</span></span><br><span class="line"><span class="comment"># 读取背景图片</span></span><br><span class="line">background_Image = np.array(Image.open(path.join(d, <span class="string">"wave.png"</span>)))</span><br><span class="line"><span class="comment"># 提取背景图片颜色</span></span><br><span class="line">img_colors = ImageColorGenerator(background_Image)</span><br><span class="line"><span class="comment"># 设置中文停止词</span></span><br><span class="line">stopwords = set(<span class="string">''</span>)</span><br><span class="line">stopwords.update([<span class="string">'但是'</span>,<span class="string">'一个'</span>,<span class="string">'自己'</span>,<span class="string">'因此'</span>,<span class="string">'没有'</span>,<span class="string">'很多'</span>,<span class="string">'可以'</span>,<span class="string">'这个'</span>,<span class="string">'虽然'</span>,<span class="string">'因为'</span>,<span class="string">'这样'</span>,<span class="string">'已经'</span>,<span class="string">'现在'</span>,<span class="string">'一些'</span>,<span class="string">'比如'</span>,<span class="string">'不是'</span>,<span class="string">'当然'</span>,<span class="string">'可能'</span>,<span class="string">'如果'</span>,<span class="string">'就是'</span>,<span class="string">'同时'</span>,<span class="string">'比如'</span>,<span class="string">'这些'</span>,<span class="string">'必须'</span>,<span class="string">'由于'</span>,<span class="string">'而且'</span>,<span class="string">'并且'</span>,<span class="string">'他们'</span>])</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">        font_path = font_path, <span class="comment"># 中文需设置路径</span></span><br><span class="line">        margin = <span class="number">2</span>, <span class="comment"># 页面边缘</span></span><br><span class="line">        mask = background_Image,</span><br><span class="line">        scale = <span class="number">2</span>,</span><br><span class="line">        max_words = <span class="number">200</span>, <span class="comment"># 最多词个数</span></span><br><span class="line">        min_font_size = <span class="number">4</span>, <span class="comment">#</span></span><br><span class="line">        stopwords = stopwords,</span><br><span class="line">        random_state = <span class="number">42</span>,</span><br><span class="line">        background_color = <span class="string">'white'</span>, <span class="comment"># 背景颜色</span></span><br><span class="line">        <span class="comment"># background_color = '#C3481A', # 背景颜色</span></span><br><span class="line">        max_font_size = <span class="number">100</span>,</span><br><span class="line">        )</span><br><span class="line">wc.generate(text)</span><br><span class="line"><span class="comment"># 获取文本词排序，可调整 stopwords</span></span><br><span class="line">process_word = WordCloud.process_text(wc,text)</span><br><span class="line">sort = sorted(process_word.items(),key=<span class="keyword">lambda</span> e:e[<span class="number">1</span>],reverse=<span class="keyword">True</span>)</span><br><span class="line">print(sort[:<span class="number">50</span>]) <span class="comment"># 获取文本词频最高的前50个词</span></span><br><span class="line"><span class="comment"># 设置为背景色，若不想要背景图片颜色，就注释掉</span></span><br><span class="line">wc.recolor(color_func=img_colors)</span><br><span class="line"><span class="comment"># 存储图像</span></span><br><span class="line">wc.to_file(<span class="string">'浪潮之巅basic.png'</span>)</span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">plt.imshow(wc,interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们对比以下 stopwords 添加前后的效果就可以大致看出效果。</p><p>stopwords 添加之前：</p><p><img src="http://media.makcyun.top/18-11-13/56340530.jpg" alt=""></p><p>stopwords 添加之后：</p><p><img src="http://media.makcyun.top/18-11-13/83641740.jpg" alt=""></p><p>可以看到，stopwords.update() 这种方法需要手动去添加，比较麻烦一些，而且如果 stopwords 过多的话，添加就比较费时了。下面介绍第 2 种 自动去除 stopwords 的方法。</p><h3 id="2-2-stopwords-库自动遍历删除"><a href="#2-2-stopwords-库自动遍历删除" class="headerlink" title="2.2. stopwords 库自动遍历删除"></a>2.2. stopwords 库自动遍历删除</h3><p>这种方法的思路也比较简单。主要分为 2 个步骤：</p><ul><li><p>利用已有的中文 stopwords 词库，对原文本进行分词后，遍历词库去除停止词，然后生成新的文本文件。</p></li><li><p>根据新的文件绘制词云图，便不会再出现 stopwords，如果发现 stopwords 词库不全可以进行补充，然后再次生成词云图即可。</p></li></ul><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对原文本分词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_words</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 获取当前文件路径</span></span><br><span class="line">    d = path.dirname(__file__) <span class="keyword">if</span> <span class="string">"__file__"</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> os.getcwd()</span><br><span class="line">    text = open(path.join(d,<span class="string">r'langchao.txt'</span>),encoding=<span class="string">'UTF-8-SIG'</span>).read()</span><br><span class="line">    text = jieba.cut(text,cut_all=<span class="keyword">False</span>)</span><br><span class="line">    content = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> text:</span><br><span class="line">        content += i</span><br><span class="line">        content += <span class="string">" "</span></span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载stopwords</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_stopwords</span><span class="params">()</span>:</span></span><br><span class="line">    filepath = path.join(d,<span class="string">r'stopwords_cn.txt'</span>)</span><br><span class="line">    stopwords = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> open(filepath,encoding=<span class="string">'utf-8'</span>).readlines()]</span><br><span class="line">    <span class="comment"># print(stopwords) # ok</span></span><br><span class="line">    <span class="keyword">return</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除原文stopwords,并生成新的文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move_stopwwords</span><span class="params">(content,stopwords)</span>:</span></span><br><span class="line">    content_after = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> content:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords:</span><br><span class="line">            <span class="keyword">if</span> word != <span class="string">'\t'</span><span class="keyword">and</span><span class="string">'\n'</span>:</span><br><span class="line">                content_after += word</span><br><span class="line"></span><br><span class="line">    content_after = content_after.replace(<span class="string">"   "</span>, <span class="string">" "</span>).replace(<span class="string">"  "</span>, <span class="string">" "</span>)</span><br><span class="line">    <span class="comment"># print(content_after)</span></span><br><span class="line">    <span class="comment"># 写入去停止词后生成的新文本</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'langchao2.txt'</span>,<span class="string">'w'</span>,encoding=<span class="string">'UTF-8-SIG'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(content_after)</span><br></pre></td></tr></table></figure><p>网上有很多中文 stopwords 词库资料，这里选取了一套包含近 <a href="https://blog.csdn.net/shijiebei2009/article/details/39696571" target="_blank" rel="noopener">2000 个词汇和标点符号的词库</a>：stopwords_cn.txt，结构形式如下：</p><p><img src="http://media.makcyun.top/18-11-13/83684654.jpg" alt=""></p><p>遍历该 stopwords 词库，删除停止词获得新的文本，然后利用第一种方法绘制词云图即可。</p><p>首先输出一下文本词频最高的部分词汇，可以看到常见的停止词已经没有了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'公司'</span>, <span class="number">1462</span>), (<span class="string">'美国'</span>, <span class="number">366</span>), (<span class="string">'IBM'</span>, <span class="number">322</span>), (<span class="string">'微软'</span>, <span class="number">320</span>), (<span class="string">'市场'</span>, <span class="number">287</span>), (<span class="string">'投资'</span>, <span class="number">263</span>), (<span class="string">'世界'</span>, <span class="number">236</span>), (<span class="string">'硅谷'</span>, <span class="number">235</span>), (<span class="string">'技术'</span>, <span class="number">234</span>), (<span class="string">'发展'</span>, <span class="number">225</span>), (<span class="string">'计算机'</span>, <span class="number">218</span>), (<span class="string">'摩托罗拉'</span>, <span class="number">203</span>)...]</span><br></pre></td></tr></table></figure><p>最终绘制词云图，效果如下：</p><p><img src="http://media.makcyun.top/18-11-13/18929834.jpg" alt=""></p><h2 id="3-Frenquency-词云图"><a href="#3-Frenquency-词云图" class="headerlink" title="3. Frenquency 词云图"></a>3. Frenquency 词云图</h2><p>除了直接读入文本生成词云，还可以使用 <strong>DataFrame</strong> 或者 <strong>字典格式</strong> 的词频作为输入绘制词云。</p><p>下面，以此前我们爬过的一份 10 年「世界大学排名 TOP500 强」 数据为例，介绍如何绘制词云图。</p><p>数据为 5001行 x 6 列，我们想根据各国 TOP 500 强大学的数量之和，展示各国顶尖大学数量的一个大概情况对比。</p><p>​</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">world_rankuniversityscorequantityyearcountry</span><br><span class="line"><span class="number">1</span>哈佛大学<span class="number">100</span><span class="number">500</span><span class="number">2009</span>USA</span><br><span class="line"><span class="number">2</span>斯坦福大学<span class="number">73.1</span><span class="number">499</span><span class="number">2009</span>USA</span><br><span class="line"><span class="number">3</span>加州大学-伯克利<span class="number">71</span><span class="number">498</span><span class="number">2009</span>USA</span><br><span class="line"><span class="number">4</span>剑桥大学<span class="number">70.2</span><span class="number">497</span><span class="number">2009</span>UK</span><br><span class="line"><span class="number">5</span>麻省理工学院<span class="number">69.5</span><span class="number">496</span><span class="number">2009</span>USA</span><br><span class="line">...</span><br><span class="line"><span class="number">496</span>犹他州立大学<span class="number">2018</span>USA</span><br><span class="line"><span class="number">497</span>圣拉斐尔生命健康大学<span class="number">2018</span>Italy</span><br><span class="line"><span class="number">498</span>早稻田大学<span class="number">2018</span>Japan</span><br><span class="line"><span class="number">499</span>韦恩州立大学<span class="number">2018</span>USA</span><br><span class="line"><span class="number">500</span>西弗吉尼亚大学<span class="number">2018</span>USA</span><br></pre></td></tr></table></figure><p>有两种格式可以直接生成频率词云图，第一种是 Series 生成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.dates <span class="keyword">as</span> mdate</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'university.csv'</span>,encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">df = df.groupby(by = <span class="string">'country'</span>).count()</span><br><span class="line">df = df[<span class="string">'world_rank'</span>].sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line">print(df[:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">country</span><br><span class="line">USA               <span class="number">1459</span></span><br><span class="line">Germany            <span class="number">382</span></span><br><span class="line">UK                 <span class="number">379</span></span><br><span class="line">China              <span class="number">320</span></span><br><span class="line">France             <span class="number">210</span></span><br><span class="line">Canada             <span class="number">209</span></span><br><span class="line">Japan              <span class="number">206</span></span><br><span class="line">Australia          <span class="number">199</span></span><br><span class="line">Italy              <span class="number">195</span></span><br><span class="line">Netherlands        <span class="number">122</span></span><br></pre></td></tr></table></figure><p>第二种转换为 dict 字典生成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = dict(df)</span><br><span class="line">print(df)</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">&#123;<span class="string">'USA'</span>: <span class="number">1459</span>, <span class="string">'Germany'</span>: <span class="number">382</span>, <span class="string">'UK'</span>: <span class="number">379</span>, <span class="string">'China'</span>: <span class="number">320</span>, <span class="string">'France'</span>: <span class="number">210</span>,..&#125;</span><br></pre></td></tr></table></figure><p>这两种数据都可以快速生成词云图，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">font_path=<span class="string">'C:\Windows\Fonts\SourceHanSansCN-Regular.otf'</span>  <span class="comment"># 思源黑</span></span><br><span class="line">wordcloud = WordCloud(</span><br><span class="line">    background_color = <span class="string">'#F3F3F3'</span>,</span><br><span class="line">    font_path = font_path,</span><br><span class="line">    width = <span class="number">5000</span>,</span><br><span class="line">    height = <span class="number">300</span>,</span><br><span class="line">    margin = <span class="number">2</span>,</span><br><span class="line">    max_font_size = <span class="number">200</span>,</span><br><span class="line">    random_state = <span class="number">42</span>,</span><br><span class="line">    scale = <span class="number">2</span>,</span><br><span class="line">    colormap = <span class="string">'viridis'</span>,  <span class="comment"># 默认virdis</span></span><br><span class="line">    )</span><br><span class="line">wordcloud.generate_from_frequencies(df)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># wordcloud.fit_words(df)</span></span><br><span class="line">plt.imshow(wordcloud,interpolation = <span class="string">'bilinear'</span>)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="http://media.makcyun.top/18-11-13/73471381.jpg" alt=""></p><p>可以看到，美国最为突出，其次是德国、英国、中国等。</p><p>文中代码及素材可以在公众号后台回复 <strong>词云 </strong>或者在下面的链接中获取：</p><p>本文完。</p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython4.html">做 PPT 没灵感？澎湃网 1500 期信息图送给你</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython7.html">国内创业公司的信息都在这里了</a></p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎扫一扫识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;英文和中文词云图绘制总结。&lt;/p&gt;
    
    </summary>
    
      <category term="Python可视化" scheme="https://www.makcyun.top/categories/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="WordCloud 词云" scheme="https://www.makcyun.top/tags/WordCloud-%E8%AF%8D%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>程序员是如何在 5 分钟内搞定公众号排版的</title>
    <link href="https://www.makcyun.top/2018/11/10/weekly_sharing4.html"/>
    <id>https://www.makcyun.top/2018/11/10/weekly_sharing4.html</id>
    <published>2018-11-10T07:16:24.000Z</published>
    <updated>2019-03-15T08:56:48.687Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>放弃 Word ，放弃 135 编辑器，用这套方法又快又好搞定公众号排版。</p><a id="more"></a><p>这是每周分享的第 4 期。</p><p>先简单介绍下这个栏目，顾名思义，就是会在每个周末分享一篇文章。内容主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p>好，下面开始进入正题。</p><p>文章主要内容：</p><ul><li>Markdown 语法介绍</li><li>Markdown 编辑器推荐 Typora</li><li>VS Code 给标题添加编号</li><li>Md2All 渲染 Markdown</li></ul><p>正如微信公众号的口号所言「再小的个体，都有自己的品牌」，于是越来越多的人开始拥有自己的公众号，这几乎成为了不少人写作的标配。在我的微信好友中，拥有公众号的人数比例达到了 30% ，这是很夸张的一个比例。大家在公众号书写出于各自的目的，但是有一点是共通的，就是要「排版」。</p><p>虽然每个人都有自己的一套排版技巧和排版风格，但我发现有些人写作 1小时、排版也要 1 小时，才勉强能排出比较好看的文章，更有甚者排版时间也花了，文章看起来依然惨不忍睹。而有些人写作 1 小时，排版只要 1 分钟，排出来却非常好看。这些人肯定是有独到的排版技巧的。</p><p>我的公众号开通时间算是比较早的 (2014年)，这 4 年，我使用了不下 3 套排版方法。</p><p><img src="http://media.makcyun.top/18-11-10/69343741.jpg" alt=""></p><p>可以看到，我的排版风格一直在变化，大致可以分为 3 个阶段。</p><p>早期 （2014 年）那会儿还不太会玩公众号，文章内容都是直接从 Word 中复制进去然后就发布，根本谈不上排版。中期 (2016 年)，我使用了很多人推荐的 秀米 和135 这些公众号排版编辑器，套用了很多模板自带的样式，文章排版出来也变得好看很多。到今年，我换成了 Markdwon 和 Md2All 的组合方式。因为，我发现前两种方式有几个比较明显的缺点：</p><ul><li>在自己电脑上的 Word 中排版地很好看，但在别人的电脑上打开时却惨不忍睹，等于没有排版。这是 Word 的一大硬伤。</li><li>使用秀米编辑器排版虽然有很多好看的样式，但是其实不需要那么多花哨的样式，而且耗时比较长。</li><li>这两个平台文章可移植性差，排版好以后仅适用于公众号，如果想发布到其他平台，需要重新排版，太麻烦。</li></ul><p>如果用一句话来形容 Markdwon 和 Md2All 的这种排版的特点，那就是：<code>Word 好比素颜，Markdown + Md2ALL 则是底妆 + 精妆。</code></p><h2 id="1-关于-Markdwon"><a href="#1-关于-Markdwon" class="headerlink" title="1. 关于 Markdwon"></a>1. 关于 Markdwon</h2><p>几个月前，我开始搜索有什么更好用的公众号排版方式，看到很多人推荐使用 Markdwon 编辑器排版。于是赶紧了解了下是怎么玩的，原来 Markdown 语言诞生于 2004 年，起初主要是程序员使用，到现在渐渐成为了很多写作者的第一选择，这种方法最大的特点就是：<code>写文和排版可以同步进行，从而大大节省排版时间。</code></p><p>什么意思呢，就是在文章需要进行排版的地方使用一些诸如：「<code># - &gt; ! [] ()</code>」 这几种符号，就可以自动识别为：标题、序号、引用、图片、超链接等这些排版格式。这样就省去了在 Word 中去进行：插入列表级别、插入图片、插入链接这些麻烦的操作，手要不停地在键盘和鼠标中切换。使用它，手几乎可以不用离开键盘，从而更加专注于写作。</p><p>上面的这些符号就是 Markdown 语言中的固定几种语法符号，简单易学，学会以后能够节省很多排版时间。</p><p>如果你此前没有使用过 Markdown 写作，那么推荐两个教程给你，以供参考。看了之后如果对这种写作方式感兴趣的话，那么强烈建议你放弃现在的 Word 开始拥抱 Markdown 吧。</p><p><a href="https://www.jianshu.com/p/q81RER" target="_blank" rel="noopener">献给写作者的 Markdown 新手指南</a></p><p><a href="https://github.com/xirong/my-markdown" target="_blank" rel="noopener">markdown 介绍/学习/工具/资料</a></p><h2 id="2-Typora-和-Markeditor"><a href="#2-Typora-和-Markeditor" class="headerlink" title="2. Typora 和 Markeditor"></a>2. Typora 和 Markeditor</h2><p><code>使用 Markdown 写作，最好是找一款支持 Markdown 语法的编辑器。</code></p><p>十多年来，随着 Markdwon 越来越流行，各种各样的 Makdown 编辑器如雨后春笋般涌出，多的让人目不暇接。到底哪款编辑器最好用？这个是没有固定答案的，不仅不同的人的喜欢不同，而且同一个人在不同时期喜好也是变化的，只有多去尝试几款，通过对比后就能发现你喜欢的，我个人就用过不下 5 款编辑器。这里给你推荐些获取 Markdwon 编辑器的资源：</p><ul><li><a href="https://www.ctolib.com/categories/javascript-markdown-pg-1.html" target="_blank" rel="noopener">CTOLib 码库：123 款 Markdwon 编辑器详细介绍</a></li></ul><p><img src="http://media.makcyun.top/18-11-10/87025783.jpg" alt=""></p><ul><li><a href="https://sspai.com/post/32483" target="_blank" rel="noopener">少数派：18 款优秀的 Markdown 写作工具</a></li></ul><p><img src="http://media.makcyun.top/18-11-10/22998989.jpg" alt=""></p><ul><li><a href="https://github.com/topics/markdown-editor" target="_blank" rel="noopener">GitHub：数百款 Markdown 编辑器</a></li></ul><p><img src="http://media.makcyun.top/18-11-10/60945598.jpg" alt=""></p><p><code>最终的选用原则只有一个：「选用着最舒服的」。</code></p><p>为了不至于让你挑花眼，我推荐两款给你：<a href="https://typora.io/" target="_blank" rel="noopener">Typora</a> 和 <a href="https://www.markeditor.com/" target="_blank" rel="noopener">Markeditor</a> 。二者都是开源的，也都支持 Mac 和 Windows。</p><p>相比于 Markeditor，我更喜欢 Typora，也是我现在微信公众号排版使用的编辑器。</p><p><img src="http://media2.makcyun.top/typora.gif" alt=""></p><p>它有这么几个优点：</p><ul><li><p>「What You See Is What You Mean」，也就是「所见即所得」</p><p>据我所知，它是目前唯一款能够做到实时预览 Markdown 效果的编辑器。其他大多数编辑器，包括 Markeditor 在内，要么只能先写然后再预览，要么左右分屏一边写一边预览。事实上，当 Markdwon 比较熟练以后，只需要 Typora 这种能够实时预览的效果。</p></li><li><p>足够多的快捷键，代替 Markdown 语法符号输入</p><p>举个例子，如果你想插入一幅图片，那么基本的操作是这样来写：！<a href="http://media.makcyun.top/18-11-10/60945598.jpg" target="_blank" rel="noopener"></a>，而在 Typroa 里，只需要输入快捷键：Ctrl + Shift + I，就可以快速插入图片了。同理，它还可以插入表格、超链接等。也就是说，你早先记得那些语法，在编辑器里可以不用了，取而代之的是快捷键。这样一来，Markdown 更加容易写了。不过，语法是一定要掌握的。</p></li></ul><p>上面，用了不少篇幅去介绍 Markdown 和编辑器。 <strong>现在，我们回到今天所要说的「又快又好地排版」这个问题上来。</strong></p><h2 id="3-给标题添加编号"><a href="#3-给标题添加编号" class="headerlink" title="3. 给标题添加编号"></a>3. 给标题添加编号</h2><p>用 Markdown 编辑好文章，其实我们已经完成一多半的排版工作。接下来「标题添加编号这个功能」其实是选择性的，可有可无，不过我觉得添加上编号会让文章显得更加有秩序。</p><p><img src="http://media.makcyun.top/18-11-10/48689535.jpg" alt=""></p><p>你可能会问，这个编号是手动添加上去的么？</p><p>当然不是，这里推荐一款可以自动添加编号的 VS Code 插件：<strong><a href="https://segmentfault.com/a/1190000010338264" target="_blank" rel="noopener">Markdown add index</a></strong>。</p><p>使用很简单，只需要 3 步：</p><ul><li>安装 VS Code 软件（编程的人应该都用过这款颜值很高的 IDE 吧）</li><li>在里面搜索下载 Markdown add index 这个插件</li><li>打开命令窗口 (Ctrl + Shift + P)，输入 Markdown add index，然后回车，就会发现文章中的标题已经自动添加上序号了。如果后期标题有顺序调整，只需要再次执行该命令，就会自动更改。这比手动去改，方便太多。</li></ul><p><img src="http://media.makcyun.top/18-11-10/22399881.jpg" alt=""></p><h2 id="4-Md2All-渲染文章"><a href="#4-Md2All-渲染文章" class="headerlink" title="4. Md2All 渲染文章"></a>4. Md2All 渲染文章</h2><p>完成以上工作后，我们只需要再做最后一步的「文章渲染」就大功告成了。渲染的目的就是让公众号的文章变得更加好看，也就好比女生化妆的最后一步「精妆」。</p><p>之前很多人会推荐使用 Markdwon Here 插件来渲染，但用过之后发现其实并不太好用。这里，推荐一款可以说是 Markdown Here 的增强版插件： <strong>Md2All</strong> 。</p><p><img src="http://media.makcyun.top/18-11-10/29639072.jpg" alt=""></p><p>它有这几个优点：</p><ul><li><p>排版样式非常多，支持自定义 CSS 样式</p><p>我的公众号排版风格，就是在里面预先自定义好 CSS 样式。CSS 样式，其实就是类似下面这种预先设置好的文章排版格式，比如：字体、间距、列表、表格这些。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">p</span> &#123;  <span class="comment">/*段落选择器*/</span></span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">1.5em</span> <span class="number">5px</span> <span class="meta">!important</span>;</span><br><span class="line">  <span class="comment">/*颜色*/</span></span><br><span class="line">  <span class="attribute">color</span>:<span class="number">#565656</span>;</span><br><span class="line">  <span class="comment">/*字体*/</span></span><br><span class="line">  <span class="attribute">font-family</span>:<span class="string">'微软雅黑'</span>;</span><br><span class="line">  <span class="comment">/*字号*/</span></span><br><span class="line">  <span class="attribute">font-size</span>:<span class="number">15px</span>;</span><br><span class="line">  <span class="comment">/*行间距，可用百分比，数值倍数，像素设置，还包括text-indent缩进、letter-spacing字间距、*/</span></span><br><span class="line">  <span class="attribute">line-height</span>:<span class="number">2</span>;</span><br><span class="line">  <span class="comment">/*段间距，一般用margin属性调整*/</span></span><br><span class="line">  <span class="attribute">margin-bottom</span>:<span class="number">20px</span>;</span><br><span class="line">  <span class="comment">/*页边距用padding属性调整*/</span></span><br><span class="line">   <span class="attribute">letter-spacing</span>: <span class="number">0.5px</span>;</span><br><span class="line">   <span class="comment">/*文字间距*/</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">ul</span>, <span class="selector-tag">ol</span> &#123;  <span class="comment">/*无序、有序列表 缩进15px*/</span></span><br><span class="line">  <span class="attribute">padding-left</span>: <span class="number">25px</span>;</span><br><span class="line">  <span class="attribute">font-family</span>:<span class="string">'微软雅黑'</span>;</span><br><span class="line">  <span class="attribute">font-size</span>: <span class="number">11px</span>;</span><br><span class="line">  <span class="attribute">letter-spacing</span>: <span class="number">0.5px</span>;</span><br><span class="line">   <span class="comment">/*文字间距*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>支持数十种代码高亮主题样式</p><p>这个就有点牛逼了，内置了很多种非常不错的颜色主题，而且还可以显示代码行数。</p><p><img src="http://media.makcyun.top/18-11-10/78440523.jpg" alt=""></p></li><li><p>可对接七牛云账号，支持直接上传图片</p><p>这个很方便，文章要插入图片，只需把图片拖拽进去，就会自动返回 URL 链接。至于七牛云，我是非常建议去注册个账号，可以把它当作文章图片的仓库。</p></li></ul><p>关于这款插件更多的功能，可以参考下面这个教程：</p><p><a href="https://www.cnblogs.com/garyyan/p/8329343.html" target="_blank" rel="noopener">玩转公众号markdown排版</a></p><h2 id="5-复制到公众号，大功告成"><a href="#5-复制到公众号，大功告成" class="headerlink" title="5. 复制到公众号，大功告成"></a>5. 复制到公众号，大功告成</h2><p>完成以上 3 步，就可以点击 Md2All 的「复制」按钮，然后打开微信公众号后台复制进去，格式会原模原样地保留下来，然后，发布就可以了。</p><h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><p>本文所说的微信公众号排版，看似非常复杂，要用到很多软件，好像 5 分钟之内不可能排版好。其实，这些只是一次性工作，当你前期花点时间配置好各项操作后，以后的排版工作就「一劳永逸」了。</p><p>5 分钟之内完成，是可以做得到的。</p><p>本文完。</p><hr><p><strong>推荐阅读：</strong></p><p><a href="https://www.makcyun.top/fuli01.html">每周分享第1期：关于 PDF 处理软件，你需要的都在这里了</a></p><p><a href="https://www.makcyun.top/weekly_sharing2.html">每周分享第 2 期：一掏出手机，就暴露了程序猿身份</a></p><p><a href="https://www.makcyun.top/weekly_sharing3.html">每周分享第 3 期：安卓最好用的电子书阅读器</a></p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;放弃 Word ，放弃 135 编辑器，用这套方法又快又好搞定公众号排版。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="技能 GET" scheme="https://www.makcyun.top/tags/%E6%8A%80%E8%83%BD-GET/"/>
    
  </entry>
  
  <entry>
    <title>pyspider 爬取并分析虎嗅网 5 万篇文章</title>
    <link href="https://www.makcyun.top/2018/11/04/web_scraping_withpython9.html"/>
    <id>https://www.makcyun.top/2018/11/04/web_scraping_withpython9.html</id>
    <published>2018-11-04T08:16:24.000Z</published>
    <updated>2019-03-07T00:21:04.975Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。</p><a id="more"></a><p><strong>摘要：</strong> 不少时候，一篇文章能否得到广泛的传播，除了文章本身实打实的质量以外，一个好的标题也至关重要。本文爬取了虎嗅网建站至今共 5 万条新闻标题内容，助你找到起文章标题的技巧与灵感。同时，分享一些值得关注的文章和作者。</p><h2 id="1-分析背景"><a href="#1-分析背景" class="headerlink" title="1. 分析背景"></a>1. 分析背景</h2><h3 id="1-1-为什么选择虎嗅"><a href="#1-1-为什么选择虎嗅" class="headerlink" title="1.1. 为什么选择虎嗅"></a>1.1. 为什么选择虎嗅</h3><p>在众多新媒体网站中，「虎嗅」网的文章内容和质量还算不错。在「新榜」科技类公众号排名中，它位居榜单第 3 名，还是比较受欢迎的。所以选择爬取该网站的文章信息，顺便从中了解一下这几年科技互联网都出现了哪些热点信息。</p><p><img src="http://media.makcyun.top/18-11-4/22792947.jpg" alt=""></p><blockquote><p>「关于虎嗅」</p><p>虎嗅网创办于 2012 年 5 月，是一个聚合优质创新信息与人群的新媒体平台。该平台专注于贡献原创、深度、犀利优质的商业资讯，围绕创新创业的观点进行剖析与交流。虎嗅网的核心，是关注互联网及传统产业的融合、明星公司的起落轨迹、产业潮汐的动力与趋势。</p></blockquote><h3 id="1-2-分析内容"><a href="#1-2-分析内容" class="headerlink" title="1.2. 分析内容"></a>1.2. 分析内容</h3><ul><li>分析虎嗅网 5 万篇文章的基本情况，包括收藏数、评论数等</li><li>发掘最受欢迎和最不受欢迎的文章及作者</li><li>分析文章标题形式（长度、句式）与受欢迎程度之间的关系</li><li>展现近些年科技互联网行业的热门词汇</li></ul><h3 id="1-3-分析工具"><a href="#1-3-分析工具" class="headerlink" title="1.3. 分析工具"></a>1.3. 分析工具</h3><ul><li>Python</li><li>pyspider</li><li>MongoDB</li><li>Matplotlib</li><li>WordCloud</li><li>Jieba</li></ul><h2 id="2-数据抓取"><a href="#2-数据抓取" class="headerlink" title="2. 数据抓取"></a>2. 数据抓取</h2><p>使用 pyspider 抓取了虎嗅网的主页文章，文章抓取时期为 2012 年建站至 2018 年 11 月 1 日，共计约 5 万篇文章。抓取 了 7 个字段信息：文章标题、作者、发文时间、评论数、收藏数、摘要和文章链接。</p><h3 id="2-1-目标网站分析"><a href="#2-1-目标网站分析" class="headerlink" title="2.1. 目标网站分析"></a>2.1. 目标网站分析</h3><p>这是要爬取的 <a href="https://www.huxiu.com/" target="_blank" rel="noopener">网页界面</a>，可以看到是通过 AJAX 加载的。</p><p><img src="http://media.makcyun.top/18-11-4/38979674.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-11-5/33467436.jpg" alt=""></p><p>右键打开开发者工具查看翻页规律，可以看到 URL 请求是 POST 类型，下拉到底部查看 Form Data，表单需提交参数只有 3 项。经尝试， 只提交 page 参数就能成功获取页面的信息，其他两项参数无关紧要，所以构造分页爬取非常简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huxiu_hash_code: <span class="number">39</span>bcd9c3fe9bc69a6b682343ee3f024a</span><br><span class="line">page: <span class="number">4</span></span><br><span class="line">last_dateline: <span class="number">1541123160</span></span><br></pre></td></tr></table></figure><p>接着，切换选项卡到 Preview 和 Response 查看网页内容，可以看到数据都位于 data 字段里。total_page 为 2004，表示一共有 2004 页的文章内容，每一页有 25 篇文章，总共约 5 万篇，也就是我们要爬取的数量。</p><p><img src="http://media.makcyun.top/18-11-5/80730129.jpg" alt=""></p><p>以上，我们就找到了所需内容，接下来可以开始构造爬虫，整个爬取思路比较简单。之前我们也练习过这一类 Ajax 文章的爬取，可以参考：</p><p><a href="https://www.makcyun.top/web_scraping_withpython4.html">抓取澎湃网建站至今 1500 期信息图栏目图片</a></p><h3 id="2-2-pyspider-介绍"><a href="#2-2-pyspider-介绍" class="headerlink" title="2.2. pyspider 介绍"></a>2.2. pyspider 介绍</h3><p>和之前文章不同的是，这里我们使用一种新的工具来进行爬取，叫做：pyspider 框架。由国人 binux 大神开发，GitHub Star 数超过 12 K，足以证明它的知名度。可以说，学习爬虫不能不会使用这个框架。</p><p>网上关于这个框架的介绍和实操案例非常多，这里仅简单介绍一下。</p><p>我们之前的爬虫都是在 Sublime 、PyCharm 这种 IDE 窗口中执行的，整个爬取过程可以说是处在黑箱中，内部运行的些细节并不太清楚。而 pyspider 一大亮点就在于提供了一个可视化的 WebUI 界面，能够清楚地查看爬虫的运行情况。</p><p><img src="http://media.makcyun.top/18-11-5/65831925.jpg" alt=""></p><p>pyspider 的架构主要分为 Scheduler(调度器)、Fetcher(抓取器)、Processer(处理器)三个部分。Monitor(监控器)对整个爬取过程进行监控，Result Worker(结果处理器)处理最后抓取的结果。</p><p><img src="http://media.makcyun.top/18-11-5/51865737.jpg" alt=""></p><p>我们看看该框架的运行流程大致是怎么样的：</p><ul><li>一个 pyppider 爬虫项目对应一个 Python 脚本，脚本里定义了一个 Handler 主类。爬取时首先调用 on_start() 方法生成最初的抓取任务，然后发送给 Scheduler。</li><li>Scheduler 将抓取任务分发给 Fetcher 进行抓取，Fetcher 执行然后得到 Response、随后将 Response 发送给 Processer。</li><li>Processer 处理响应并提取出新的 URL 然后生成新的抓取任务，然后通过消息队列的方式通知 Scheduler 当前抓取任务执行情况，并将新生成的抓取任务发送给 Scheduler。如果生成了新的提取结果，则将其发送到结果队列等待 Result Worker 处理。</li><li>Scheduler 接收到新的抓取任务，然后查询数据库，判断其如果是新的抓取任务或者是需要重试的任务就继续进行调度，然后将其发送回 Fetcher 进行抓取。</li><li>不断重复以上工作、直到所有的任务都执行完毕，抓取结束。</li><li>抓取结束后、程序会回调 on_finished() 方法，这里可以定义后处理过程。</li></ul><p>该框架比较容易上手，网页右边是代码区，先定义类（Class）然后在里面添加爬虫的各种方法（也可以称为函数），运行的过程会在左上方显示，左下方则是输出结果的区域。</p><p>这里，分享几个不错的教程以供参考：</p><p>GitHub 项目地址：<a href="https://github.com/binux/pyspider" target="_blank" rel="noopener">https://github.com/binux/pyspider</a></p><p>官方主页：<a href="http://docs.pyspider.org/en/latest/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/</a></p><p>pyspider 中文网：<a href="http://www.pyspider.cn/page/1.html" target="_blank" rel="noopener">http://www.pyspider.cn/page/1.html</a></p><p>pyspider 爬虫原理剖析：<a href="http://python.jobbole.com/81109/" target="_blank" rel="noopener">http://python.jobbole.com/81109/</a></p><p>pyspider 爬淘宝图案例实操：<a href="https://cuiqingcai.com/2652.html" target="_blank" rel="noopener">https://cuiqingcai.com/2652.html</a></p><p>安装好该框架后，下面我们可以就开始爬取了。</p><h3 id="2-3-抓取数据"><a href="#2-3-抓取数据" class="headerlink" title="2.3. 抓取数据"></a>2.3. 抓取数据</h3><p>CMD 命令窗口执行：pyspider all 命令，然后浏览器输入：<a href="http://localhost:5000/" target="_blank" rel="noopener">http://localhost:5000/</a> 就可以启动 pyspider 。</p><p>点击 Create 新建一个项目，Project Name 命名为：huxiu，因为要爬取的 URL 是 POST 类型，所以这里可以先不填写，之后可以在代码中添加，再次点击 Creat 便完成了该项目的新建。</p><p><img src="http://media.makcyun.top/18-11-5/80480010.jpg" alt=""></p><p>新项目建立好后会自动生成一部分模板代码，我们只需在此基础上进行修改和完善，然后就可以运行爬虫项目了。现在，简单梳理下代码编写步骤。</p><p><img src="http://media.makcyun.top/18-11-5/13729703.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config:&#123;</span><br><span class="line">        <span class="string">"headers"</span>:&#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span></span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">3</span>): <span class="comment"># 先循环1页</span></span><br><span class="line">            print(<span class="string">'正在爬取第 %s 页'</span> % page)</span><br><span class="line">            self.crawl(<span class="string">'https://www.huxiu.com/v2_action/article_list'</span>,method=<span class="string">'POST'</span>,data=&#123;<span class="string">'page'</span>:page&#125;, callback=self.index_page)</span><br></pre></td></tr></table></figure><p>这里，首先定义了一个 Handler 主类，整个爬虫项目都主要在该类下完成。 接着，可以将爬虫基本的一些基本配置，比如 Headers、代理等设置写在下面的 crawl_config 属性中。（如果你还没有习惯从函数（def）转换到类（Class）的代码写法，那么需要先了解一下类的相关知识，之后我也会单独用一篇文章介绍一下。）</p><p>下面的 on_start() 方法是程序的入口，也就是说程序启动后会首先从这里开始运行。首先，我们将要爬取的 URL传入 crawl() 方法，同时将 URL 修改成虎嗅网的：<a href="https://www.huxiu.com/v2_action/article_list。由于" target="_blank" rel="noopener">https://www.huxiu.com/v2_action/article_list。由于</a> URL 是 POST 请求，所以我们还需要增加两个参数：method 和 data。method 表示 HTTP 请求方式，默认是 GET，这里我们需要设置为 POST；data 是 POST 请求表单参数，只需要添加一个 page 参数即可。</p><p>接着，通过 callback 参数定义一个 index_page() 方法，用来解析 crawl() 方法爬取 URL 成功后返回的 Response 响应。在后面的 index_page() 方法中，可以使用 PyQuery 提取响应中的所需内容。具体提取方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        content = response.json[<span class="string">'data'</span>]</span><br><span class="line">        <span class="comment"># 注意，在sublime中，json后面需要添加()，pyspider 中则不用</span></span><br><span class="line">        doc = pq(content)</span><br><span class="line">        lis = doc(<span class="string">'.mod-art'</span>).items()</span><br><span class="line">        data = [&#123;</span><br><span class="line">            <span class="string">'title'</span>: item(<span class="string">'.msubstr-row2'</span>).text(),</span><br><span class="line">            <span class="string">'url'</span>:<span class="string">'https://www.huxiu.com'</span>+ str(item(<span class="string">'.msubstr-row2'</span>).attr(<span class="string">'href'</span>)),</span><br><span class="line">            <span class="string">'name'</span>: item(<span class="string">'.author-name'</span>).text(),</span><br><span class="line">            <span class="string">'write_time'</span>:item(<span class="string">'.time'</span>).text(),</span><br><span class="line">            <span class="string">'comment'</span>:item(<span class="string">'.icon-cmt+ em'</span>).text(),</span><br><span class="line">            <span class="string">'favorites'</span>:item(<span class="string">'.icon-fvr+ em'</span>).text(),</span><br><span class="line">            <span class="string">'abstract'</span>:item(<span class="string">'.mob-sub'</span>).text()</span><br><span class="line">            &#125; <span class="keyword">for</span> item <span class="keyword">in</span> lis ]   <span class="comment"># 列表生成式结果返回每页提取出25条字典信息构成的list</span></span><br><span class="line">        print(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>这里，网页返回的 Response 是 json 格式，待提取的信息存放在其中的 data 键值中，由一段 HTML 代码构成。我们可以使用 response.json[‘data’] 获取该 HTML 信息，接着使用 PyQuery 搭配 CSS 语法提取出文章标题、链接、作者等所需信息。这里使用了列表生成式，能够精简代码并且转换为方便的 list 格式，便于后续存储到 MongoDB 中。我们输出并查看一下第 2 页的提取结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由25个 dict 构成的 list</span></span><br><span class="line">[&#123;<span class="string">'title'</span>: <span class="string">'想要长生不老？杀死体内的“僵尸细胞”吧'</span>, <span class="string">'url'</span>: <span class="string">'https://www.huxiu.com/article/270086.html'</span>, <span class="string">'name'</span>: <span class="string">'造就Talk'</span>, <span class="string">'write_time'</span>: <span class="string">'19小时前'</span>, <span class="string">'comment'</span>: <span class="string">'4'</span>, <span class="string">'favorites'</span>: <span class="string">'28'</span>, <span class="string">'abstract'</span>: <span class="string">'如果有了最终疗法，也不应该是每天都需要接受治疗'</span>&#125;, </span><br><span class="line"> &#123;<span class="string">'title'</span>: <span class="string">'日本步入下流社会，我们还在买买买'</span>, <span class="string">'url'</span>: <span class="string">'https://www.huxiu.com/article/270112.html'</span>, <span class="string">'name'</span>: <span class="string">'腾讯《大家》©'</span>, <span class="string">'write_time'</span>: <span class="string">'20小时前'</span>, <span class="string">'comment'</span>: <span class="string">'13'</span>, <span class="string">'favorites'</span>: <span class="string">'142'</span>, <span class="string">'abstract'</span>: <span class="string">'我买，故我在'</span>&#125;</span><br><span class="line">...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>可以看到，成功得到所需数据，然后就可以保存了，可以选择输出为 CSV、MySQL、MongoDB 等方式，这里我们选择保存到 MongoDB 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>,<span class="number">27017</span>)</span><br><span class="line">db = client.Huxiu</span><br><span class="line">mongo_collection = db.huxiu_news</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_result</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            self.save_to_mongo(result)  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self,result)</span>:</span></span><br><span class="line">    df = pd.DataFrame(result)</span><br><span class="line">    <span class="comment">#print(df)</span></span><br><span class="line">    content = json.loads(df.T.to_json()).values()</span><br><span class="line">    <span class="keyword">if</span> mongo_collection.insert_many(content):</span><br><span class="line">        print(<span class="string">'存储到 mongondb 成功'</span>)</span><br><span class="line">        <span class="comment"># 随机暂停</span></span><br><span class="line">        sleep = np.random.randint(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">        time.sleep(sleep)</span><br></pre></td></tr></table></figure><p>上面，定义了一个 on_result() 方法，该方法专门用来获取 return 的结果数据。这里用来接收上面 index_page() 返回的 data 数据，在该方法里再定义一个存储到 MongoDB 的方法就可以保存到 MongoDB 中。关于数据如何存储到 MongoDB 中，我们在之前的 <a href="https://www.makcyun.top/web_scraping_withpython7.html">一篇文章</a> 中有过介绍，如果忘记了可以回顾一下。</p><p>下面，我们来测试一下整个爬取和存储过程。点击左上角的 run 就可以顺利运行单个网页的抓取、解析和存储，结果如下：</p><p><img src="http://media.makcyun.top/18-11-6/80607479.jpg" alt=""></p><p>上面完成了单页面的爬取，接下来，我们需要爬取全部 2000 余页内容。</p><p>需要修改两个地方，首先在 on_start() 方法中将 for 循环页数 3 改为 2002。改好以后，如果我们直接点击 run ，会发现还是只能爬取第 2 页的结果。这是因为，pyspider 以 URL的 MD5 值作为 唯一 ID 编号，ID 编号相同的话就视为同一个任务，便不会再重复爬取。由于 GET 请求的 分页URL 通常是有差异的，所以 ID 编号会不同，也就自然能够爬取多页。但这里 POST 请求的分页 URL 是相同的，所以爬完第 2 页，后面的页数便不会再爬取。</p><p>那有没有解决办法呢？ 当然是有的，我们需要重新写下 ID 编号的生成方式，方法很简单，在 on_start() 方法前面添加下面 2 行代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_taskid</span><span class="params">(self,task)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> md5string(task[<span class="string">'url'</span>]+json.dumps(task[<span class="string">'fetch'</span>].get(<span class="string">'data'</span>,<span class="string">''</span>)))</span><br></pre></td></tr></table></figure><p>这样，我们再点击 run 就能够顺利爬取 2000 页的结果了，我这里一共抓取了 49,996 条结果，耗时 2 小时左右完成。</p><p><img src="http://media.makcyun.top/18-11-6/36910200.jpg" alt=""></p><p>以上，就完成了数据的获取。有了数据我们就可以着手分析，不过这之前还需简单地进行一下数据的清洗、处理。</p><h2 id="3-数据清洗处理"><a href="#3-数据清洗处理" class="headerlink" title="3. 数据清洗处理"></a>3. 数据清洗处理</h2><p>首先，我们需要从 MongoDB 中读取数据，并转换为 DataFrame。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'Huxiu'</span>]</span><br><span class="line">collection = db[<span class="string">'huxiu_news'</span>]</span><br><span class="line"><span class="comment"># 将数据库数据转为DataFrame</span></span><br><span class="line">data = pd.DataFrame(list(collection.find()))</span><br></pre></td></tr></table></figure><p>下面我们看一下数据的总体情况，可以看到数据的维度是 49996 行 × 8 列。发现多了一列无用的 _id 需删除，同时 name 列有一些特殊符号，比如© 需删除。另外，数据格式全部为 Object 字符串格式，需要将 comment 和 favorites 两列更改为数值格式、 write_time 列更改为日期格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">print(data.shape)  <span class="comment"># 查看行数和列数</span></span><br><span class="line">print(data.info()) <span class="comment"># 查看总体情况</span></span><br><span class="line">print(data.head()) <span class="comment"># 输出前5行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">(<span class="number">49996</span>, <span class="number">8</span>)</span><br><span class="line">Data columns (total <span class="number">8</span> columns):</span><br><span class="line">_id           <span class="number">49996</span> non-null object</span><br><span class="line">abstract      <span class="number">49996</span> non-null object</span><br><span class="line">comment       <span class="number">49996</span> non-null object</span><br><span class="line">favorites     <span class="number">49996</span> non-null object</span><br><span class="line">name          <span class="number">49996</span> non-null object</span><br><span class="line">title         <span class="number">49996</span> non-null object</span><br><span class="line">url           <span class="number">49996</span> non-null object</span><br><span class="line">write_time    <span class="number">49996</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">8</span>)</span><br><span class="line">    </span><br><span class="line">_idabstractcommentfavoritesnametitleurlwrite_time</span><br><span class="line"><span class="number">0</span><span class="number">5</span>bdc2“在你们看到…<span class="number">22</span><span class="number">50</span>普象工业设计小站©看了苹果屌https://<span class="number">10</span>小时前</span><br><span class="line"><span class="number">1</span><span class="number">5</span>bdc2中国”绿卡”号称“世界最难拿”<span class="number">9</span><span class="number">16</span>经济观察报©递交材料厚https://<span class="number">10</span>小时前</span><br><span class="line"><span class="number">2</span><span class="number">5</span>bdc2鲜衣怒马少年时<span class="number">2</span><span class="number">13</span>小马宋金庸小说陪https://<span class="number">11</span>小时前</span><br><span class="line"><span class="number">3</span><span class="number">5</span>bdc2预告还是预警？<span class="number">3</span><span class="number">10</span>Cuba Libre阿里即将发https://<span class="number">11</span>小时前</span><br><span class="line"><span class="number">4</span><span class="number">5</span>bdc2库克：咋回事？<span class="number">2</span><span class="number">3</span>Cuba Libre【虎嗅早报https://<span class="number">11</span>小时前</span><br></pre></td></tr></table></figure><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除无用_id列</span></span><br><span class="line">data.drop([<span class="string">'_id'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 替换掉特殊字符©</span></span><br><span class="line">data[<span class="string">'name'</span>].replace(<span class="string">'©'</span>,<span class="string">''</span>,inplace=<span class="keyword">True</span>,regex=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 字符更改为数值</span></span><br><span class="line">data = data.apply(pd.to_numeric,errors=<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># 更该日期格式</span></span><br><span class="line">data[<span class="string">'write_time'</span>] = data[<span class="string">'write_time'</span>].replace(<span class="string">'.*前'</span>,<span class="string">'2018-10-31'</span>,regex=<span class="keyword">True</span>) </span><br><span class="line"><span class="comment"># 为了方便，将write_time列，包含几小时前和几天前的行，都替换为10月31日最后1天。</span></span><br><span class="line">data[<span class="string">'write_time'</span>] = pd.to_datetime(data[<span class="string">'write_time'</span>])</span><br></pre></td></tr></table></figure><p>下面，我们看一下数据是否有重复，如果有，那么需要删除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断整行是否有重复值</span></span><br><span class="line">print(any(data.duplicated()))</span><br><span class="line"><span class="comment"># 显示True，表明有重复值，进一步提取出重复值数量</span></span><br><span class="line">data_duplicated = data.duplicated().value_counts()</span><br><span class="line">print(data_duplicated) <span class="comment"># 显示2 True ，表明有2个重复值</span></span><br><span class="line"><span class="comment"># 删除重复值</span></span><br><span class="line">data = data.drop_duplicates(keep=<span class="string">'first'</span>)</span><br><span class="line"><span class="comment"># 删除部分行后，index中断，需重新设置index</span></span><br><span class="line">data = data.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#结果：</span></span><br><span class="line"><span class="keyword">True</span> </span><br><span class="line"><span class="keyword">False</span>    <span class="number">49994</span></span><br><span class="line"><span class="keyword">True</span>         <span class="number">2</span></span><br></pre></td></tr></table></figure><p>然后，我们再增加两列数据，一列是文章标题长度列，一列是年份列，便于后面进行分析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'title_length'</span>] = data[<span class="string">'title'</span>].apply(len)</span><br><span class="line">data[<span class="string">'year'</span>] = data[<span class="string">'write_time'</span>].dt.year</span><br><span class="line">Data columns (total <span class="number">9</span> columns):</span><br><span class="line">abstract        <span class="number">49994</span> non-null object</span><br><span class="line">comment         <span class="number">49994</span> non-null int64</span><br><span class="line">favorites       <span class="number">49994</span> non-null int64</span><br><span class="line">name            <span class="number">49994</span> non-null object</span><br><span class="line">title           <span class="number">49994</span> non-null object</span><br><span class="line">url             <span class="number">49994</span> non-null object</span><br><span class="line">write_time      <span class="number">49994</span> non-null datetime64[ns]</span><br><span class="line">title_length    <span class="number">49994</span> non-null int64</span><br><span class="line">year            <span class="number">49994</span> non-null int64</span><br></pre></td></tr></table></figure><p>以上，就完成了基本的数据清洗处理过程，针对这 9 列数据可以开始进行分析了。</p><h2 id="4-描述性数据分析"><a href="#4-描述性数据分析" class="headerlink" title="4. 描述性数据分析"></a>4. 描述性数据分析</h2><p>通常，数据分析主要分为四类： 「描述型分析」、「诊断型分析」「预测型分析」「规范型分析」。「描述型分析」是用来概括、表述事物整体状况以及事物间关联、类属关系的统计方法，是这四类中最为常见的数据分析类型。通过统计处理可以简洁地用几个统计值来表示一组数据地集中性（如平均值、中位数和众数等）和离散型(反映数据的波动性大小，如方差、标准差等)。</p><p>这里，我们主要进行描述性分析，数据主要为数值型数据（包括离散型变量和连续型变量）和文本数据。</p><h3 id="4-1-总体情况"><a href="#4-1-总体情况" class="headerlink" title="4.1. 总体情况"></a>4.1. 总体情况</h3><p>先来看一下总体情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(data.describe())</span><br><span class="line">             comment     favorites  title_length </span><br><span class="line">count  <span class="number">49994.000000</span>  <span class="number">49994.000000</span>  <span class="number">49994.000000</span>  </span><br><span class="line">mean      <span class="number">10.860203</span>     <span class="number">34.081810</span>     <span class="number">22.775333</span>  </span><br><span class="line">std       <span class="number">24.085969</span>     <span class="number">48.276213</span>      <span class="number">9.540142</span>  </span><br><span class="line">min        <span class="number">0.000000</span>      <span class="number">0.000000</span>      <span class="number">1.000000</span>  </span><br><span class="line"><span class="number">25</span>%        <span class="number">3.000000</span>      <span class="number">9.000000</span>     <span class="number">17.000000</span>  </span><br><span class="line"><span class="number">50</span>%        <span class="number">6.000000</span>     <span class="number">19.000000</span>     <span class="number">22.000000</span>  </span><br><span class="line"><span class="number">75</span>%       <span class="number">12.000000</span>     <span class="number">40.000000</span>     <span class="number">28.000000</span>  </span><br><span class="line">max     <span class="number">2376.000000</span>   <span class="number">1113.000000</span>    <span class="number">224.000000</span></span><br></pre></td></tr></table></figure><p>这里，使用了 data.describe() 方法对数值型变量进行统计分析。从上面可以简要得出以下几个结论：</p><ul><li>读者的评论和收藏热情都不算太高，大部分文章（75 %）的评论数量为十几条，收藏数量不过几十个。这和一些微信大 V 公众号动辄百万级阅读、数万级评论和收藏量相比，虎嗅网的确相对小众一些。不过也正是因为小众，也才深得部分人的喜欢。</li><li>评论数最多的文章有 2376 条，收藏数最多的文章有 1113 个收藏量，说明还是有一些潜在的比较火或者质量比较好的文章。</li><li>最长的文章标题长达 224 个字，大部分文章标题长度在 20 来个字左右，所以标题最好不要太长或过短。</li></ul><p>对于非数值型变量（name、write_time），使用 describe() 方法会产生另外一种汇总统计。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(data[<span class="string">'name'</span>].describe())</span><br><span class="line">print(data[<span class="string">'write_time'</span>].describe())</span><br><span class="line"><span class="comment"># 结果：</span></span><br><span class="line">count     <span class="number">49994</span></span><br><span class="line">unique     <span class="number">3162</span></span><br><span class="line">top          虎嗅</span><br><span class="line">freq      <span class="number">10513</span></span><br><span class="line">Name: name, dtype: object</span><br><span class="line">count                   <span class="number">49994</span></span><br><span class="line">unique                   <span class="number">2397</span></span><br><span class="line">top       <span class="number">2014</span><span class="number">-07</span><span class="number">-10</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">freq                      <span class="number">274</span></span><br><span class="line">first     <span class="number">2012</span><span class="number">-04</span><span class="number">-03</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">last      <span class="number">2018</span><span class="number">-10</span><span class="number">-31</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br></pre></td></tr></table></figure><p>unique 表示唯一值数量，top 表示出现次数最多的变量，freq 表示该变量出现的次数，所以可以简单得出以下几个结论：</p><ul><li>在文章来源方面，3162 个作者贡献了这 5 万篇文章，其中自家官网「虎嗅」写的数量最多，超过了 1 万篇，这也很自然。</li><li>在文章发表时间方面，最早的一篇文章来自于 2012年 4 月 3 日。 6 年多时间，发文数最多的 1 天 是 2014 年 7 月 10 日，一共发了 274 篇文章。</li></ul><h3 id="4-2-不同时期文章发布的数量变化"><a href="#4-2-不同时期文章发布的数量变化" class="headerlink" title="4.2. 不同时期文章发布的数量变化"></a>4.2. 不同时期文章发布的数量变化</h3><p><img src="http://media.makcyun.top/18-11-6/47981964.jpg" alt=""></p><p>可以看到 ，以季度为时间尺度的6 年间，前几年发文数量比较稳定，大概在1750 篇左右，个别季度数量激增到 2000 篇以上。2016 年之后文章开始增加到 2000 篇以上，可能跟网站知名度提升有关。首尾两个季度日期不全，所以数量比较少。</p><p>具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis1</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># # 汇总统计</span></span><br><span class="line">    <span class="comment"># print(data.describe())</span></span><br><span class="line">    <span class="comment"># print(data['name'].describe())</span></span><br><span class="line">    <span class="comment"># print(data['write_time'].describe())</span></span><br><span class="line">    </span><br><span class="line">    data.set_index(data[<span class="string">'write_time'</span>],inplace=<span class="keyword">True</span>)</span><br><span class="line">    data = data.resample(<span class="string">'Q'</span>).count()[<span class="string">'name'</span>]  <span class="comment"># 以季度汇总</span></span><br><span class="line">    data = data.to_period(<span class="string">'Q'</span>)</span><br><span class="line">    <span class="comment"># 创建x,y轴标签</span></span><br><span class="line">    x = np.arange(<span class="number">0</span>,len(data),<span class="number">1</span>)</span><br><span class="line">    ax1.plot(x,data.values, <span class="comment">#x、y坐标</span></span><br><span class="line">        color = color_line , <span class="comment">#折线图颜色为红色</span></span><br><span class="line">        marker = <span class="string">'o'</span>,markersize = <span class="number">4</span> <span class="comment">#标记形状、大小设置</span></span><br><span class="line">        )</span><br><span class="line">    ax1.set_xticks(x) <span class="comment"># 设置x轴标签为自然数序列</span></span><br><span class="line">    ax1.set_xticklabels(data.index) <span class="comment"># 更改x轴标签值为年份</span></span><br><span class="line">    plt.xticks(rotation=<span class="number">90</span>) <span class="comment"># 旋转90度，不至太拥挤</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data.values):</span><br><span class="line">        plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors,fontsize=fontsize_text )</span><br><span class="line">        <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line">    <span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">    plt.title(<span class="string">'虎嗅网文章数量发布变化(2012-2018)'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.xlabel(<span class="string">'时期'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章(篇)'</span>)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘</span></span><br><span class="line">    plt.savefig(<span class="string">'虎嗅网文章数量发布变化.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="4-3-文章收藏量-TOP-10"><a href="#4-3-文章收藏量-TOP-10" class="headerlink" title="4.3. 文章收藏量 TOP 10"></a>4.3. 文章收藏量 TOP 10</h3><p>接下来，到了我们比较关心的问题：几万篇文章里，到底哪些文章写得比较好或者比较火？</p><table><thead><tr><th>序号</th><th>title</th><th>favorites</th><th>comment</th></tr></thead><tbody><tr><td>1</td><td>读完这10本书，你就能站在智商鄙视链的顶端了</td><td>1113</td><td>13</td></tr><tr><td>2</td><td>京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利</td><td>867</td><td>10</td></tr><tr><td>3</td><td>离职创业？先读完这22本书再说</td><td>860</td><td>9</td></tr><tr><td>4</td><td>货币如水，覆水难收</td><td>784</td><td>39</td></tr><tr><td>5</td><td>自杀经济学</td><td>778</td><td>119</td></tr><tr><td>6</td><td>2016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里</td><td>774</td><td>39</td></tr><tr><td>7</td><td>真正强大的商业分析能力是怎样炼成的？</td><td>746</td><td>18</td></tr><tr><td>8</td><td>腾讯没有梦想</td><td>705</td><td>32</td></tr><tr><td>9</td><td>段永平连答53问，核心是“不为清单”</td><td>703</td><td>27</td></tr><tr><td>10</td><td>王健林的滑铁卢</td><td>701</td><td>92</td></tr></tbody></table><p>此处选取了「favorites」(收藏数量)作为衡量标准。毕竟，一般好的文章，我们都会有收藏的习惯。</p><p>第一名「<a href="https://www.huxiu.com/article/123650.html" target="_blank" rel="noopener">读完这10本书，你就能站在智商鄙视链的顶端了</a> 」以 1113 次收藏位居第一，并且遥遥领先于后者，看来大家都怀有「想早日攀上人生巅峰，一览众人小」的想法啊。打开这篇文章的链接，文中提到了这几本书：《思考，快与慢》、《思考的技术》、《麦肯锡入职第一课：让职场新人一生受用的逻辑思考力》等。一本都没看过，看来这辈子是很难登上人生巅峰了。</p><p>发现两个有意思的地方。</p><p>第一，<strong>文章标题都比较短小精炼。</strong></p><p>第二，文章收藏量虽然比较高，但评论数都不多，猜测这是因为 <strong>大家都喜欢做伸手党</strong>？</p><h3 id="4-4-历年文章收藏量-TOP3"><a href="#4-4-历年文章收藏量-TOP3" class="headerlink" title="4.4. 历年文章收藏量 TOP3"></a>4.4. 历年文章收藏量 TOP3</h3><p>在了解文章的总体排名之后，我们来看看历年的文章排名是怎样的。这里，每年选取了收藏量最多的 3 篇文章。</p><table><thead><tr><th>year</th><th>title</th><th>favorites</th></tr></thead><tbody><tr><td>2012</td><td>产品的思路——来自腾讯张小龙的分享（全版）</td><td>187</td></tr><tr><td></td><td>Fab CEO：创办四家公司教给我的90件事</td><td>163</td></tr><tr><td></td><td>张小龙：微信背后的产品观</td><td>162</td></tr><tr><td>2013</td><td>创业者手记：我所犯的那些入门错误</td><td>473</td></tr><tr><td></td><td>马化腾三小时讲话实录：千亿美金这个线，其实很恐怖</td><td>391</td></tr><tr><td></td><td>雕爷亲身谈：白手起家的我如何在30岁之前赚到1000万。读《MBA教不了的创富课》</td><td>354</td></tr><tr><td>2014</td><td>85后，突变的一代</td><td>528</td></tr><tr><td></td><td>雕爷自述：什么是我做餐饮时琢磨、而大部分“外人”无法涉猎的思考？</td><td>521</td></tr><tr><td></td><td>据说这40张PPT是蚂蚁金服的内部培训资料……</td><td>485</td></tr><tr><td>2015</td><td>读完这10本书，你就能站在智商鄙视链的顶端了</td><td>1113</td></tr><tr><td></td><td>京东打脸央视：你所谓的翻新iPhone均为正品，我们保留向警方报案的权利</td><td>867</td></tr><tr><td></td><td>离职创业？先读完这22本书再说</td><td>860</td></tr><tr><td>2016</td><td>蝗虫般的刷客大军：手握千万手机号，分秒间薅干一家平台</td><td>554</td></tr><tr><td></td><td>准CEO必读的这20本书，你读过几本？</td><td>548</td></tr><tr><td></td><td>运营简史：一文读懂互联网运营的20年发展与演变</td><td>503</td></tr><tr><td>2017</td><td>2016年已经起飞的5只黑天鹅，都在罗振宇这份跨年演讲全文里</td><td>774</td></tr><tr><td></td><td>真正强大的商业分析能力是怎样炼成的？</td><td>746</td></tr><tr><td></td><td>王健林的滑铁卢</td><td>701</td></tr><tr><td>2018</td><td>货币如水，覆水难收</td><td>784</td></tr><tr><td></td><td>自杀经济学</td><td>778</td></tr><tr><td></td><td>腾讯没有梦想</td><td>705</td></tr></tbody></table><p><img src="http://media.makcyun.top/18-11-8/86110562.jpg" alt=""></p><p>可以看到，文章收藏量基本是逐年递增的，但 2015 年的 3 篇文章的收藏量却是最高的，包揽了总排名的前 3 名，不知道这一年的文章有什么特别之处。</p><p>以上只罗列了一小部分文章的标题，可以看到标题起地都蛮有水准的。关于标题的重要性，有这样通俗的说法：「<code>一篇好文章，标题占一半</code>」，一个好的标题可以大大增强文章的传播力和吸引力。文章标题虽只有短短数十字，但要想起好，里面也是很有很多技巧的。</p><p>好在，这里提供了 5 万个标题可以参考。<code>如需，可以在公众号后台回复「虎嗅」得到这份 CSV 文件。</code></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis2</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># # 总收藏排名</span></span><br><span class="line">    <span class="comment"># top = data.sort_values(['favorites'],ascending = False)</span></span><br><span class="line">    <span class="comment"># # 收藏前10</span></span><br><span class="line">    <span class="comment"># top.index = (range(1,len(top.index)+1)) # 重置index，并从1开始编号</span></span><br><span class="line">    <span class="comment"># print(top[:10][['title','favorites','comment']])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按年份排名</span></span><br><span class="line">    <span class="comment"># # 增加一列年份列</span></span><br><span class="line">    <span class="comment"># data['year'] = data['write_time'].dt.year</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(data)</span>:</span></span><br><span class="line">        top = data.sort_values(<span class="string">'favorites'</span>,ascending=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> top[:<span class="number">3</span>]</span><br><span class="line">    data = data.groupby(by=[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line">    print(data[[<span class="string">'title'</span>,<span class="string">'favorites'</span>]])</span><br><span class="line">    <span class="comment"># 增加每年top123列，列依次值为1、2、3</span></span><br><span class="line">    data[<span class="string">'add'</span>] = <span class="number">1</span> <span class="comment"># 辅助</span></span><br><span class="line">    data[<span class="string">'top'</span>] = data.groupby(by=<span class="string">'year'</span>)[<span class="string">'add'</span>].cumsum()</span><br><span class="line">    data_reshape = data.pivot_table(index=<span class="string">'year'</span>,columns=<span class="string">'top'</span>,values=<span class="string">'favorites'</span>).reset_index()</span><br><span class="line">    <span class="comment"># print(data_reshape)  # ok</span></span><br><span class="line">    data_reshape.plot(</span><br><span class="line">        <span class="comment"># x='year',</span></span><br><span class="line">        y=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">        kind=<span class="string">'bar'</span>,</span><br><span class="line">        width=<span class="number">0.3</span>,</span><br><span class="line">        color=[<span class="string">'#1362A3'</span>,<span class="string">'#3297EA'</span>,<span class="string">'#8EC6F5'</span>]  <span class="comment"># 设置不同的颜色</span></span><br><span class="line">        <span class="comment"># title='虎嗅网历年收藏数最多的3篇文章'</span></span><br><span class="line">        )</span><br><span class="line">    plt.xlabel(<span class="string">'Year'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章收藏数量'</span>)</span><br><span class="line">    plt.title(<span class="string">'历年 TOP3 文章收藏量比较'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line">    <span class="comment"># plt.savefig('历年 Top3 文章收藏量比较.png',dpi=200)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="4-4-1-最高产作者-TOP20"><a href="#4-4-1-最高产作者-TOP20" class="headerlink" title="4.4.1. 最高产作者 TOP20"></a>4.4.1. 最高产作者 TOP20</h4><p>上面，我们从收藏量指标进行了分析,下面，我们关注一下发布文章的作者（个人/媒体）。前面提到发文最多的是虎嗅官方，有一万多篇文章，这里我们筛除官媒，看看还有哪些比较高产的作者。</p><p><img src="http://media.makcyun.top/18-11-8/9931236.jpg" alt=""></p><p>可以看到，前 20 名作者的发文量差距都不太大。发文比较多的有「娱乐资本论」、「Eastland」、「发条橙子」这类媒体号；也有虎嗅官网团队的作者：发条橙子、周超臣、张博文等；还有部分独立作者：假装FBI、孙永杰等。可以尝试关注一下这些高产作者。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis3</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = data.groupby(data[<span class="string">'name'</span>])[<span class="string">'title'</span>].count()</span><br><span class="line">    data = data.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># pandas 直接绘制,.invert_yaxis()颠倒顺序</span></span><br><span class="line">    data[<span class="number">1</span>:<span class="number">21</span>].plot(kind=<span class="string">'barh'</span>,color=color_line).invert_yaxis()</span><br><span class="line">    <span class="keyword">for</span> y,x <span class="keyword">in</span> enumerate(list(data[<span class="number">1</span>:<span class="number">21</span>].values)):</span><br><span class="line">        plt.text(x+<span class="number">12</span>,y+<span class="number">0.2</span>,<span class="string">'%s'</span> %round(x,<span class="number">1</span>),ha=<span class="string">'center'</span>,color=colors)</span><br><span class="line">    plt.xlabel(<span class="string">'文章数量'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'作者'</span>)</span><br><span class="line">    plt.title(<span class="string">'发文数量最多的 TOP20 作者'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(<span class="string">'发文数量最多的TOP20作者.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="4-4-2-平均文章收藏量最多作者-TOP-10"><a href="#4-4-2-平均文章收藏量最多作者-TOP-10" class="headerlink" title="4.4.2. 平均文章收藏量最多作者 TOP 10"></a>4.4.2. 平均文章收藏量最多作者 TOP 10</h4><p>我们关注一个作者除了是因为文章高产以外，可能更看重的是其文章水准。这里我们选择「文章平均收藏量」（总收藏量/文章数）这个指标，来看看文章水准比较高的作者是哪些人。</p><p>这里，为了避免出现「某作者只写了一篇高收藏率的文章」这种不能代表其真实水准的情况，我们将筛选范围定在至少发布过 5 篇文章的作者们。</p><table><thead><tr><th>name</th><th>total_favorites</th><th>ariticls_num</th><th>avg_favorites</th></tr></thead><tbody><tr><td>重读</td><td>1947</td><td>6</td><td>324</td></tr><tr><td>楼台</td><td>2302</td><td>8</td><td>287</td></tr><tr><td>彭萦</td><td>2487</td><td>9</td><td>276</td></tr><tr><td>曹山石</td><td>1187</td><td>5</td><td>237</td></tr><tr><td>饭统戴老板</td><td>7870</td><td>36</td><td>218</td></tr><tr><td>笔记侠</td><td>1586</td><td>8</td><td>198</td></tr><tr><td>辩手李慕阳</td><td>11989</td><td>62</td><td>193</td></tr><tr><td>李录</td><td>2370</td><td>13</td><td>182</td></tr><tr><td>高晓松</td><td>889</td><td>5</td><td>177</td></tr><tr><td>宁南山</td><td>2827</td><td>16</td><td>176</td></tr></tbody></table><p>可以看到，前 10 名作者包括：遥遥领先的 <strong>重读</strong>、两位高产又有质量的 <strong>辩手李慕阳</strong> 和 <strong>饭统戴老板</strong> ，还有大众比较熟悉的 <strong>高晓松</strong>、<strong>宁南山 </strong>等。</p><p>如果你将这份名单和上面那份高产作者名单进行对比，会发现他们没有出现在这个名单中。相比于数量，质量可能更重要吧。</p><p>下面，我们就来看看排名第一的 <strong>重读</strong> 都写了哪些高收藏量文章。</p><table><thead><tr><th>order</th><th>title</th><th>favorites</th><th>write_time</th></tr></thead><tbody><tr><td>1</td><td>我采访出200多万字素材，还原了阿里系崛起前传</td><td>231</td><td>2018/10/31</td></tr><tr><td>2</td><td>阿里史上最强人事地震回顾：中供铁军何以被生生解体</td><td>494</td><td>2018/4/9</td></tr><tr><td>3</td><td>马云“斩”卫哲：复原阿里史上最震撼的人事地震</td><td>578</td><td>2018/3/15</td></tr><tr><td>4</td><td>重读一场马云发起、针对卫哲的批斗会</td><td>269</td><td>2017/8/31</td></tr><tr><td>5</td><td>阿里“中供系”前世今生：马云麾下最神秘的子弟兵</td><td>203</td><td>2017/5/10</td></tr><tr><td>6</td><td>揭秘马云麾下最神秘的子弟兵：阿里“中供系”的前世今生</td><td>172</td><td>2017/4/26</td></tr></tbody></table><p>居然写的都是清一色关于马老板家的文章。</p><p>了解了前十名作者之后，我们顺便也看看那些处于最后十名的都是哪些作者。</p><table><thead><tr><th>name</th><th>total_favorites</th><th>ariticls_num</th><th>avg_favorites</th></tr></thead><tbody><tr><td>于斌</td><td>25</td><td>11</td><td>2</td></tr><tr><td>朝克图</td><td>33</td><td>23</td><td>1</td></tr><tr><td>东风日产</td><td>24</td><td>13</td><td>1</td></tr><tr><td>董晓常</td><td>14</td><td>8</td><td>1</td></tr><tr><td>蔡钰</td><td>31</td><td>16</td><td>1</td></tr><tr><td>马继华</td><td>12</td><td>11</td><td>1</td></tr><tr><td>angeljie</td><td>7</td><td>5</td><td>1</td></tr><tr><td>薛开元</td><td>6</td><td>6</td><td>1</td></tr><tr><td>pookylee</td><td>15</td><td>24</td><td>0</td></tr><tr><td>Yang Yemeng</td><td>0</td><td>7</td><td>0</td></tr></tbody></table><p>一对比，就能看到他们的文章收藏量就比较寒碜了。尤其好奇最后一位作者 <strong>Yang Yemeng</strong> ，他写了 7 篇文章，竟然一个收藏都没有。</p><p>来看看他究竟写了些什么文章。</p><p><img src="http://media.makcyun.top/18-11-7/59717401.jpg" alt=""></p><p>原来写的全都是英文文章，看来大家并不太钟意阅读英文类的文章啊。</p><p>具体实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis4</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = pd.pivot_table(data,values=[<span class="string">'favorites'</span>],index=<span class="string">'name'</span>,aggfunc=[np.sum,np.size])</span><br><span class="line">    data[<span class="string">'avg'</span>] = data[(<span class="string">'sum'</span>,<span class="string">'favorites'</span>)]/data[(<span class="string">'size'</span>,<span class="string">'favorites'</span>)]</span><br><span class="line">    <span class="comment"># 平均收藏数取整</span></span><br><span class="line">    <span class="comment"># data['avg'] = data['avg'].round(decimals=1)</span></span><br><span class="line">    data[<span class="string">'avg'</span>] = data[<span class="string">'avg'</span>].astype(<span class="string">'int'</span>)</span><br><span class="line">    <span class="comment"># flatten 平铺列</span></span><br><span class="line">    data.columns = data.columns.get_level_values(<span class="number">0</span>)</span><br><span class="line">    data.columns = [<span class="string">'total_favorites'</span>,<span class="string">'ariticls_num'</span>,<span class="string">'avg_favorites'</span>]</span><br><span class="line">    <span class="comment"># 筛选出文章数至少5篇的</span></span><br><span class="line">    data=data.query(<span class="string">'ariticls_num &gt; 4'</span>)</span><br><span class="line">    data = data.sort_values(by=[<span class="string">'avg_favorites'</span>],ascending=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># # 查看平均收藏率第一名详情</span></span><br><span class="line">    <span class="comment"># data = data.query('name == "重读"')</span></span><br><span class="line">    <span class="comment"># # 查看平均收藏率倒数第一名详情</span></span><br><span class="line">    <span class="comment"># data = data.query('name == "Yang Yemeng"')</span></span><br><span class="line">    <span class="comment"># print(data[['title','favorites','write_time']])</span></span><br><span class="line">    print(data[:<span class="number">10</span>]) <span class="comment"># 前10名</span></span><br><span class="line">    print(data[<span class="number">-10</span>:])<span class="comment"># 后10名</span></span><br></pre></td></tr></table></figure><h3 id="4-5-文章评论数最多-TOP10"><a href="#4-5-文章评论数最多-TOP10" class="headerlink" title="4.5. 文章评论数最多 TOP10"></a>4.5. 文章评论数最多 TOP10</h3><p>说完了收藏量。下面，我们再来看看评论数量最多的文章是哪些。</p><table><thead><tr><th>order</th><th>title</th><th>comment</th><th>favorites</th></tr></thead><tbody><tr><td>1</td><td>喜瓜2.0—明星社交应用的中国式引进与创新</td><td>2376</td><td>3</td></tr><tr><td>2</td><td>百度，请给“儿子们”好好起个名字</td><td>1297</td><td>9</td></tr><tr><td>3</td><td>三星S5为什么对凤凰新闻客户端下注？</td><td>1157</td><td>1</td></tr><tr><td>4</td><td>三星Tab S：马是什么样的马？鞍又是什么样的鞍？</td><td>951</td><td>0</td></tr><tr><td>5</td><td>三星，正在重塑你的营销观</td><td>914</td><td>1</td></tr><tr><td>6</td><td>马化腾，你就把微信卖给运营商得了！</td><td>743</td><td>20</td></tr><tr><td>7</td><td>【文字直播】罗永浩 VS 王自如 网络公开辩论</td><td>711</td><td>33</td></tr><tr><td>8</td><td>看三星Hub如何推动数字内容消费变革</td><td>684</td><td>1</td></tr><tr><td>9</td><td>三星要重新定义软件与内容商店新模式，SO?</td><td>670</td><td>0</td></tr><tr><td>10</td><td>三星Hub——数字内容交互新模式</td><td>611</td><td>0</td></tr></tbody></table><p>基本上都是和 <strong>三星</strong> 有关的文章，这些文章大多来自 2014 年，那几年 <strong>三星</strong> 好像是挺火的，不过这两年国内基本上都见不到三星的影子了，世界变化真快。</p><p>发现了两个有意思的现象。</p><p>第一，上面关于 <strong>三星</strong> 和前面 <strong>阿里</strong> 的这些批量文章，它们「霸占」了评论和收藏榜，结合知乎上曾经的一篇关于介绍虎嗅这个网站的文章：<a href="https://www.zhihu.com/question/20799239/answer/20698562" target="_blank" rel="noopener">虎嗅网其实是这样的</a> ，貌似能发现些微妙的事情。</p><p>第二，这些文章评论数和收藏数两个指标几乎呈极端趋势，评论量多的文章收藏量却很少，评论量少的文章收藏量却很多。</p><p>我们进一步观察下这两个参数的关系。</p><p><img src="http://media.makcyun.top/18-11-6/85687626.jpg" alt=""></p><p>可以看到，大多数点都位于左下角，意味着这些文章收藏量和评论数都比较低。但也存在少部分位于上方和右侧的异常值，表明这些文章呈现 「多评论、少收藏」或者「少评论、多收藏」的特点。</p><h3 id="4-6-文章标题长度"><a href="#4-6-文章标题长度" class="headerlink" title="4.6. 文章标题长度"></a>4.6. 文章标题长度</h3><p>下面，我们再来看看文章标题的长度和收藏量之间有没有什么关系。</p><p><img src="http://media.makcyun.top/18-11-6/93399033.jpg" alt=""></p><p>大致可以看出两点现象：</p><p>第一，<strong>收藏量高的文章，他们的标题都比较短</strong>（右侧的部分散点）。</p><p>第二，<strong>标题很长的文章，它们的收藏量都非常低</strong>（左边形成了一条垂直线）。</p><p>看来，文章起标题时最好不要起太长的。</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis5</span><span class="params">(data)</span>:</span></span><br><span class="line">    plt.scatter(</span><br><span class="line">        x=data[<span class="string">'favorites'</span>],</span><br><span class="line">        y =data[<span class="string">'comment'</span>],</span><br><span class="line">        s=data[<span class="string">'title_length'</span>]/<span class="number">2</span>,</span><br><span class="line">        )</span><br><span class="line">    plt.xlabel(<span class="string">'文章收藏量'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'文章评论数'</span>)</span><br><span class="line">    plt.title(<span class="string">'文章标题长度与收藏量和评论数之间的关系'</span>,color = colors,fontsize=fontsize_title)</span><br><span class="line">    plt.tight_layout() </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="文章标题形式"><a href="#文章标题形式" class="headerlink" title="文章标题形式"></a>文章标题形式</h3><p>下面，我们看看作者在起文章标题的时候，在标点符号方面有没有什么偏好。</p><p>可以看到，五万篇文章中，大多数文章的标题是陈述性标题。三分之一（34.8%） 的文章标题使用了问号「？」，而仅有 5% 的文章用了叹号「！」。通常，问号会让人们产生好奇，从而想去点开文章；而叹号则会带来一种紧张或者压迫感，使人不太想去点开。所以，<strong>可以尝试多用问号而少用叹号。</strong></p><p><img src="http://media.makcyun.top/18-11-8/44309942.jpg" alt=""></p><h3 id="4-7-文本分析"><a href="#4-7-文本分析" class="headerlink" title="4.7. 文本分析"></a>4.7. 文本分析</h3><p>最后，我们从这 5 万篇文章中的标题和摘要中，来看看虎嗅网的文章主要关注的都是哪些主题领域。</p><p>这里首先运用了 jieba 分词包对标题进行了分词，然后用 WordCloud 做成了词云图，因虎嗅网含有「虎」字，故选取了一张老虎头像。（关于 jieba 和 WordCloud 两个包，之后再详细介绍）</p><p><img src="http://media.makcyun.top/18-11-8/24683879.jpg" alt=""></p><p>可以看到文章的主题内容侧重于：互联网、知名公司、电商、投资这些领域。这和网站本身对外宣传的核心内容，即「关注互联网与移动互联网一系列明星公司的起落轨迹、产业潮汐的动力与趋势，以及互联网与移动互联网如何改造传统产业」大致相符合。</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis6</span><span class="params">(data)</span>:</span></span><br><span class="line">    text=<span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="string">'title'</span>].values:</span><br><span class="line">        symbol_to_replace = <span class="string">'[!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@，。?★、…【】《》？“”‘’！[\\]^_`&#123;|&#125;~]+'</span></span><br><span class="line">        i = re.sub(symbol_to_replace,<span class="string">''</span>,i)</span><br><span class="line">        text+=<span class="string">' '</span>.join(jieba.cut(i,cut_all=<span class="keyword">False</span>))</span><br><span class="line">    d = path.dirname(__file__) <span class="keyword">if</span> <span class="string">"__file__"</span> <span class="keyword">in</span> locals() <span class="keyword">else</span> os.getcwd()</span><br><span class="line"></span><br><span class="line">    background_Image = np.array(Image.open(path.join(d, <span class="string">"tiger.png"</span>)))</span><br><span class="line">    font_path = <span class="string">'C:\Windows\Fonts\SourceHanSansCN-Regular.otf'</span>  <span class="comment"># 思源黑字体</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加stopswords</span></span><br><span class="line">    stopwords = set()</span><br><span class="line">    <span class="comment"># 先运行对text进行词频统计再排序，再选择要增加的停用词</span></span><br><span class="line">    stopwords.update([<span class="string">'如何'</span>,<span class="string">'怎么'</span>,<span class="string">'一个'</span>,<span class="string">'什么'</span>,<span class="string">'为什么'</span>,<span class="string">'还是'</span>,<span class="string">'我们'</span>,<span class="string">'为何'</span>,<span class="string">'可能'</span>,<span class="string">'不是'</span>,<span class="string">'没有'</span>,<span class="string">'哪些'</span>,<span class="string">'成为'</span>,<span class="string">'可以'</span>,<span class="string">'背后'</span>,<span class="string">'到底'</span>,<span class="string">'就是'</span>,<span class="string">'这么'</span>,<span class="string">'不要'</span>,<span class="string">'怎样'</span>,<span class="string">'为了'</span>,<span class="string">'能否'</span>,<span class="string">'你们'</span>,<span class="string">'还有'</span>,<span class="string">'这样'</span>,<span class="string">'这个'</span>,<span class="string">'真的'</span>,<span class="string">'那些'</span>])</span><br><span class="line">    wc = WordCloud(</span><br><span class="line">        background_color = <span class="string">'black'</span>,</span><br><span class="line">        font_path = font_path,</span><br><span class="line">        mask = background_Image,</span><br><span class="line">        stopwords = stopwords,</span><br><span class="line">        max_words = <span class="number">2000</span>,</span><br><span class="line">        margin =<span class="number">2</span>,</span><br><span class="line">        max_font_size = <span class="number">100</span>,</span><br><span class="line">        random_state = <span class="number">42</span>,</span><br><span class="line">        scale = <span class="number">2</span>,</span><br><span class="line">    )</span><br><span class="line">    wc.generate_from_text(text)</span><br><span class="line">    process_word = WordCloud.process_text(wc, text)</span><br><span class="line">    <span class="comment"># 下面是字典排序</span></span><br><span class="line">    sort = sorted(process_word.items(),key=<span class="keyword">lambda</span> e:e[<span class="number">1</span>],reverse=<span class="keyword">True</span>) <span class="comment"># sort为list</span></span><br><span class="line">    print(sort[:<span class="number">50</span>])  <span class="comment"># 输出前词频最高的前50个，然后筛选出不需要的stopwords，添加到前面的stopwords.update()方法中</span></span><br><span class="line">    img_colors = ImageColorGenerator(background_Image)</span><br><span class="line">    wc.recolor(color_func=img_colors)  <span class="comment"># 颜色跟随图片颜色</span></span><br><span class="line">    plt.imshow(wc,interpolation=<span class="string">'bilinear'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘</span></span><br><span class="line">    plt.savefig(<span class="string">'huxiu20.png'</span>,dpi=<span class="number">200</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>上面的关键词是这几年总体的概况，而科技互联网行业每年的发展都是不同的，所以，我们再来看看历年的一些关键词，透过这些关键词看看这几年互联网行业、科技热点、知名公司都有些什么不同变化。</p><p><img src="http://media.makcyun.top/18-11-7/78186420.jpg" alt=""></p><p>可以看到每年的关键词都有一些相同之处，但也不同的地方：</p><ul><li>中国互联网、公司、苹果、腾讯、阿里等这些热门关键词一直都是热门，这几家公司真是稳地一批啊。</li><li>每年会有新热点涌现：比如 2013 年的微信（刚开始火）、2016 年的直播（各大直播平台如雨后春笋般出现）、2017年的 iPhone（上市十周年）、2018年的小米（上市）。</li><li>不断有新的热门技术出现：2013 - 2015 年的 O2O、2016 年的 VR、2017 年的 AI 、2018 年的「区块链」。这些科技前沿技术也是这几年大家口耳相传的热门词汇。</li></ul><p>通过这一幅图，就看出了这几年科技互联网行业、明星公司、热点信息的风云变化。</p><h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><ul><li>本文简要分析了虎嗅网 5 万篇文章信息，大致了解了近些年科技互联网的千变万化。</li><li>发掘了那些优秀的文章和作者，能够节省宝贵的时间成本。</li><li>一篇文章要想传播广泛，文章本身的质量和标题各占一半，文中的5 万个标题相信能够带来一些灵感。</li><li>本文尚未做深入的文本挖掘，而文本挖掘可能比数据挖掘涵盖的信息量更大，更有价值。进行这些分析需要机器学习和深度学习的知识，待后期学习后再来补充。</li></ul><p>本文完。</p><p>文中的完整代码和素材可以在公众号后台回复「<strong>虎嗅</strong>」 或者在下面的链接中获取：</p><p><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><hr><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython4.html">做 PPT 没灵感？澎湃网 1500 期信息图送给你</a></p><p><a href="https://www.makcyun.top/web_scraping_withpython7.html">听说你想创业找投资？国内创业公司的信息都在这里了</a></p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎扫一扫识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;数据抓取、清洗、分析一条龙，感受科技互连网世界的千变万化。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫,Python分析,Python可视化" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB-Python%E5%88%86%E6%9E%90-Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="Python分析" scheme="https://www.makcyun.top/tags/Python%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>安卓最好用的电子书阅读器</title>
    <link href="https://www.makcyun.top/2018/11/02/weekly_sharing3.html"/>
    <id>https://www.makcyun.top/2018/11/02/weekly_sharing3.html</id>
    <published>2018-11-02T07:16:24.000Z</published>
    <updated>2018-12-26T11:48:29.795Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>推荐一款最好用的手机阅读 App。</p><a id="more"></a><p>这是每周分享的第 3 期。</p><p>先简单介绍下这个栏目，顾名思义，就是会在每个周末分享一篇文章。内容主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p>好，下面开始进入正题。</p><p>我们大多数人每天当中花时间花的最多的事情，应该就是看手机了，聊天 玩游戏、看视频、或者阅读学习等。</p><p><img src="http://media.makcyun.top/18-11-2/75427908.jpg" alt=""></p><p>我呢，手机阅读的时间相对多一些。自从学习 Python 以来，手机上下载了很多的 PDF 电子书，然后在 WPS 中阅读。起初一直觉得很方便，各个文件之间的切换也非常顺畅。不过，用久了之后发现有几个不方便的地方：</p><ul><li><p>目录跳转麻烦</p><p>有时看书不是从头看到尾的，而是跳跃地阅读部分章节。需要先定位到所在目录页数，再滑动到相应位置，还要看其他章节时，则需重复同样的操作。这样一来，需要花费较多的时间在目录和内容的切换上，真正阅读的时间减少了。</p></li><li><p>不支持其他电子书格式</p><p>除了 PDF 格式以外，WPS 几乎不支持 txt、epub、mobi 等格式。</p></li><li><p>阅读功能单一</p><p>缺少诸如：蓝光过滤、字体背景设置、滚动翻页等功能。</p></li><li><p>缺少自定义性功能</p><p>WPS 走的是简洁路线，提供的自定义功能很少，用来查看文件还可以，但是阅读就显得力不从心了。</p></li></ul><p>发现了这些痛点之后，我便开始寻找替代品，前后试用了多款 App。最近，终于找到一款能够解决以上所有问题的阅读器。这款堪称 「安卓最好用的阅读器」，名叫：「<strong>静读天下</strong>」。</p><p>很多非常棒的 App 都具备一个特质是：提供 <strong>高度可自定义</strong> 的使用功能以满足不同用户的喜好与需求。而 <strong>静读天下</strong> 就是其中的佼佼者之一。用了它之后，阅读体验得到了大大提升，<br>下面一一道来它的各种好。</p><h2 id="1-看书"><a href="#1-看书" class="headerlink" title="1. 看书"></a>1. 看书</h2><h3 id="1-1-目录快速跳转定位"><a href="#1-1-目录快速跳转定位" class="headerlink" title="1.1. 目录快速跳转定位"></a>1.1. 目录快速跳转定位</h3><p>它最实用的一个功能就是「<strong>能够自动识别电子书的目录</strong>」。单击屏幕即可呼出章节目录，然后点击想查看的章节内容即可快速跳转到相应位置。看完以后如果想看其他章节内容，只需要重复操作一次即可。这样，就缩短了查找内容的时间，从而可以更加专注于阅读。</p><p><img src="http://media2.makcyun.top/%E9%9D%99%E8%AF%BB%E5%A4%A9%E4%B8%8B.gif" alt=""></p><h3 id="1-2-便捷的护眼功能"><a href="#1-2-便捷的护眼功能" class="headerlink" title="1.2. 便捷的护眼功能"></a>1.2. 便捷的护眼功能</h3><p>每天看书的时间是在变化的，白天和晚上的光线会有所不同。因而护眼阅读就显得很有必要。<br>虽然手机上有调节亮度的功能，但功能不全且需要额外操作。所幸，该 App 里只需通过触屏手势就能实现 「快速调节亮度」 和 「蓝光过滤 」功能。另外，还能够自动切换白天/晚上模式。</p><p><img src="http://media.makcyun.top/18-11-2/26133427.jpg" alt=""></p><h3 id="1-3-字体、配色、背景设置"><a href="#1-3-字体、配色、背景设置" class="headerlink" title="1.3. 字体、配色、背景设置"></a>1.3. 字体、配色、背景设置</h3><p>它支持的电子书格式除了 PDF 以外，还支持 txt、epub、mobi 等多种格式。这些格式支持设置字体、配色、背景等功能。<br>比如，看惯了无衬线字体，想体验下有棱角的衬线字体，那么可以下载自己喜欢的字体然后使用。字体的大小、颜色、行距、留白都可以凭自己喜好设置，甚至还可以竖版阅读。</p><p><img src="http://media.makcyun.top/18-11-2/52757050.jpg" alt=""></p><p>除此之外，背景也是可以随意更换的，内置十几种背景，不够还可以下载自己自己喜欢的，可以说提供了无限选择。</p><p><img src="http://media.makcyun.top/18-11-2/28095706.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-11-2/71065070.jpg" alt=""></p><h3 id="1-4-添加批注"><a href="#1-4-添加批注" class="headerlink" title="1.4. 添加批注"></a>1.4. 添加批注</h3><p>有时，会边阅读边做些批注，比如添加：高亮、下划线、删除线、标注等。它提供了丰富的批注功能，批注好后可以在书签页面里看到，方便下次快速查找到。</p><h3 id="1-5-书籍分类"><a href="#1-5-书籍分类" class="headerlink" title="1.5. 书籍分类"></a>1.5. 书籍分类</h3><p>如果手机里有很多且类别不同的电子书，有了这个书籍分类功能会更方便。比如，我将电子书分为 Pyhon、数学、机器学习等类别，想看哪一类就筛选即可。</p><p><img src="http://media.makcyun.top/18-11-2/11726230.jpg" alt=""></p><h3 id="1-6-添加书签"><a href="#1-6-添加书签" class="headerlink" title="1.6. 添加书签"></a>1.6. 添加书签</h3><p>当需要暂停阅读时，可以添加书签，下次再看时直接点击即可快速回到上次的阅读位置。</p><h3 id="1-7-数百种自定义组合功能"><a href="#1-7-数百种自定义组合功能" class="headerlink" title="1.7. 数百种自定义组合功能"></a>1.7. 数百种自定义组合功能</h3><p><img src="http://media.makcyun.top/18-11-2/73620836.jpg" alt=""></p><p>可以看到，上面所实现的能都是通过设定功能所对应的触屏手势完成的。该 App 可以实现的功能包括：翻页、搜索、书签、配色、导航等 15 种事件。具体功能对应的操作包括：屏幕点击、滑动手势、手机物理按键等 21 种操作。</p><p>恩，也就是说，如果你喜欢追求极端个性化操作的话，App 为你提供了 300 多种方案，足以满足你对快捷键的癖好。</p><h2 id="2-下书"><a href="#2-下书" class="headerlink" title="2. 下书"></a>2. 下书</h2><p>要阅读，那就得有电子书。这里推荐两个非常棒的电子书下载 App：「搜书大师」 和 「小寻书」，利用它们基本都能找到你所需的书。</p><p><img src="http://media.makcyun.top/18-11-2/61777342.jpg" alt=""></p><h2 id="3-导入"><a href="#3-导入" class="headerlink" title="3. 导入"></a>3. 导入</h2><p>下载好以后，定位到电子书所在位置，然后导入即可。另外，还可以设置过滤来选择导入的文件类型。</p><p>OK，还有很多好用的功能，这里就不再往下罗列了，感兴趣的话可以到官网进一步了解：</p><p><a href="https://www.moondownload.com/chinese.html" target="_blank" rel="noopener">https://www.moondownload.com/chinese.html</a></p><p>如果你想体验下这 <strong>3</strong> 款 App，可以在网上搜索下载或者在公众号后台回复<code>「阅读器」</code>得到。</p><p>本文完。</p><hr><p><strong>推荐阅读：</strong></p><p><a href="https://www.makcyun.top/fuli01.html">每周分享第1期：关于 PDF 处理软件，你需要的都在这里了</a></p><p><a href="https://www.makcyun.top/weekly_sharing2.html">每周分享第 2 期：一掏出手机，就暴露了程序猿身份</a></p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><p><center>欢迎长按识别关注我的公众号</center><br>–&gt;</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;推荐一款最好用的手机阅读 App。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
      <category term="佳软" scheme="https://www.makcyun.top/tags/%E4%BD%B3%E8%BD%AF/"/>
    
      <category term="APP" scheme="https://www.makcyun.top/tags/APP/"/>
    
  </entry>
  
  <entry>
    <title>每周分享第 2 期：一掏出手机，就暴露了程序猿身份</title>
    <link href="https://www.makcyun.top/2018/10/26/weekly_sharing2.html"/>
    <id>https://www.makcyun.top/2018/10/26/weekly_sharing2.html</id>
    <published>2018-10-26T08:16:24.000Z</published>
    <updated>2018-12-19T03:12:32.968Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>推荐一款神级桌面 App：Aris 终端桌面。</p><a id="more"></a><p>这是「每周分享」的第 2 期。</p><p>先简单介绍下这个栏目。顾名思义，就是会在每个周末分享一篇文章。内容涵盖范围会比较广，主要集中在软件、App、PPT、摄影、Ps 等几个方面。你可以在公众号里面的 「不务正业」菜单里集中查看。另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p>好，下面开始进入正题。</p><p><img src="http://media2.makcyun.top/3.gif" alt=""></p><p>自从我的手机桌面变成这个样子以后，每当在电梯里掏出手机时，总隐隐约约能感觉到有异样的眼神。我猜他们心里在想：「这人要么是个程序猿，要么就是个装 X 犯。」</p><p>谁还没点极客精神，是吧？</p><p>这一期，想向你推荐这款我用了几个月并且爱不释手的 App：「<strong>Aris 终端桌面</strong>」，有 <strong>3</strong> 个原因。</p><p><code>首先，它能用来装 X</code>。</p><p>提供了多种桌面主题，且高度可自定义化。只要你想，就可以打造出独一无二的桌面来。</p><p><img src="http://media.makcyun.top/18-10-23/99042177.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-10-23/60690017.jpg" alt=""></p><p>其次，它体积只有 4 M大，且非常省电。光这两个特点就足以秒杀众多其他桌面。</p><p>最后，也是最重要的，它可以改变我们玩手机的方式。`</p><p>如果说一般的手机桌面是 Windows 的话，这个 App 桌面就是 Linux，一切操作都靠命令来完成，快捷方便。<br>它有很多实用的功能 ，下面一一道来。</p><h3 id="1-打开-App"><a href="#1-打开-App" class="headerlink" title="1. 打开 App"></a>1. 打开 App</h3><p>它最实用的功能就是通常输入不超过 <strong>3 个字母</strong> 就能够快速打开手机上的 App。不用像普通桌面需要 <strong>切换或者下拉屏幕</strong> 挨个去找那么复杂。<br>比如打开 「微信」、「支付宝」分别只需要输入字母 <strong>W</strong> 和 <strong>ZF</strong> 就能打开，<code>1 秒钟都要不到</code>。</p><p>对于平常不太常用的功能比如「设置」、「闹钟」这些，仍然只需要输入几个字母，很快就能打开，不用再到处去找了。</p><p><img src="http://media.makcyun.top/18-10-23/33781571.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="2-打开微信「扫一扫」"><a href="#2-打开微信「扫一扫」" class="headerlink" title="2. 打开微信「扫一扫」"></a>2. 打开微信「扫一扫」</h3><p>我们每天几乎都要打开微信「扫一扫」付款或者添加好友。常规操作需要打开微信右上角的「+ 号」，再点击扫一扫。而使用它，只需要输入「wes」就能调用 wescan 命令直接打开「扫一扫」，比常规操作快多了。</p><p><img src="http://media.makcyun.top/18-10-24/51970718.jpg" alt=""></p><h3 id="3-打电话"><a href="#3-打电话" class="headerlink" title="3. 打电话"></a>3. 打电话</h3><p>打电话也很方便，像搜索 App 一样，可以直接输入联系人拼音字母，然后拨号就行了。省去进到电话 App 里面去拨打的步骤。</p><p>上面只是常规操作，下面介绍些<code>「骚」</code>操作。</p><h3 id="4-打开手电筒-WIFI"><a href="#4-打开手电筒-WIFI" class="headerlink" title="4. 打开手电筒 / WIFI"></a>4. 打开手电筒 / WIFI</h3><p>当需要打开手电筒时，那么只要输入 「to」就能打开手电筒的命令「Torch」，用完要关上就再输入一次「to」就可以；当走到一个可以连 WIFI的地方，只需要输入 WI，就能打开 WIFI命令。当然，其他很多工具都可以打开。</p><p><img src="http://media.makcyun.top/18-10-23/34263305.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="5-查天气"><a href="#5-查天气" class="headerlink" title="5. 查天气"></a>5. 查天气</h3><p>如果你想查一下某个城市的天气，比如「北京」那么只需要输入：beijing weather。</p><p><img src="http://media.makcyun.top/18-10-23/56479602.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="6-查股价"><a href="#6-查股价" class="headerlink" title="6. 查股价"></a>6. 查股价</h3><p>如果你炒股，还可以查询股票股价，比如查询下「万科」今天的股价，只需要输入：sz.000002 stock。</p><p><img src="http://media.makcyun.top/18-10-23/30371194.jpg" alt=""></p><center>(双击看高清大图)</center><h3 id="7-翻译"><a href="#7-翻译" class="headerlink" title="7. 翻译"></a>7. 翻译</h3><p>还可以翻译单词，比如遇到一个不熟悉的词： charming，那么输入：charming translating ，就会返回该词的翻译：迷人的。</p><h3 id="8-发送-apk-到其他软件"><a href="#8-发送-apk-到其他软件" class="headerlink" title="8. 发送 apk 到其他软件"></a>8. 发送 apk 到其他软件</h3><p>这是个非常实用的功能，可以把手机上的 App 直接打包成 Apk 文件发给别人。比如我手机上有个好用的 WPS Office，然后想通过 QQ 发送给朋友，<br>则可以使用下面的命令：<br>wpsoffice apk qq 就能发送了。</p><p><img src="http://media.makcyun.top/18-10-23/76995835.jpg" alt=""></p><p>还有很多好用的功能，这里就不再往下罗列了，可以看官方文档进一步了解这款 App 是怎么使用的：</p><p><a href="https://7doger.gitbooks.io/aris/" target="_blank" rel="noopener">https://7doger.gitbooks.io/aris/</a></p><p>到这里，如果你想下载下来把玩一下，可以在网上搜索下载或者在后台回复<code>「aris」</code>得到这款 App。</p><p>最后，知乎上有一篇讲 <a href="https://www.zhihu.com/question/20311673/answer/30516935" target="_blank" rel="noopener">「极客精神」</a>的文章我觉得写地很好，分享给你。</p><blockquote><p>所谓的“极客精神”，描述起来就这样简单——“好奇之心与改变之力”。你不需要一定是个程序员或者是产品经理，也不一定变成一副科技宅的模样，如果你对世界充满好奇心和探索精神，并愿意自己去创造哪怕一些改变——这，其实就够了。</p></blockquote><p>本文完。</p><p><strong>推荐阅读：</strong></p><p><a href="https://www.makcyun.top/fuli01.html">每周分享第1期：关于 PDF 处理软件，你需要的都在这里了</a></p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;推荐一款神级桌面 App：Aris 终端桌面。&lt;/p&gt;
    
    </summary>
    
      <category term="每周分享" scheme="https://www.makcyun.top/categories/%E6%AF%8F%E5%91%A8%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="神器" scheme="https://www.makcyun.top/tags/%E7%A5%9E%E5%99%A8/"/>
    
      <category term="佳软" scheme="https://www.makcyun.top/tags/%E4%BD%B3%E8%BD%AF/"/>
    
      <category term="APP" scheme="https://www.makcyun.top/tags/APP/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(8)：模拟登录方法汇总</title>
    <link href="https://www.makcyun.top/2018/10/20/web_scraping_withpython8.html"/>
    <id>https://www.makcyun.top/2018/10/20/web_scraping_withpython8.html</id>
    <published>2018-10-20T08:16:24.000Z</published>
    <updated>2018-12-19T03:12:32.967Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>对于很多要先登录的网站来说，模拟登录往往是爬虫的第一道坎。本文介绍 POST 请求登录、获取 Cookies 登录、Seleium 模拟登录三种方法。</p><a id="more"></a><p><strong>摘要：</strong> 在进行爬虫时，除了常见的不用登录就能爬取的网站，还有一类需要先登录的网站。比如豆瓣、知乎，以及上一篇文章中的桔子网。这一类网站又可以分为：只需输入帐号密码、除了帐号密码还需输入或点击验证码等类型。本文以只需输入账号密码就能登录的桔子网为例，介绍模拟登录常用的 3 种方法。</p><ul><li>POST 请求方法：需要在后台获取登录的 URL并填写请求体参数，然后 POST 请求登录，相对麻烦；</li><li>添加 Cookies 方法：先登录将获取到的 Cookies 加入 Headers 中，最后用 GET 方法请求登录，这种最为方便；</li><li>Selenium 模拟登录：代替手工操作，自动完成账号和密码的输入，简单但速度比较慢。</li></ul><p>下面，我们用代码分别实现上述 3 种方法。</p><h2 id="1-目标网页"><a href="#1-目标网页" class="headerlink" title="1. 目标网页"></a>1. 目标网页</h2><p>这是我们要获取内容的网页：</p><p><a href="http://radar.itjuzi.com/investevent" target="_blank" rel="noopener">http://radar.itjuzi.com/investevent</a></p><p><img src="http://media.makcyun.top/18-10-19/68214534.jpg" alt=""></p><p>这个网页需要先登录才能看到数据信息，登录界面如下：</p><p><img src="http://media.makcyun.top/18-10-17/98135777.jpg" alt=""></p><p>可以看到，只需要输入账号和密码就可以登录，不用输验证码，比较简单。下面我们利用一个测试账号和密码，来实现模拟登录。</p><h2 id="2-POST-提交请求登录"><a href="#2-POST-提交请求登录" class="headerlink" title="2. POST 提交请求登录"></a>2. POST 提交请求登录</h2><p>首先，我们要找到 POST 请求的 URL。</p><p>有两种方法，第一种是在网页 devtools 查看请求，第二种是在 Fiddler 软件中查看。</p><p>先说第一种方法。</p><p><img src="http://media.makcyun.top/18-10-29/47863090.jpg" alt=""></p><p>在登录界面输入账号密码，并打开开发者工具，清空所有请求，接着点击登录按钮，这时便会看到有大量请求产生。哪一个才是 POST 请求的 URL呢？这个需要一点经验，因为是登录，所以可以尝试点击带有 「login」字眼的请求。这里我们点击第四个请求，在右侧 Headers 中可以看到请求的 URL，请求方式是 POST类型，说明 URL 找对了。</p><p><img src="http://media.makcyun.top/18-10-29/84079751.jpg" alt=""></p><p>接着，我们下拉到 Form Data，这里有几个参数，包括 identify 和 password，这两个参数正是我们登录时需要输入的账号和密码，也就是 POST 请求需要携带的参数。</p><p><img src="http://media.makcyun.top/18-10-29/10155339.jpg" alt=""></p><p>参数构造非常简单，接下来只需要利用 Requests.post 方法请求登录网站，然后就可以爬取内容了。</p><p>下面，我们尝试用 Fiddler 获取 POST 请求。</p><p>如果你对 Fiddler 还不太熟悉或者没有电脑上没有安装，可以先了解和安装一下。</p><blockquote><p>Fiddler 是位于客户端和服务器端的 HTTP 代理，也是目前最常用的 HTTP 抓包工具之一 。 它能够记录客户端和服务器之间的所有 HTTP 请求，可以针对特定的 HTTP 请求，分析请求数据、设置断点、调试 web 应用、修改请求的数据，甚至可以修改服务器返回的数据，功能非常强大，是 web 调试的利器。</p></blockquote><p>Fiddler 下载地址：</p><p><a href="https://www.telerik.com/download/fiddler" target="_blank" rel="noopener">https://www.telerik.com/download/fiddler</a></p><p>使用教程：</p><p><a href="https://zhuanlan.zhihu.com/p/37374178" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37374178</a></p><p><a href="http://www.hangge.com/blog/cache/detail_1697.html" target="_blank" rel="noopener">http://www.hangge.com/blog/cache/detail_1697.html</a></p><p>下面，我们就通过 Fiddler 截取登录请求。</p><p>当点击登录时，官场 Fiddler 页面，左侧可以看到抓取了大量请求。通过观察，第15个请求的 URL中含有「login」字段，很有可能是登录的 POST 请求。我们点击该请求，回到右侧，分别点击「inspectors」、「Headers」，可以看到就是 POST 请求，该 URL 和上面的方法获取的 URL 是一致的。</p><p><img src="http://media.makcyun.top/18-10-29/35336276.jpg" alt=""></p><p>接着，切换到右侧的 Webforms 选项，可以看到 Body 请求体。也和上面方法中得到的一致。</p><p><img src="http://media.makcyun.top/18-10-29/71870548.jpg" alt=""></p><p>获取到 URL 和请求体参数之后，下面就可以开始用 Requests.post 方法模拟登录了。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'identity'</span>:<span class="string">'irw27812@awsoo.com'</span>,   </span><br><span class="line">    <span class="string">'password'</span>:<span class="string">'test2018'</span>,</span><br><span class="line">&#125;</span><br><span class="line">url =<span class="string">'https://www.itjuzi.com/user/login?redirect=&amp;flag=&amp;radar_coupon='</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">session.post(url,headers = headers,data = data)</span><br><span class="line"><span class="comment"># 登录后，我们需要获取另一个网页中的内容</span></span><br><span class="line">response = session.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>,headers = headers)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>使用 session.post 方法提交登录请求，然后用 session.get 方法请求目标网页，并输出 HTML代码。可以看到，成功获取到了网页内容。</p><p><img src="http://media.makcyun.top/18-10-19/19273353.jpg" alt=""></p><p>下面，介绍第 2 种方法。</p><h2 id="3-获取-Cookies，直接请求登录"><a href="#3-获取-Cookies，直接请求登录" class="headerlink" title="3. 获取 Cookies，直接请求登录"></a>3. 获取 Cookies，直接请求登录</h2><p>上面一种方法，我们需要去后台获取 POST 请求链接和参数，比较麻烦。下面，我们可以尝试先登录，获取 Cookie，然后将该 Cookie 添加到 Headers 中去，然后用 GET 方法请求即可，过程简单很多。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://www.itjuzi.com/user/login?redirect=&amp;flag=&amp;radar_coupon='</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">response = session.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>, headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>可以看到，添加了 Cookie 后就不用再 POST 请求了，直接 GET 请求目标网页即可。可以看到，也能成功获取到网页内容。</p><p><img src="http://media.makcyun.top/18-10-19/5798761.jpg" alt=""></p><p>下面介绍第 3 种方法。</p><h2 id="4-Selenium-模拟登录"><a href="#4-Selenium-模拟登录" class="headerlink" title="4. Selenium 模拟登录"></a>4. Selenium 模拟登录</h2><p>这个方法很直接，利用 Selenium 代替手动方法去自动输入账号密码然后登录就行了。</p><p>关于 Selenium 的使用，在之前的一篇文章中有详细介绍，如果你不熟悉可以回顾一下：</p><p><a href="https://www.makcyun.top/web_scraping_withpython5.html">https://www.makcyun.top/web_scraping_withpython5.html</a></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.maximize_window()  <span class="comment"># 最大化窗口</span></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>) <span class="comment"># 等待加载10s</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">'https://www.itjuzi.com/user/login'</span>)</span><br><span class="line">    input = wait.until(EC.presence_of_element_located(</span><br><span class="line">        (By.XPATH, <span class="string">'//*[@id="create_account_email"]'</span>)))</span><br><span class="line">    input.send_keys(<span class="string">'irw27812@awsoo.com'</span>)</span><br><span class="line">    input = wait.until(EC.presence_of_element_located(</span><br><span class="line">        (By.XPATH, <span class="string">'//*[@id="create_account_password"]'</span>)))</span><br><span class="line">    input.send_keys(<span class="string">'test2018'</span>)</span><br><span class="line">    submit = wait.until(EC.element_to_be_clickable(</span><br><span class="line">        (By.XPATH, <span class="string">'//*[@id="login_btn"]'</span>)))</span><br><span class="line">    submit.click() <span class="comment"># 点击登录按钮</span></span><br><span class="line">    get_page_index()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_index</span><span class="params">()</span>:</span></span><br><span class="line">    browser.get(<span class="string">'http://radar.itjuzi.com/investevent'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(browser.page_source)  <span class="comment"># 输出网页源码</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(str(e))</span><br><span class="line">login()</span><br></pre></td></tr></table></figure><p>这里，我们在网页中首先定位了账号节点位置：<code>&#39;//*[@id=&quot;create_account_email&quot;]&#39;</code>，然后用 input.send_keys 方法输入账号，同理，定位了密码框位置并输入了密码。接着定位 <strong>登录</strong> 按钮的位置：<code>//*[@id=&quot;login_btn&quot;]</code>，然后用 submit.click() 方法实现点击登录按钮操作，从而完成登录。可以看到，也能成功获取到网页内容。</p><p><img src="http://media.makcyun.top/18-10-19/8322177.jpg" alt=""></p><p>以上就是模拟需登录网站的几种方法。当登录进去后，就可以开始爬取所需内容了。</p><h2 id="5-总结："><a href="#5-总结：" class="headerlink" title="5. 总结："></a>5. 总结：</h2><ul><li>本文分别实现了模拟登录的 3 种操作方法，建议优先选择第 2 种，即先获取 Cookies 再 Get 请求直接登录的方法。</li><li>本文模拟登录的网站，仅需输入账号密码，不需要获取相关加密参数，比如 Authenticity_token ，同时也无需输入验证码，所以方法比较简单。但是还有很多网站模拟登录时，需要处理加密参数、验证码输入等问题。后续将会介绍。</li></ul><p>推荐阅读：</p><p><a href="https://www.makcyun.top/web_scraping_withpython5.html">Python爬虫(5)：Selenium 爬取东方财富网股票财务报表</a></p><p>本文完。</p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;对于很多要先登录的网站来说，模拟登录往往是爬虫的第一道坎。本文介绍 POST 请求登录、获取 Cookies 登录、Seleium 模拟登录三种方法。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="模拟登录" scheme="https://www.makcyun.top/tags/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(7)：IT桔子网，国内创业公司的信息都在这里了</title>
    <link href="https://www.makcyun.top/2018/10/17/web_scraping_withpython7.html"/>
    <id>https://www.makcyun.top/2018/10/17/web_scraping_withpython7.html</id>
    <published>2018-10-17T08:16:24.000Z</published>
    <updated>2018-12-19T03:12:32.966Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>以 IT 桔子网为例，介绍需登录网站的爬取方法。爬取该网站数据库中的信息：创业公司投融资情况、投资机构信息、独角兽公司。</p><a id="more"></a><p><strong>摘要：</strong> 之前爬的网站都是不需要登录就可以爬取的，但还有很多网站的内容需要先登录才能爬，比如 IT 桔子、豆瓣、知乎等。这时采用之前的方法就不行了，需要先登录再去爬。本文以 IT 桔子网站为例，介绍模拟登录方法，然后爬取该网站数据库中的数据信息，并保存到 MongoDB 数据库中。</p><h2 id="1-网站介绍"><a href="#1-网站介绍" class="headerlink" title="1. 网站介绍"></a>1. 网站介绍</h2><p>网址：<a href="https://www.itjuzi.com/" target="_blank" rel="noopener">https://www.itjuzi.com/</a></p><p><img src="http://media.makcyun.top/18-10-17/90303578.jpg" alt=""></p><p>对于创投圈的人来说，国内的<code>IT桔子</code>和国外的 <code>Crunchbase</code>应该算是必备网站。今天，我们要说的就是IT桔子，这个网站提供了很多有价值的信息。</p><p>信息的价值体现在哪里呢，举个简单的例子。你运营了一个不错的公众号，有着 10 万粉丝群，这时候你想找投资继续做大，但你不知道该到哪里去找投资。这时候你偶然发现了IT桔子，在网站上你看到了同领域的大 V 号信息，他们得到了好几家公司上千万的投资。你看着心生羡慕，也跃跃欲试。于是，你经过综合对比分析，对自己公众号估值 200 万，然后就去找投资大 V 号的那几家公司洽谈。由于目的性明确，准备也充分，你很快就得到了融资。</p><p>这个例子中，IT 桔子提供了创业公司（运营公众号也是创业）融资金额和投资机构等相关宝贵的信息。当然，这只是非常小的一点价值，该网站数据库提供了自 2000 年以来的海量数据，包括：超过11 万家的创业公司、6 万多条投融资信息、7 千多家投资机构以及其他数据，可以说是非常全了。这些数据的背后蕴含着大量有价值的信息。</p><p><img src="http://media.makcyun.top/18-10-20/32908700.jpg" alt=""></p><p>接下来，本文尝试爬取该网站数据库的信息，之后做一些数据分析，尝试找到一些有意思的信息。</p><p><strong>主要内容：</strong></p><ul><li>模拟登录</li><li>分析 Ajax 然后抓取</li><li>存储到 MongoDB 数据库</li><li>导出 csv</li><li>数据分析(后期)</li></ul><h2 id="2-模拟登录"><a href="#2-模拟登录" class="headerlink" title="2. 模拟登录"></a>2. 模拟登录</h2><h3 id="2-1-Session-和-Cookies"><a href="#2-1-Session-和-Cookies" class="headerlink" title="2.1. Session 和 Cookies"></a>2.1. Session 和 Cookies</h3><p>观察这个网站，是需要先登录才能看到数据信息的，但是好在不用输验证码。我们需要利用账号和密码，然后实现模拟登录。</p><p><img src="http://media.makcyun.top/18-10-17/98135777.jpg" alt=""></p><p>模拟登录的方法有好几种，比如 Post 直接提交账号、先登录获取 Cookies 再直接请求、Selenium 模拟登录等。其中：</p><ul><li><p>Post 方法需要在后台获取登录的 url，填写表单参数然后再请求，比较麻烦；</p></li><li><p>直接复制 Cookies 的方法就是先登录账号，复制出 Cookies 并添加到 Headers 中，再用 <strong>requests.get</strong> 方法提交请求，这种方法最为方便；</p></li><li><p>Selenium 模拟登录方法是直接输入账号、密码，也比较方便，但速度会有点慢。</p></li></ul><p>之后，会单独介绍几种方法的具体实现的步骤。这里，我们就先采用第二种方法。</p><p>首先，需要先了解两个知识点：Session 和 Cookies。</p><blockquote><p>Session 在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。所以我们可以理解为 Cookies 里面保存了登录的凭证，有了它我们只需要在下次请求携带 Cookies 发送 Request 而不必重新输入用户名、密码等信息重新登录了。<br>因此在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 Cookies 放在 Request Headers 里面直接请求。</p></blockquote><p>更多知识，可以参考崔庆才大神的文章：</p><p><a href="https://germey.gitbooks.io/python3webspider/content/2.4-Session%E5%92%8CCookies.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/2.4-Session%E5%92%8CCookies.html</a></p><p>在了解 Cookies 知识后，我们就可以进入正题了。</p><h3 id="2-2-Requests-请求登录"><a href="#2-2-Requests-请求登录" class="headerlink" title="2.2. Requests 请求登录"></a>2.2. Requests 请求登录</h3><p>首先，利用已有的账号和密码，登录进网站主页，然后右键-检查，打开第一个 <code>www.itjuzi.com</code> 请求：</p><p><img src="http://media.makcyun.top/18-10-18/84996738.jpg" alt=""></p><p>向下拉到 Requests Headers 选项，可以看到有很多字段信息，这些信息之后我们都要添加到 Get 请求中去。</p><p>定位到下方的 Cookie 字段，可以看到有很多 Cookie 值和名称，这是在登录后自动产生的。我们将整个 Cookies 复制 Request Headers 里，然后请求网页就可以顺利登陆然后爬取。如果不加 Cookies，那么就卡在登录界面，也就无法进行后面的爬取，所以 Cookies 很重要，需要把它放到 Request Headers 里去。</p><p>下面，我们按照 json 格式开始构造 Request Headers。这里推荐一个好用的网站，可以帮我们自动构造 Request Headers：<a href="https://curl.trillworks.com/" target="_blank" rel="noopener">https://curl.trillworks.com/</a></p><p><img src="http://media.makcyun.top/18-10-18/58555393.jpg" alt=""></p><p>使用方法也很简单，右键复制<code>cURL链接</code>到这个网页中。</p><p><img src="http://media.makcyun.top/18-10-19/77493357.jpg" alt=""></p><p>将 cURL 复制到左边选框，默认选择语言为 Python，然后右侧就会自动构造后 requests 请求，包括 headers，复制下来直接可以用。登录好以后，我们就转到投融资速递网页中（url：<a href="http://radar.itjuzi.com/investevent" target="_blank" rel="noopener">http://radar.itjuzi.com/investevent</a>），然后就可以获取该页面网页内容了。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">    <span class="string">'Cache-Control'</span>: <span class="string">'max-age=0'</span>,</span><br><span class="line">    <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'</span>,</span><br><span class="line">    <span class="string">'DNT'</span>: <span class="string">'1'</span>,</span><br><span class="line">    <span class="string">'Referer'</span>: <span class="string">'http://radar.itjuzi.com/investevent'</span>,</span><br><span class="line">    <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">    <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7'</span>,</span><br><span class="line">    <span class="string">'If-None-Match'</span>: <span class="string">'W/^\\^5bc7df15-19cdc^\\^'</span>,</span><br><span class="line">    <span class="string">'If-Modified-Since'</span>: <span class="string">'Thu, 18 Oct 2018 01:17:09 GMT'</span>,</span><br><span class="line">    <span class="comment"># 主页cookie</span></span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://radar.itjuzi.com/investevent'</span>   <span class="comment"># 投融资信息</span></span><br><span class="line">s = requests.Session()</span><br><span class="line">response = s.get(url,headers = headers)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-10-18/21995265.jpg" alt=""></p><p>可以看到，添加 Cookie 后，我们请求投融资信息网页就成功了。这里如果不加 Cookie 的结果就什么也得不到：</p><p><img src="http://media.makcyun.top/18-10-18/26869412.jpg" alt=""></p><p>好，这样就算成功登录了。但是整个 headers 请求头的参数太多，是否一定需要带这么多参数呢？ 这里就去尝试看哪些参数是请求一定要的，哪些则是不用的，不用的可以去掉以精简代码。经过尝试，仅需要下面三个参数就能请求成功。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">    <span class="comment"># 主页cookie</span></span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'复制你的cookie'</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>Tips：爬取失效的时候需要重新注册帐号，然后生成新的 Cookie</p><p>如果你没那么多邮箱账号，那么推荐一个可生成随机账号的免费邮箱，用来接收注册激活链接：</p><p><a href="https://10minutemail.net/" target="_blank" rel="noopener">https://10minutemail.net/</a></p></blockquote><h2 id="3-网页爬取分析"><a href="#3-网页爬取分析" class="headerlink" title="3. 网页爬取分析"></a>3. 网页爬取分析</h2><p>在成功登录以后，我们就可以通过分析网页结构来采取相应的爬取方法。这里，我们将爬取投融资速递、创业公司、投资机构和千里马等几个子板块的数据。首先，以投融资速递信息为例。</p><p>网址：<a href="http://radar.itjuzi.com/investevent" target="_blank" rel="noopener">http://radar.itjuzi.com/investevent</a></p><h3 id="3-1-分析网页"><a href="#3-1-分析网页" class="headerlink" title="3.1. 分析网页"></a>3.1. 分析网页</h3><p><img src="http://media.makcyun.top/18-10-18/54988425.jpg" alt=""></p><p>可以看到，投融资事件信息网页中的数据是表格形式。经尝试点击翻页，发现url不变，可以初步判定网页数据采用 Ajax 形式呈现。切换到后台，点击翻页可以发现出现有规律的 <code>info?locatiop 开头的请求</code>，页数随 page 参数而发生规律的变化。</p><p><img src="http://media.makcyun.top/18-10-18/23929665.jpg" alt=""></p><p>点击请求查分别查看 Response 和 Preview，发现表格数据是 json 格式，这就太好了。因为 json 格式的数据非常整齐也很容易抓取。上一篇文章，我们正好解决了 json 格式数据的处理方法，如果不太熟悉可以回顾一下：</p><p><a href="https://www.makcyun.top/web_scraping_withpython6.html">https://www.makcyun.top/web_scraping_withpython6.html</a></p><p>接着，我们就可以通过构造 url 参数，然后用 Get 请求就可以获取网页中的表格数据，最后再加个循环，就可以爬取多页数据了。</p><h3 id="3-2-构造-url"><a href="#3-2-构造-url" class="headerlink" title="3.2. 构造 url"></a>3.2. 构造 url</h3><p>下面，我们来构造一下 url，切换到 Headers 选项卡，拉到最底部可以看到 url 需带的请求参数。这里有三项，很好理解。location：in，表示国内数据； orderby：def，表示默认排序；page：1，表示第一页。所以，只需要改变 page 参数就可以查看其他页的结果，非常简单。</p><p><img src="http://media.makcyun.top/18-10-18/23739456.jpg" alt=""></p><p>这里，如果我们对表格进行筛选，比如行业选择教育、时间选择 2018 年，那么相应的请求参数也会增加。通过构造参数就可以爬取指定的数据，这样就不用全部爬下来了，也对网站友好点。</p><p><img src="http://media.makcyun.top/18-10-18/96770843.jpg" alt=""></p><h3 id="3-3-爬取数据"><a href="#3-3-爬取数据" class="headerlink" title="3.3. 爬取数据"></a>3.3. 爬取数据</h3><p>到这儿，我们就可以直接开始爬了。可以使用函数，也可以用定义类（Class）的方法。考虑到，Python 是一种面向对象的编程，类（Class）是面向对象最重要的概念之一，运用类的思想编程非常重要。所以，这里我们尝试采用类的方法来实现爬虫。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line">ua = UserAgent()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ITjuzi</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: ua.random,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">            <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.url = <span class="string">'http://radar.itjuzi.com/investevent/info?'</span>    <span class="comment"># investevent</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(self, page)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        1 获取投融资事件数据</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;                <span class="comment"># invsestevent</span></span><br><span class="line">            <span class="string">'location'</span>: <span class="string">'in'</span>,</span><br><span class="line">            <span class="string">'orderby'</span>: <span class="string">'def'</span>,</span><br><span class="line">            <span class="string">'page'</span>: page,</span><br><span class="line">            <span class="string">'date'</span>: <span class="number">2018</span>  <span class="comment"># 年份</span></span><br><span class="line">        &#125;</span><br><span class="line">        response = self.session.get(</span><br><span class="line">            self.url, params=params, headers=self.headers).json()</span><br><span class="line">        print(response)</span><br><span class="line">        <span class="comment"># self.save_to_mongo(response)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = itjuzi()</span><br><span class="line">    spider.get_table(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如果你之前一直是用 Def 函数的写法，而没有接触过 Class 类的写法，可能会看地比较别扭，我之前就是这样的，搞不懂<code>为什么要有 self</code>、<code>为什么用__init__</code>。这种思维的转变可以通过看教程和别人写的实际案例去揣摩。这里，我先略过，之后会单独介绍。</p><p>简单解释一下上面代码的意思。首先定义了一个类（class），类名是 <strong>ITjuzi</strong>，类名通常是大写开头的单词。后面的 <strong>(object)</strong> 表示该类是从哪个类继承下来的，这个可以暂时不用管，填上就可以。然后定义了一个特殊的<code>__init__</code>方法，<code>__init__</code>方法的第一个参数永远是 <code>self</code>，之后是其他想要设置的属性参数。在这个方法里可以绑定些确定而且必须的属性，比如 headers、url 等。</p><p>在 headers 里面，User-Agent 没有使用固定的 UA，而是借助 <code>fake_useragent包</code> 生成随机的 UA：<code>ua.random</code>。因为，这个网站做了一定的反爬措施，这样可以起到一定的反爬效果，后续我们会再说到。接着，定义了一个 get_table() 函数，这个函数和普通函数没有什么区别，除了第一个参数永远是实例变量 <code>self</code>。在 session.get（）方法中传入 url、请求参数和 headers，请求网页并指定获取的数据类型为 json 格式，然后就可以顺利输出 2018 年投融资信息的第 1 页数据：</p><p><img src="http://media.makcyun.top/18-10-18/606314.jpg" alt=""></p><h3 id="3-4-存储到-MongoDB"><a href="#3-4-存储到-MongoDB" class="headerlink" title="3.4. 存储到 MongoDB"></a>3.4. 存储到 MongoDB</h3><p>数据爬取下来了，那么我们放到哪里呢？可以选择存储到 csv 中，但 json 数据中存在多层嵌套，csv 不能够直观展现。这里，我们可以尝试之前没有用过的 MongoDB 数据库，当作练习。另外，数据存储到该数据库中，后期也可以导出 csv，一举两得。</p><p>关于 MongoDB 的安装与基本使用，可以参考下面这两篇教程，之后我也会单独再写一下：</p><blockquote><p>安装</p><p><a href="https://germey.gitbooks.io/python3webspider/content/1.4.2-MongoDB%E7%9A%84%E5%AE%89%E8%A3%85.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/1.4.2-MongoDB%E7%9A%84%E5%AE%89%E8%A3%85.html</a></p><p>使用</p><p><a href="https://germey.gitbooks.io/python3webspider/content/5.3.1-MongoDB%E5%AD%98%E5%82%A8.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/5.3.1-MongoDB%E5%AD%98%E5%82%A8.html</a></p><p>可视化工具可采用 Robo 3T (之前叫 RoboMongo)</p><p><a href="https://www.mongodb.com/" target="_blank" rel="noopener">https://www.mongodb.com/</a></p></blockquote><p>下面我们就将上面返回的 json 数据，存储到 MongoDB 中去：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># mongodb数据库初始化</span></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line"><span class="comment"># 指定数据库</span></span><br><span class="line">db = client.ITjuzi</span><br><span class="line"><span class="comment"># 指定集合，类似于mysql中的表</span></span><br><span class="line">mongo_collection1 = db.itjuzi_investevent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = response[<span class="string">'data'</span>][<span class="string">'rows'</span>]  <span class="comment"># dict可以连续选取字典层内的内容</span></span><br><span class="line">            df = pd.DataFrame(data)</span><br><span class="line">            table = json.loads(df.T.to_json()).values()</span><br><span class="line">            <span class="keyword">if</span> mongo_collection1.insert_many(table):  <span class="comment"># investment</span></span><br><span class="line">                print(<span class="string">'存储到mongodb成功'</span>)</span><br><span class="line">                sleep = np.random.randint(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">            time.sleep(sleep)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            print(<span class="string">'存储到mongodb失败'</span>)</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">spider_itjuzi</span><span class="params">(self, start_page, end_page)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">            print(<span class="string">'下载第%s页:'</span> % (page))</span><br><span class="line">            self.get_table(page)</span><br><span class="line">        print(<span class="string">'下载完成'</span>)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = ITjuzi()</span><br><span class="line">    spider.spider_itjuzi(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>这里，安装好 MongoingDB 数据库、Robo 3T 和 pymongo 库后，我们就可以开始使用了。</p><p>首先，对数据库进行初始化，然后指定（如果没有则生成）数据将要存放的数据库和集合名称。接着，定义了<strong>save_to_mongo </strong>函数。由于表格里面的数据存储在键为 <strong>rows</strong> 的 value 值中，可使用 <code>response[&#39;data&#39;][&#39;rows&#39;]</code> 来获取到 json 里面的嵌套数据，然后转为 DataFrame。DataFrame 存储 MongoDB 参考了 <a href="https://stackoverflow.com/questions/20167194/insert-a-pandas-dataframe-into-mongodb-using-pymongo" target="_blank" rel="noopener">stackoverflow</a> 上面的一个回答：<strong>json.loads(df.T.to_json()).values()</strong>。</p><p>然后，使用 <strong>mongo_collection1.insert_many(table)</strong> 方法将数据插入到 mongo_collection1，也就是 itjuzi_investevent 集合中。爬取完一页数据后，设置随机延时 3-6 s，避免爬取太频繁，这也能起到一定的反爬作用。</p><p>最后，我们定义一个分页循环爬取函数 <strong>spider_itjuzi</strong>，利用 for 循环设置爬取起始页数就可以了，爬取结果如下：</p><p><img src="http://media.makcyun.top/18-10-18/32245012.jpg" alt=""></p><p>打开 Robo 3T，可以看到数据成功存储到 MongoDB 中了：</p><p><img src="http://media.makcyun.top/18-10-18/48807142.jpg" alt=""></p><p>好，以上，我们就基本上完成了 2018 年投融资信息数据表的爬取，如果你想爬其他年份或者更多页的数据，更改相应的参数即可。</p><h3 id="3-5-导出到-csv"><a href="#3-5-导出到-csv" class="headerlink" title="3.5. 导出到 csv"></a>3.5. 导出到 csv</h3><p>数据存好后，如果还不太熟悉 MongoDB 的对数据的操作，那么我们可以将数据导出为 csv，在 excel 中操作。MongoDB不能直接导出 csv，但操作起来也不麻烦，利用<code>mongoexport</code>命令，几行代码就可以输出 csv。</p><p><code>mongoexport</code>导出 csv 的方法：</p><p><a href="https://docs.mongodb.com/manual/reference/program/mongoexport/#mongoexport-fields-example" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/program/mongoexport/#mongoexport-fields-example</a></p><p>首先，运行 cmd，切换路径到 MongoDB 安装文件夹中的 bin 目录下，我这里是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd C:\Program Files\MongoDB\Server\<span class="number">4.0</span>\bin</span><br></pre></td></tr></table></figure><p>接着，在桌面新建一个txt文件，命名为<code>fields</code>，在里面输入我们需要输出的表格列名，如下所示：</p><p><img src="http://media.makcyun.top/Fv3SDvCH-YELedOEbTOWjyIIbzNW" alt=""></p><p>然后，利用<code>mongoexport</code>命令，按照：表格所在的数据库、集合、输出格式、导出列名文件位置和输出文件名的格式，编写好命令并运行就可以导出了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongoexport --db ITjuzi --collection itjuzi_investevent --type=csv --fieldFile C:\Users\sony\Desktop\fields.txt --out C:\Users\sony\Desktop\investevent.csv</span><br></pre></td></tr></table></figure><p>cmd 命令：</p><p><img src="http://media.makcyun.top/FhWiQ9majkIgqMWDKAnAr-dij5xw" alt=""></p><p>导出 csv 结果如下：<br><img src="http://media.makcyun.top/FhoU_6rggZJ0foJcRnfayTGxS0u4" alt=""></p><p><strong>Tips：直接用 excel 打开可能会是乱码，需先用 Notepad++ 转换为 UTF-8 编码，然后 excel 再打开就正常了。</strong></p><p>以上，我们就完成了整个数据表的爬取。</p><h3 id="3-6-完整代码"><a href="#3-6-完整代码" class="headerlink" title="3.6. 完整代码"></a>3.6. 完整代码</h3><p>下面，可以再尝试爬取创业公司、投资机构和千里马的数据。他们的数据结构形式是一样的，只需要更换相应的参数就可以了，感兴趣的话可以尝试下。将上面的代码再稍微整理一下，完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="keyword">import</span> socket  <span class="comment"># 断线重试</span></span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="comment"># 随机ua</span></span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># mongodb数据库初始化</span></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line"><span class="comment"># 获得数据库</span></span><br><span class="line">db = client.ITjuzi</span><br><span class="line"><span class="comment"># 获得集合</span></span><br><span class="line">mongo_collection1 = db.itjuzi_investevent</span><br><span class="line">mongo_collection2 = db.itjuzi_company</span><br><span class="line">mongo_collection3 = db.itjuzi_investment</span><br><span class="line">mongo_collection4 = db.itjuzi_horse</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">itjuzi</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: ua.random,</span><br><span class="line">            <span class="string">'X-Requested-With'</span>: <span class="string">'XMLHttpRequest'</span>,</span><br><span class="line">            <span class="comment"># 主页cookie</span></span><br><span class="line">            <span class="string">'Cookie'</span>: <span class="string">'你的cookie'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.url = <span class="string">'http://radar.itjuzi.com/investevent/info?'</span>    <span class="comment"># investevent</span></span><br><span class="line">        <span class="comment"># self.url = 'http://radar.itjuzi.com/company/infonew?'       # company</span></span><br><span class="line">        <span class="comment"># self.url = 'http://radar.itjuzi.com/investment/info?'       # investment</span></span><br><span class="line">        <span class="comment"># self.url = 'https://www.itjuzi.com/horse'               # horse</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(self, page)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        1 获取投融资事件网页内容</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;                <span class="comment"># 1 invsestevent</span></span><br><span class="line">        <span class="string">'location'</span>: <span class="string">'in'</span>,</span><br><span class="line">        <span class="string">'orderby'</span>: <span class="string">'def'</span>,</span><br><span class="line">        <span class="string">'page'</span>: page,</span><br><span class="line">        <span class="string">'date'</span>:<span class="number">2018</span>  <span class="comment">#年份  </span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # #</span></span><br><span class="line">        <span class="comment"># params = &#123;                  # 2 company</span></span><br><span class="line">        <span class="comment">#     'page': page,</span></span><br><span class="line">        <span class="comment">#     # 'scope[]': 1,  # 行业 1教育</span></span><br><span class="line">        <span class="comment">#     'orderby': 'pv',</span></span><br><span class="line">        <span class="comment">#     'born_year[]': 2018,  # 只能单年，不能多年筛选，会保留最后一个</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">        <span class="comment"># # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # #</span></span><br><span class="line">        <span class="comment"># params = &#123;                  # 3 investment</span></span><br><span class="line">        <span class="comment"># 'orderby': 'num',</span></span><br><span class="line">        <span class="comment"># 'page': page</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">        <span class="comment"># # # # # # # # # # # # # # # # # # # # # # # # # ## # # # # # # # # # # #</span></span><br><span class="line">        <span class="comment"># params = &#123;                  # 4 horse</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">        <span class="comment"># 可能会遇到请求失败，则设置3次重新请求</span></span><br><span class="line">        retrytimes = <span class="number">3</span></span><br><span class="line">        <span class="keyword">while</span> retrytimes:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                response = self.session.get(</span><br><span class="line">                    self.url, params=params, headers=self.headers,timeout = (<span class="number">5</span>,<span class="number">20</span>)).json()</span><br><span class="line">                self.save_to_mongo(response)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> socket.timeout:</span><br><span class="line">                print(<span class="string">'下载第&#123;&#125;页，第&#123;&#125;次网页请求超时'</span> .format(page,retrytimes))</span><br><span class="line">                retrytimes -=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = response[<span class="string">'data'</span>][<span class="string">'rows'</span>]  <span class="comment"># dict可以连续选取字典层内的内容</span></span><br><span class="line">            <span class="comment"># data =response  # 爬取千里马时需替换为此data</span></span><br><span class="line">            df = pd.DataFrame(data)</span><br><span class="line">            table = json.loads(df.T.to_json()).values()</span><br><span class="line">            <span class="keyword">if</span> mongo_collection1.insert_many(table):      <span class="comment"># investevent</span></span><br><span class="line">            <span class="comment"># if mongo_collection2.insert_many(table):    # company</span></span><br><span class="line">            <span class="comment"># if mongo_collection3.insert_many(table):    # investment</span></span><br><span class="line">            <span class="comment"># if mongo_collection4.insert_many(table):    # horse </span></span><br><span class="line">                print(<span class="string">'存储到mongodb成功'</span>)</span><br><span class="line">                sleep = np.random.randint(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">                time.sleep(sleep)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            print(<span class="string">'存储到mongodb失败'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_itjuzi</span><span class="params">(self, start_page, end_page)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">            print(<span class="string">'下载第%s页:'</span> % (page))</span><br><span class="line">            self.get_table(page)</span><br><span class="line">        print(<span class="string">'下载完成'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    spider = itjuzi()</span><br><span class="line">    spider.spider_itjuzi(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>源代码也可以在公众号后台回复：「<strong>it桔子</strong>」，或者在下面的链接中获取：</p><p><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><p>后续文章，我们将对这些数据进行分析，尝试从中找出一些有意思的发现。</p><h2 id="4-总结："><a href="#4-总结：" class="headerlink" title="4. 总结："></a>4. 总结：</h2><ul><li>本文以 IT 桔子网为例，介绍了需登录网站的爬取方法。即：先模拟登录再爬取数据信息。但是还有一些网站登录时需要输入验证码，这让爬取难度又增加，后期会再进行介绍。</li><li>IT 桔子相比之前的爬虫网站，反爬措施高了很多。本文通过设置随机延时、随机 UserAgent，可一定程度上增加爬虫的稳定性。但是仍然会受到反爬措施的限制，后期可尝试通过设置 IP 代理池进一步提升爬虫效率。</li><li>上面的爬虫程序在爬取过程容易中断，接着再进行爬取即可。但是手动修改非常不方便，也容易造成数据重复爬取或者漏爬。所以，为了更完整地爬取，需增加断点续传的功能。</li></ul><p>本文完。</p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎扫一扫关注我的微信公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;以 IT 桔子网为例，介绍需登录网站的爬取方法。爬取该网站数据库中的信息：创业公司投融资情况、投资机构信息、独角兽公司。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="模拟登录" scheme="https://www.makcyun.top/tags/%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95/"/>
    
      <category term="MongoDB" scheme="https://www.makcyun.top/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(6)：50 行代码爬取东方财富网上市公司 10 年近百万行财务报表数据</title>
    <link href="https://www.makcyun.top/2018/10/12/web_scraping_withpython6.html"/>
    <id>https://www.makcyun.top/2018/10/12/web_scraping_withpython6.html</id>
    <published>2018-10-12T08:16:24.000Z</published>
    <updated>2019-05-12T09:43:55.949Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>通过分析网址 JavaScript 请求，以比 Selenium 快 100 倍的方法，快速爬取东方财富网各上市公司历年的财务报表数据。</p><a id="more"></a><p><strong>摘要：</strong> 上一篇文章，我们用Selenium成功爬取了东方财富网的财务报表数据，但是速度非常慢，爬取 70 页需要好几十分钟。为了加快速度，本文分析网页JavaScript请求，找到数据接口然后快速爬取财务报表数据。</p><h2 id="1-JavaScript请求分析"><a href="#1-JavaScript请求分析" class="headerlink" title="1. JavaScript请求分析"></a>1. JavaScript请求分析</h2><p><img src="http://media.makcyun.top/18-10-4/90719379.jpg" alt=""></p><p>上一篇文章，我们简单分了东方财富网财务报表网页后台的js请求，文章回顾：（<a href="https://www.makcyun.top/web_scraping_withpython5.html">https://www.makcyun.top/web_scraping_withpython5.html</a>）</p><p>接下来，我们深入分析。首先，点击报表底部的下一页，然后观察左侧<strong>Name</strong>列，看会弹出什么新的请求来：</p><p><img src="http://media.makcyun.top/18-10-5/136165.jpg" alt=""></p><p>可以看到，当不断点击下一页时，会相应弹出以<strong>get？type</strong>开头的请求。点击右边Headers选项卡，可以看到请求的URL，网址非常长，先不管它，后续我们会分析各项参数。接着，点击右侧的Preview和Response，可以看到里面有很多整齐的数据，尝试猜测这可能是财务报表中的数据，经过和表格进行对比，发现这正是我们所需的数据，太好了。</p><p><img src="http://media.makcyun.top/Fpsj517KOZ63Z-9vrzIYjgLyfS80" alt=""></p><p>然后将URL复制到新链接中打开看看，可以看到表格中的数据完美地显示出来了。竟然不用添加Headers、UA去请求就能获取到，看来东方财富网很大方啊。</p><p><img src="http://media.makcyun.top/Fjn2ZLvaUCdam1U-2RDk_N17zGOy" alt=""></p><p>到这里，爬取思路已经很清晰了。首先，用Request请求该URL，将获取到的数据进行正则匹配，将数据转变为json格式，然后写入本地文件，最后再加一个分页循环爬取就OK了。这比之前的Selenium要简单很多，而且速度应该会快很多倍。下面我们就先来尝试爬一页数据看看。</p><h2 id="2-爬取单页"><a href="#2-爬取单页" class="headerlink" title="2. 爬取单页"></a>2. 爬取单页</h2><h3 id="2-1-抓取分析"><a href="#2-1-抓取分析" class="headerlink" title="2.1. 抓取分析"></a>2.1. 抓取分析</h3><p>这里仍然以<code>2018年中报的利润表</code>为例，抓取该网页的第一页表格数据，网页url为：<a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a></p><p>表格第一页的js请求的url为：<a href="http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?type=CWBB_LRB&amp;token=70f12f2f4f091e459a279469fe49eca5&amp;st=noticedate&amp;sr=-1&amp;p=2&amp;ps=50&amp;js=var%20spmVUpAF={pages:(tp" target="_blank" rel="noopener">http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?type=CWBB_LRB&amp;token=70f12f2f4f091e459a279469fe49eca5&amp;st=noticedate&amp;sr=-1&amp;p=2&amp;ps=50&amp;js=var%20spmVUpAF={pages:(tp),data:%20(x)}&amp;filter=(reportdate=^2018-06-30^)&amp;rt=51312886</a>,data:%20(x)}&amp;filter=(reportdate=^2018-06-30^)&amp;rt=51312886)</p><p>下面，我们通过分析该url，来抓取表格内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">()</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'type'</span>: <span class="string">'CWBB_LRB'</span>,  <span class="comment"># 表格类型,LRB为利润表缩写，必须</span></span><br><span class="line">        <span class="string">'token'</span>: <span class="string">'70f12f2f4f091e459a279469fe49eca5'</span>,  <span class="comment"># 访问令牌，必须</span></span><br><span class="line">        <span class="string">'st'</span>: <span class="string">'noticedate'</span>,  <span class="comment"># 公告日期</span></span><br><span class="line">        <span class="string">'sr'</span>: <span class="number">-1</span>,  <span class="comment"># 保持-1不用改动即可</span></span><br><span class="line">        <span class="string">'p'</span>: <span class="number">1</span>,  <span class="comment"># 表格页数</span></span><br><span class="line">        <span class="string">'ps'</span>: <span class="number">50</span>,  <span class="comment"># 每页显示多少条信息</span></span><br><span class="line">        <span class="string">'js'</span>: <span class="string">'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;'</span>,  <span class="comment"># js函数，必须</span></span><br><span class="line">        <span class="string">'filter'</span>: <span class="string">'(reportdate=^2018-06-30^)'</span>,  <span class="comment"># 筛选条件</span></span><br><span class="line">        <span class="comment"># 'rt': 51294261  可不用</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?'</span></span><br><span class="line">    response = requests.get(url, params=params).text</span><br><span class="line">    print(response)</span><br><span class="line">get_table()</span><br></pre></td></tr></table></figure><p>这里我们定义了一个get_table()方法，来输出抓取的第一页表格内容。params为url请求中所包含的参数。</p><p>这里对重要参数进行简单说明：<strong>type</strong>为7个表格的类型说明，将type拆成两部分：’CWBB_‘ 和’LRB’，资产负债表等后3个表是以’CWBB_’ 开头，业绩报表至预约披露时间表等前4个表是以’YJBB20_‘开头的；’LRB’为利润表的首字母缩写，同理业绩报表则为’YJBB’。所以，如果要爬取不同的表格，就需要更改type参数。’filter’为表格筛选参数，这里筛选出年中报的数据。不同的表格筛选条件会不一样，所以当type类型更改的时候，也要相应修改filter类型。</p><p>params参数设置好之后，将url和params参数一起传进requests.get()方法中，这样就构造好了请求连接。几行代码就可以成功获取网页第一页的表格数据了：</p><p><img src="http://media.makcyun.top/FiX9K5Z5Y_Hue7c16dZaXZ_IpQ5O" alt=""></p><p>可以看到，表格信息存储在LFtlXDqn变量中，pages表示表格有72页。data为表格数据，是一个由多个字典构成的列表，每个字典是表格的一行数据。我们可以通过正则表达式分别提取出pages和data数据。</p><h3 id="2-2-正则表达式提取表格"><a href="#2-2-正则表达式提取表格" class="headerlink" title="2.2. 正则表达式提取表格"></a>2.2. 正则表达式提取表格</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定页数</span></span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line">pat = re.compile(<span class="string">'var.*?&#123;pages:(\d+),data:.*?'</span>)</span><br><span class="line">page_all = re.search(pat, response)</span><br><span class="line">print(page_all.group(<span class="number">1</span>))</span><br><span class="line">结果：</span><br><span class="line"><span class="number">72</span></span><br></pre></td></tr></table></figure><p>这里用<code>\d+</code>匹配页数中的数值，然后用re.search()方法提取出来。group(1)表示输出第一个结果，这里就是()中的页数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取出list，可以使用json.dumps和json.loads</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">pattern = re.compile(<span class="string">'var.*?data: (.*)&#125;'</span>, re.S)</span><br><span class="line">items = re.search(pattern, response)</span><br><span class="line">data = items.group(<span class="number">1</span>)</span><br><span class="line">print(data)</span><br><span class="line">print(type(data))</span><br><span class="line">结果如下：</span><br><span class="line">[&#123;<span class="string">'scode'</span>: <span class="string">'600478'</span>, <span class="string">'hycode'</span>: <span class="string">'016040'</span>, <span class="string">'companycode'</span>: <span class="string">'10001305'</span>, <span class="string">'sname'</span>: <span class="string">'科力远'</span>, <span class="string">'publishname'</span>: <span class="string">'材料行业'</span>...</span><br><span class="line"><span class="string">'sjltz'</span>: <span class="number">10.466665</span>, <span class="string">'kcfjcxsyjlr'</span>: <span class="number">46691230.93</span>, <span class="string">'sjlktz'</span>: <span class="number">10.4666649042</span>, <span class="string">'eutime'</span>: <span class="string">'2018/9/6 20:18:42'</span>, <span class="string">'yyzc'</span>: <span class="number">14238766.31</span>&#125;]</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">str</span>'&gt;</span></span><br></pre></td></tr></table></figure><p>这里在匹配表格数据用了<code>(.*)</code>表示贪婪匹配，因为data中有很多个字典，每个字典都是以’}’结尾，所以我们利用贪婪匹配到最后一个’}’，这样才能获取data所有数据。多数情况下，我们可能会用到(.*?)，这表示非贪婪匹配，意味着之多匹配一个’}’，这样的话，我们只能匹配到第一行数据，显然是不对的。</p><h3 id="2-3-json-loads-输出表格"><a href="#2-3-json-loads-输出表格" class="headerlink" title="2.3. json.loads()输出表格"></a>2.3. json.loads()输出表格</h3><p>这里提取出来的list是str字符型的，我们需要转换为list列表类型。为什么要转换为list类型呢，因为无法用操作list的方法去操作str，比如list切片。转换为list后，我们可以对list进行切片，比如data[0]可以获取第一个{}中的数据，也就是表格第一行，这样方便后续构造循环从而逐行输出表格数据。这里采用json.loads()方法将str转换为list。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = json.loads(data)</span><br><span class="line"><span class="comment"># print(data) 和上面的一样</span></span><br><span class="line">print(type(data))</span><br><span class="line">print(data[<span class="number">0</span>])</span><br><span class="line">结果如下：</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line">&#123;'scode': '600478', 'hycode': '016040', 'companycode': '10001305', 'sname': '科力远', 'publishname': '材料行业', 'reporttimetypecode': '002', 'combinetypecode': '001', 'dataajusttype': '2', 'mkt': 'shzb', 'noticedate': '2018-10-13T00:00:00', 'reportdate': '2018-06-30T00:00:00', 'parentnetprofit': -46515200.15, 'totaloperatereve': 683459458.22, 'totaloperateexp': 824933386.17, 'totaloperateexp_tb': -0.0597570689015973, 'operateexp': 601335611.67, 'operateexp_tb': -0.105421872593886, 'saleexp': 27004422.05, 'manageexp': 141680603.83, 'financeexp': 33258589.95, 'operateprofit': -94535963.65, 'sumprofit': -92632216.61, 'incometax': -8809471.54, 'operatereve': '-', 'intnreve': '-', 'intnreve_tb': '-', 'commnreve': '-', 'commnreve_tb': '-', 'operatetax': 7777267.21, 'operatemanageexp': '-', 'commreve_commexp': '-', 'intreve_intexp': '-', 'premiumearned': '-', 'premiumearned_tb': '-', 'investincome': '-', 'surrenderpremium': '-', 'indemnityexp': '-', 'tystz': -0.092852, 'yltz': 0.178351, 'sjltz': 0.399524, 'kcfjcxsyjlr': -58082725.17, 'sjlktz': 0.2475682609, 'eutime': '2018/10/12 21:01:36', 'yyzc': 601335611.67&#125;</span><br></pre></td></tr></table></figure><p>接下来我们就将表格内容输入到csv文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入csv文件</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        w = csv.writer(f)</span><br><span class="line">        w.writerow(d.values())</span><br></pre></td></tr></table></figure><p>通过for循环，依次取出表格中的每一行字典数据{}，然后用with…open的方法写入’eastmoney.csv’文件中。</p><p><strong>tips</strong>：’a’表示可重复写入；encoding=’utf_8_sig’ 能保持csv文件的汉字不会乱码；newline为空能避免每行数据中产生空行。</p><p>这样，第一页50行的表格数据就成功输出到csv文件中去了：</p><p><img src="http://media.makcyun.top/Fn4lk33DpaNp2SyAK2HB5_IaKClv" alt=""></p><p>这里，我们还可以在输出表格之前添加上表头：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加列标题</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_header</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = list(data[<span class="number">0</span>].keys())</span><br><span class="line">        print(headers)  </span><br><span class="line">        print(len(headers)) <span class="comment"># 输出list长度，也就是有多少列</span></span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow(headers)</span><br></pre></td></tr></table></figure><p>这里，data[0]表示list的一个字典中的数据，data[0].keys()表示获取字典中的key键值，也就是列标题。外面再加一个list序列化（结果如下），然后将该list输出到’eastmoney.csv’中作为表格的列标题即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'scode'</span>, <span class="string">'hycode'</span>, <span class="string">'companycode'</span>, <span class="string">'sname'</span>, <span class="string">'publishname'</span>, <span class="string">'reporttimetypecode'</span>, <span class="string">'combinetypecode'</span>, <span class="string">'dataajusttype'</span>, <span class="string">'mkt'</span>, <span class="string">'noticedate'</span>, <span class="string">'reportdate'</span>, <span class="string">'parentnetprofit'</span>, <span class="string">'totaloperatereve'</span>, <span class="string">'totaloperateexp'</span>, <span class="string">'totaloperateexp_tb'</span>, <span class="string">'operateexp'</span>, <span class="string">'operateexp_tb'</span>, <span class="string">'saleexp'</span>, <span class="string">'manageexp'</span>, <span class="string">'financeexp'</span>, <span class="string">'operateprofit'</span>, <span class="string">'sumprofit'</span>, <span class="string">'incometax'</span>, <span class="string">'operatereve'</span>, <span class="string">'intnreve'</span>, <span class="string">'intnreve_tb'</span>, <span class="string">'commnreve'</span>, <span class="string">'commnreve_tb'</span>, <span class="string">'operatetax'</span>, <span class="string">'operatemanageexp'</span>, <span class="string">'commreve_commexp'</span>, <span class="string">'intreve_intexp'</span>, <span class="string">'premiumearned'</span>, <span class="string">'premiumearned_tb'</span>, <span class="string">'investincome'</span>, <span class="string">'surrenderpremium'</span>, <span class="string">'indemnityexp'</span>, <span class="string">'tystz'</span>, <span class="string">'yltz'</span>, <span class="string">'sjltz'</span>, <span class="string">'kcfjcxsyjlr'</span>, <span class="string">'sjlktz'</span>, <span class="string">'eutime'</span>, <span class="string">'yyzc'</span>]</span><br><span class="line"><span class="number">44</span> <span class="comment"># 一共有44个字段，也就是说表格有44列。</span></span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/FktoPi6Qd0WRm8soOZNY6L5LByHf" alt=""></p><p>以上，就完成了单页表格的爬取和下载到本地的过程。</p><h2 id="3-多页表格爬取"><a href="#3-多页表格爬取" class="headerlink" title="3. 多页表格爬取"></a>3. 多页表格爬取</h2><p>将上述代码整理为相应的函数，再添加for循环，仅50行代码就可以爬取72页的利润报表数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(page)</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'type'</span>: <span class="string">'CWBB_LRB'</span>,  <span class="comment"># 表格类型,LRB为利润表缩写，必须</span></span><br><span class="line">        <span class="string">'token'</span>: <span class="string">'70f12f2f4f091e459a279469fe49eca5'</span>,  <span class="comment"># 访问令牌，必须</span></span><br><span class="line">        <span class="string">'st'</span>: <span class="string">'noticedate'</span>,  <span class="comment"># 公告日期</span></span><br><span class="line">        <span class="string">'sr'</span>: <span class="number">-1</span>,  <span class="comment"># 保持-1不用改动即可</span></span><br><span class="line">        <span class="string">'p'</span>: page,  <span class="comment"># 表格页数</span></span><br><span class="line">        <span class="string">'ps'</span>: <span class="number">50</span>,  <span class="comment"># 每页显示多少条信息</span></span><br><span class="line">        <span class="string">'js'</span>: <span class="string">'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;'</span>,  <span class="comment"># js函数，必须</span></span><br><span class="line">        <span class="string">'filter'</span>: <span class="string">'(reportdate=^2018-06-30^)'</span>,  <span class="comment"># 筛选条件，如果不选则默认下载全部时期的数据</span></span><br><span class="line">        <span class="comment"># 'rt': 51294261  可不用</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?'</span></span><br><span class="line">    response = requests.get(url, params=params).text</span><br><span class="line">  <span class="comment"># 确定页数</span></span><br><span class="line">    pat = re.compile(<span class="string">'var.*?&#123;pages:(\d+),data:.*?'</span>)</span><br><span class="line">    page_all = re.search(pat, response)  <span class="comment"># 总页数</span></span><br><span class="line">    pattern = re.compile(<span class="string">'var.*?data: (.*)&#125;'</span>, re.S)</span><br><span class="line">    items = re.search(pattern, response)</span><br><span class="line">    data = items.group(<span class="number">1</span>)</span><br><span class="line">    data = json.loads(data)</span><br><span class="line">    print(<span class="string">'\n正在下载第 %s 页表格'</span> % page)</span><br><span class="line">    <span class="keyword">return</span> page_all,data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_header</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = list(data[<span class="number">0</span>].keys())</span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow(headers)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_table</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'eastmoney.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">            w = csv.writer(f)</span><br><span class="line">            w.writerow(d.values())</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    data = get_table(page)[<span class="number">1</span>]</span><br><span class="line">    write_table(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start_time = time.time()  <span class="comment"># 下载开始时间</span></span><br><span class="line">    <span class="comment"># 写入表头</span></span><br><span class="line">    write_header(get_table(<span class="number">1</span>))</span><br><span class="line">    page_all = get_table(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    page_all = int(page_all.group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, page_all):</span><br><span class="line">        main(page)</span><br><span class="line">    end_time = time.time() - start_time  <span class="comment"># 结束时间</span></span><br><span class="line">    print(<span class="string">'下载用时: &#123;:.1f&#125; s'</span> .format(end_time))</span><br></pre></td></tr></table></figure><p>整个下载只用了20多秒，而之前用selenium花了几十分钟，这效率提升了足有100倍！</p><p><img src="http://media.makcyun.top/Fs-EixHunxcirWSlKPZhF_A1Ariu" alt=""></p><p>这里，如果我们想下载全部时期（从2007年-2018年）利润报表数据，也很简单。只要将<code>type</code>中的<code>filter</code>参数注释掉，意味着也就是不筛选日期，那么就可以下载全部时期的数据。这里当我们取消注释filter列，将会发现总页数page_all会从2018年中报的72页增加到2528页，全部下载完成后，表格有超过12万行的数据。基于这些数据，可以尝试从中进行一些有价值的数据分析。</p><p><img src="http://media.makcyun.top/FuP6aIzr6A7c52EgxAvGiptpoeCX" alt=""></p><h2 id="4-通用代码构造"><a href="#4-通用代码构造" class="headerlink" title="4. 通用代码构造"></a>4. 通用代码构造</h2><p>以上代码实现了2018年中报利润报表的爬取，但如果不想局限于该报表，还想爬取其他报表或者其他任意时期的数据，那么就需要手动地去修改代码中相应的字段，很不方便。所以上面的代码可以说是简短但不够强大。</p><p>为了能够灵活实现爬取任意类别和任意时期的报表数据，需要对代码再进行一些加工，就可以构造出通用强大的爬虫程序了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">e.g: http://data.eastmoney.com/bbsj/201806/lrb.html</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置文件保存在D盘eastmoney文件夹下</span></span><br><span class="line">file_path = <span class="string">'D:\\eastmoney'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">    os.mkdir(file_path)</span><br><span class="line">os.chdir(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 设置表格爬取时期、类别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_table</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line">    print(<span class="string">'\t\t\t\t东方财富网报表下载'</span>)</span><br><span class="line">    print(<span class="string">'作者：高级农民工  2018.10.10'</span>)</span><br><span class="line">    print(<span class="string">'--------------'</span>)</span><br><span class="line">    year = int(float(input(<span class="string">'请输入要查询的年份(四位数2007-2018)：\n'</span>)))</span><br><span class="line">    <span class="comment"># int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10</span></span><br><span class="line">    <span class="keyword">while</span> (year &lt; <span class="number">2007</span> <span class="keyword">or</span> year &gt; <span class="number">2018</span>):</span><br><span class="line">        year = int(float(input(<span class="string">'年份数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    quarter = int(float(input(<span class="string">'请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：\n'</span>)))</span><br><span class="line">    <span class="keyword">while</span> (quarter &lt; <span class="number">1</span> <span class="keyword">or</span> quarter &gt; <span class="number">4</span>):</span><br><span class="line">        quarter = int(float(input(<span class="string">'季度数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充，</span></span><br><span class="line">    <span class="comment"># http://www.runoob.com/python/att-string-format.html</span></span><br><span class="line">    quarter = <span class="string">'&#123;:02d&#125;'</span>.format(quarter * <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># quarter = '%02d' %(int(month)*3)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定季度所对应的最后一天是30还是31号</span></span><br><span class="line">    <span class="keyword">if</span> (quarter == <span class="string">'06'</span>) <span class="keyword">or</span> (quarter == <span class="string">'09'</span>):</span><br><span class="line">        day = <span class="number">30</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        day = <span class="number">31</span></span><br><span class="line">    date = <span class="string">'&#123;&#125;-&#123;&#125;-&#123;&#125;'</span> .format(year, quarter, day)</span><br><span class="line">    <span class="comment"># print('date:', date)  # 测试日期 ok</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2 设置财务报表种类</span></span><br><span class="line">    tables = int(</span><br><span class="line">        input(<span class="string">'请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): \n'</span>))</span><br><span class="line"></span><br><span class="line">    dict_tables = &#123;<span class="number">1</span>: <span class="string">'业绩报表'</span>, <span class="number">2</span>: <span class="string">'业绩快报表'</span>, <span class="number">3</span>: <span class="string">'业绩预告表'</span>,</span><br><span class="line">                   <span class="number">4</span>: <span class="string">'预约披露时间表'</span>, <span class="number">5</span>: <span class="string">'资产负债表'</span>, <span class="number">6</span>: <span class="string">'利润表'</span>, <span class="number">7</span>: <span class="string">'现金流量表'</span>&#125;</span><br><span class="line"></span><br><span class="line">    dict = &#123;<span class="number">1</span>: <span class="string">'YJBB'</span>, <span class="number">2</span>: <span class="string">'YJKB'</span>, <span class="number">3</span>: <span class="string">'YJYG'</span>,</span><br><span class="line">            <span class="number">4</span>: <span class="string">'YYPL'</span>, <span class="number">5</span>: <span class="string">'ZCFZB'</span>, <span class="number">6</span>: <span class="string">'LRB'</span>, <span class="number">7</span>: <span class="string">'XJLLB'</span>&#125;</span><br><span class="line">    category = dict[tables]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># js请求参数里的type，第1-4个表的前缀是'YJBB20_'，后3个表是'CWBB_'</span></span><br><span class="line">    <span class="comment"># 设置set_table()中的type、st、sr、filter参数</span></span><br><span class="line">    <span class="keyword">if</span> tables == <span class="number">1</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'latestnoticedate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter =  <span class="string">"(securitytypecode in ('058001001','058001002'))(reportdate=^%s^)"</span> %(date)</span><br><span class="line">    <span class="keyword">elif</span> tables == <span class="number">2</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'ldate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter = <span class="string">"(securitytypecode in ('058001001','058001002'))(rdate=^%s^)"</span> %(date)</span><br><span class="line">    <span class="keyword">elif</span> tables == <span class="number">3</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'ndate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter=<span class="string">" (IsLatest='T')(enddate=^2018-06-30^)"</span></span><br><span class="line">    <span class="keyword">elif</span> tables == <span class="number">4</span>:</span><br><span class="line">        category_type = <span class="string">'YJBB20_'</span></span><br><span class="line">        st = <span class="string">'frdate'</span></span><br><span class="line">        sr = <span class="number">1</span></span><br><span class="line">        filter =  <span class="string">"(securitytypecode ='058001001')(reportdate=^%s^)"</span> %(date)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        category_type = <span class="string">'CWBB_'</span></span><br><span class="line">        st = <span class="string">'noticedate'</span></span><br><span class="line">        sr = <span class="number">-1</span></span><br><span class="line">        filter = <span class="string">'(reportdate=^%s^)'</span> % (date)</span><br><span class="line"></span><br><span class="line">    category_type = category_type + category</span><br><span class="line">    <span class="comment"># print(category_type)</span></span><br><span class="line">    <span class="comment"># 设置set_table()中的filter参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'date'</span>:date,</span><br><span class="line">    <span class="string">'category'</span>:dict_tables[tables],</span><br><span class="line">    <span class="string">'category_type'</span>:category_type,</span><br><span class="line">    <span class="string">'st'</span>:st,</span><br><span class="line">    <span class="string">'sr'</span>:sr,</span><br><span class="line">    <span class="string">'filter'</span>:filter</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 设置表格爬取起始页数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">page_choose</span><span class="params">(page_all)</span>:</span></span><br><span class="line">    <span class="comment"># 选择爬取页数范围</span></span><br><span class="line">    start_page = int(input(<span class="string">'请输入下载起始页数：\n'</span>))</span><br><span class="line">    nums = input(<span class="string">'请输入要下载的页数，（若需下载全部则按回车）：\n'</span>)</span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断输入的是数值还是回车空格</span></span><br><span class="line">    <span class="keyword">if</span> nums.isdigit():</span><br><span class="line">        end_page = start_page + int(nums)</span><br><span class="line">    <span class="keyword">elif</span> nums == <span class="string">''</span>:</span><br><span class="line">        end_page = int(page_all.group(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'页数输入错误'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回所需的起始页数，供后续程序调用</span></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'start_page'</span>: start_page,</span><br><span class="line">        <span class="string">'end_page'</span>: end_page</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 表格正式爬取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_table</span><span class="params">(date, category_type,st,sr,filter,page)</span>:</span></span><br><span class="line">    <span class="comment"># 参数设置</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="comment"># 'type': 'CWBB_LRB',</span></span><br><span class="line">        <span class="string">'type'</span>: category_type,  <span class="comment"># 表格类型</span></span><br><span class="line">        <span class="string">'token'</span>: <span class="string">'70f12f2f4f091e459a279469fe49eca5'</span>,</span><br><span class="line">        <span class="string">'st'</span>: st,</span><br><span class="line">        <span class="string">'sr'</span>: sr,</span><br><span class="line">        <span class="string">'p'</span>: page,</span><br><span class="line">        <span class="string">'ps'</span>: <span class="number">50</span>,  <span class="comment"># 每页显示多少条信息</span></span><br><span class="line">        <span class="string">'js'</span>: <span class="string">'var LFtlXDqn=&#123;pages:(tp),data: (x)&#125;'</span>,</span><br><span class="line">        <span class="string">'filter'</span>: filter,</span><br><span class="line">        <span class="comment"># 'rt': 51294261  可不用</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://dcfm.eastmoney.com/em_mutisvcexpandinterface/api/js/get?'</span></span><br><span class="line">    response = requests.get(url, params=params).text</span><br><span class="line">    <span class="comment"># 确定页数</span></span><br><span class="line">    pat = re.compile(<span class="string">'var.*?&#123;pages:(\d+),data:.*?'</span>)</span><br><span class="line">    page_all = re.search(pat, response)</span><br><span class="line">    <span class="comment"># print(page_all.group(1))  # ok</span></span><br><span class="line">    <span class="comment"># 提取出list，可以使用json.dumps和json.loads</span></span><br><span class="line">    pattern = re.compile(<span class="string">'var.*?data: (.*)&#125;'</span>, re.S)</span><br><span class="line">    items = re.search(pattern, response)</span><br><span class="line">    <span class="comment"># 等价于</span></span><br><span class="line">    <span class="comment"># items = re.findall(pattern,response)</span></span><br><span class="line">    <span class="comment"># print(items[0])</span></span><br><span class="line">    data = items.group(<span class="number">1</span>)</span><br><span class="line">    data = json.loads(data)</span><br><span class="line">    <span class="keyword">return</span> page_all, data,page</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 写入表头</span></span><br><span class="line"><span class="comment"># 方法1 借助csv包，最常用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_header</span><span class="params">(data,category)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'&#123;&#125;.csv'</span> .format(category), <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        headers = list(data[<span class="number">0</span>].keys())</span><br><span class="line">        <span class="comment"># print(headers)  # 测试 ok</span></span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow(headers)</span><br><span class="line"><span class="comment"># 5 写入表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_table</span><span class="params">(data,page,category)</span>:</span></span><br><span class="line">    print(<span class="string">'\n正在下载第 %s 页表格'</span> % page)</span><br><span class="line">    <span class="comment"># 写入文件方法1</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'&#123;&#125;.csv'</span> .format(category), <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">            w = csv.writer(f)</span><br><span class="line">            w.writerow(d.values())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(date, category_type,st,sr,filter,page)</span>:</span></span><br><span class="line">    func = get_table(date, category_type,st,sr,filter,page)</span><br><span class="line">    data = func[<span class="number">1</span>]</span><br><span class="line">    page = func[<span class="number">2</span>]</span><br><span class="line">    write_table(data,page,category)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 获取总页数，确定起始爬取页数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> set_table():</span><br><span class="line">        date = i.get(<span class="string">'date'</span>)</span><br><span class="line">        category = i.get(<span class="string">'category'</span>)</span><br><span class="line">        category_type = i.get(<span class="string">'category_type'</span>)</span><br><span class="line">        st = i.get(<span class="string">'st'</span>)</span><br><span class="line">        sr = i.get(<span class="string">'sr'</span>)</span><br><span class="line">        filter = i.get(<span class="string">'filter'</span>)</span><br><span class="line">    constant = get_table(date,category_type,st,sr,filter, <span class="number">1</span>)</span><br><span class="line">    page_all = constant[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> page_choose(page_all):</span><br><span class="line">        start_page = i.get(<span class="string">'start_page'</span>)</span><br><span class="line">        end_page = i.get(<span class="string">'end_page'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先写入表头</span></span><br><span class="line">    write_header(constant[<span class="number">1</span>],category)</span><br><span class="line">    start_time = time.time()  <span class="comment"># 下载开始时间</span></span><br><span class="line">    <span class="comment"># 爬取表格主程序</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">        main(date,category_type,st,sr,filter, page)</span><br><span class="line">    end_time = time.time() - start_time  <span class="comment"># 结束时间</span></span><br><span class="line">    print(<span class="string">'下载完成'</span>)</span><br><span class="line">    print(<span class="string">'下载用时: &#123;:.1f&#125; s'</span> .format(end_time))</span><br></pre></td></tr></table></figure><p>以爬取2018年中业绩报表为例，感受一下比selenium快得多的爬取效果（视频链接）：</p><p><a href="https://v.qq.com/x/page/a0519bfxajc.html" target="_blank" rel="noopener">https://v.qq.com/x/page/a0519bfxajc.html</a></p><p>利用上面的程序，我们可以下载任意时期和任意报表的数据。这里，我下载完成了2018年中报所有7个报表的数据。</p><p>文中代码和素材资源可以在下面的链接中获取：</p><p><a href="https://github.com/makcyun/eastmoney_spider" target="_blank" rel="noopener">https://github.com/makcyun/eastmoney_spider</a></p><p>本文完。</p><p><img src="http://media.makcyun.top/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;通过分析网址 JavaScript 请求，以比 Selenium 快 100 倍的方法，快速爬取东方财富网各上市公司历年的财务报表数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(5)：Selenium 爬取东方财富网股票财务报表</title>
    <link href="https://www.makcyun.top/2018/10/02/web_scraping_withpython5.html"/>
    <id>https://www.makcyun.top/2018/10/02/web_scraping_withpython5.html</id>
    <published>2018-10-02T09:18:57.000Z</published>
    <updated>2018-12-19T03:12:32.963Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>利用Selenium爬取东方财富网各上市公司历年的财务报表数据。</p><a id="more"></a><p><strong>摘要：</strong> 现在很多网页都采取JavaScript进行动态渲染，其中包括Ajax技术。上一篇文章通过分析Ajax接口数据，顺利爬取了澎湃新闻网动态网页中的图片。但有的网页虽然也Ajax技术，但接口参数可能是加密的无法直接获得，比如淘宝；有的动态网页也采用JavaScript，但不是Ajax技术，比如Echarts官网。所以，当遇到这两类网页时，上一篇文章中的方法便不再奏效，需要新的采取新的方法，这其中包括干脆、直接、好用的的Selenium大法。东方财富网的财务报表网页也是通过JavaScript动态加载的，本文利用Selenium方法爬取该网站上市公司的财务报表数据。</p><p>[TOC]</p><h2 id="1-实战背景"><a href="#1-实战背景" class="headerlink" title="1. 实战背景"></a>1. 实战背景</h2><p>很多网站都提供上市公司的公告、财务报表等金融投资信息和数据，比如：腾讯财经、网易财经、新浪财经、东方财富网等。这之中，发现东方财富网的数据非常齐全。</p><p><code>东方财富网</code>有一个数据中心：<a href="http://data.eastmoney.com/center/" target="_blank" rel="noopener">http://data.eastmoney.com/center/</a>，该数据中心提供包括特色数据、研究报告、年报季报等在内的大量数据（见下图）。</p><p><img src="http://media.makcyun.top/18-10-4/3257803.jpg" alt=""></p><p>以年报季报类别为例，我们点开该分类查看一下2018年中报（见下图），可以看到该分类下又包括：业绩报表、业绩快报、利润表等7个报表的数据。以业绩报表为例，报表包含全部3000多只股票的业绩报表数据，一共有70多页。</p><p><img src="http://media.makcyun.top/18-10-4/90719379.jpg" alt=""></p><p>假如，我们想获取所有股票2018年中的业绩报表数据，然后对该数据进行一些分析。采取手动复制的方法，70多页可以勉强完成。但如果想获取任意一年、任意季度、任意报表的数据，要再通过手动复制的方法，工作量会非常地大。举个例子，假设要获取10年间（40个季度）、所有7个报表的数据，那么手动复制的工作量大约将是：40×7×70（每个报表大约70页），差不多要重复性地复制2万次！！！可以说是人工不可能完成的任务。所以，本文的目标就是利用Selenium自动化技术，爬取年报季报类别下，任意一年（网站有数据至今）、任意财务报表数据。我们所需要做的，仅是简单输入几个字符，其他就全部交给电脑，然后过一会儿打开excel，就可以看到所需数据”静静地躺在那里”，是不是挺酷的？</p><p>好，下面我们就开始实操一下。首先，需要分析要爬取的网页对象。</p><h2 id="2-网页分析"><a href="#2-网页分析" class="headerlink" title="2. 网页分析"></a>2. 网页分析</h2><p>之前，我们已经爬过表格型的数据，所以对表格数据的结构应该不会太陌生，如果忘了，可以再看一下这篇文章：<a href="https://www.makcyun.top/web_scraping_withpython2.html">https://www.makcyun.top/web_scraping_withpython2.html</a></p><p>我们这里以上面的2018年中报的业绩报表为例，查看一下表格的形式。</p><p>网址url：<a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a>，<code>bbsj</code>代表年报季报，<code>201803</code>代表<code>2018年一季报</code>，类似地，201806表示年中报；<code>lrb</code>是<code>利润表</code>的首字母缩写，同理，<code>yjbb</code>表示<code>业绩报表</code>。可以看出，该网址格式很简单，便于构造url。</p><p>接着，我们点击<code>下一页</code>按钮，可以看到表格更新后，url没有发生改变，可以判定是采用了Javscript。那么，我们首先判断是不是采用了Ajax加载的。方法也很简单，右键检查或按F12，切换到network并选择下面的XHR，再按F5刷新。可以看到只有一个Ajax请求，点击下一页也并没有生成新的Ajax请求，可以判断该网页结构不是常见的那种点击下一页或者下拉会源源不断出现的Ajax请求类型，那么便无法构造url来实现分页爬取。</p><p><img src="http://media.makcyun.top/18-10-4/90923672.jpg" alt=""></p><p>XHR选项里没有找到我们需要的请求，接下来试试看能不能再JS里找到表格的数据请求。将选项选为JS，再次F5刷新，可以看到出现了很多JS请求，然后我们点击几次下一页，会发现弹出新的请求来，然后右边为响应的请求信息。url链接非常长，看上去很复杂。好，这里我们先在这里打住不往下了。</p><p>可以看到，通过分析后台元素来爬取该动态网页的方法，相对比较复杂。那么有没有干脆、直截了当地就能够抓取表格内容的方法呢？有的，就是本文接下来要介绍的Selenium大法。</p><p><img src="http://media.makcyun.top/18-10-5/136165.jpg" alt=""></p><h2 id="3-Selenium知识"><a href="#3-Selenium知识" class="headerlink" title="3. Selenium知识"></a>3. Selenium知识</h2><p>Selenium 是什么？一句话，自动化测试工具。它是为了测试而出生的，但在近几年火热的爬虫领域中，它摇身一变，变成了爬虫的利器。直白点说， <strong>Seleninm能控制浏览器, 像人一样”上网”</strong>。比如，可以实现网页自动翻页、登录网站、发送邮件、下载图片/音乐/视频等等。举个例子，写几行python代码就可以用Selenium实现登录<a href="https://www.itjuzi.com/" target="_blank" rel="noopener">IT桔子</a>，然后浏览网页的功能。</p><p><img src="http://media2.makcyun.top/selenium.gif" alt=""></p><p>怎么样，仅用几行代码就能实现自动上网操作，是不是挺神奇的？当然，这仅仅是Selenium最简单的功能，还有很多更加丰富的操作，可以参考以下几篇教程：</p><p>参考网站：</p><blockquote><p>Selenium官网： <a href="https://selenium-python.readthedocs.io/" target="_blank" rel="noopener">https://selenium-python.readthedocs.io/</a></p><p>SeleniumPython文档（英文版）：<a href="http://selenium-python.readthedocs.org/index.html" target="_blank" rel="noopener">http://selenium-python.readthedocs.org/index.html</a></p><p>SeleniumPython文档（中文版）：<a href="https://selenium-python-zh.readthedocs.io/en/latest/faq.html" target="_blank" rel="noopener">https://selenium-python-zh.readthedocs.io/en/latest/faq.html</a></p><p>Selenium 基本操作：<a href="https://www.yukunweb.com/2017/7/python-spider-Selenium-PhantomJS-basic/" target="_blank" rel="noopener">https://www.yukunweb.com/2017/7/python-spider-Selenium-PhantomJS-basic/</a></p><p>Selenium爬取淘宝信息实战：<a href="https://cuiqingcai.com/2852.html" target="_blank" rel="noopener">https://cuiqingcai.com/2852.html</a></p></blockquote><p>只需要记住重要的一点就是：<code>Selenium能做到&quot;可见即可爬&quot;</code>。也就是说网页上你能看到的东西，Selenium基本上都能爬取下来。包括上面我们提到的东方财富网的财务报表数据，它也能够做到，而且非常简单直接，不用去后台查看用了什么JavaScript技术或者Ajax参数。下面我们就实际来操练下吧。</p><h2 id="4-编码实现"><a href="#4-编码实现" class="headerlink" title="4. 编码实现"></a>4. 编码实现</h2><h3 id="4-1-思路"><a href="#4-1-思路" class="headerlink" title="4.1. 思路"></a>4.1. 思路</h3><ul><li>安装配置好Selenium运行的相关环境，浏览器可以用Chrome、Firefox、PhantomJS等，我用的是Chrome；</li><li>东方财富网的财务报表数据不用登录可直接获得，Selenium更加方便爬取；</li><li>先以单个网页中的财务报表为例，表格数据结构简单，可先直接定位到整个表格，然后一次性获取所有td节点对应的表格单元内容；</li><li>接着循环分页爬取所有上市公司的数据，并保存为csv文件。</li><li>重新构造灵活的url，实现可以爬取任意时期、任意一张财务报表的数据。</li></ul><p>根据上述思路，下面就用代码一步步来实现。</p><h3 id="4-2-爬取单页表格"><a href="#4-2-爬取单页表格" class="headerlink" title="4.2. 爬取单页表格"></a>4.2. 爬取单页表格</h3><p>我们先以<code>2018年中报的利润表</code>为例，抓取该网页的第一页表格数据，网页url：<a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a></p><p><img src="http://media.makcyun.top/18-10-5/446556.jpg" alt=""></p><p>快速定位到表格所在的节点：<strong>id = dt_1</strong>，然后可以用Selenium进行抓取了，方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="comment"># 当测试好能够顺利爬取后，为加快爬取速度可设置无头模式，即不弹出浏览器</span></span><br><span class="line"><span class="comment"># 添加无头headlesss 1使用chrome headless,2使用PhantomJS</span></span><br><span class="line"><span class="comment"># 使用 PhantomJS 会警告高不建议使用phantomjs，建议chrome headless</span></span><br><span class="line"><span class="comment"># chrome_options = webdriver.ChromeOptions()</span></span><br><span class="line"><span class="comment"># chrome_options.add_argument('--headless')</span></span><br><span class="line"><span class="comment"># browser = webdriver.Chrome(chrome_options=chrome_options)</span></span><br><span class="line"><span class="comment"># browser = webdriver.PhantomJS()</span></span><br><span class="line"><span class="comment"># browser.maximize_window()  # 最大化窗口,可以选择设置</span></span><br><span class="line"></span><br><span class="line">browser.get(<span class="string">'http://data.eastmoney.com/bbsj/201806/lrb.html'</span>)</span><br><span class="line">element = browser.find_element_by_css_selector(<span class="string">'#dt_1'</span>)  <span class="comment"># 定位表格，element是WebElement类型</span></span><br><span class="line"><span class="comment"># 提取表格内容td</span></span><br><span class="line">td_content = element.find_elements_by_tag_name(<span class="string">"td"</span>) <span class="comment"># 进一步定位到表格内容所在的td节点</span></span><br><span class="line">lst = []  <span class="comment"># 存储为list</span></span><br><span class="line"><span class="keyword">for</span> td <span class="keyword">in</span> td_content:</span><br><span class="line">    lst.append(td.text)</span><br><span class="line">print(lst) <span class="comment"># 输出表格内容</span></span><br></pre></td></tr></table></figure><p>这里，使用Chrome浏览器构造一个Webdriver对象，赋值给变量browser，browser调用get()方法请求想要抓取的网页。接着使用<code>find_element_by_css_selector</code>方法查找表格所在的节点：<strong>‘#dt_1’</strong>。</p><p>这里推荐一款小巧、快速定位css/xpath的Chrome插件：<strong>SelectorGadget</strong>，使用这个插件就不用再去源代码中手动定位节点那么麻烦了。</p><p>插件地址：<a href="https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb" target="_blank" rel="noopener">https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb</a></p><p>紧接着再向下定位到<strong>td</strong>节点，因为网页中有很多个td节点，所以要用<strong>find_elements</strong>方法。然后，遍历数据节点存储到list中。打印查看一下结果：</p><p><img src="http://media.makcyun.top/18-10-5/53703391.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># list形式:</span></span><br><span class="line">[<span class="string">'1'</span>, <span class="string">'002161'</span>, <span class="string">'远望谷'</span>, ...<span class="string">'-7960万'</span>, <span class="string">'09-29'</span>,</span><br><span class="line"> <span class="string">'2'</span>,<span class="string">'002316'</span>, <span class="string">'亚联发展'</span>, ...<span class="string">'1.79亿'</span>, <span class="string">'09-29'</span>, </span><br><span class="line"> <span class="string">'3'</span>,...</span><br><span class="line"> <span class="string">'50'</span>, <span class="string">'002683'</span>, <span class="string">'宏大爆破'</span>,...<span class="string">'1.37亿'</span>, <span class="string">'09-01'</span>]</span><br></pre></td></tr></table></figure><p>是不是很方便，几行代码就能抓取下来这一页表格，除了速度有点慢。</p><p>为了便于后续存储，我们将list转换为DataFrame。首先需要把这一个大的list分割为多行多列的子list，实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 确定表格列数</span></span><br><span class="line">col = len(element.find_elements_by_css_selector(<span class="string">'tr:nth-child(1) td'</span>))</span><br><span class="line"><span class="comment"># 通过定位一行td的数量，可获得表格的列数，然后将list拆分为对应列数的子list</span></span><br><span class="line">lst = [lst[i:i + col] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(lst), col)]</span><br><span class="line"><span class="comment"># 原网页中打开"详细"链接可以查看更详细的数据，这里我们把url提取出来，方便后期查看</span></span><br><span class="line">lst_link = []</span><br><span class="line">links = element.find_elements_by_css_selector(<span class="string">'#dt_1 a.red'</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">    url = link.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">    lst_link.append(url)</span><br><span class="line">lst_link = pd.Series(lst_link)</span><br><span class="line"><span class="comment"># list转为dataframe</span></span><br><span class="line">df_table = pd.DataFrame(lst)</span><br><span class="line"><span class="comment"># 添加url列</span></span><br><span class="line">df_table[<span class="string">'url'</span>] = lst_link</span><br><span class="line">print(df_table.head())  <span class="comment"># 查看DataFrame</span></span><br></pre></td></tr></table></figure><p>这里，要将list分割为子list，只需要确定表格有多少列即可，然后将每相隔这么多数量的值划分为一个子list。如果我们数一下该表的列数，可以发现一共有16列。但是这里不能使用这个数字，因为除了利润表，其他报表的列数并不是16，所以当后期爬取其他表格可能就会报错。这里仍然通过f<strong>ind_elements_by_css_selector</strong>方法，定位首行td节点的数量，便可获得表格的列数，然后将list拆分为对应列数的子list。同时，原网页中打开”详细”列的链接可以查看更详细的数据，这里我们把url提取出来，并增加一列到DataFrame中，方便后期查看。打印查看一下输出结果：</p><p><img src="http://media.makcyun.top/18-10-5/20285338.jpg" alt=""></p><p>可以看到，表格所有的数据我们都抓取到了，下面只需要进行分页循环爬取就行了。</p><p>这里，没有抓取表头是因为表头有合并单元格，处理起来就非常麻烦。建议表格抓取下来后，在excel中复制表头进去就行了。如果，实在想要用代码完成，可以参考这篇文章：<a href="https://blog.csdn.net/weixin_39461443/article/details/75456962" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39461443/article/details/75456962</a></p><h3 id="4-3-分页爬取"><a href="#4-3-分页爬取" class="headerlink" title="4.3. 分页爬取"></a>4.3. 分页爬取</h3><p>上面完成了单页表格的爬取，下面我们来实现分页爬取。</p><p>首先，我们先实现Selenium模拟翻页跳转操作，成功后再爬取每页的表格内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.maximize_window()  <span class="comment"># 最大化窗口,可以选择设置</span></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        browser.get(<span class="string">'http://data.eastmoney.com/bbsj/201806/lrb.html'</span>)</span><br><span class="line">        print(<span class="string">'正在爬取第： %s 页'</span> % page)</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"dt_1"</span>)))</span><br><span class="line">        <span class="comment"># 判断是否是第1页，如果大于1就输入跳转，否则等待加载完成。</span></span><br><span class="line">        <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 确定页数输入框</span></span><br><span class="line">            input = wait.until(EC.presence_of_element_located(</span><br><span class="line">                (By.XPATH, <span class="string">'//*[@id="PageContgopage"]'</span>)))</span><br><span class="line">            input.click()</span><br><span class="line">            input.clear()</span><br><span class="line">            input.send_keys(page)</span><br><span class="line">            submit = wait.until(EC.element_to_be_clickable(</span><br><span class="line">                (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; a.btn_link'</span>)))</span><br><span class="line">            submit.click()</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 确认成功跳转到输入框中的指定页</span></span><br><span class="line">        wait.until(EC.text_to_be_present_in_element(</span><br><span class="line">            (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; span.at'</span>), str(page)))</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):  <span class="comment"># 测试翻4页</span></span><br><span class="line">        index_page(page)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>这里，我们先加载了相关包，使用WebDriverWait对象，设置最长10s的显式等待时间，以便网页加载出表格。判断表格是否加载出来，用到了<strong>EC.presence_of_element_located</strong>条件。表格加载出来后，设置一个页面判断，如果在第1页就等待页面加载完成，如果大于第1页就开始跳转。</p><p>要完成跳转操作，我们需要通过获取输入框input节点，然后用clear()方法清空输入框，再通过send_keys()方法填写相应的页码，接着通过submit.click()方法击下一页完成翻页跳转。</p><p>这里，我们测试一下前4页跳转效果，可以看到网页成功跳转了。下面就可以对每一页应用第一页爬取表格内容的方法，抓取每一页的表格，转为DataFrame然后存储到csv文件中去。</p><p><img src="" alt="http://media2.makcyun.top/selenium%E5%88%86%E9%A1%B5%E5%8E%8B%E7%BC%A9.gif"></p><h3 id="4-4-通用爬虫构造"><a href="#4-4-通用爬虫构造" class="headerlink" title="4.4. 通用爬虫构造"></a>4.4. 通用爬虫构造</h3><p>上面，我们完成了<strong>2018年中报利润表： <a href="http://data.eastmoney.com/bbsj/201806/lrb.html" target="_blank" rel="noopener">http://data.eastmoney.com/bbsj/201806/lrb.html</a></strong>，一个网页表格的爬取。但如果我们想爬取任意时期、任意一张报表的表格，比如2017年3季度的利润表、2016年全年的业绩报表、2015年1季度的现金流量表等等。上面的代码就行不通了，下面我们对代码进行一下改造，变成更通用的爬虫。从图中可以看到，东方财富网年报季报有7张表格，财务报表最早从2007年开始每季度一次。基于这两个维度，可重新构造url的形式，然后爬取表格数据。下面，我们用代码进行实现：</p><p><img src="http://media.makcyun.top/18-10-6/3087007.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构url</span></span><br><span class="line"><span class="comment"># 1 设置财务报表获取时期</span></span><br><span class="line">year = int(float(input(<span class="string">'请输入要查询的年份(四位数2007-2018)：  '</span>)))</span><br><span class="line"><span class="comment"># int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会</span></span><br><span class="line"><span class="keyword">while</span> (year &lt; <span class="number">2007</span> <span class="keyword">or</span> year &gt; <span class="number">2018</span>):</span><br><span class="line">    year = int(float(input(<span class="string">'年份数值输入错误，请重新输入：'</span>)))</span><br><span class="line">quarter = int(float(input(<span class="string">'请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：  '</span>)))</span><br><span class="line"><span class="keyword">while</span> (quarter &lt; <span class="number">1</span> <span class="keyword">or</span> quarter &gt; <span class="number">4</span>):</span><br><span class="line">    quarter = int(float(input(<span class="string">'季度数值输入错误，请重新输入：  '</span>)))</span><br><span class="line"><span class="comment"># 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充</span></span><br><span class="line">quarter = <span class="string">'&#123;:02d&#125;'</span>.format(quarter * <span class="number">3</span>)</span><br><span class="line"><span class="comment"># quarter = '%02d' %(int(month)*3)</span></span><br><span class="line">date = <span class="string">'&#123;&#125;&#123;&#125;'</span> .format(year, quarter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 设置财务报表种类</span></span><br><span class="line">tables = int(</span><br><span class="line">    input(<span class="string">'请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表):  '</span>))</span><br><span class="line">dict_tables = &#123;<span class="number">1</span>: <span class="string">'业绩报表'</span>, <span class="number">2</span>: <span class="string">'业绩快报表'</span>, <span class="number">3</span>: <span class="string">'业绩预告表'</span>,</span><br><span class="line">               <span class="number">4</span>: <span class="string">'预约披露时间表'</span>, <span class="number">5</span>: <span class="string">'资产负债表'</span>, <span class="number">6</span>: <span class="string">'利润表'</span>, <span class="number">7</span>: <span class="string">'现金流量表'</span>&#125;</span><br><span class="line">dict = &#123;<span class="number">1</span>: <span class="string">'yjbb'</span>, <span class="number">2</span>: <span class="string">'yjkb/13'</span>, <span class="number">3</span>: <span class="string">'yjyg'</span>,</span><br><span class="line">        <span class="number">4</span>: <span class="string">'yysj'</span>, <span class="number">5</span>: <span class="string">'zcfz'</span>, <span class="number">6</span>: <span class="string">'lrb'</span>, <span class="number">7</span>: <span class="string">'xjll'</span>&#125;</span><br><span class="line">category = dict[tables]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 设置url</span></span><br><span class="line">url = <span class="string">'http://data.eastmoney.com/&#123;&#125;/&#123;&#125;/&#123;&#125;.html'</span> .format(<span class="string">'bbsj'</span>, date, category)</span><br><span class="line">print(url)  <span class="comment"># 测试输出的url</span></span><br></pre></td></tr></table></figure><p><img src="http://media2.makcyun.top/%E9%80%9A%E7%94%A8url.gif" alt=""></p><p>经过上面的设置，我们通过输入想要获得指定时期、制定财务报表类型的数值，就能返回相应的url链接。将该链接应用到前面的爬虫中，就可以爬取相应的报表内容了。</p><p>另外，除了从第一页开始爬取到最后一页的结果以外，我们还可以自定义设置想要爬取的页数。比如起始页数从第1页开始，然后爬取10页。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 选择爬取页数范围</span></span><br><span class="line">start_page = int(input(<span class="string">'请输入下载起始页数：\n'</span>))</span><br><span class="line">nums = input(<span class="string">'请输入要下载的页数，（若需下载全部则按回车）：\n'</span>)</span><br><span class="line"><span class="comment"># 确定网页中的最后一页</span></span><br><span class="line">browser.get(url)</span><br><span class="line"><span class="comment"># 确定最后一页页数不直接用数字而是采用定位，因为不同时间段的页码会不一样</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    page = browser.find_element_by_css_selector(<span class="string">'.next+ a'</span>)  <span class="comment"># next节点后面的a节点</span></span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    page = browser.find_element_by_css_selector(<span class="string">'.at+ a'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'没有找到该节点'</span>)</span><br><span class="line"><span class="comment"># 上面用try.except是因为绝大多数页码定位可用'.next+ a'，但是业绩快报表有的只有2页，无'.next+ a'节点</span></span><br><span class="line">end_page = int(page.text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> nums.isdigit():</span><br><span class="line">    end_page = start_page + int(nums)</span><br><span class="line"><span class="keyword">elif</span> nums == <span class="string">''</span>:</span><br><span class="line">    end_page = end_page</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'页数输入错误'</span>)</span><br><span class="line"><span class="comment"># 输入准备下载表格类型</span></span><br><span class="line">print(<span class="string">'准备下载:&#123;&#125;-&#123;&#125;'</span> .format(date, dict_tables[tables]))</span><br></pre></td></tr></table></figure><p>经过上面的设置，我们就可以实现自定义时期和财务报表类型的表格爬取了，将代码再稍微整理一下，可实现下面的爬虫效果：</p><p>视频截图：</p><p><img src="http://media.makcyun.top/18-10-6/66845185.jpg" alt=""></p><p>视频地址：<a href="https://v.qq.com/x/page/y07335thsn2.html" target="_blank" rel="noopener">https://v.qq.com/x/page/y07335thsn2.html</a></p><p>背景中类似黑客帝国的代码雨效果，其实是动态网页效果。素材来源于下面这个网站，该网站还有很多酷炫的动态背景可以下载下来。</p><p><a href="http://wallpaper.upupoo.com/store/paperDetail-1783830052.htm" target="_blank" rel="noopener">http://wallpaper.upupoo.com/store/paperDetail-1783830052.htm</a></p><h3 id="4-5-完整代码"><a href="#4-5-完整代码" class="headerlink" title="4.5. 完整代码"></a>4.5. 完整代码</h3><p>整个爬虫的完整代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先chrome，后phantomjs</span></span><br><span class="line"><span class="comment"># browser = webdriver.Chrome()</span></span><br><span class="line"><span class="comment"># 添加无头headlesss</span></span><br><span class="line">chrome_options = webdriver.ChromeOptions()</span><br><span class="line">chrome_options.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">browser = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># browser = webdriver.PhantomJS() # 会报警高提示不建议使用phantomjs，建议chrome添加无头</span></span><br><span class="line">browser.maximize_window()  <span class="comment"># 最大化窗口</span></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        print(<span class="string">'正在爬取第： %s 页'</span> % page)</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.ID, <span class="string">"dt_1"</span>)))</span><br><span class="line">        <span class="comment"># 判断是否是第1页，如果大于1就输入跳转，否则等待加载完成。</span></span><br><span class="line">        <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 确定页数输入框</span></span><br><span class="line">            input = wait.until(EC.presence_of_element_located(</span><br><span class="line">                (By.XPATH, <span class="string">'//*[@id="PageContgopage"]'</span>)))</span><br><span class="line">            input.click()</span><br><span class="line">            input.clear()</span><br><span class="line">            input.send_keys(page)</span><br><span class="line">            submit = wait.until(EC.element_to_be_clickable(</span><br><span class="line">                (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; a.btn_link'</span>)))</span><br><span class="line">            submit.click()</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 确认成功跳转到输入框中的指定页</span></span><br><span class="line">        wait.until(EC.text_to_be_present_in_element(</span><br><span class="line">            (By.CSS_SELECTOR, <span class="string">'#PageCont &gt; span.at'</span>), str(page)))</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_table</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 提取表格第一种方法</span></span><br><span class="line">    <span class="comment"># element = wait.until(EC.presence_of_element_located((By.ID, "dt_1")))</span></span><br><span class="line">    <span class="comment"># 第二种方法</span></span><br><span class="line">    element = browser.find_element_by_css_selector(<span class="string">'#dt_1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取表格内容td</span></span><br><span class="line">    td_content = element.find_elements_by_tag_name(<span class="string">"td"</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> td <span class="keyword">in</span> td_content:</span><br><span class="line">        <span class="comment"># print(type(td.text)) # str</span></span><br><span class="line">        lst.append(td.text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定表格列数</span></span><br><span class="line">    col = len(element.find_elements_by_css_selector(<span class="string">'tr:nth-child(1) td'</span>))</span><br><span class="line">    <span class="comment"># 通过定位一行td的数量，可获得表格的列数，然后将list拆分为对应列数的子list</span></span><br><span class="line">    lst = [lst[i:i + col] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(lst), col)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 原网页中打开"详细"链接，可以查看更详细的数据，这里我们把url提取出来，方便后期查看</span></span><br><span class="line">    lst_link = []</span><br><span class="line">    links = element.find_elements_by_css_selector(<span class="string">'#dt_1 a.red'</span>)</span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">        url = link.get_attribute(<span class="string">'href'</span>)</span><br><span class="line">        lst_link.append(url)</span><br><span class="line"></span><br><span class="line">    lst_link = pd.Series(lst_link)</span><br><span class="line">    <span class="comment"># list转为dataframe</span></span><br><span class="line">    df_table = pd.DataFrame(lst)</span><br><span class="line">    <span class="comment"># 添加url列</span></span><br><span class="line">    df_table[<span class="string">'url'</span>] = lst_link</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(df_table.head())</span></span><br><span class="line">    <span class="keyword">return</span> df_table</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file</span><span class="params">(df_table, category)</span>:</span></span><br><span class="line">    <span class="comment"># 设置文件保存在D盘eastmoney文件夹下</span></span><br><span class="line">    file_path = <span class="string">'D:\\eastmoney'</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">        os.mkdir(file_path)</span><br><span class="line">    os.chdir(file_path)</span><br><span class="line">    df_table.to_csv(<span class="string">'&#123;&#125;.csv'</span> .format(category), mode=<span class="string">'a'</span>,</span><br><span class="line">                    encoding=<span class="string">'utf_8_sig'</span>, index=<span class="number">0</span>, header=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置表格获取时间、类型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_table</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line">    print(<span class="string">'\t\t\t\t东方财富网报表下载'</span>)</span><br><span class="line">    print(<span class="string">'作者：高级农民工  2018.10.6'</span>)</span><br><span class="line">    print(<span class="string">'--------------'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1 设置财务报表获取时期</span></span><br><span class="line">    year = int(float(input(<span class="string">'请输入要查询的年份(四位数2007-2018)：\n'</span>)))</span><br><span class="line">    <span class="comment"># int表示取整，里面加float是因为输入的是str，直接int会报错，float则不会</span></span><br><span class="line">    <span class="comment"># https://stackoverflow.com/questions/1841565/valueerror-invalid-literal-for-int-with-base-10</span></span><br><span class="line">    <span class="keyword">while</span> (year &lt; <span class="number">2007</span> <span class="keyword">or</span> year &gt; <span class="number">2018</span>):</span><br><span class="line">        year = int(float(input(<span class="string">'年份数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    quarter = int(float(input(<span class="string">'请输入小写数字季度(1:1季报，2-年中报，3：3季报，4-年报)：\n'</span>)))</span><br><span class="line">    <span class="keyword">while</span> (quarter &lt; <span class="number">1</span> <span class="keyword">or</span> quarter &gt; <span class="number">4</span>):</span><br><span class="line">        quarter = int(float(input(<span class="string">'季度数值输入错误，请重新输入：\n'</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为所需的quarter 两种方法,2表示两位数，0表示不满2位用0补充，</span></span><br><span class="line">    <span class="comment"># http://www.runoob.com/python/att-string-format.html</span></span><br><span class="line">    quarter = <span class="string">'&#123;:02d&#125;'</span>.format(quarter * <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># quarter = '%02d' %(int(month)*3)</span></span><br><span class="line">    date = <span class="string">'&#123;&#125;&#123;&#125;'</span> .format(year, quarter)</span><br><span class="line">    <span class="comment"># print(date) 测试日期 ok</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2 设置财务报表种类</span></span><br><span class="line">    tables = int(</span><br><span class="line">        input(<span class="string">'请输入查询的报表种类对应的数字(1-业绩报表；2-业绩快报表：3-业绩预告表；4-预约披露时间表；5-资产负债表；6-利润表；7-现金流量表): \n'</span>))</span><br><span class="line"></span><br><span class="line">    dict_tables = &#123;<span class="number">1</span>: <span class="string">'业绩报表'</span>, <span class="number">2</span>: <span class="string">'业绩快报表'</span>, <span class="number">3</span>: <span class="string">'业绩预告表'</span>,</span><br><span class="line">                   <span class="number">4</span>: <span class="string">'预约披露时间表'</span>, <span class="number">5</span>: <span class="string">'资产负债表'</span>, <span class="number">6</span>: <span class="string">'利润表'</span>, <span class="number">7</span>: <span class="string">'现金流量表'</span>&#125;</span><br><span class="line">    dict = &#123;<span class="number">1</span>: <span class="string">'yjbb'</span>, <span class="number">2</span>: <span class="string">'yjkb/13'</span>, <span class="number">3</span>: <span class="string">'yjyg'</span>,</span><br><span class="line">            <span class="number">4</span>: <span class="string">'yysj'</span>, <span class="number">5</span>: <span class="string">'zcfz'</span>, <span class="number">6</span>: <span class="string">'lrb'</span>, <span class="number">7</span>: <span class="string">'xjll'</span>&#125;</span><br><span class="line">    category = dict[tables]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3 设置url</span></span><br><span class="line">    <span class="comment"># url = 'http://data.eastmoney.com/bbsj/201803/lrb.html' eg.</span></span><br><span class="line">    url = <span class="string">'http://data.eastmoney.com/&#123;&#125;/&#123;&#125;/&#123;&#125;.html'</span> .format(</span><br><span class="line">        <span class="string">'bbsj'</span>, date, category)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 4 选择爬取页数范围</span></span><br><span class="line">    start_page = int(input(<span class="string">'请输入下载起始页数：\n'</span>))</span><br><span class="line">    nums = input(<span class="string">'请输入要下载的页数，（若需下载全部则按回车）：\n'</span>)</span><br><span class="line">    print(<span class="string">'*'</span> * <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定网页中的最后一页</span></span><br><span class="line">    browser.get(url)</span><br><span class="line">    <span class="comment"># 确定最后一页页数不直接用数字而是采用定位，因为不同时间段的页码会不一样</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        page = browser.find_element_by_css_selector(<span class="string">'.next+ a'</span>)  <span class="comment"># next节点后面的a节点</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        page = browser.find_element_by_css_selector(<span class="string">'.at+ a'</span>)</span><br><span class="line">    <span class="comment"># else:</span></span><br><span class="line">    <span class="comment">#     print('没有找到该节点')</span></span><br><span class="line">    <span class="comment"># 上面用try.except是因为绝大多数页码定位可用'.next+ a'，但是业绩快报表有的只有2页，无'.next+ a'节点</span></span><br><span class="line">    end_page = int(page.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> nums.isdigit():</span><br><span class="line">        end_page = start_page + int(nums)</span><br><span class="line">    <span class="keyword">elif</span> nums == <span class="string">''</span>:</span><br><span class="line">        end_page = end_page</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'页数输入错误'</span>)</span><br><span class="line">    <span class="comment"># 输入准备下载表格类型</span></span><br><span class="line">    print(<span class="string">'准备下载:&#123;&#125;-&#123;&#125;'</span> .format(date, dict_tables[tables]))</span><br><span class="line">    print(url)</span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>: url,</span><br><span class="line">        <span class="string">'category'</span>: dict_tables[tables],</span><br><span class="line">        <span class="string">'start_page'</span>: start_page,</span><br><span class="line">        <span class="string">'end_page'</span>: end_page</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(category, page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        index_page(page)</span><br><span class="line">        <span class="comment"># parse_table() #测试print</span></span><br><span class="line">        df_table = parse_table()</span><br><span class="line">        write_to_file(df_table, category)</span><br><span class="line">        print(<span class="string">'第 %s 页抓取完成'</span> % page)</span><br><span class="line">        print(<span class="string">'--------------'</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'网页爬取失败，请检查网页中表格内容是否存在'</span>)</span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> set_table():</span><br><span class="line">        <span class="comment"># url = i.get('url')</span></span><br><span class="line">        category = i.get(<span class="string">'category'</span>)</span><br><span class="line">        start_page = i.get(<span class="string">'start_page'</span>)</span><br><span class="line">        end_page = i.get(<span class="string">'end_page'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(start_page, end_page):</span><br><span class="line">        <span class="comment"># for page in range(44,pageall+1): # 如果下载中断，可以尝试手动更改网页继续下载</span></span><br><span class="line">        main(category, page)</span><br><span class="line">    print(<span class="string">'全部抓取完成'</span>)</span><br></pre></td></tr></table></figure><p>这里，我下载了所有上市公司的部分报表。</p><p>2018年中报业绩报表：</p><p><img src="http://media.makcyun.top/18-10-6/90979397.jpg" alt=""></p><p>2017年报的利润表：</p><p><img src="http://media.makcyun.top/18-10-6/66373311.jpg" alt=""></p><p>如果你想下载更多的报表，可以使用文中的代码，代码和素材资源可以在下面的链接中获取：</p><p><a href="https://github.com/makcyun/eastmoney_spider" target="_blank" rel="noopener">https://github.com/makcyun/eastmoney_spider</a></p><p>另外，爬虫还可以再完善一下，比如增加爬取上市公司的公告信息，设置可以爬任意一家（数家/行业）的公司数据而不用全部。</p><p>还有一个问题是，Selenium爬取的速度很慢而且很占用内存，建议尽量先尝试采用Requests请求的方法，抓不到的时候再考虑这个。文章开头在进行网页分析的时候，我们初步分析了表格JS的请求数据，是否能从该请求中找到我们需要的表格数据呢？ 后续文章，我们换一个思路再来尝试爬取一次。</p><p>本文完。</p><p><img src="http://media.makcyun.top/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;利用Selenium爬取东方财富网各上市公司历年的财务报表数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Pyhton可视化(3): 中国66家环保股上市公司市值Top20强</title>
    <link href="https://www.makcyun.top/2018/09/20/Python_visualization03.html"/>
    <id>https://www.makcyun.top/2018/09/20/Python_visualization03.html</id>
    <published>2018-09-20T05:27:22.000Z</published>
    <updated>2019-02-27T08:43:57.199Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Tushare包提取中国环保股上市公司近10年市值排名，并结合D3.js做动态数据可视化表。</p><a id="more"></a><p><strong>摘要：</strong> 之前介绍过Tushare包和D3.js可视化动态表格，本文将二者进行结合，制作中国上市公司环境保护股Top20强，近十年的市值变化动态表。</p><p>制作近10年期间，环保板块市值最高的20只股票动态变化，需要获得各只股票在不同年份的市值。获取特定股票的市值可以利用<code>pro.daily_basic</code>接口获取到每日的市值，然后利用Resample函数获得年均市值。但获取环保板块所有几十只股票的数据，用手动输入股票代码就不是很方便，此时，可以利用该包另外一个接口<code>ts.get_stock_basics()接口</code>获取所有股票基本数据，该接口能够返回股票代码、行业类别等数据。两个接口合二为一就可以提取出所需的数据，下面开始详细实现步骤。</p><h2 id="1-提取所有股票代码"><a href="#1-提取所有股票代码" class="headerlink" title="1. 提取所有股票代码"></a>1. 提取所有股票代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="comment"># 获取所有股票列表</span></span><br><span class="line">data = ts.get_stock_basics()</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 返回数据如下，所有列值可以参考：http://tushare.org/fundamental.html</span></span><br><span class="line">         name industry area      pe  outstanding  totals  totalAssets  \</span><br><span class="line">code                                                                    </span><br><span class="line"><span class="number">002936</span>    N郑银       银行   河南    <span class="number">8.27</span>         <span class="number">6.00</span>   <span class="number">59.22</span>  <span class="number">44363604.00</span>   </span><br><span class="line"><span class="number">600856</span>   中天能源     供气供热   吉林   <span class="number">21.28</span>        <span class="number">13.43</span>   <span class="number">13.67</span>   <span class="number">1712831.63</span>   </span><br><span class="line"><span class="number">300021</span>   大禹节水     农业综合   甘肃   <span class="number">35.27</span>         <span class="number">6.48</span>    <span class="number">7.97</span>    <span class="number">359294.91</span>   </span><br><span class="line"><span class="number">603111</span>   康尼机电     运输设备   江苏    <span class="number">0.00</span>         <span class="number">7.38</span>    <span class="number">9.93</span>    <span class="number">734670.69</span>   </span><br><span class="line"><span class="number">000498</span>   山东路桥     建筑施工   山东   <span class="number">19.52</span>         <span class="number">4.41</span>   <span class="number">11.20</span>   <span class="number">1926262.38</span></span><br></pre></td></tr></table></figure><p>可以看到，index是股票代码，name股票名称，industry是行业分类。我们需要获取环保类（可以获取任意行业类别，也可以全部获取所有股票，为了后期数据提取量耗时短一些，所以选择提取环保类股票）的股票代码和股票名称，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data = data[data.industry ==<span class="string">'环境保护'</span>]</span><br><span class="line">print(data.head()) <span class="comment">#返回的环保股数据</span></span><br><span class="line">print(<span class="string">'环保股股票数量为'</span>：len(data.industry)) <span class="comment">#计算环保股股票数量</span></span><br><span class="line">结果如下：</span><br><span class="line">        name industry area     pe  outstanding  totals  totalAssets  \</span><br><span class="line">code                                                                  </span><br><span class="line"><span class="number">300056</span>   三维丝     环境保护   福建   <span class="number">0.00</span>         <span class="number">2.37</span>    <span class="number">3.85</span>    <span class="number">266673.63</span>   </span><br><span class="line"><span class="number">002549</span>  凯美特气     环境保护   湖南  <span class="number">34.44</span>         <span class="number">6.20</span>    <span class="number">6.24</span>    <span class="number">122630.13</span>   </span><br><span class="line"><span class="number">300422</span>   博世科     环境保护   广西  <span class="number">19.22</span>         <span class="number">2.64</span>    <span class="number">3.56</span>    <span class="number">509822.44</span>   </span><br><span class="line"><span class="number">601330</span>  绿色动力     环境保护   深圳  <span class="number">59.83</span>         <span class="number">1.16</span>   <span class="number">11.61</span>    <span class="number">784969.25</span>   </span><br><span class="line"><span class="number">000820</span>  神雾节能     环境保护   辽宁   <span class="number">0.00</span>         <span class="number">2.88</span>    <span class="number">6.37</span>    <span class="number">284674.34</span>   </span><br><span class="line">环保股股票数量为： <span class="number">66</span></span><br></pre></td></tr></table></figure><p>可以看到，环境保护股一共有66只，下面我们将用这66只股票的代码和名称，输入到<code>pro.daily_basic()接口</code>中，获取每只股票的每日数据，其中包括每日市值。时间期限从2009年1月1日至2018年9月10日，共10年的逐日数据。</p><h2 id="2-提股票每日市值"><a href="#2-提股票每日市值" class="headerlink" title="2. 提股票每日市值"></a>2. 提股票每日市值</h2><p>每日基本指标的数据接口：<a href="https://tushare.pro/document/2?doc_id=32" target="_blank" rel="noopener">https://tushare.pro/document/2?doc_id=32</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pro = ts.pro_api()</span><br><span class="line">pro.daily_basic(ts_code=<span class="string">''</span>, trade_date=<span class="string">''</span>,start_date = <span class="string">''</span>,end_date = <span class="string">''</span>)</span><br><span class="line"><span class="comment"># ts_code是股票代码，格式为000002.SZ,可以为一只股票，也可以是列表组成的多支股票</span></span><br><span class="line"><span class="comment"># 后面三个是交易日期，可以为固定日期，也可以为一个时期，格式'20180919'</span></span><br></pre></td></tr></table></figure><p>该接口股票代码的格式是<code>000002.SZ</code>，而上面股票代码格式是：<code>000002</code>，没有带后缀.SZ，由此需要添加上，然后就可获取每只股票近10年的逐日市值数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'code2'</span>] = data.index</span><br><span class="line"><span class="comment"># apply方法添加.SZ后缀</span></span><br><span class="line">data[<span class="string">'code2'</span>] = data[<span class="string">'code2'</span>].apply(<span class="keyword">lambda</span> i:i+<span class="string">'.SZ'</span>)</span><br><span class="line">data = data.set_index([<span class="string">'code2'</span>])</span><br><span class="line"><span class="comment"># 将code和name转为dict，因为我们只需要表格中的代码和名称列</span></span><br><span class="line">data = data[<span class="string">'name'</span>]</span><br><span class="line">data = data.to_dict()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(data) #测试返回的环保股字典数据 ok</span></span><br><span class="line">&#123;<span class="string">'300056.SZ'</span>: <span class="string">'三维丝'</span>, <span class="string">'002549.SZ'</span>: <span class="string">'凯美特气'</span>, <span class="string">'300422.SZ'</span>: <span class="string">'博世科'</span>, <span class="string">'601330.SZ'</span>: <span class="string">'绿色动力'</span>, <span class="string">'000820.SZ'</span>: <span class="string">'神雾节能'</span>, <span class="string">'300072.SZ'</span>: <span class="string">'三聚环保'</span>, <span class="string">'300055.SZ'</span>: <span class="string">'万邦达'</span>, <span class="string">'002717.SZ'</span>: <span class="string">'岭南股份'</span>, <span class="string">'300070.SZ'</span>: <span class="string">'碧水源'</span>, <span class="string">'000504.SZ'</span>: <span class="string">'南华生物'</span>, <span class="string">'300203.SZ'</span>: <span class="string">'聚光科技'</span>, <span class="string">'002672.SZ'</span>: <span class="string">'东江环保'</span>, <span class="string">'000967.SZ'</span>: <span class="string">'盈峰环境'</span>, <span class="string">'002322.SZ'</span>: <span class="string">'理工环科'</span>, <span class="string">'300272.SZ'</span>: <span class="string">'开能健康'</span>, <span class="string">'300495.SZ'</span>: <span class="string">'美尚生态'</span>, <span class="string">'603717.SZ'</span>: <span class="string">'天域生态'</span>, <span class="string">'300266.SZ'</span>: <span class="string">'兴源环境'</span>, <span class="string">'603126.SZ'</span>: <span class="string">'中材节能'</span>, <span class="string">'002200.SZ'</span>: <span class="string">'云投生态'</span>, <span class="string">'300385.SZ'</span>: <span class="string">'雪浪环境'</span>, <span class="string">'603200.SZ'</span>: <span class="string">'上海洗霸'</span>, <span class="string">'000826.SZ'</span>: <span class="string">'启迪桑德'</span>, <span class="string">'300262.SZ'</span>: <span class="string">'巴安水务'</span>, <span class="string">'002887.SZ'</span>: <span class="string">'绿茵生态'</span>, <span class="string">'603568.SZ'</span>: <span class="string">'伟明环保'</span>, <span class="string">'300631.SZ'</span>: <span class="string">'久吾高科'</span>, <span class="string">'002616.SZ'</span>: <span class="string">'长青集团'</span>, <span class="string">'300156.SZ'</span>: <span class="string">'神雾环保'</span>, <span class="string">'000920.SZ'</span>: <span class="string">'南方汇通'</span>, <span class="string">'600008.SZ'</span>: <span class="string">'首创股份'</span>, <span class="string">'601200.SZ'</span>: <span class="string">'上海环境'</span>, <span class="string">'603955.SZ'</span>: <span class="string">'大千生态'</span>, <span class="string">'603177.SZ'</span>: <span class="string">'德创环保'</span>, <span class="string">'600481.SZ'</span>: <span class="string">'双良节能'</span>, <span class="string">'300190.SZ'</span>: <span class="string">'维尔利'</span>, <span class="string">'603588.SZ'</span>: <span class="string">'高能环境'</span>, <span class="string">'002034.SZ'</span>: <span class="string">'旺能环境'</span>, <span class="string">'603817.SZ'</span>: <span class="string">'海峡环保'</span>, <span class="string">'002499.SZ'</span>: <span class="string">'科林环保'</span>, <span class="string">'603822.SZ'</span>: <span class="string">'嘉澳环保'</span>, <span class="string">'300664.SZ'</span>: <span class="string">'鹏鹞环保'</span>, <span class="string">'300332.SZ'</span>: <span class="string">'天壕环境'</span>, <span class="string">'600526.SZ'</span>: <span class="string">'菲达环保'</span>, <span class="string">'600874.SZ'</span>: <span class="string">'创业环保'</span>, <span class="string">'600292.SZ'</span>: <span class="string">'远达环保'</span>, <span class="string">'603903.SZ'</span>: <span class="string">'中持股份'</span>, <span class="string">'300172.SZ'</span>: <span class="string">'中电环保'</span>, <span class="string">'000544.SZ'</span>: <span class="string">'中原环保'</span>, <span class="string">'300692.SZ'</span>: <span class="string">'中环环保'</span>, <span class="string">'600388.SZ'</span>: <span class="string">'龙净环保'</span>, <span class="string">'300425.SZ'</span>: <span class="string">'环能科技'</span>, <span class="string">'300388.SZ'</span>: <span class="string">'国祯环保'</span>, <span class="string">'300362.SZ'</span>: <span class="string">'天翔环境'</span>, <span class="string">'300197.SZ'</span>: <span class="string">'铁汉生态'</span>, <span class="string">'300187.SZ'</span>: <span class="string">'永清环保'</span>, <span class="string">'300090.SZ'</span>: <span class="string">'盛运环保'</span>, <span class="string">'002573.SZ'</span>: <span class="string">'清新环境'</span>, <span class="string">'000035.SZ'</span>: <span class="string">'中国天楹'</span>, <span class="string">'603797.SZ'</span>: <span class="string">'联泰环保'</span>, <span class="string">'603603.SZ'</span>: <span class="string">'博天环境'</span>, <span class="string">'300137.SZ'</span>: <span class="string">'先河环保'</span>, <span class="string">'300355.SZ'</span>: <span class="string">'蒙草生态'</span>, <span class="string">'300152.SZ'</span>: <span class="string">'科融环境'</span>, <span class="string">'002658.SZ'</span>: <span class="string">'雪迪龙'</span>, <span class="string">'600217.SZ'</span>: <span class="string">'中再资环'</span>&#125;</span><br></pre></td></tr></table></figure><p>可以看到，很完整地显示了环保股的股票代码和名称，下面通过for循环即可获取每日数据。为了方便，将上式代码命名为一个函数get_code()，return data 为上面的dict。</p><h2 id="3-提取环保股公司数据"><a href="#3-提取环保股公司数据" class="headerlink" title="3. 提取环保股公司数据"></a>3. 提取环保股公司数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ts_codes = get_code()</span><br><span class="line">start = <span class="string">'20090101'</span></span><br><span class="line">end = <span class="string">'201809010'</span></span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> ts_codes.items():</span><br><span class="line">data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) <span class="comment"># 获取每只股票时间段数据</span></span><br><span class="line">    <span class="comment"># 添加代码列和名称列</span></span><br><span class="line">    <span class="comment"># 替换掉末尾的.SZ,regex设置为true才行</span></span><br><span class="line">    data[<span class="string">'code'</span>] = data[<span class="string">'ts_code'</span>].replace(<span class="string">'.SZ'</span>,<span class="string">''</span>,regex = <span class="keyword">True</span>)</span><br><span class="line">    data[<span class="string">'name'</span>] = value</span><br><span class="line">    <span class="comment"># 存储结果</span></span><br><span class="line">    data.to_csv(<span class="string">'environment.csv'</span>,mode=<span class="string">'a'</span>,encoding = <span class="string">'utf_8_sig'</span>,index = <span class="keyword">False</span>,header = <span class="number">0</span>)</span><br><span class="line">    print(<span class="string">'数据提取完毕'</span>)</span><br></pre></td></tr></table></figure><p>表格结果如下，66只股票10年一共产生了75933行数据。如果提前全部3000多家股票的数据，那么数据量会达到几百万行，量太大，所以这里仅提取了66支。其中，选中的列为每日市值（万元）。下面就可以根据日期、市值得到各只股票每年的市值均值，然后绘制股票动态表。</p><p><img src="http://media.makcyun.top/18-9-20/15865243.jpg" alt=""></p><h2 id="4-绘制Top20强动态表"><a href="#4-绘制Top20强动态表" class="headerlink" title="4. 绘制Top20强动态表"></a>4. 绘制Top20强动态表</h2><p>首先读取上面的表格，获取DataFrame信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'environment.csv'</span>,encoding = <span class="string">'utf-8'</span>,converters = &#123;<span class="string">'code'</span>:str&#125;)</span><br><span class="line"><span class="comment"># converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示</span></span><br><span class="line">print(df.info())</span><br><span class="line"></span><br><span class="line">Data columns (total <span class="number">17</span> columns):</span><br><span class="line">ts_code          <span class="number">75932</span> non-null object</span><br><span class="line">trade_date       <span class="number">75932</span> non-null int64</span><br><span class="line">close            <span class="number">75932</span> non-null float64</span><br><span class="line">turnover_rate    <span class="number">75932</span> non-null float64</span><br><span class="line">volume_ratio     <span class="number">0</span> non-null float64</span><br><span class="line">pe               <span class="number">70861</span> non-null float64</span><br><span class="line">e_ttm            <span class="number">69254</span> non-null float64</span><br><span class="line">pb               <span class="number">73713</span> non-null float64</span><br><span class="line">ps               <span class="number">75932</span> non-null float64</span><br><span class="line">ps_ttm           <span class="number">75840</span> non-null float64</span><br><span class="line">total_share      <span class="number">75932</span> non-null float64</span><br><span class="line">float_share      <span class="number">75932</span> non-null float64</span><br><span class="line">free_share       <span class="number">75932</span> non-null float64</span><br><span class="line">total_mv         <span class="number">75932</span> non-null float64</span><br><span class="line">circ_mv          <span class="number">75932</span> non-null float64</span><br><span class="line">code             <span class="number">75932</span> non-null int64</span><br><span class="line">name             <span class="number">75932</span> non-null object</span><br><span class="line">dtypes: float64(<span class="number">13</span>), int64(<span class="number">2</span>), object(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>可以看到trade_date交易日期是整形，需将交易日期先转换为字符型再转换为datetime日期型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="comment"># trade_date是int型，需转为字符型</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = df[<span class="string">'trade_date'</span>].apply(str)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="comment"># df['trade_date'] = df['trade_date'].astype(str)</span></span><br><span class="line"><span class="comment"># 将object转为datatime</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = pd.to_datetime(df[<span class="string">'trade_date'</span>],format = <span class="string">'%Y%m%d'</span>,errors = <span class="string">'ignore'</span>) <span class="comment">#errors忽略无法转换的数据，不然会报错</span></span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">Data columns (total <span class="number">17</span> columns):</span><br><span class="line">ts_code          <span class="number">75932</span> non-null object</span><br><span class="line">trade_date       <span class="number">75932</span> non-null datetime64[ns]</span><br><span class="line">close            <span class="number">75932</span> non-null float64</span><br><span class="line">turnover_rate    <span class="number">75932</span> non-null float64</span><br><span class="line">volume_ratio     <span class="number">0</span> non-null float64</span><br><span class="line">pe               <span class="number">70861</span> non-null float64</span><br><span class="line">e_ttm            <span class="number">69254</span> non-null float64</span><br><span class="line">pb               <span class="number">73713</span> non-null float64</span><br><span class="line">ps               <span class="number">75932</span> non-null float64</span><br><span class="line">ps_ttm           <span class="number">75840</span> non-null float64</span><br><span class="line">total_share      <span class="number">75932</span> non-null float64</span><br><span class="line">float_share      <span class="number">75932</span> non-null float64</span><br><span class="line">free_share       <span class="number">75932</span> non-null float64</span><br><span class="line">total_mv         <span class="number">75932</span> non-null float64</span><br><span class="line">circ_mv          <span class="number">75932</span> non-null float64</span><br><span class="line">code             <span class="number">75932</span> non-null int64</span><br><span class="line">name             <span class="number">75932</span> non-null object</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置总市值数字格式由万元变为亿元</span></span><br><span class="line">df[<span class="string">'total_mv'</span>] = (df[<span class="string">'total_mv'</span>]/<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># 保留四列,并将交易日期设为index</span></span><br><span class="line">df = df[[<span class="string">'ts_code'</span>,<span class="string">'trade_date'</span>,<span class="string">'total_mv'</span>,<span class="string">'name'</span>]]</span><br><span class="line">df = df.set_index(<span class="string">'trade_date'</span>)</span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 结果如下:</span></span><br><span class="line">               ts_code   total_mv  name</span><br><span class="line">trade_date                            </span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-30</span>  <span class="number">300090.</span>SZ  <span class="number">36.034715</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-29</span>  <span class="number">300090.</span>SZ  <span class="number">38.014644</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-28</span>  <span class="number">300090.</span>SZ  <span class="number">39.202602</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-27</span>  <span class="number">300090.</span>SZ  <span class="number">40.126569</span>  盛运环保</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-24</span>  <span class="number">300090.</span>SZ  <span class="number">38.938611</span>  盛运环保</span><br></pre></td></tr></table></figure><p>接下来，求出每只股票每年的市值平均值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">求平均市值时需切片同一股票，这里股票名称切片赋值为value变量，也就是dict字典里<span class="number">66</span>只股票名称</span><br><span class="line">df = df[df.name == value]</span><br><span class="line"><span class="comment"># 不能用query方法,会报错 df = df.query('name == value')</span></span><br><span class="line"><span class="comment"># resampe按年统计数据</span></span><br><span class="line">df = df.resample(<span class="string">'AS'</span>).mean()  <span class="comment">#年平均市值</span></span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">              total_mv code</span><br><span class="line">trade_date                 </span><br><span class="line"><span class="number">2009</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">25.184678</span>  三维丝</span><br><span class="line"><span class="number">2010</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">50.672849</span>  三维丝</span><br><span class="line"><span class="number">2011</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">46.488004</span>  三维丝</span><br><span class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">39.214508</span>  三维丝</span><br><span class="line"><span class="number">2013</span><span class="number">-01</span><span class="number">-01</span>   <span class="number">59.110332</span>  三维丝</span><br><span class="line"><span class="comment"># 再用to_period按年显示市值数据</span></span><br><span class="line">df = df.to_period(<span class="string">'A'</span>)</span><br><span class="line">print(df.head())</span><br><span class="line"><span class="comment"># 结果如下:</span></span><br><span class="line">                  total_mv code</span><br><span class="line">trade_date                 </span><br><span class="line"><span class="number">2009</span>         <span class="number">25.184678</span>  三维丝</span><br><span class="line"><span class="number">2010</span>         <span class="number">50.672849</span>  三维丝</span><br><span class="line"><span class="number">2011</span>         <span class="number">46.488004</span>  三维丝</span><br><span class="line"><span class="number">2012</span>         <span class="number">39.214508</span>  三维丝</span><br><span class="line"><span class="number">2013</span>         <span class="number">59.110332</span>  三维丝</span><br></pre></td></tr></table></figure><p>经过以上处理，基本就获得了想要的数据。为了能够满足D3.js模板表格条件，再做一点修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加code列</span></span><br><span class="line">df[<span class="string">'code'</span>] = value</span><br><span class="line"><span class="comment"># 重置index</span></span><br><span class="line">df = df.reset_index()</span><br><span class="line"><span class="comment"># 重命名为d3.js格式</span></span><br><span class="line"><span class="comment"># 增加一列空type</span></span><br><span class="line">df[<span class="string">'type'</span>] = <span class="string">''</span></span><br><span class="line">df = df[[<span class="string">'code'</span>,<span class="string">'type'</span>,<span class="string">'total_mv'</span>,<span class="string">'trade_date'</span>]]</span><br><span class="line">df.rename(columns = &#123;<span class="string">'code'</span>:<span class="string">'name'</span>,<span class="string">'total_mv'</span>:<span class="string">'value'</span>,<span class="string">'type'</span>:<span class="string">'type'</span>,<span class="string">'trade_date'</span>:<span class="string">'date'</span>&#125;)</span><br><span class="line"><span class="comment"># df.to_csv('parse_environment.csv',mode='a',encoding = 'utf_8_sig',index = False,float_format = '%.1f',header = 0)   # float_format = '%.1f' #设置输出浮点数格式为1位小数</span></span><br></pre></td></tr></table></figure><p>最终，生成parse_environment.csv文件如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">nametypevaluedate</span><br><span class="line">中国天楹<span class="number">8.8</span><span class="number">2009</span></span><br><span class="line">中国天楹<span class="number">9.8</span><span class="number">2010</span></span><br><span class="line">中国天楹<span class="number">15</span><span class="number">2011</span></span><br><span class="line">中国天楹<span class="number">18.8</span><span class="number">2012</span></span><br><span class="line">中国天楹<span class="number">22.5</span><span class="number">2013</span></span><br><span class="line">...</span><br><span class="line">东方园林<span class="number">177.1</span><span class="number">2014</span></span><br><span class="line">东方园林<span class="number">320.5</span><span class="number">2015</span></span><br><span class="line">东方园林<span class="number">288.4</span><span class="number">2016</span></span><br><span class="line">东方园林<span class="number">481</span><span class="number">2017</span></span><br><span class="line">东方园林<span class="number">461.5</span><span class="number">2018</span></span><br></pre></td></tr></table></figure><p>可绘制出动态可视化表格，见下面视频。可以看到前几年市值龙头由东方园林、碧桂园轮流坐庄，近两年三聚环保强势崛起，市值增长迅猛，跃居头名。</p><p><a href="https://www.bilibili.com/video/av32087716/" target="_blank" rel="noopener">https://www.bilibili.com/video/av32087716/</a></p><p>本文仅对比了环保股企业的市值变化，你还可以分析互联网股、金融股等100多种行业的企业市值对比。另外，Tushare包返回的参数还可以做更多其他分析。</p><p>文章完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">ts.set_token(<span class="string">'404ba015bd44c01cf09c8183dcd89bb9b25749057ff72b5f8671b9e6'</span>)</span><br><span class="line">pro = ts.pro_api()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_code</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 所有股票列表</span></span><br><span class="line">    data = ts.get_stock_basics()</span><br><span class="line">    <span class="comment"># data = data.query('industry == "环境保护"')</span></span><br><span class="line">    <span class="comment"># 或者</span></span><br><span class="line">    data = data[data.industry ==<span class="string">'环境保护'</span>]</span><br><span class="line">    <span class="comment"># 提取股票代码code并转化为list</span></span><br><span class="line">    data[<span class="string">'code2'</span>] = data.index</span><br><span class="line">    <span class="comment"># apply方法添加.SZ后缀</span></span><br><span class="line">    data[<span class="string">'code2'</span>] = data[<span class="string">'code2'</span>].apply(<span class="keyword">lambda</span> i:i+<span class="string">'.SZ'</span>)</span><br><span class="line">    data = data.set_index([<span class="string">'code2'</span>])</span><br><span class="line">    <span class="comment"># 将code和name转为dict</span></span><br><span class="line">    data = data[<span class="string">'name'</span>]</span><br><span class="line">    data = data.to_dict()</span><br><span class="line">    <span class="comment"># 增加东方园林</span></span><br><span class="line">    data[<span class="string">'002310.SZ'</span>] = <span class="string">'东方园林'</span></span><br><span class="line">    <span class="comment"># print(data) #测试返回的环保股dict ok</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stock</span><span class="params">(key,start,end,value)</span>:</span></span><br><span class="line">    data = pro.daily_basic(ts_code=key, start_date=start, end_date=end) <span class="comment"># 获取每只股票时间段数据</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 替换掉末尾的.SZ,regex设置为true才行</span></span><br><span class="line">    data[<span class="string">'code'</span>] = data[<span class="string">'ts_code'</span>].replace(<span class="string">'.SZ'</span>,<span class="string">''</span>,regex = <span class="keyword">True</span>)</span><br><span class="line">    data[<span class="string">'name'</span>] = value</span><br><span class="line">    <span class="comment"># print(data)</span></span><br><span class="line">    data.to_csv(<span class="string">'environment.csv'</span>,mode=<span class="string">'a'</span>,encoding = <span class="string">'utf_8_sig'</span>,index = <span class="keyword">False</span>,header = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_code</span><span class="params">()</span>:</span></span><br><span class="line">    df = pd.read_csv(<span class="string">'environment.csv'</span>,encoding = <span class="string">'utf-8'</span>,converters = &#123;<span class="string">'code'</span>:str&#125;)</span><br><span class="line">    <span class="comment"># converters = &#123;'code':str&#125; 将数字前面不显示的0转为str显示</span></span><br><span class="line">    df.columns = [<span class="string">'ts_code'</span>,<span class="string">'trade_date'</span>,<span class="string">'close'</span>,<span class="string">'turnover_rate'</span>,<span class="string">'volume_ratio'</span>,<span class="string">'pe'</span>,<span class="string">'e_ttm'</span>,<span class="string">'pb'</span>,<span class="string">'ps'</span>,<span class="string">'ps_ttm'</span>,<span class="string">'total_share'</span>,<span class="string">'float_share'</span>,<span class="string">'free_share'</span>,<span class="string">'total_mv'</span>,<span class="string">'circ_mv'</span>, <span class="string">'code'</span>,<span class="string">'name'</span>]</span><br><span class="line">    <span class="comment"># trade_date是int型，需转为字符型</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = df[<span class="string">'trade_date'</span>].apply(str)</span><br><span class="line"><span class="comment"># 或者df['trade_date'] = df['trade_date'].astype(str)</span></span><br><span class="line"><span class="comment"># 将object转为datatime</span></span><br><span class="line">df[<span class="string">'trade_date'</span>] = pd.to_datetime(df[<span class="string">'trade_date'</span>],format = <span class="string">'%Y%m%d'</span>,errors = <span class="string">'ignore'</span>) <span class="comment">#errors忽略无法转换的数据，不然会报错</span></span><br><span class="line">    <span class="comment">## 设置总市值数字格式由万元变为亿元</span></span><br><span class="line">    df[<span class="string">'total_mv'</span>] = (df[<span class="string">'total_mv'</span>]/<span class="number">10000</span>)</span><br><span class="line">    <span class="comment"># 保留四列,并将交易日期设为index</span></span><br><span class="line">    df = df[[<span class="string">'ts_code'</span>,<span class="string">'trade_date'</span>,<span class="string">'total_mv'</span>,<span class="string">'name'</span>]]</span><br><span class="line">    df = df.set_index(<span class="string">'trade_date'</span>)</span><br><span class="line"></span><br><span class="line">    df = df[df.name == value]</span><br><span class="line">    <span class="comment"># # 不能用query方法</span></span><br><span class="line">    <span class="comment"># # df = df.query('name == ')</span></span><br><span class="line">    df = df.resample(<span class="string">'AS'</span>).mean()/<span class="number">10000</span>  <span class="comment">#年平均市值</span></span><br><span class="line">    df = df.to_period(<span class="string">'A'</span>)</span><br><span class="line">    <span class="comment"># # 增加code列</span></span><br><span class="line">    df[<span class="string">'code'</span>] = value</span><br><span class="line">    <span class="comment"># # 重置index</span></span><br><span class="line">    df = df.reset_index()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重命名为d3.js格式</span></span><br><span class="line">    <span class="comment"># 增加一列空type</span></span><br><span class="line">    df[<span class="string">'type'</span>] = <span class="string">''</span></span><br><span class="line">    df = df[[<span class="string">'code'</span>,<span class="string">'type'</span>,<span class="string">'total_mv'</span>,<span class="string">'trade_date'</span>]]</span><br><span class="line">    df.rename(columns = &#123;<span class="string">'code'</span>:<span class="string">'name'</span>,<span class="string">'total_mv'</span>:<span class="string">'value'</span>,<span class="string">'type'</span>:<span class="string">'type'</span>,<span class="string">'trade_date'</span>:<span class="string">'date'</span>&#125;)</span><br><span class="line">    df.to_csv(<span class="string">'parse_environment.csv'</span>,mode=<span class="string">'a'</span>,encoding = <span class="string">'utf_8_sig'</span>,index = <span class="keyword">False</span>,float_format = <span class="string">'%.1f'</span>,header = <span class="number">0</span>)</span><br><span class="line">    float_format = <span class="string">'%.1f'</span> <span class="comment">#设置输出浮点数格式</span></span><br><span class="line">    <span class="comment"># print(df)</span></span><br><span class="line">    <span class="comment"># print(df.info())</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># get_code()  #提取环保股dict</span></span><br><span class="line">    start = <span class="string">'20090101'</span></span><br><span class="line">    end = <span class="string">'201809010'</span></span><br><span class="line">    ts_codes = get_code()</span><br><span class="line">    <span class="comment"># dict_values转list</span></span><br><span class="line">    keys = list(ts_codes.keys())</span><br><span class="line">    values = list(ts_codes.values())</span><br><span class="line">    <span class="keyword">for</span> key,value <span class="keyword">in</span> ts_codes.items():</span><br><span class="line">        stock(key,start,end,value)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> values:</span><br><span class="line">        parse_code(value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>文件素材可以在这里获得：</p><p><a href="https://github.com/makcyun/web_scraping_with_python/tree/master/Tushare" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python/tree/master/Tushare</a></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Tushare包提取中国环保股上市公司近10年市值排名，并结合D3.js做动态数据可视化表。&lt;/p&gt;
    
    </summary>
    
      <category term="Python可视化" scheme="https://www.makcyun.top/categories/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>我的白血病妻子</title>
    <link href="https://www.makcyun.top/2018/09/15/life01.html"/>
    <id>https://www.makcyun.top/2018/09/15/life01.html</id>
    <published>2018-09-15T11:27:22.000Z</published>
    <updated>2018-12-19T03:12:32.974Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p><b><em>世界骨髓捐献者日。</em></b><br><a id="more"></a></p><p><img src="http://media.makcyun.top/18-9-15/81039212.jpg" alt=""></p><p>当夜晚来临，人们结束一天的忙碌，正赶着回家之际，我的一天才刚刚开始。守夜时间是晚七时至凌晨七时，我称之为”深夜医院”。你问我有没有人，我会说人还挺少的。</p><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>这是一篇关于一个年轻的家庭和白血病作斗争的生活随笔。<br><strong>人物介绍</strong>：我，苏克，27岁。妻子，”23”，25岁。儿子，馍馍，2岁半。<br><strong>时间</strong> ： 2018年5月至今<br><strong>地点</strong> ：东莞—广州—北京</p><h2 id="2-天使与恶魔"><a href="#2-天使与恶魔" class="headerlink" title="2. 天使与恶魔"></a>2. 天使与恶魔</h2><p><em>5月18日</em><br>23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”<br>我：”老婆，太棒了，我就说你可以的，你都找不到工作，那很多人更找不到了！”</p><p>23：”一直说想换工作，这次终于是跨出这一步了，很紧张啊！”<br>我：”不要紧张，你肯定能很快适应新工作的。”</p><p>23：”好吧，我答应 HR 6月1号去签合同，再上几天班就把年假休了，放松一下。”<br>我：”对啊，好好休息休息，我要是也有年假，就陪你出去玩了。”</p><p>23：”你在深圳，周末能有空回来就不错了…”<br>我：”以后，回东莞来就有时间了…”</p><p><em>一周后 5月25日 周五</em><br>23：”老公，我住院了。”<br>我：”啊？怎么回事？”</p><p>23：” 背痛了两天，痛地不行了，还发烧…”<br>我：”还是没有缓解啊，我刚下班，现在从梅林关坐大巴回来，到了马上去医院…”</p><p><em>一个多小时后…</em><br>我：”怎么样？医生怎么说?”<br>23：”医生看了血常规，说白细胞很高(29万多，正常是1万以内)，让我转到血液科来了，现在打消炎针退烧…”<br>我：”希望没大碍，早点退烧吧，这几天晚上我陪你。”</p><p><em>半夜…</em><br>23：”老公，我很热，出了好多汗，衣服全湿了…”<br>我：”我给你换，你额头不烫了，多出点汗烧就退下来了。”<br>23：”我背好疼…”<br>我：”忍受不了的痛么？我刚去找了医生，医生说忍一忍，坚强一点。”</p><p><em>3天后 周一</em><br>我：” 老婆，我得赶去深圳上班了，希望你这周能退烧。”<br>23：”我也希望，我这周五还要去公司签劳动合同呢。”</p><p><em>6月1日 周五</em><br>我：”老婆，今天儿童节，我在网上买了好多书给儿子，以后周末回来给他讲故事，争取做个会讲故事的好爸爸。”<br>23：”好，你快回来吧，我这几天反复高烧到40度，医生也找不到原因，让做骨穿还建议转院去广州”<br>我：”啊？太奇怪了，这次发烧怎么这么顽固，骨穿是做什么？我马上坐车回来…”</p><p><em>6月2日 转院到广州一家医院的血液科</em><br>当地医院的医生找不到病因，让我们转院到广州去。我一直不明白，发烧为什么要住到血液科病区。到了之后，医生给23抽了骨髓，说是查找病因。</p><p><em>一天后</em><br>医生单独跟我我和家里人说：”确诊病人患了急性淋巴细胞白血病，之前反复高烧和骨骼疼痛是这种病的常见临床表现。这种病得马上开始化疗，不然会有生命危险。” 听完，我一时没有反应过来。</p><p>后来了解到，如果一切顺利的话，治疗和康复时长大概要2年，治疗费用在100万左右；如果不顺利，生命可能只剩不到2年。</p><p>我突然明白了：原来这不是普通地发烧和背疼啊。不过，”白血病是什么病? 严重么？好治么？”</p><p>23那几天一直问：”检测结果有没有出来？到底是怎么回事？”</p><p>我说：”医生说需要十几天才能检测出来，别着急，这边的医生肯定会把你治好的。”</p><p>我偷偷百度了下，大致了解到白血病俗称”血癌”，是癌症的一种。后面一直在想，为什么23会得上这个病？<br>不管怎样，首先要做好长期住院治疗的准备，那么得尽快辞职了。</p><p><em>6月8日</em><br>我回到深圳，火速辞了职、退了租房。</p><p>晚上，关了灯，望着窗外灯火通明的深圳，看着这个刚刚来了3个月的城市。各种思绪开始涌上心头：23会很顺利地治好病么？未来的生活会是怎样？2年后进入社会我们还有竞争力么？很多刚毕业的人感叹生活艰难，一出来就要变成”房奴”。那我，如果不靠父母亲友，岂不是要变成”病奴”了？</p><p>想了好一会儿却什么没想出来，那么就顺其自然、虔诚祈祷吧。转而，开始回忆起这几年的走过的点点滴滴，想从曾面对过的种种困难中找到信心。</p><hr><p><br></p><h2 id="3-时光倒流"><a href="#3-时光倒流" class="headerlink" title="3. 时光倒流"></a>3. 时光倒流</h2><h3 id="3-1-2014年-浪漫邂逅"><a href="#3-1-2014年-浪漫邂逅" class="headerlink" title="3.1. 2014年-浪漫邂逅"></a>3.1. 2014年-浪漫邂逅</h3><p>2014年夏天，我本科毕业，从成都开启了筹划已久的70天毕业旅行。对于一个在新疆长大的内地人来说，22、3岁才第一次见到大海，那是一种难以言表的心情。整个旅途，我都尽可能地享受和体验每一处风土人情。</p><p><img src="http://media.makcyun.top/18-9-14/12369483.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-9-13/50707742.jpg" alt=""></p><p>70天，不知不觉就快要结束，来到了最后一站：南亚岛国斯里兰卡。在一个青旅里我第一次见到了23。那种感觉用一个词形容就够了：”一见钟情”。聊了几次天之后，得知她在广州念大三，来斯里兰卡是到一个孤儿院做义工。</p><p><img src="http://media.makcyun.top/18-9-14/40605269.jpg" alt=""></p><p>一周后，我到了她所在的城市”加勒”——一座濒临印度洋的小城镇。距离回国倒数的那几天里，我用一顿大盘鸡把她”收买”了。后来，我认为这是我人生中最重要的决定之一。</p><p><img src="http://media.makcyun.top/18-9-13/47037291.jpg" alt=""></p><h3 id="3-2-2015年-约定终身"><a href="#3-2-2015年-约定终身" class="headerlink" title="3.2. 2015年-约定终身"></a>3.2. 2015年-约定终身</h3><p>回国后，我们开始异地，我在成都读研，她在广州读大四。这一年，我们的感情状况可以用”打飞的”来形容，她常来成都，我也常过去，还参加了她的毕业典礼。</p><p>最终，我们决定在8月，认识的一周年的那天领证结婚。婚礼仪式很简单，仅是请亲人朋友吃了一顿饭。</p><h3 id="3-3-2016年-馍馍出生"><a href="#3-3-2016年-馍馍出生" class="headerlink" title="3.3. 2016年-馍馍出生"></a>3.3. 2016年-馍馍出生</h3><p>2016年过年期间，感谢伟大的妈妈，把儿子馍馍带到了这个世界上。</p><p>坦诚地说，那种感觉是惊喜与压力并存。我们俩都没有什么经济来源，只能靠家里”救济”，我仅能每个月回来几天。</p><p><img src="http://media.makcyun.top/18-9-14/48590797.jpg" alt=""></p><h3 id="3-4-2017年-毕业工作"><a href="#3-4-2017年-毕业工作" class="headerlink" title="3.4. 2017年-毕业工作"></a>3.4. 2017年-毕业工作</h3><p>就这样，异地状态维持了近3年，我终于毕业，可以回去抱娃和工作。曾经，八卦地转发过一篇文章给23说：”居然有人带娃参加毕业典礼”，没想到，说的也算是自己啊。</p><p>家也从西北搬到了东南，走过这一路，仅仅花了多数人二到三分之一的时间。</p><p><img src="http://media.makcyun.top/18-9-15/54207175.jpg" alt=""></p><h3 id="3-5-2018年-再次异地"><a href="#3-5-2018年-再次异地" class="headerlink" title="3.5. 2018年-再次异地"></a>3.5. 2018年-再次异地</h3><p>在东莞工作了半年，2018年春节后，我决定还是去隔壁更大的深圳看看。<br>4月的某一天，我和23说：”要不你还是从政府出来吧，趁还年轻换个更有活力和挑战的工作环境。”<br>23：”是啊，为了馍馍在里面待了3年，我也想出来了。”<br>我：”那我们就开始找找看吧。”</p><p><em>一个月后的5月18日</em><br>23：”老公，我拿到offer了！哎，找了一个月，终于找到工作了！”(回到了文章的开头）</p><p>窗外的霓虹灯渐渐地黯淡下来，发现已经半夜，赶紧睡吧，明天得赶去广州的医院，要开始新的人生节奏了。</p><hr><p><br></p><h2 id="4-难熬的化疗"><a href="#4-难熬的化疗" class="headerlink" title="4. 难熬的化疗"></a>4. 难熬的化疗</h2><h3 id="4-1-治疗过程"><a href="#4-1-治疗过程" class="headerlink" title="4.1. 治疗过程"></a>4.1. 治疗过程</h3><p>6月2日，23在广州医院住下来，仍然在发烧并伴随强烈的骨痛。前3天体温持续超过38度，半夜最高烧到了40.3度。</p><p><img src="http://media.makcyun.top/18-9-14/13426921.jpg" alt=""></p><p>6月4日下午，瞒着23马上开始进行了化疗药物注射。说起化疗，我脑子里最先想到的是赵本山和范伟演过的一个小品，里面戏称化疗为”谈话治疗”。可实际中的化疗远比这难受地多。</p><p><img src="http://media.makcyun.top/18-9-15/4399129.jpg" alt=""></p><p>23对药物反应特别大，不停呕吐、浑身乏力、也没有任何食欲，体重开始暴瘦。连续打了三天化疗针以后，总算压制住了体内的恶性白细胞，23不再发烧和骨痛，整个人才渐渐恢复精神，开始有胃口喝粥和下床活动一下。随着在病区看到和听到医生、护士、病友关于白血病的治疗和讲解，23慢慢接受了自己得病的事实。化疗的第14天，出现了掉头发现象后，她同意剪发，过肩的长发被剪成了齐耳短发。好在，她很看得开，说：”这样很方便，至少出汗不用那么难受了”。</p><p>没有料到，头发脱落的速度是那么地快。仅仅2天过后，头发稍微一抓就是一把，整个床上和地下到处都是，打扫卫生的阿姨说”快点剪了吧，打扫卫生不方便”。坚持了几天后，在出院的那一天，给23彻底剪光了所有的头发。她依然很释怀，还在朋友圈让大家推荐戴哪个假发好，我心里感到很意外。</p><p>在病房，23每天要经历的三件事就是输液、抽血和用药。</p><p>大部分的时间都是在输液，各种各样的药物通过一袋袋的氯化钠和葡萄糖注射进23的血液里，有时是单手臂，有时双手都要打。同时，几乎每天要扎针抽血，各种颜色的抽血管，到后期手背已全部瘀青。至于服药，就像一日三餐一样稀松平常。就这样，每一天都仿佛在重复着昨天。</p><p><img src="http://media.makcyun.top/18-9-15/31936040.jpg" alt=""></p><p>整个疗程22天下来，一共输入了114袋液体，平均每天5袋，重达23200 ml，大致和一个5岁的小孩体重差不多。一共抽了81管血液，平均每天4管。注射和服用各种各样的药物加起来，有60种之多。化疗两周后，身高168 cm 的23体重降到了79斤，搀扶她上厕所，摸到的都是骨骼。多数女生喜欢越瘦越好，瘦到这种程度可能就不喜欢了吧。</p><p><img src="http://media.makcyun.top/18-9-15/84469243.jpg" alt=""></p><h3 id="4-2-23的感受"><a href="#4-2-23的感受" class="headerlink" title="4.2. 23的感受"></a>4.2. 23的感受</h3><p>尽管生理和心理都遭受着从未体验过的艰难考验，23的表现大大超出了我的心理预估。不难受的时候，她会发一些朋友圈，每个人看到都会情不自禁地被感动到。<br><img src="http://media.makcyun.top/18-9-15/98531897.jpg" alt=""></p><p>要说，会让23难受的一件事，那就是见不到儿子。从他出生下来至今，从来没有这么多天看不到他。所以，每次和馍馍视频，她都特别开心。</p><p>有一天晚上，打电话回家，发现突然找不到馍馍，23急得要命。后来，终于在一个小剧场找到了他，原来和小伙伴在一起。23想哭又有点生气，训了他几句。之后，我妈说原来儿子是想跟着小伙伴的妈妈去进剧场看节目，但他没有票就赖在那里不想走。他在那里哭着说：”我妈妈在就好了，妈妈在就会给我买票。”</p><p>听完23就哭了，我也鼻头一酸，2岁半的他竟然会讲出这样的话。</p><p><em>6月23日</em><br>住了3周院后，23终于能踏出病区，呼吸到外面的空气。回家的路上，她一直看着车窗外，没有说一句话。回到家后第一件事就是抱馍馍，馍馍居然很自然地接受了光头的妈妈。回到房间之后，23突然大哭了一场，吓了我一跳，急忙问怎么了，她说：”活着真好。”</p><h3 id="4-3-陪伴"><a href="#4-3-陪伴" class="headerlink" title="4.3. 陪伴"></a>4.3. 陪伴</h3><p>我一直都是一个大老粗，不怎么会照顾人，更不用说病人。好在，家里人给予了非常强力的支持。在23身边，做的最多的仅仅是陪伴。<br><img src="http://media.makcyun.top/18-9-15/17625322.jpg" alt=""></p><hr><p><br></p><h2 id="5-关于白血病"><a href="#5-关于白血病" class="headerlink" title="5. 关于白血病"></a>5. 关于白血病</h2><h3 id="5-1-什么是白血病"><a href="#5-1-什么是白血病" class="headerlink" title="5.1. 什么是白血病"></a>5.1. 什么是白血病</h3><p>在此之前，我的医学常识差到连感冒、发烧该吃什么药都不太清楚。白血病？我甚至有百度过：是不是血液是白色的病？(最好不要去百度)</p><p>在医院住久了之后，慢慢地开始对这个病有所了解。</p><blockquote><p><strong>白血病</strong>，是一种骨髓中的白细胞大量、异常增生的癌症。这些白细胞会破坏骨髓的造血功能，转而取代正常细胞。初期会导致病人出现发烧、贫血、感染等症状，具体发病原因至今没有完全弄清楚。</p></blockquote><p>虽然是癌症，但受益于科技发展，目前白血病是可以根治的。5年的生存率总体在50%以上，儿童高于成年人，且不同类型的白血病存活率也会不一样。根据细胞类型和发病程度，可将白血病分为四种主要类型：</p><table><thead><tr><th style="text-align:center">细胞类型</th><th style="text-align:center">急性</th><th style="text-align:center">慢性</th></tr></thead><tbody><tr><td style="text-align:center">淋巴细胞白血病</td><td style="text-align:center">急性淋巴细胞白血病</td><td style="text-align:center">慢性淋巴细胞白血病</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">（ALL）</td><td style="text-align:center">（CLL ）</td></tr><tr><td style="text-align:center">髓细胞白血病</td><td style="text-align:center">急性髓性白血病</td><td style="text-align:center">慢性粒细胞白血病</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">（AML ）</td><td style="text-align:center">（CML）</td></tr></tbody></table><p>儿童中，四分之三患上的是ALL；成人中，AML 和CLL 最为常见，《我不是药神》里面说的是CML。23患上的是ALL，并且属于其中的高危型，存活率为50%。ALL接受造血干细胞移植且5年内没有复发的话，之后终身基本上就不会再复发，也就是会和正常人一样。</p><p>普通个体患这种病的概率是十万分之1-1.5，所以很多人对这个病比较陌生，觉得离自己很遥远。其实，不用觉得陌生和遥远。像宋庆龄、肯德基创始人、居里夫人等这些名人就是罹患这种病而逝世的。根据中国红十字会统计，<a href="https://new.crcf.org.cn/html/2012-07/1733.html" target="_blank" rel="noopener">2012年中国内地有400万患者，并且每年新增4万人。</a></p><h3 id="5-2-治疗方法"><a href="#5-2-治疗方法" class="headerlink" title="5.2. 治疗方法"></a>5.2. 治疗方法</h3><p>白血病的治疗通常是采用：化疗、放疗、干细胞移植中的一种或者组合。以23所属的ALL高危类型来说，治疗主要分为三个阶段：化疗缓解、造血干细胞移植和排异控制和恢复。</p><h4 id="5-2-1-化疗"><a href="#5-2-1-化疗" class="headerlink" title="5.2.1. 化疗"></a>5.2.1. 化疗</h4><p>刚患病时，骨髓中会产生超出常人数十倍的白细胞，这么多的白细胞需要采用高强度的化疗进行治疗。最终将骨髓中的白血病原始细胞减少至&lt;5％，并从血液中清除肿瘤细胞。这个阶段的治疗通常需要3-6个疗程，然后为下一个阶段做准备。</p><h4 id="5-2-2-造血干细胞捐献移植"><a href="#5-2-2-造血干细胞捐献移植" class="headerlink" title="5.2.2. 造血干细胞捐献移植"></a>5.2.2. 造血干细胞捐献移植</h4><p>干细胞具有自我更新、繁殖并能分化成不同种类的成熟细胞的能力。造血干细胞是一群未分化的血液细胞，可以制造运输氧气的红血球、帮助凝血的血小板、抵抗感染的白血球等。所以，造血干细胞可用来治疗许多包括白血病在内的血液疾病、肿瘤等。它存在于骨髓、婴儿脐带血、以及成人周边血液中。</p><p>不少人会认为捐献造血干细胞就是抽取骨髓，而抽取骨髓会有很大风险甚至会残疾。所以常出现志愿者反悔的情形，给出的理由常常是”父母不同意”、”儿女不同意”，甚至是”丈母娘不同意”、”领导不同意”。其实，骨髓捐献造血干细胞已经是很久以前的方法了，现在90%以上的捐献者采用的捐献周边血干细胞。其实，这就是一个加强版的抽血过程，血液中左手臂抽出经过一个血液分离机，提取出干细胞，然后再经过右手臂回输到自己身体中。整个过程持续半天，最终抽取出200-300ml的血液就算完成捐献了。</p><p>对于病人来说，造血干细胞移植，就是利用供者的干细胞的造血能力全部换掉自身体内的血液，从而彻底清除癌变的白细胞，然后重新造血。因此，病人的血型在移植完后会变成供体的血型。</p><p>供体的选择，通常是按照：兄弟姐妹、直系亲属、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序去筛选寻找。</p><p>判断供体和不合适，其实和常人输液一样，如果你是A型血，那你需要同样是A型血的人，B型血的人就爱莫能助。只不过移植不是用血型来判断，而是根据HLA（人类白细胞抗原）。</p><blockquote><p>HLA（人类白细胞抗原）是编码人类的主要组织相容性复合体的基因，负责调节人体免疫系统。简单地说，HLA是人体生物学的”身份证”，由父母遗传，能识别”自己”和”非己”，会通过免疫反应排除”非己”。所以，HLA在造血干细胞移植的成败中起着重要的作用。造血干细胞移植就需要供体和受体HLA配型相同，如果不相同就会发生致命的排斥反应。</p></blockquote><p>由于不同人种、不同种族、不同个体的HLA差别很大，所以要采用一种方法来确定HLA。判断两个人的HLA是否相同，只需要抽8ml的血，做HLA高分辨检测化验，然后将HLA上5组基因的数字进行对比就能判定。</p><p>简单地举个例子，供体1的10组数字和受体全部对应，称为”全相合”，这是最理想的，移植效果也最好；供体2有5组数字和受体对应，称为”半相合”，如果是直系亲属，那么可以移植，如果是非血缘供提，那么无法移植；供体3一组数字都对不上，称为”零相合”，完全不可用。类似地还会有9组数字对应的情况，称为9个点匹配，以此类推。</p><p><img src="http://media.makcyun.top/18-9-15/47625856.jpg" alt=""></p><p>尽管判断HLA是否相同很简单，但是供体的选择范围其实很有限。按照兄弟姐妹、父母子女、堂（表）兄弟姐妹和骨髓库志愿者这样的顺序来解释一下。</p><ul><li><p>兄弟姐妹</p><p>根据染色体的基本常识，正常情况下，子女的染色体一半来自父亲、一半来自母亲。可以知道，兄弟姐妹有25%的概率是全相合、50%的概率是半相合、剩下25%则是零相合。可以说，只要有兄弟姐妹，供体基本就找到了，但很多人没有兄弟姐妹，尤其90后。</p></li><li><p>父母子女</p><p>没有兄弟姐妹的人，可以将父母子、女身作为潜在的供体对象。绝大多数情况下，父母和子女之间是半相合，这是最常见的。目前，国内半相合移植技术发展地很好，很多患者才得以生存下来。但是也有条件限制，比如在健康的条件下，父母年龄最好不要超过60岁，子女体重要达到一定要求才行。</p></li><li><p>堂（表）兄弟姐妹</p><p>在堂（表）兄弟姐妹中，这个概率其实就比较低了，但如果前两类人群中无法找到，也只能去试一下。</p></li><li><p>骨髓库志愿者</p><p>上面三类人群都无法找到，那么只能寄希望于骨髓库中的志愿者了。没有血缘关系的人，HLA能匹配上的概率非常低，只有数十万到上百万分之一！但依然得抱着希望去尝试，因为这是最后一根救命稻草。</p></li></ul><h4 id="5-2-3-控制排异"><a href="#5-2-3-控制排异" class="headerlink" title="5.2.3. 控制排异"></a>5.2.3. 控制排异</h4><p>移植完成后，供体的干细胞进入病人身体，自身细胞会视为外来敌人然后会发生排斥。可以比喻成战争，视战役的惨烈程度分为两种情况：第一种小规模或者和平停战，那么移植算成功，慢慢恢复就能康复起来；第二种是拼到了两败俱伤的境地，就会引起严重的排异反应，这就会致命。控制排异和恢复的过程需要持续数年。</p><h3 id="5-3-治疗费用"><a href="#5-3-治疗费用" class="headerlink" title="5.3. 治疗费用"></a>5.3. 治疗费用</h3><p>根据治疗方案就可以大致计算白血病的治疗费用。23的治疗费用主要包括：化疗费用+移植费用+排异控制费用+服药费用。简单做了一下费用预估：</p><ul><li>化疗费用15-40万</li><li>移植费用30-50万</li><li>排异控制费用：可能几万，也可能几百万</li><li>口服药（几万到数十万）。</li></ul><p>拿口服药举个例子，服用进口药，60片共3g要8500元。目前，千足金（99.99%）的价格是265元/g，这种药的单价是其10倍不止。一瓶够服用一个月，服用时间按年计算。</p><p>所以，白血病可能50万能治好，也可能花了500万，仍然留不住病人。</p><h2 id="6-希望之光"><a href="#6-希望之光" class="headerlink" title="6. 希望之光"></a>6. 希望之光</h2><p>很多亲朋好友在得知我们的情况后，非常关心并且给予了很多的帮助，让我们不要有压力。</p><p>我觉得，压力大不大最好是去对比一下。一对比，很容易就能得出答案。3个多月以来，住了3家医院，接触和听说过不少病友的故事。比如：有怀身孕6个月的妻子照顾患病的丈夫、这辈子几乎无法再当妈妈的未婚女生、撇下2岁多孩子消失的父亲等等。</p><p>相比之下，我们的压力不算大，唯一的压力来自于供体的寻找。</p><p><img src="http://media.makcyun.top/18-9-15/14687057.jpg" alt=""></p><p><em>9月13日</em><br>医生告诉了一个久违的好消息：在台湾骨髓库找到了一个全相合且有意愿的供者。全家听到后都特别的开心，祈祷这位同胞体检能够通过，最终顺利捐出造血干细胞。</p><p><img src="http://media.makcyun.top/18-9-16/47265044.jpg" alt=""></p><p><em>2015年，世界骨髓捐献者协会宣布：每年9月的第3个周六，为”世界骨髓捐献者日”。</em></p><p>截至2018年9月，全球骨髓库有<a href="https://worldmarrowdonorday.org/" target="_blank" rel="noopener">3250万志愿者</a>，占全世界75亿人的千分之四左右。中国内地排在美国、德国和巴西之后，位列世界第四，志愿者的人数为250万，仅占总人口比例的0.18%，比其他国家（地区）低一个数量级。250万这个数字，还只是登记在库内的初查人数，实际上真正愿意做志愿者的人，会比这低很多。</p><p><strong>从2001年成立到现在，17年只捐献了7600例，意味着最多有7600个患者得到了救治，每年只有平均450人。</strong>第一大骨髓库美国，30年来也不过，只有8万人得到了救治。<strong>可以说，不仅国内，全世界骨髓库都很需要志愿者的加入。</strong></p><table><thead><tr><th style="text-align:center">骨髓库</th><th style="text-align:center">成立时间（年）</th><th style="text-align:center">库容志愿者人数（万）</th><th style="text-align:center">人口（亿）</th><th style="text-align:center">占人口比例</th><th style="text-align:center">捐献造血干细胞例数</th></tr></thead><tbody><tr><td style="text-align:center">美国</td><td style="text-align:center">1988</td><td style="text-align:center">900</td><td style="text-align:center">3.3</td><td style="text-align:center">2.73%</td><td style="text-align:center"><a href="https://bethematch.org/workarea/downloadasset.aspx?id=1968" target="_blank" rel="noopener">80,000</a></td></tr><tr><td style="text-align:center">德国</td><td style="text-align:center">1993</td><td style="text-align:center">400</td><td style="text-align:center">0.83</td><td style="text-align:center">9.37%</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">巴西</td><td style="text-align:center">-</td><td style="text-align:center">500</td><td style="text-align:center">2.12</td><td style="text-align:center">2.36%</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">中国</td><td style="text-align:center">2001</td><td style="text-align:center"><a href="http://www.cmdp.org.cn/" target="_blank" rel="noopener">250</a></td><td style="text-align:center">13.9</td><td style="text-align:center"><strong>0.18%</strong></td><td style="text-align:center">7,600</td></tr><tr><td style="text-align:center">台湾</td><td style="text-align:center">1993</td><td style="text-align:center"><a href="http://btcscc.tzuchi.com.tw/index.php?option=com_content&amp;view=article&amp;id=1146%3A2014-01-02-06-00-06&amp;catid=185%3A2013-09-24-06-23-25&amp;Itemid=558&amp;lang=tw" target="_blank" rel="noopener">43</a></td><td style="text-align:center">0.24</td><td style="text-align:center">1.79%</td><td style="text-align:center">5,100</td></tr></tbody></table><p><em>参考来源：<a href="https://www.wmda.info/wp-content/uploads/2018/06/20180531-GTR-SearchMatch-2017.pdf" target="_blank" rel="noopener">WMDA global trend report 2017</a>、 <a href="http://lnmdp.org.admin.yuntai.net/readnews.aspx?n_id=1817&amp;type=4" target="_blank" rel="noopener">http://lnmdp.org.admin.yuntai.net/readnews.aspx?n_id=1817&amp;type=4</a></em></p><p>假如，大陆志愿者的人口比例，能够达到台湾的比例，<strong>那么，这一数字将会从250万变成2500万！</strong></p><p>成为志愿者的了解渠道：</p><p><a href="http://www.cmdp.org.cn/show/1020166.html" target="_blank" rel="noopener">http://www.cmdp.org.cn/show/1020166.html</a></p><p>本文完。</p><p>链接：</p><blockquote><p>世界骨髓捐献者协会WMDA：<a href="https://www.wmda.info/" target="_blank" rel="noopener">https://www.wmda.info/</a></p><p>世界骨髓捐献者日网站WMDD：<a href="https://worldmarrowdonorday.org/" target="_blank" rel="noopener">https://worldmarrowdonorday.org/</a></p><p>中华骨髓库CMDP：<a href="http://www.cmdp.org.cn/" target="_blank" rel="noopener">http://www.cmdp.org.cn/</a></p><p>台湾慈濟骨髓幹細胞中心BTCSCC：<a href="http://btcscc.tzuchi.com.tw/" target="_blank" rel="noopener">http://btcscc.tzuchi.com.tw/</a></p><p>香港骨髓庫：<a href="https://www5.ha.org.hk/rcbts/hkarticle.asp?bid=56&amp;MenuID=6#.W52iwfl4nDc" target="_blank" rel="noopener">https://www5.ha.org.hk/rcbts/hkarticle.asp?bid=56&amp;MenuID=6#.W52iwfl4nDc</a></p><p>美国骨髓库NMDP：<a href="https://bethematch.org/" target="_blank" rel="noopener">https://bethematch.org/</a></p><p>欧洲骨髓库EMDIS：<a href="www.emdis.net/">www.emdis.net/</a></p><p>德国骨髓库ZKRD：<a href="https://www.zkrd.de/de/" target="_blank" rel="noopener">https://www.zkrd.de/de/</a></p><p>日本骨髓库JMDP： <a href="http://www.jmdp.or.jp/" target="_blank" rel="noopener">http://www.jmdp.or.jp/</a></p><hr><p>白血病介绍：<a href="https://en.m.wikipedia.org/wiki/Leukemia" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Leukemia</a></p><p>急性淋巴细胞白血病介绍： <a href="https://en.m.wikipedia.org/wiki/Acute_lymphoblastic_leukemia" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Acute_lymphoblastic_leukemia</a></p><p>HLA介绍：<a href="https://en.m.wikipedia.org/wiki/Human_leukocyte_antigen" target="_blank" rel="noopener">https://en.m.wikipedia.org/wiki/Human_leukocyte_antigen</a></p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;b&gt;&lt;em&gt;世界骨髓捐献者日。&lt;/em&gt;&lt;/b&gt;&lt;br&gt;
    
    </summary>
    
      <category term="生活随笔" scheme="https://www.makcyun.top/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活随笔" scheme="https://www.makcyun.top/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/"/>
    
      <category term="白血病" scheme="https://www.makcyun.top/tags/%E7%99%BD%E8%A1%80%E7%97%85/"/>
    
  </entry>
  
  <entry>
    <title>Python数据处理分析(1)：日期型数据处理</title>
    <link href="https://www.makcyun.top/2018/09/11/python_data_analysis&amp;mining01.html"/>
    <id>https://www.makcyun.top/2018/09/11/python_data_analysis&amp;mining01.html</id>
    <published>2018-09-11T01:36:41.000Z</published>
    <updated>2019-02-27T08:26:09.570Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>以调用大名鼎鼎的Tushare包案例，介绍日期数据的处理。</p><a id="more"></a><p><strong>摘要：</strong> Python数据处理分析中，日期型数据的处理是相对复杂且非常重要的一环。本文以调用Tushare包获得股票的各种信息数据为案例，介绍日期数据的处理。</p><p>之前的<a href="https://www.makcyun.top/web_scraping_withpython2.html">一篇文章</a>用爬虫实现了上市公司信息的抓取。但还有更简单的方法，就是调用Tushare包，可以很便捷地拿到干净的各种股市数据。</p><p>强烈推荐一下这款由国内团队开发的包，Github上目前Star数 6000+。Tushare是一个开源免费、强大的python金融财经数据接口包。调用该包返回的数据格式基本是Pandas DataFrame类型，非常便于后续处理分析。包的数据来源于新浪财经、腾讯财经、上交所和深交所，比较齐全，质量也很可靠。</p><blockquote><p>参考：<br><a href="https://tushare.pro/document/2" target="_blank" rel="noopener">https://tushare.pro/document/2</a><br><a href="https://github.com/waditu/Tushare" target="_blank" rel="noopener">https://github.com/waditu/Tushare</a></p></blockquote><p>下面我们就来简单体检一下这款包的便利，然后利用它返回的数据处理其中的日期型数据。</p><h2 id="1-获取数据"><a href="#1-获取数据" class="headerlink" title="1. 获取数据"></a>1. 获取数据</h2><p>接口使用前提：首先在官网注册成功后获得token，然后通过下面命令下载Tushare包，然后在程序中调用就可以使用了。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip instasll tushare</span><br></pre></td></tr></table></figure><p></p><p>可以获得的信息接口非常多，包括：行情数据、基础数据、财务数据板块等。<br><img src="http://media.makcyun.top/18-9-11/86932586.jpg" alt=""></p><p>下面就简单使用下部分接口。首先，获取国内股票列表数据。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">ts.set_token(<span class="string">'你的token'</span>)</span><br><span class="line">pro = ts.pro_api()</span><br><span class="line">data = pro.stock_basic(exchange_id=<span class="string">''</span>, is_hs=<span class="string">''</span>, fields=<span class="string">'symbol,name,is_hs,list_date,list_status'</span>)</span><br><span class="line">print(data)</span><br><span class="line"><span class="comment"># ''表示获取全部</span></span><br></pre></td></tr></table></figure><p></p><p><code>exchange_id</code>表示股票代码，可以获取特定股票的基础信息，为空则获取全部;<code>is_hs</code>表示是否沪深港通，为空表示提取所有股市；<code>fields</code>表示想要提取的信息列表。</p><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">        ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line"><span class="number">0</span>     <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L   <span class="number">19910403</span>     S</span><br><span class="line"><span class="number">1</span>     <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L   <span class="number">19910129</span>     S</span><br><span class="line"><span class="number">2</span>     <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L   <span class="number">19910114</span>     N</span><br><span class="line"><span class="number">3</span>     <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L   <span class="number">19901210</span>     N</span><br><span class="line"><span class="number">4</span>     <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L   <span class="number">19920427</span>     S</span><br><span class="line"><span class="number">5</span>     <span class="number">000007.</span>SZ  <span class="number">000007</span>   全新好           L   <span class="number">19920413</span>     N</span><br><span class="line"><span class="number">6</span>     <span class="number">000008.</span>SZ  <span class="number">000008</span>  神州高铁           L   <span class="number">19920507</span>     S</span><br><span class="line"><span class="number">7</span>     <span class="number">000009.</span>SZ  <span class="number">000009</span>  中国宝安           L   <span class="number">19910625</span>     S</span><br><span class="line"><span class="number">8</span>     <span class="number">000010.</span>SZ  <span class="number">000010</span>  美丽生态           L   <span class="number">19951027</span>     N</span><br><span class="line"><span class="number">9</span>     <span class="number">000011.</span>SZ  <span class="number">000011</span>  深物业A           L   <span class="number">19920330</span>     S</span><br><span class="line"><span class="number">10</span>    <span class="number">000012.</span>SZ  <span class="number">000012</span>   南玻A           L   <span class="number">19920228</span>     S</span><br><span class="line">···</span><br><span class="line"><span class="number">3532</span>  <span class="number">603987.</span>SH  <span class="number">603987</span>   康德莱           L   <span class="number">20161121</span>     N</span><br><span class="line"><span class="number">3533</span>  <span class="number">603988.</span>SH  <span class="number">603988</span>  中电电机           L   <span class="number">20141104</span>     N</span><br><span class="line"><span class="number">3534</span>  <span class="number">603989.</span>SH  <span class="number">603989</span>  艾华集团           L   <span class="number">20150515</span>     H</span><br><span class="line"><span class="number">3535</span>  <span class="number">603990.</span>SH  <span class="number">603990</span>  麦迪科技           L   <span class="number">20161208</span>     N</span><br><span class="line"><span class="number">3536</span>  <span class="number">603991.</span>SH  <span class="number">603991</span>  至正股份           L   <span class="number">20170308</span>     N</span><br><span class="line"><span class="number">3537</span>  <span class="number">603993.</span>SH  <span class="number">603993</span>  洛阳钼业           L   <span class="number">20121009</span>     H</span><br><span class="line"><span class="number">3538</span>  <span class="number">603996.</span>SH  <span class="number">603996</span>  中新科技           L   <span class="number">20151222</span>     N</span><br><span class="line"><span class="number">3539</span>  <span class="number">603997.</span>SH  <span class="number">603997</span>  继峰股份           L   <span class="number">20150302</span>     H</span><br><span class="line"><span class="number">3540</span>  <span class="number">603998.</span>SH  <span class="number">603998</span>  方盛制药           L   <span class="number">20141205</span>     N</span><br><span class="line"><span class="number">3541</span>  <span class="number">603999.</span>SH  <span class="number">603999</span>  读者传媒           L   <span class="number">20151210</span>     N</span><br></pre></td></tr></table></figure><p>很轻松地就能获得3542家上市公司的基本情况。下面就将这个数据作为日期型处理的基础数据。</p><h2 id="2-日期型数据处理"><a href="#2-日期型数据处理" class="headerlink" title="2. 日期型数据处理"></a>2. 日期型数据处理</h2><p>查看一下数据结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">RangeIndex: <span class="number">3542</span> entries, <span class="number">0</span> to <span class="number">3541</span></span><br><span class="line">Data columns (total <span class="number">6</span> columns):</span><br><span class="line">ts_code        <span class="number">3542</span> non-null object</span><br><span class="line">symbol         <span class="number">3542</span> non-null object</span><br><span class="line">name           <span class="number">3542</span> non-null object</span><br><span class="line">list_status    <span class="number">3542</span> non-null object</span><br><span class="line">list_date      <span class="number">3542</span> non-null object</span><br><span class="line">is_hs          <span class="number">3542</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>所有列都是object字符型。这里想对日期做数据分析，比如可以统计一下历年上市公司数量。需更改日期型数据字符型为日期型。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'list_date'</span>] = pd.to_datetime(data[<span class="string">'list_date'</span>])</span><br></pre></td></tr></table></figure><p></p><p>pd.to_datetime将’list_date’列格式改为datetime格式，再来看一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">RangeIndex: <span class="number">3542</span> entries, <span class="number">0</span> to <span class="number">3541</span></span><br><span class="line">Data columns (total <span class="number">6</span> columns):</span><br><span class="line">ts_code        <span class="number">3542</span> non-null object</span><br><span class="line">symbol         <span class="number">3542</span> non-null object</span><br><span class="line">name           <span class="number">3542</span> non-null object</span><br><span class="line">list_status    <span class="number">3542</span> non-null object</span><br><span class="line">list_date      <span class="number">3542</span> non-null datetime64[ns]</span><br><span class="line">is_hs          <span class="number">3542</span> non-null object</span><br><span class="line">dtypes: object(<span class="number">6</span>)</span><br></pre></td></tr></table></figure><h3 id="2-1-按日期切片筛选数据"><a href="#2-1-按日期切片筛选数据" class="headerlink" title="2.1. 按日期切片筛选数据"></a>2.1. 按日期切片筛选数据</h3><p>有时候我们需要按年、季度、月、日这样的日期格式来筛选提取相应的数据。</p><h4 id="2-1-1-按年度"><a href="#2-1-1-按年度" class="headerlink" title="2.1.1. 按年度"></a>2.1.1. 按年度</h4><ul><li>获取单一年份数据，比如2017年</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data = data.set_index(data[<span class="string">'list_date'</span>])</span><br><span class="line">data = data[<span class="string">'2017'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>  <span class="number">001965.</span>SZ  <span class="number">001965</span>  招商公路           L <span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>     S</span><br><span class="line"><span class="number">2017</span><span class="number">-03</span><span class="number">-24</span>  <span class="number">002774.</span>SZ  <span class="number">002774</span>  快意电梯           L <span class="number">2017</span><span class="number">-03</span><span class="number">-24</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">002824.</span>SZ  <span class="number">002824</span>  和胜股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">002838.</span>SZ  <span class="number">002838</span>  道恩股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>  <span class="number">002839.</span>SZ  <span class="number">002839</span>  张家港行           L <span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>     S</span><br></pre></td></tr></table></figure><ul><li>获取多个年份，比如2015-2017</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="string">'2015'</span>:<span class="string">'2017'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-26</span>  <span class="number">000166.</span>SZ  <span class="number">000166</span>  申万宏源           L <span class="number">2015</span><span class="number">-01</span><span class="number">-26</span>     S</span><br><span class="line"><span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>  <span class="number">001965.</span>SZ  <span class="number">001965</span>  招商公路           L <span class="number">2017</span><span class="number">-12</span><span class="number">-25</span>     S</span><br><span class="line"><span class="number">2015</span><span class="number">-12</span><span class="number">-30</span>  <span class="number">001979.</span>SZ  <span class="number">001979</span>  招商蛇口           L <span class="number">2015</span><span class="number">-12</span><span class="number">-30</span>     S</span><br><span class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-27</span>  <span class="number">002734.</span>SZ  <span class="number">002734</span>  利民股份           L <span class="number">2015</span><span class="number">-01</span><span class="number">-27</span>     N</span><br><span class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-22</span>  <span class="number">002739.</span>SZ  <span class="number">002739</span>  万达电影           L <span class="number">2015</span><span class="number">-01</span><span class="number">-22</span>     S</span><br></pre></td></tr></table></figure><h4 id="2-1-2-按月度"><a href="#2-1-2-按月度" class="headerlink" title="2.1.2. 按月度"></a>2.1.2. 按月度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="string">'2017-1'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">002824.</span>SZ  <span class="number">002824</span>  和胜股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">002838.</span>SZ  <span class="number">002838</span>  道恩股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-06</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>  <span class="number">002839.</span>SZ  <span class="number">002839</span>  张家港行           L <span class="number">2017</span><span class="number">-01</span><span class="number">-24</span>     S</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-10</span>  <span class="number">002840.</span>SZ  <span class="number">002840</span>  华统股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-19</span>  <span class="number">002841.</span>SZ  <span class="number">002841</span>  视源股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-19</span>     S</span><br></pre></td></tr></table></figure><h4 id="2-1-3-按具体天"><a href="#2-1-3-按具体天" class="headerlink" title="2.1.3. 按具体天"></a>2.1.3. 按具体天</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data[<span class="string">'2017-1-12'</span>]</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">              ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                       </span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">002824.</span>SZ  <span class="number">002824</span>  和胜股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">300584.</span>SZ  <span class="number">300584</span>  海辰药业           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     N</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">603628.</span>SH  <span class="number">603628</span>  清源股份           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     H</span><br><span class="line"><span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>  <span class="number">603639.</span>SH  <span class="number">603639</span>   海利尔           L <span class="number">2017</span><span class="number">-01</span><span class="number">-12</span>     H</span><br></pre></td></tr></table></figure><h3 id="2-2-to-period按日期显示数据"><a href="#2-2-to-period按日期显示数据" class="headerlink" title="2.2. to_period按日期显示数据"></a>2.2. to_period按日期显示数据</h3><p>dataframe.to_period方法只是用于显示数据，但不会进行统计。</p><h4 id="2-2-1-按年度"><a href="#2-2-1-按年度" class="headerlink" title="2.2.1. 按年度"></a>2.2.1. 按年度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data.to_period(<span class="string">'A'</span>)  <span class="comment"># 'A'默认是从'A-DEC'开始算,也可以根据情况设置为'A-JAN'</span></span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">             ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                      </span><br><span class="line"><span class="number">1991</span>       <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L <span class="number">1991</span><span class="number">-04</span><span class="number">-03</span>     S</span><br><span class="line"><span class="number">1991</span>       <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L <span class="number">1991</span><span class="number">-01</span><span class="number">-29</span>     S</span><br><span class="line"><span class="number">1991</span>       <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L <span class="number">1991</span><span class="number">-01</span><span class="number">-14</span>     N</span><br><span class="line"><span class="number">1990</span>       <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L <span class="number">1990</span><span class="number">-12</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">1992</span>       <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L <span class="number">1992</span><span class="number">-04</span><span class="number">-27</span>     S</span><br></pre></td></tr></table></figure><p>可以看到，相比上面筛选数据时是按原始的日期，这里利用to_period方法，设置参数为’A’后，可以直接显示为年，这在后期可视化绘图时非常有用。</p><h4 id="2-2-2-按季度"><a href="#2-2-2-按季度" class="headerlink" title="2.2.2. 按季度"></a>2.2.2. 按季度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data.to_period(<span class="string">'Q'</span>)   <span class="comment"># 'Q'默认是从'Q-DEC'开始算,也可以根据情况设置为“Q-SEP”，“Q-FEB”等</span></span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">             ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                      </span><br><span class="line"><span class="number">1991</span>Q2     <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L <span class="number">1991</span><span class="number">-04</span><span class="number">-03</span>     S</span><br><span class="line"><span class="number">1991</span>Q1     <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L <span class="number">1991</span><span class="number">-01</span><span class="number">-29</span>     S</span><br><span class="line"><span class="number">1991</span>Q1     <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L <span class="number">1991</span><span class="number">-01</span><span class="number">-14</span>     N</span><br><span class="line"><span class="number">1990</span>Q4     <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L <span class="number">1990</span><span class="number">-12</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">1992</span>Q2     <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L <span class="number">1992</span><span class="number">-04</span><span class="number">-27</span>     S</span><br></pre></td></tr></table></figure><h4 id="2-2-3-按月度"><a href="#2-2-3-按月度" class="headerlink" title="2.2.3. 按月度"></a>2.2.3. 按月度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = data.to_period(<span class="string">'M'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">             ts_code  symbol  name list_status  list_date is_hs</span><br><span class="line">list_date                                                      </span><br><span class="line"><span class="number">1991</span><span class="number">-04</span>    <span class="number">000001.</span>SZ  <span class="number">000001</span>  平安银行           L <span class="number">1991</span><span class="number">-04</span><span class="number">-03</span>     S</span><br><span class="line"><span class="number">1991</span><span class="number">-01</span>    <span class="number">000002.</span>SZ  <span class="number">000002</span>   万科A           L <span class="number">1991</span><span class="number">-01</span><span class="number">-29</span>     S</span><br><span class="line"><span class="number">1991</span><span class="number">-01</span>    <span class="number">000004.</span>SZ  <span class="number">000004</span>  国农科技           L <span class="number">1991</span><span class="number">-01</span><span class="number">-14</span>     N</span><br><span class="line"><span class="number">1990</span><span class="number">-12</span>    <span class="number">000005.</span>SZ  <span class="number">000005</span>  世纪星源           L <span class="number">1990</span><span class="number">-12</span><span class="number">-10</span>     N</span><br><span class="line"><span class="number">1992</span><span class="number">-04</span>    <span class="number">000006.</span>SZ  <span class="number">000006</span>  深振业A           L <span class="number">1992</span><span class="number">-04</span><span class="number">-27</span>     S</span><br></pre></td></tr></table></figure><h3 id="2-3-resample按日期统计数据"><a href="#2-3-resample按日期统计数据" class="headerlink" title="2.3. resample按日期统计数据"></a>2.3. resample按日期统计数据</h3><p>按日期进行统计数据，可以利用resample方法。</p><h4 id="2-3-1-按年度"><a href="#2-3-1-按年度" class="headerlink" title="2.3.1. 按年度"></a>2.3.1. 按年度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data.resample(<span class="string">'AS'</span>).count()[<span class="string">'name'</span>]  <span class="comment"># count对各年上市公司数量进行计数</span></span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span><span class="number">-01</span><span class="number">-01</span>      <span class="number">7</span></span><br><span class="line"><span class="number">1991</span><span class="number">-01</span><span class="number">-01</span>      <span class="number">4</span></span><br><span class="line"><span class="number">1992</span><span class="number">-01</span><span class="number">-01</span>     <span class="number">37</span></span><br><span class="line"><span class="number">1993</span><span class="number">-01</span><span class="number">-01</span>    <span class="number">106</span></span><br><span class="line"><span class="number">1994</span><span class="number">-01</span><span class="number">-01</span>     <span class="number">99</span></span><br></pre></td></tr></table></figure><h4 id="2-3-2-按季度"><a href="#2-3-2-按季度" class="headerlink" title="2.3.2. 按季度"></a>2.3.2. 按季度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data.resample(<span class="string">'Q'</span>).count()[<span class="string">'name'</span>]  </span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span><span class="number">-12</span><span class="number">-31</span>    <span class="number">7</span></span><br><span class="line"><span class="number">1991</span><span class="number">-03</span><span class="number">-31</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1991</span><span class="number">-06</span><span class="number">-30</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1991</span><span class="number">-09</span><span class="number">-30</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1991</span><span class="number">-12</span><span class="number">-31</span>    <span class="number">0</span></span><br></pre></td></tr></table></figure><h4 id="2-3-3-按月度"><a href="#2-3-3-按月度" class="headerlink" title="2.3.3. 按月度"></a>2.3.3. 按月度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = data.resample(<span class="string">'M'</span>).count()[<span class="string">'name'</span>]  </span><br><span class="line">print(data.head())</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span><span class="number">-12</span><span class="number">-31</span>    <span class="number">7</span></span><br><span class="line"><span class="number">1991</span><span class="number">-01</span><span class="number">-31</span>    <span class="number">2</span></span><br><span class="line"><span class="number">1991</span><span class="number">-02</span><span class="number">-28</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1991</span><span class="number">-03</span><span class="number">-31</span>    <span class="number">0</span></span><br><span class="line"><span class="number">1991</span><span class="number">-04</span><span class="number">-30</span>    <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="2-4-统计和显示结合"><a href="#2-4-统计和显示结合" class="headerlink" title="2.4. 统计和显示结合"></a>2.4. 统计和显示结合</h3><p>利用前面的resample和to.period方法，可以按年、季度、月份汇总数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 汇总各年上市公司数量</span></span><br><span class="line">data = data.set_index([<span class="string">'list_date'</span>])</span><br><span class="line">data = data.resample(<span class="string">'AS'</span>).count()[<span class="string">'ts_code'</span>]</span><br><span class="line">data = data.to_period(<span class="string">'A'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line">print(data.tail())</span><br></pre></td></tr></table></figure><p>结果如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">list_date</span><br><span class="line"><span class="number">1990</span>      <span class="number">7</span></span><br><span class="line"><span class="number">1991</span>      <span class="number">4</span></span><br><span class="line"><span class="number">1992</span>     <span class="number">37</span></span><br><span class="line"><span class="number">1993</span>    <span class="number">106</span></span><br><span class="line"><span class="number">1994</span>     <span class="number">99</span></span><br><span class="line">Freq: A-DEC, Name: name, dtype: int64</span><br><span class="line">list_date</span><br><span class="line"><span class="number">2014</span>    <span class="number">124</span></span><br><span class="line"><span class="number">2015</span>    <span class="number">223</span></span><br><span class="line"><span class="number">2016</span>    <span class="number">227</span></span><br><span class="line"><span class="number">2017</span>    <span class="number">438</span></span><br><span class="line"><span class="number">2018</span>     <span class="number">78</span></span><br><span class="line">Freq: A-DEC, Name: name, dtype: int64</span><br></pre></td></tr></table></figure><p></p><p>基于上述数据，可以利用matplotlib绘制出历年上市公司数量的折线图：</p><p><img src="http://media.makcyun.top/18-9-17/25609726.jpg" alt=""></p><p>折线图的具体绘制方法，见后续文章。</p><p>以上就是简单利用了Tushare的一个接口返回的数据，介绍了日期型数据的转换和处理。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;以调用大名鼎鼎的Tushare包案例，介绍日期数据的处理。&lt;/p&gt;
    
    </summary>
    
      <category term="Python数据清洗处理" scheme="https://www.makcyun.top/categories/Python%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E5%A4%84%E7%90%86/"/>
    
    
      <category term="Python数据分析" scheme="https://www.makcyun.top/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="Python包" scheme="https://www.makcyun.top/tags/Python%E5%8C%85/"/>
    
      <category term="Tushare" scheme="https://www.makcyun.top/tags/Tushare/"/>
    
  </entry>
  
  <entry>
    <title>Python可视化(2)：折线图</title>
    <link href="https://www.makcyun.top/2018/09/11/Python__visualization02.html"/>
    <id>https://www.makcyun.top/2018/09/11/Python__visualization02.html</id>
    <published>2018-09-11T01:36:41.000Z</published>
    <updated>2019-02-27T08:43:20.531Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>日期型数据的折线图绘制。</p><a id="more"></a><p><strong>摘要：</strong> 利用matplotlib绘制横轴为日期格式的折线图时，存在不少技巧。本文借助Tushare包返回的股票数据，介绍日期折线图绘制的方法。</p><p><a href="https://www.makcyun.top/python_data_analysis01.html">上一篇文章</a>的最后讲到了折线图的绘制，本文详细介绍绘制方法。</p><p>折线图绘制的数据源，采用Tushare包获取上市公司基本数据表，格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read(<span class="string">'get_stock_basics.csv'</span>,encoding = <span class="string">'utf8'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line"></span><br><span class="line">ts_codesymbolnamelist_statuslist_dateis_hs</span><br><span class="line"><span class="number">000001.</span>SZ<span class="number">1</span>平安银行L<span class="number">19910403</span>S</span><br><span class="line"><span class="number">000002.</span>SZ<span class="number">2</span>万科AL<span class="number">19910129</span>S</span><br><span class="line"><span class="number">000004.</span>SZ<span class="number">4</span>国农科技L<span class="number">19910114</span>N</span><br><span class="line"><span class="number">000005.</span>SZ<span class="number">5</span>世纪星源L<span class="number">19901210</span>N</span><br></pre></td></tr></table></figure><p>然后利用<code>resample</code>和<code>to.period</code>方法汇总各年度的上市公司数量数据，格式为Pandas.Series数组格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 汇总各年上市公司数量</span></span><br><span class="line">data = data.set_index([<span class="string">'list_date'</span>])</span><br><span class="line">data = data.resample(<span class="string">'AS'</span>).count()[<span class="string">'ts_code'</span>]</span><br><span class="line">data = data.to_period(<span class="string">'A'</span>)</span><br><span class="line">print(data.head())</span><br><span class="line">print(data.tail())</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line">list_date</span><br><span class="line"><span class="number">1990</span>      <span class="number">7</span></span><br><span class="line"><span class="number">1991</span>      <span class="number">4</span></span><br><span class="line"><span class="number">1992</span>     <span class="number">37</span></span><br><span class="line"><span class="number">1993</span>    <span class="number">106</span></span><br><span class="line"><span class="number">1994</span>     <span class="number">99</span></span><br><span class="line">...</span><br><span class="line">list_date</span><br><span class="line"><span class="number">2014</span>    <span class="number">124</span></span><br><span class="line"><span class="number">2015</span>    <span class="number">223</span></span><br><span class="line"><span class="number">2016</span>    <span class="number">227</span></span><br><span class="line"><span class="number">2017</span>    <span class="number">438</span></span><br><span class="line"><span class="number">2018</span>     <span class="number">78</span></span><br></pre></td></tr></table></figure><h2 id="1-Series直接绘制折线图"><a href="#1-Series直接绘制折线图" class="headerlink" title="1. Series直接绘制折线图"></a>1. Series直接绘制折线图</h2><p>首先，我们可以直接利用pandas的数组Series绘制折线图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)  <span class="comment"># 设置绘图风格</span></span><br><span class="line">fig = plt.figure(figsize = (<span class="number">10</span>,<span class="number">6</span>))  <span class="comment"># 设置图框的大小</span></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">data.plot() <span class="comment"># 绘制折线图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment">#设置标题颜色为灰色</span></span><br><span class="line">plt.title(<span class="string">'历年中国内地上市公司数量变化'</span>,color = colors1,fontsize = <span class="number">18</span>)</span><br><span class="line">plt.xlabel(<span class="string">'年份'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(家)'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://media.makcyun.top/18-9-18/15047153.jpg" alt=""></p><p>可以发现，图中存在两个问题：一是缺少数值标签，二是横坐标年份被自动分割了。我们希望能够添加上数值标签，然后坐标轴显示每一年的年份值。接下来，需要采用新的方法重新绘制折线图。</p><h2 id="2-折线图完善"><a href="#2-折线图完善" class="headerlink" title="2. 折线图完善"></a>2. 折线图完善</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建x,y轴标签</span></span><br><span class="line">x = np.arange(<span class="number">0</span>,len(data),<span class="number">1</span>)</span><br><span class="line">    ax1.plot(x,data.values, <span class="comment">#x、y坐标</span></span><br><span class="line">    color = <span class="string">'#C42022'</span>, <span class="comment">#折线图颜色为红色</span></span><br><span class="line">    marker = <span class="string">'o'</span>,markersize = <span class="number">4</span> <span class="comment">#标记形状、大小设置</span></span><br><span class="line">    )</span><br><span class="line">ax1.set_xticks(x) <span class="comment"># 设置x轴标签为自然数序列</span></span><br><span class="line">ax1.set_xticklabels(data.index) <span class="comment"># 更改x轴标签值为年份</span></span><br><span class="line">plt.xticks(rotation=<span class="number">90</span>) <span class="comment"># 旋转90度，不至太拥挤</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data.values):</span><br><span class="line">    plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors1,fontsize = <span class="number">10</span> )</span><br><span class="line">    <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line"><span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">plt.title(<span class="string">'历年中国内地上市公司数量变化'</span>,color = colors1,fontsize = <span class="number">18</span>)</span><br><span class="line">plt.xlabel(<span class="string">'年份'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(家)'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('stock.png',bbox_inches = 'tight',dpi = 300)</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>完善后的折线图如下：</p><p><img src="http://media.makcyun.top/18-9-19/84420184.jpg" alt=""></p><p>可以看到，x轴逐年的数据都显示并且数值标签也添加上了。</p><h2 id="3-多元折线图"><a href="#3-多元折线图" class="headerlink" title="3. 多元折线图"></a>3. 多元折线图</h2><p>上面介绍了一元折线图的绘制，当需要绘制多元折线图时，方法也很简单，只要重复绘图函数即可。这里我们以二元折线图为例，绘制国内两家知名地产公司万科和保利地产2017年的市值变化对比折线图。</p><h3 id="3-1-数据来源"><a href="#3-1-数据来源" class="headerlink" title="3.1. 数据来源"></a>3.1. 数据来源</h3><p>数据源仍然采用tushare包的<code>pro.daily_basic()</code>接口，该接口能够返回股票的每日股市数据，其中包括每日市值<code>total_mv</code>。我们需要的两只股票分别是万科地产(000002.SZ)和保利地产(600048.SH)，下面就来获取两只股票2017年的市值数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line">ts.set_token(<span class="string">'你的token'</span>)  <span class="comment"># 官网注册后可以获得</span></span><br><span class="line">pro = ts.pro_api()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stock</span><span class="params">()</span>:</span></span><br><span class="line">    lst = []</span><br><span class="line">    ts_codes = [<span class="string">'000002.SZ'</span>, <span class="string">'600048.SH'</span>]</span><br><span class="line">    <span class="keyword">for</span> ts_code <span class="keyword">in</span> ts_codes:</span><br><span class="line">        data = pro.daily_basic(</span><br><span class="line">            ts_code=ts_code, start_date=<span class="string">'20170101'</span>, end_date=<span class="string">'20180101'</span>)</span><br><span class="line">    print(lst)</span><br><span class="line">    reutrn lst</span><br><span class="line"><span class="comment"># 结果如下，total_mv为当日市值（万元）：</span></span><br><span class="line">    <span class="comment">#万科地产数据</span></span><br><span class="line">    ts_codetrade_dateclose…total_mvcirc_mv</span><br><span class="line"><span class="number">0</span><span class="number">000002.</span>SZ<span class="number">20171229</span><span class="number">31.06</span>…<span class="number">3.43E+07</span><span class="number">3.02E+07</span></span><br><span class="line"><span class="number">1</span><span class="number">000002.</span>SZ<span class="number">20171228</span><span class="number">30.7</span>…<span class="number">3.39E+07</span><span class="number">2.98E+07</span></span><br><span class="line"><span class="number">2</span><span class="number">000002.</span>SZ<span class="number">20171227</span><span class="number">30.79</span>…<span class="number">3.40E+07</span><span class="number">2.99E+07</span></span><br><span class="line"><span class="number">3</span><span class="number">000002.</span>SZ<span class="number">20171226</span><span class="number">30.5</span>…<span class="number">3.37E+07</span><span class="number">2.96E+07</span></span><br><span class="line"><span class="number">4</span><span class="number">000002.</span>SZ<span class="number">20171225</span><span class="number">30.37</span>…<span class="number">3.35E+07</span><span class="number">2.95E+07</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#保利地产数据</span></span><br><span class="line">    ts_codetrade_dateclose…total_mvcirc_mv</span><br><span class="line"><span class="number">0</span><span class="number">600048.</span>SH<span class="number">20171229</span><span class="number">14.15</span>…<span class="number">1.68E+07</span><span class="number">1.66E+07</span></span><br><span class="line"><span class="number">1</span><span class="number">600048.</span>SH<span class="number">20171228</span><span class="number">13.71</span>…<span class="number">1.63E+07</span><span class="number">1.61E+07</span></span><br><span class="line"><span class="number">2</span><span class="number">600048.</span>SH<span class="number">20171227</span><span class="number">13.65</span>…<span class="number">1.62E+07</span><span class="number">1.60E+07</span></span><br><span class="line"><span class="number">3</span><span class="number">600048.</span>SH<span class="number">20171226</span><span class="number">13.85</span>…<span class="number">1.64E+07</span><span class="number">1.63E+07</span></span><br><span class="line"><span class="number">4</span><span class="number">600048.</span>SH<span class="number">20171225</span><span class="number">13.55</span>…<span class="number">1.61E+07</span><span class="number">1.59E+07</span></span><br></pre></td></tr></table></figure><p>下面对数据作进一步修改，从DataFrame中提取total_mv列，index设置为日期。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">'trade_date'</span>] = pd.to_datetime(data[<span class="string">'trade_date'</span>])</span><br><span class="line"><span class="comment"># 设置index为日期</span></span><br><span class="line">data = data.set_index(data[<span class="string">'trade_date'</span>]).sort_index(ascending=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 按月汇总和显示</span></span><br><span class="line">data = data.resample(<span class="string">'m'</span>)</span><br><span class="line">data = data.to_period()</span><br><span class="line"><span class="comment"># 市值改为亿元</span></span><br><span class="line">market_value = data[<span class="string">'total_mv'</span>]/<span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二者结果分别如下，万科地产：</span></span><br><span class="line"><span class="number">2017</span><span class="number">-01</span>    <span class="number">2291.973270</span></span><br><span class="line"><span class="number">2017</span><span class="number">-02</span>    <span class="number">2286.331037</span></span><br><span class="line"><span class="number">2017</span><span class="number">-03</span>    <span class="number">2306.894790</span></span><br><span class="line"><span class="number">2017</span><span class="number">-04</span>    <span class="number">2266.337906</span></span><br><span class="line"><span class="number">2017</span><span class="number">-05</span>    <span class="number">2131.053098</span></span><br><span class="line"><span class="number">2017</span><span class="number">-06</span>    <span class="number">2457.716659</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span>    <span class="number">2686.982164</span></span><br><span class="line"><span class="number">2017</span><span class="number">-08</span>    <span class="number">2524.462077</span></span><br><span class="line"><span class="number">2017</span><span class="number">-09</span>    <span class="number">2904.085487</span></span><br><span class="line"><span class="number">2017</span><span class="number">-10</span>    <span class="number">2976.999550</span></span><br><span class="line"><span class="number">2017</span><span class="number">-11</span>    <span class="number">3263.374043</span></span><br><span class="line"><span class="number">2017</span><span class="number">-12</span>    <span class="number">3317.107474</span></span><br><span class="line"><span class="comment"># 保利地产：</span></span><br><span class="line"><span class="number">2017</span><span class="number">-01</span>    <span class="number">1089.008286</span></span><br><span class="line"><span class="number">2017</span><span class="number">-02</span>    <span class="number">1120.023350</span></span><br><span class="line"><span class="number">2017</span><span class="number">-03</span>    <span class="number">1145.731640</span></span><br><span class="line"><span class="number">2017</span><span class="number">-04</span>    <span class="number">1153.760435</span></span><br><span class="line"><span class="number">2017</span><span class="number">-05</span>    <span class="number">1108.230609</span></span><br><span class="line"><span class="number">2017</span><span class="number">-06</span>    <span class="number">1157.276044</span></span><br><span class="line"><span class="number">2017</span><span class="number">-07</span>    <span class="number">1244.966905</span></span><br><span class="line"><span class="number">2017</span><span class="number">-08</span>    <span class="number">1203.580209</span></span><br><span class="line"><span class="number">2017</span><span class="number">-09</span>    <span class="number">1290.706606</span></span><br><span class="line"><span class="number">2017</span><span class="number">-10</span>    <span class="number">1244.438756</span></span><br><span class="line"><span class="number">2017</span><span class="number">-11</span>    <span class="number">1336.661916</span></span><br><span class="line"><span class="number">2017</span><span class="number">-12</span>    <span class="number">1531.150616</span></span><br></pre></td></tr></table></figure><h3 id="3-2-绘制二元折线图"><a href="#3-2-绘制二元折线图" class="headerlink" title="3.2. 绘制二元折线图"></a>3.2. 绘制二元折线图</h3><p>利用上面的Series数据就可以作图了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 设置绘图风格</span><br><span class="line">plt.style.use(&apos;ggplot&apos;)</span><br><span class="line">fig = plt.figure(figsize = (10,6))</span><br><span class="line">colors1 = &apos;#6D6D6D&apos;  #标题颜色</span><br><span class="line"></span><br><span class="line"># data1万科，data2保利</span><br><span class="line">data1 = lst[0]</span><br><span class="line">data2 = lst[1]</span><br><span class="line"># 绘制第一条折线图</span><br><span class="line">data1.plot(</span><br><span class="line">color = &apos;#C42022&apos;, #折线图颜色</span><br><span class="line">marker = &apos;o&apos;,markersize = 4, #标记形状、大小设置</span><br><span class="line">label = &apos;万科&apos;</span><br><span class="line">)</span><br><span class="line"># 绘制第二条折线图</span><br><span class="line">data2.plot(</span><br><span class="line">color = &apos;#4191C0&apos;, #折线图颜色</span><br><span class="line">marker = &apos;o&apos;,markersize = 4, #标记形状、大小设置</span><br><span class="line">label = &apos;保利&apos;</span><br><span class="line">)</span><br><span class="line"># 还可以绘制更多条</span><br><span class="line"># 设置标题及横纵坐标轴标题</span><br><span class="line">plt.title(&apos;2017年万科与保利地产市值对比&apos;,color = colors1,fontsize = 18)</span><br><span class="line">plt.xlabel(&apos;月份&apos;)</span><br><span class="line">plt.ylabel(&apos;市值(亿元)&apos;)</span><br><span class="line">plt.savefig(&apos;stock1.png&apos;,bbox_inches = &apos;tight&apos;,dpi = 300)</span><br><span class="line">plt.legend() # 显示图例</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>绘图结果如下：</p><p><img src="http://media.makcyun.top/18-9-28/31064491.jpg" alt=""></p><p>如果想添加数值标签，则可以使用下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制第一条折线图</span></span><br><span class="line"><span class="comment"># 创建x,y轴标签</span></span><br><span class="line">x = np.arange(<span class="number">0</span>,len(data1),<span class="number">1</span>)</span><br><span class="line">ax1.plot(x,data1.values, <span class="comment">#x、y坐标</span></span><br><span class="line">color = <span class="string">'#C42022'</span>, <span class="comment">#折线图颜色红色</span></span><br><span class="line">marker = <span class="string">'o'</span>,markersize = <span class="number">4</span>, <span class="comment">#标记形状、大小设置</span></span><br><span class="line">label = <span class="string">'万科'</span></span><br><span class="line">)</span><br><span class="line">ax1.set_xticks(x) <span class="comment"># 设置x轴标签</span></span><br><span class="line">ax1.set_xticklabels(data1.index) <span class="comment"># 设置x轴标签值</span></span><br><span class="line"><span class="comment"># plt.xticks(rotation=90)</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data1.values):</span><br><span class="line">    plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors1,fontsize = <span class="number">10</span> )</span><br><span class="line">    <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制第二条折线图</span></span><br><span class="line">x = np.arange(<span class="number">0</span>,len(data2),<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">ax1.plot(x,data2.values, <span class="comment">#x、y坐标</span></span><br><span class="line">color = <span class="string">'#4191C0'</span>, <span class="comment">#折线图颜色蓝色</span></span><br><span class="line">marker = <span class="string">'o'</span>,markersize = <span class="number">4</span>, <span class="comment">#标记形状、大小设置</span></span><br><span class="line">label = <span class="string">'保利'</span></span><br><span class="line">)</span><br><span class="line">ax1.set_xticks(x) <span class="comment"># 设置x轴标签</span></span><br><span class="line">ax1.set_xticklabels(data2.index) <span class="comment"># 设置x轴标签值</span></span><br><span class="line"><span class="comment"># plt.xticks(rotation=90)</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x,data2.values):</span><br><span class="line">    plt.text(x,y + <span class="number">10</span>,<span class="string">'%.0f'</span> %y,ha = <span class="string">'center'</span>,color = colors1,fontsize = <span class="number">10</span> )</span><br><span class="line">    <span class="comment"># '%.0f' %y 设置标签格式不带小数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置标题及横纵坐标轴标题</span></span><br><span class="line">plt.title(<span class="string">'2017年万科与保利地产市值对比'</span>,color = colors1,fontsize = <span class="number">18</span>)</span><br><span class="line">plt.xlabel(<span class="string">'月份'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'市值(亿元)'</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'stock1.png'</span>,bbox_inches = <span class="string">'tight'</span>,dpi = <span class="number">300</span>)</span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>结果如下图所示：</p><p><img src="http://media.makcyun.top/18-9-28/45373948.jpg" alt=""></p><p>可以看到，两只股票市值从2017年初开始一直在上涨，万科的市值是保利的2倍左右。</p><p>本文仅简单提取了两只股票的市值数据，只要你愿意，3000多只股票的数据都可以拿来绘图。</p><p>文章代码及素材可在下面链接中获得：</p><p><a href="https://github.com/makcyun/web_scraping_with_python/tree/master/date_plot" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python/tree/master/date_plot</a></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;日期型数据的折线图绘制。&lt;/p&gt;
    
    </summary>
    
      <category term="Python可视化" scheme="https://www.makcyun.top/categories/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>每周分享第 1 期：关于 PDF 阅读处理软件，你需要的都在这里了</title>
    <link href="https://www.makcyun.top/2018/09/08/fuli01.html"/>
    <id>https://www.makcyun.top/2018/09/08/fuli01.html</id>
    <published>2018-09-08T12:16:37.349Z</published>
    <updated>2018-12-19T03:12:32.968Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>分享几款佳软。</p><a id="more"></a><p>从今天起，我将开一个新的栏目「<strong>每周分享</strong>」，顾名思义，就是会在每个周末分享一篇文章。</p><p>内容涵盖范围会比较广，主要集中在软件、App、PPT、摄影Ps等几个方面。之后，你可以在公众号里面的 「不务正业」菜单里集中查看。插一句，另一个「不误正业」菜单会是以 Python 为主的编程文章。</p><p><code>为什么会想起开这个栏目呢，主要有两个原因。</code></p><p>第一，我的公众号前不久更名为「<strong>第2大脑</strong>」，是因为我想在这里分享一些一直存在我大脑里的好用的软件、App等黑科技玩意儿。</p><p>一直以来，我对软件、App这块儿保有很强的好奇心，总喜欢尝试些新鲜有趣或者能提高效率的东西。时间长了，发现积累了蛮多，但是都散乱地分布在电脑、手机或者其他地方，而没有好好地进行总结，也没拿出来分享过。本着开源精神，接下来我想陆陆续续把一些可能会对你用的东西，分享出来。</p><p>第二，大多数 Python 类公众号的内容基本上都是跟 Python 相关的，我觉得太单调。虽然「<strong>人生苦短，快用 Python</strong>」没错，但是同时关注一些别的，既不枯燥也能让生活更加有意思，是不是。</p><hr><p>好，说完了原因，那么下面就直奔主题吧。</p><p>第 1 期，就先分享几款「<strong>PDF 软件</strong>」。</p><p>为什么呢？ 因为，之前遇到过很多人在处理和 PDF 文件相关的问题时，总是很头疼，到处求助软件。这里我就分享几款，个人觉得非常好用的 PDF 软件，几乎能满足你对 PDF 所有的需求。有了它们，之后再遇到格式转换、提取分割、复制、压缩等问题时，很轻松就能解决了。</p><p>先推荐一款难得的国产良心软件。</p><h2 id="1-Foxit-Reader"><a href="#1-Foxit-Reader" class="headerlink" title="1. Foxit Reader"></a>1. <a href="https://www.foxitsoftware.com/pdf-reader/" target="_blank" rel="noopener">Foxit Reader</a></h2><p>从网站全英文 Style 就可以看出这款国产软件是走国际路线的。口碑非常好，足以媲美大名鼎鼎的 Adobe Acrobat，但体积要小地多。这款软件的功能主要是浏览 PDF ，同时打开几十个 PDF 文件也不会卡。</p><p><img src="http://media.makcyun.top/18-9-8/96139143.jpg" alt=""></p><p>除了正常浏览 PDF以外，更多的需求是对 PDF进行处理，最常见的就是 PDF 转换为 Word/Excel/PPT。</p><h2 id="2-Solid-PDF-Converter"><a href="#2-Solid-PDF-Converter" class="headerlink" title="2. Solid PDF Converter"></a>2. <a href="https://www.soliddocuments.com/zh/features.htm?product=SolidConverterPDF" target="_blank" rel="noopener">Solid PDF Converter</a></h2><p>在 PDF文件是可以 <strong>复制</strong> 的前提下，这款软件可以说是 PDF 转换为 Office 格式最好用的软件。价格不便宜，官方售价 100 美元。</p><p><img src="http://media.makcyun.top/18-9-8/15794080.jpg" alt=""></p><p>当 PDF 文件不能复制（扫描件或者图片）时，上面的软件就无能为力了，这时就需要下面这款大名鼎鼎的 OCR （文字识别）软件了。</p><h2 id="3-最好用的OCR文字识别软件"><a href="#3-最好用的OCR文字识别软件" class="headerlink" title="3. 最好用的OCR文字识别软件"></a>3. <a href="https://www.abbyy.com/en-us/finereader/" target="_blank" rel="noopener">最好用的OCR文字识别软件</a></h2><p><img src="http://media.makcyun.top/18-9-8/50294768.jpg" alt=""></p><p>它到底有多好，一句话就够了：知乎上 <a href="https://www.zhihu.com/topic/19556393/top-answers" target="_blank" rel="noopener">PDF</a> 话题中排名第一的软件。</p><p>当你需要从 PDF 中复制，但却无法选择时，这款软件能帮你搞定一切。另外，手机拍的文字照片，也能转成 Word。</p><p>这么好用，自然价格也不便宜，官方售价 500 RMB。</p><h2 id="4-Small-PDF"><a href="#4-Small-PDF" class="headerlink" title="4. Small PDF"></a>4. <a href="https://smallpdf.com/cn" target="_blank" rel="noopener">Small PDF</a></h2><p>有时，我们得到的 PDF 文件体积很大，打开或者传输给别人很不方便。那么就需要进行压缩了，压缩效果最好的软件就是这款 Small PDF了。 它的网页版是我们经常在搜索时会遇到的。</p><p>除此之外，它还有很多其他非常实用的功能。</p><p><img src="http://media.makcyun.top/18-9-8/89711681.jpg" alt=""></p><h2 id="5-PDFdo"><a href="#5-PDFdo" class="headerlink" title="5. PDFdo"></a>5. <a href="http://www.pdfdo.com/" target="_blank" rel="noopener">PDFdo</a></h2><p>除上面的之外，还会有几类需求，比如：只需要 PDF 文件中的某一部分，那么需要从 PDF 中进行提取或者分割；要将多个 PDF 合并成一个文件，那么需要合并 PDF；要将好几个 Word 文件同时转换为 PDF，那么有批量转换就最好了。</p><p>如果有上面的需求，那么这款软件是最佳选择，速度非常快。官方售价 99 RMB。</p><p><img src="http://media.makcyun.top/18-9-8/19958488.jpg" alt=""></p><p>以上这 5 款软件就足以应对和 PDF 相关的大部分问题了，但他们都是电脑软件。</p><p>这里再推荐一款手机上的软件。</p><h2 id="6-WPS-Office"><a href="#6-WPS-Office" class="headerlink" title="6. WPS Office"></a>6. WPS Office</h2><p><img src="http://media.makcyun.top/18-9-9/89469122.jpg" alt=""></p><p>别看到是「WPS」就摇头，手机版的和电脑版的 WPS 可谓 「天壤之别」。</p><p>除了阅读以外，还拥有：转换为 Word、提取 PDF、合并 PDF、转为图片、拍照扫描等几项核心功能。这款软件有免费版，不过高级版是需要付费的。</p><p>以上就是我一直在用的几款 PDF 软件，推荐给你。</p><p>有条件请支持正版，没有条件，就百度/谷歌/某宝搜索吧，实在找不到可以给我留言。</p><p>本文完。</p><p><img src="http://media.makcyun.top/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%85%B3%E6%B3%A8.jpg" alt=""></p><center>欢迎长按识别关注我的公众号</center><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;分享几款佳软。&lt;/p&gt;
    
    </summary>
    
      <category term="python爬虫" scheme="https://www.makcyun.top/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="pdf" scheme="https://www.makcyun.top/tags/pdf/"/>
    
      <category term="佳软" scheme="https://www.makcyun.top/tags/%E4%BD%B3%E8%BD%AF/"/>
    
  </entry>
  
  <entry>
    <title>Pyhton可视化(1): 20秒纵览十年中国大学排行榜变化</title>
    <link href="https://www.makcyun.top/2018/09/05/Python_visualization01.html"/>
    <id>https://www.makcyun.top/2018/09/05/Python_visualization01.html</id>
    <published>2018-09-05T05:27:22.000Z</published>
    <updated>2019-02-27T08:43:04.713Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>Python爬取近十年中国大学Top20强并结合D3.js做动态数据可视化表。</p><a id="more"></a><p><strong>摘要：</strong>：最近在朋友圈看到一个很酷炫的动态数据可视化表，介绍了新中国成立后各省GDP的发展历程，非常惊叹竟然还有这种操作，也想试试。于是，照葫芦画瓢虎，在网上爬取了历年中国大学学术排行榜，制作了一个中国大学排名Top20强动态表。</p><h2 id="1-作品介绍"><a href="#1-作品介绍" class="headerlink" title="1. 作品介绍"></a>1. 作品介绍</h2><p>这里先放一下这个动态表是什么样的：<br><a href="https://www.bilibili.com/video/av24503002" target="_blank" rel="noopener">https://www.bilibili.com/video/av24503002</a></p><p>不知道你看完是什么感觉，至少我是挺震惊的，想看看作者是怎么做出来的，于是追到了作者的B站主页，发现了更多有意思的动态视频：<br><img src="http://media.makcyun.top/18-9-6/84261830.jpg" alt=""></p><p>这些作品的作者是：@Jannchie见齐，他的主页：<br><a href="https://space.bilibili.com/1850091/#/video" target="_blank" rel="noopener">https://space.bilibili.com/1850091/#/video</a></p><p>这些会动的图表是如何做出来的呢？他用到的是一个动态图形显示数据的JavaScript库：<strong>D3.js</strong>，一种前端技术。难怪不是一般地酷炫。<br>那么，如果不会D3.js是不是就做不出来了呢？当然不是，Jannchie非常Open地给出了一个手把手简单教程：<br><a href="https://www.bilibili.com/video/av28087807" target="_blank" rel="noopener">https://www.bilibili.com/video/av28087807</a></p><p>他同时还开放了程序源码，你只需要做2步就能够实现：</p><ul><li><p>到他的Github主页下载源码到本地电脑：<br><a href="https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js" target="_blank" rel="noopener">https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</a><br><img src="http://media.makcyun.top/18-9-6/38399216.jpg" alt=""></p></li><li><p>打开<code>dist</code>文件夹里面的<code>exampe.csv</code>文件，放进你想要展示的数据，再用浏览器打开<code>bargraph.html</code>网页，就可以实现动态效果了。</p></li></ul><p>下面，我们稍微再说详细一点，实现这种效果的关键点。<br>最重要的是要有数据。观察一下上面的作品可以看到，横向柱状图中的数据要满足两个条件：一是要有多个对比的对象，二是要在时间上连续。这样才可以做出动态效果来。</p><p>看完后我立马就有了一个想法：<strong>想看看近十年中国的各个大学排名是个什么情况</strong>。下面我们就通过实实例来操作下。</p><h2 id="2-案例操作：中国大学Top20强"><a href="#2-案例操作：中国大学Top20强" class="headerlink" title="2. 案例操作：中国大学Top20强"></a>2. 案例操作：中国大学Top20强</h2><h3 id="2-1-数据来源"><a href="#2-1-数据来源" class="headerlink" title="2.1. 数据来源"></a>2.1. 数据来源</h3><p>世界上最权威的大学排名有4类，分别是：</p><ul><li>原上海交通大学的ARWU<br><a href="http://www.shanghairanking.com/ARWU2018.html" target="_blank" rel="noopener">http://www.shanghairanking.com/ARWU2018.html</a></li><li>英国教育组织的QS<br><a href="https://www.topuniversities.com/university-rankings/world-university-rankings/2018" target="_blank" rel="noopener">https://www.topuniversities.com/university-rankings/world-university-rankings/2018</a></li><li>泰晤士的THE<br><a href="https://www.timeshighereducation.com/world-university-rankings" target="_blank" rel="noopener">https://www.timeshighereducation.com/world-university-rankings</a></li><li>美国的usnews<br><a href="https://www.usnews.com/best-colleges/rankings" target="_blank" rel="noopener">https://www.usnews.com/best-colleges/rankings</a></li></ul><p>关于，这四类排名的更多介绍，可以看这个：<br><a href="https://www.zhihu.com/question/20825030/answer/71336291" target="_blank" rel="noopener">https://www.zhihu.com/question/20825030/answer/71336291</a></p><p>这里，我们选取相对比较权威也比较符合国情的第一个ARWU的排名结果。打开官网，可以看到有英文版和中文版排名，这里选取中文版。<br>排名非常齐全，从2003年到最新的2018年都有，非常好。<br><img src="http://media.makcyun.top/18-9-6/79756091.jpg" alt=""></p><p>同时，可以看到这是世界500强的大学排名，而我们需要的是中国（包括港澳台）的大学排名。怎么办呢？ 当然不能一年年地复制然后再从500条数据里一条条筛选出中国的，这里就要用爬虫来实现了。可以参考不久前的一篇爬取表格的文章：<br><a href="https://www.makcyun.top/web_scraping_withpython2.html">https://www.makcyun.top/web_scraping_withpython2.html</a></p><h3 id="2-2-抓取数据"><a href="#2-2-抓取数据" class="headerlink" title="2.2. 抓取数据"></a>2.2. 抓取数据</h3><h4 id="2-2-1-分析url"><a href="#2-2-1-分析url" class="headerlink" title="2.2.1. 分析url"></a>2.2.1. 分析url</h4><p>首先，分析一下URL:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http://www.zuihaodaxue.com/ARWU2018.html</span><br><span class="line">http://www.zuihaodaxue.com/ARWU2017.html</span><br><span class="line">...</span><br><span class="line">http://www.zuihaodaxue.com/ARWU2009.html</span><br></pre></td></tr></table></figure><p>可以看到，url非常有规律，只有年份数字在变，很简单就能构造出for循环。<br>格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br></pre></td></tr></table></figure><p>下面就可以开始写爬虫了。</p><h4 id="2-2-2-获取网页内容"><a href="#2-2-2-获取网页内容" class="headerlink" title="2.2.2. 获取网页内容"></a>2.2.2. 获取网页内容</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="comment"># 2009-2015用'gbk'，2016-2018用'utf-8'</span></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="comment"># return response.text  # text会乱码，content没有问题</span></span><br><span class="line">        <span class="keyword">return</span> response.content</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br></pre></td></tr></table></figure><p>上面需要注意的是，不同年份网页采用的编码不同，返回response.test会乱码，返回response.content则不会。关于编码乱码的问题，以后单独写一篇文章。</p><h4 id="2-2-3-解析表格"><a href="#2-2-3-解析表格" class="headerlink" title="2.2.3. 解析表格"></a>2.2.3. 解析表格</h4><p>用read_html函数一行代码来抓取表格，然后输出：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line">print(tb)</span><br></pre></td></tr></table></figure><p></p><p>可以看到，很顺利地表格就被抓取了下来：<br><img src="http://media.makcyun.top/18-9-6/80641562.jpg" alt="http://media.makcyun.top/18-9-6/80641562.jpg"><br>但是表格需要进行处理，比如删除掉不需要的评分列，增加年份列等，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 重命名表格列，不需要的列用数字表示</span></span><br><span class="line">tb.columns = [<span class="string">'world rank'</span>,<span class="string">'university'</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="string">'score'</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">tb.drop([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],axis = <span class="number">1</span>,inplace = <span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 删除后面不需要的评分列</span></span><br><span class="line"><span class="comment"># rank列100名后是区间，需需唯一化，增加一列index作为排名</span></span><br><span class="line">tb[<span class="string">'index_rank'</span>] = tb.index</span><br><span class="line">tb[<span class="string">'index_rank'</span>] = tb[<span class="string">'index_rank'</span>].astype(int) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一列年份列</span></span><br><span class="line">tb[<span class="string">'year'</span>] = i</span><br><span class="line"><span class="comment"># read_html没有爬取country，需定义函数单独爬取</span></span><br><span class="line">tb[<span class="string">'country'</span>] = get_country(html)</span><br><span class="line"><span class="keyword">return</span> tb</span><br></pre></td></tr></table></figure><p>需要注意的是，国家没有被抓取下来，因为国家是用的图片表示的，定位到国家代码位置：<br><img src="http://media.makcyun.top/18-9-6/39145641.jpg" alt="http://media.makcyun.top/18-9-6/39145641.jpg"></p><p>可以看到美国是用英文的USA表示的，那么我们可以单独提取出src属性，然后用正则提取出国家名称就可以了，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取国家名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_country</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    countries = soup.select(<span class="string">'td &gt; a &gt; img'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> countries:</span><br><span class="line">        src = i[<span class="string">'src'</span>]</span><br><span class="line">        pattern = re.compile(<span class="string">'flag.*\/(.*?).png'</span>)</span><br><span class="line">        country = re.findall(pattern,src)[<span class="number">0</span>]</span><br><span class="line">        lst.append(country)</span><br><span class="line">    <span class="keyword">return</span> lst</span><br></pre></td></tr></table></figure><p>然后，我们就可以输出一下结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">    world rank    university  score  index_rank  year      country</span><br><span class="line"><span class="number">0</span>            <span class="number">1</span>          哈佛大学  <span class="number">100.0</span>           <span class="number">1</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">1</span>            <span class="number">2</span>         斯坦福大学   <span class="number">75.6</span>           <span class="number">2</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">2</span>            <span class="number">3</span>          剑桥大学   <span class="number">71.8</span>           <span class="number">3</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">3</span>            <span class="number">4</span>        麻省理工学院   <span class="number">69.9</span>           <span class="number">4</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">4</span>            <span class="number">5</span>      加州大学-伯克利   <span class="number">68.3</span>           <span class="number">5</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">5</span>            <span class="number">6</span>        普林斯顿大学   <span class="number">61.0</span>           <span class="number">6</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">6</span>            <span class="number">7</span>          牛津大学   <span class="number">60.0</span>           <span class="number">7</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">7</span>            <span class="number">8</span>        哥伦比亚大学   <span class="number">58.2</span>           <span class="number">8</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">8</span>            <span class="number">9</span>        加州理工学院   <span class="number">57.4</span>           <span class="number">9</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">9</span>           <span class="number">10</span>         芝加哥大学   <span class="number">55.5</span>          <span class="number">10</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">10</span>          <span class="number">11</span>      加州大学-洛杉矶   <span class="number">51.2</span>          <span class="number">11</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">11</span>          <span class="number">12</span>         康奈尔大学   <span class="number">50.7</span>          <span class="number">12</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">12</span>          <span class="number">12</span>          耶鲁大学   <span class="number">50.7</span>          <span class="number">13</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">13</span>          <span class="number">14</span>     华盛顿大学-西雅图   <span class="number">50.0</span>          <span class="number">14</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">14</span>          <span class="number">15</span>     加州大学-圣地亚哥   <span class="number">47.8</span>          <span class="number">15</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">15</span>          <span class="number">16</span>       宾夕法尼亚大学   <span class="number">46.4</span>          <span class="number">16</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">16</span>          <span class="number">17</span>        伦敦大学学院   <span class="number">46.1</span>          <span class="number">17</span>  <span class="number">2018</span>           UK</span><br><span class="line"><span class="number">17</span>          <span class="number">18</span>      约翰霍普金斯大学   <span class="number">45.4</span>          <span class="number">18</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">18</span>          <span class="number">19</span>     苏黎世联邦理工学院   <span class="number">43.9</span>          <span class="number">19</span>  <span class="number">2018</span>  Switzerland</span><br><span class="line"><span class="number">19</span>          <span class="number">20</span>    华盛顿大学-圣路易斯   <span class="number">42.1</span>          <span class="number">20</span>  <span class="number">2018</span>          USA</span><br><span class="line"><span class="number">20</span>          <span class="number">21</span>      加州大学-旧金山   <span class="number">41.9</span>          <span class="number">21</span>  <span class="number">2018</span>          USA</span><br></pre></td></tr></table></figure><p>数据很完美，接下来就可以按照D3.js模板中的example.csv文件的格式作进一步的处理了。</p><h3 id="2-3-数据处理"><a href="#2-3-数据处理" class="headerlink" title="2.3. 数据处理"></a>2.3. 数据处理</h3><p>这里先将数据输出为<code>university.csv</code>文件，结果见下表：</p><p><img src="http://media.makcyun.top/18-9-6/44505347.jpg" alt="http://media.makcyun.top/18-9-6/44505347.jpg"></p><p>10年一共5011行×6列数据。接着，读入该表作进一步数据处理，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'university.csv'</span>)</span><br><span class="line"><span class="comment"># 包含港澳台</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只包括内地</span></span><br><span class="line">df = df.query(<span class="string">"(country == 'China')"</span>)</span><br><span class="line">df[<span class="string">'index_rank_score'</span>] = df[<span class="string">'index_rank'</span>]</span><br><span class="line"><span class="comment"># 将index_rank列转为整形</span></span><br><span class="line">df[<span class="string">'index_rank'</span>] = df[<span class="string">'index_rank'</span>].astype(int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 美国</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'UnitedStates')|(country == 'USA')")</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#求topn名</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(df)</span>:</span></span><br><span class="line">    top = df.sort_values([<span class="string">'year'</span>,<span class="string">'index_rank'</span>],ascending = <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> top[:<span class="number">20</span>].reset_index()</span><br><span class="line">df = df.groupby(by =[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改列顺序</span></span><br><span class="line">df = df[[<span class="string">'university'</span>,<span class="string">'index_rank_score'</span>,<span class="string">'index_rank'</span>,<span class="string">'year'</span>]]</span><br><span class="line"><span class="comment"># 重命名列</span></span><br><span class="line">df.rename (columns = &#123;<span class="string">'university'</span>:<span class="string">'name'</span>,<span class="string">'index_rank_score'</span>:<span class="string">'type'</span>,<span class="string">'index_rank'</span>:<span class="string">'value'</span>,<span class="string">'year'</span>:<span class="string">'date'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">df.to_csv(<span class="string">'university_ranking.csv'</span>,mode =<span class="string">'w'</span>,encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># index可以设置</span></span><br></pre></td></tr></table></figure><p>上面需要注意两点：</p><ul><li>可以提取包含港澳台在内的大中华区所有的大学，也可以只提取内地的大学，还可以提取世界、美国等各种排名。</li><li>定义了一个求Topn的函数，能够按年份分别求出各年的前20名大学名单。</li></ul><p>打开输出的<code>university_ranking.csv</code>文件：<br><img src="http://media.makcyun.top/18-9-6/64400003.jpg" alt="http://media.makcyun.top/18-9-6/64400003.jpg"></p><p>结果非常好，可以直接作为D3.js的导入文件了。</p><h4 id="2-3-1-完整代码"><a href="#2-3-1-完整代码" class="headerlink" title="2.3.1. 完整代码"></a>2.3.1. 完整代码</h4><p>将代码再稍微完善一下，完整地代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"><span class="comment"># 获取网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(year)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            headers = &#123;</span><br><span class="line">                <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment"># 英文版</span></span><br><span class="line">            <span class="comment"># url = 'http://www.shanghairanking.com/ARWU%s.html' % (str(year))</span></span><br><span class="line">            <span class="comment"># 中文版</span></span><br><span class="line">            url = <span class="string">'http://www.zuihaodaxue.com/ARWU%s.html'</span> % (str(year))</span><br><span class="line">            response = requests.get(url,headers = headers)</span><br><span class="line">            <span class="comment"># 2009-2015用'gbk'，2016-2018用'utf-8'</span></span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="comment"># return response.text  # text会乱码，content没有问题</span></span><br><span class="line">                <span class="comment"># https://stackoverflow.com/questions/17011357/what-is-the-difference-between-content-and-text</span></span><br><span class="line">                <span class="keyword">return</span> response.content</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">except</span> RequestException:</span><br><span class="line">            print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html,i)</span>:</span></span><br><span class="line">        tb = pd.read_html(html)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 重命名表格列，不需要的列用数字表示</span></span><br><span class="line">        tb.columns = [<span class="string">'world rank'</span>,<span class="string">'university'</span>, <span class="number">2</span>,<span class="number">3</span>, <span class="string">'score'</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]</span><br><span class="line">        tb.drop([<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>],axis = <span class="number">1</span>,inplace = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 删除后面不需要的评分列</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># rank列100名后是区间，需需唯一化，增加一列index作为排名</span></span><br><span class="line">        tb[<span class="string">'index_rank'</span>] = tb.index</span><br><span class="line">        tb[<span class="string">'index_rank'</span>] = tb[<span class="string">'index_rank'</span>].astype(int) + <span class="number">1</span></span><br><span class="line">        <span class="comment"># 增加一列年份列</span></span><br><span class="line">        tb[<span class="string">'year'</span>] = i</span><br><span class="line">        <span class="comment"># read_html没有爬取country，需定义函数单独爬取</span></span><br><span class="line">        tb[<span class="string">'country'</span>] = get_country(html)</span><br><span class="line">        <span class="comment"># print(tb) # 测试表格ok</span></span><br><span class="line">        <span class="keyword">return</span> tb</span><br><span class="line">        <span class="comment"># print(tb.info()) # 查看表信息</span></span><br><span class="line">        <span class="comment"># print(tb.columns.values) # 查看列表名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家名称</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_country</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    countries = soup.select(<span class="string">'td &gt; a &gt; img'</span>)</span><br><span class="line">    lst = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> countries:</span><br><span class="line">        src = i[<span class="string">'src'</span>]</span><br><span class="line">        pattern = re.compile(<span class="string">'flag.*\/(.*?).png'</span>)</span><br><span class="line">        country = re.findall(pattern,src)[<span class="number">0</span>]</span><br><span class="line">        lst.append(country)</span><br><span class="line">    <span class="keyword">return</span> lst</span><br><span class="line">    <span class="comment"># print(lst) # 测试提取国家是否成功ok</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存表格为csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_csv</span><span class="params">(tb)</span>:</span></span><br><span class="line">    tb.to_csv(<span class="string">r'university.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    endtime = time.time()-start_time</span><br><span class="line">    <span class="comment"># print('程序运行了%.2f秒' %endtime)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analysis</span><span class="params">()</span>:</span></span><br><span class="line">    df = pd.read_csv(<span class="string">'university.csv'</span>)</span><br><span class="line">    <span class="comment"># 包含港澳台</span></span><br><span class="line">    <span class="comment"># df = df.query("(country == 'China')|(country == 'China-hk')|(country == 'China-tw')|(country == 'China-HongKong')|(country == 'China-Taiwan')|(country == 'Taiwan,China')|(country == 'HongKong,China')")[['university','year','index_rank']]</span></span><br><span class="line">    <span class="comment"># 只包括内地</span></span><br><span class="line">    df = df.query(<span class="string">"(country == 'China')"</span>)[[<span class="string">'university'</span>,<span class="string">'year'</span>,<span class="string">'index_rank'</span>]]</span><br><span class="line"></span><br><span class="line">    df[<span class="string">'index_rank_score'</span>] = df[<span class="string">'index_rank'</span>]</span><br><span class="line">    <span class="comment"># 将index_rank列转为整形</span></span><br><span class="line">    df[<span class="string">'index_rank'</span>] = df[<span class="string">'index_rank'</span>].astype(int)</span><br><span class="line">    <span class="comment"># 美国</span></span><br><span class="line"><span class="comment"># df = df.query("(country == 'UnitedStates')|(country == 'USA')")[['university','year','index_rank']]</span></span><br><span class="line">    <span class="comment">#求topn名</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">(df)</span>:</span></span><br><span class="line">        top = df.sort_values([<span class="string">'year'</span>,<span class="string">'index_rank'</span>],ascending = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> top[:<span class="number">20</span>].reset_index()</span><br><span class="line">    df = df.groupby(by =[<span class="string">'year'</span>]).apply(topn)</span><br><span class="line">    <span class="comment"># 更改列顺序</span></span><br><span class="line">    df = df[[<span class="string">'university'</span>,<span class="string">'index_rank_score'</span>,<span class="string">'index_rank'</span>,<span class="string">'year'</span>]]</span><br><span class="line">    <span class="comment"># 重命名列</span></span><br><span class="line">    df.rename (columns = &#123;<span class="string">'university'</span>:<span class="string">'name'</span>,<span class="string">'index_rank_score'</span>:<span class="string">'type'</span>,<span class="string">'index_rank'</span>:<span class="string">'value'</span>,<span class="string">'year'</span>:<span class="string">'date'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    df.to_csv(<span class="string">'university_ranking.csv'</span>,mode =<span class="string">'w'</span>,encoding=<span class="string">'utf_8_sig'</span>, header=<span class="keyword">True</span>, index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># index可以设置</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(year)</span>:</span></span><br><span class="line">    <span class="comment"># generate_mysql()</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2009</span>,year):  <span class="comment">#抓取10年</span></span><br><span class="line">        <span class="comment"># get_one_page(i)</span></span><br><span class="line">        html = get_one_page(i)</span><br><span class="line">        <span class="comment"># parse_one_page(html,i)  # 测试表格ok</span></span><br><span class="line">        tb = parse_one_page(html,i)</span><br><span class="line">        save_csv(tb)</span><br><span class="line">        print(i,<span class="string">'年排名提取完成完成'</span>)</span><br><span class="line">        analysis()</span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(<span class="number">2019</span>)</span><br><span class="line">    <span class="comment"># 2016-2018采用gb2312编码，2009-2015采用utf-8编码</span></span><br></pre></td></tr></table></figure><p>至此，我们已经有<code>university_ranking.csv</code>基础数据，下面就可以进行可视化呈现了。</p><h3 id="2-4-可视化呈现"><a href="#2-4-可视化呈现" class="headerlink" title="2.4. 可视化呈现"></a>2.4. 可视化呈现</h3><p>首先，到见齐的github主页：<br><a href="https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js" target="_blank" rel="noopener">https://github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</a></p><h4 id="2-4-1-克隆仓库文件"><a href="#2-4-1-克隆仓库文件" class="headerlink" title="2.4.1. 克隆仓库文件"></a>2.4.1. 克隆仓库文件</h4><p>如果你平常使用github或者Git软件的话，那么就找个合适文件存放目录，然后直接在 GitBash里分别输入下面3条命令就搭建好环境了：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 克隆项目仓库</span><br><span class="line">git clone https:<span class="comment">//github.com/Jannchie/Historical-ranking-data-visualization-based-on-d3.js</span></span><br><span class="line"># 切换到项目根目录</span><br><span class="line">cd Historical-ranking-data-visualization-based-on-d3.js</span><br><span class="line"># 安装依赖</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>如果你此前没有用过上面的软件，你可以直接点击<code>Download Zip</code>下载下来然后解压即可，不过还是强烈建议使用第一种方法，因为后面如果要自定义可视化效果的话，需要修改代码然后执行<code>npm run build</code>命令才能够看到效果。</p><h4 id="2-4-2-效果呈现"><a href="#2-4-2-效果呈现" class="headerlink" title="2.4.2. 效果呈现"></a>2.4.2. 效果呈现</h4><p>好，所有基本准备都已完成，下面就可以试试看效果了。<br>任意浏览器打开<code>bargraph.html</code>网页，点击选择文件，然后选择：前面输出的<code>university_ranking.csv</code>文件，看下效果吧：<br><a href="https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0" target="_blank" rel="noopener">https://v.youku.com/v_show/id_XMzgxMjkzMzI4NA==.html?spm=a2hzp.8244740.0.0</a></p><p>可以看到，有了大致的可视化效果，但还存在很多瑕疵，比如：表顺序颠倒了、字体不合适、配色太花哨等。可不可以修改呢？</p><p>当然是可以的，只需要分别修改文件夹中这几个文件的参数就可以了：</p><ul><li>config.js 全局设置各项功能的开关，比如配色、字体、文字名称、反转图表等等功能；</li><li>color.css 修改柱形图的配色；</li><li>stylesheet.css 具体修改配色、字体、文字名称等的css样式；</li><li>visual.js 更进一步的修改，比如图表的透明度等。</li></ul><p>知道在哪里修改了以后，那么，如何修改呢？很简单，只需要简单的几步就可以实现：</p><ul><li><p>打开网页，<code>右键-检查</code>，箭头指向想要修改的元素，然后在右侧的css样式表里，双击各项参数修改参数，修改完元素就会发生变化，可以不断微调，直至满意为止。<br><img src="http://media.makcyun.top/18-9-6/63524613.jpg" alt=""></p></li><li><p>把参数复制到四个文件中对应的文件里并保存。</p></li><li>Git Bash不断重复运行<code>npm run build</code>，之后刷新网页就可以看到优化后的效果。</li></ul><p>最后，再添加一个合适的BGM就可以了。以下是我优化之后的效果：<br><a href="https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0" target="_blank" rel="noopener">https://v.youku.com/v_show/id_XMzgxMjgyNTE2OA==.html?spm=a2hzp.8244740.0.0</a><br><em>BGM：ツナ覚醒</em></p><p>如果你不太会调整，没有关系，我会分享优化后的配置文件。</p><p>以上，就是实现动态可视化表的步骤。 同样地，只要更改数据源可以很方便地做出世界、美国等大学的动态效果，可以看看：<br>中国（含港澳台）大学排名：<br><a href="http://media2.makcyun.top/Greater_China_uni_ranking.mp4" target="_blank" rel="noopener">http://media2.makcyun.top/Greater_China_uni_ranking.mp4</a><br>美国大学排名：<br><a href="http://media2.makcyun.top/USA_uni_ranking.mp4" target="_blank" rel="noopener">http://media2.makcyun.top/USA_uni_ranking.mp4</a></p><p>文章所有的素材，在公众号后台回复<strong>大学排名</strong>就可以得到，或者到我的github下载：<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a><br>感兴趣的话就动手试试吧。</p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Python爬取近十年中国大学Top20强并结合D3.js做动态数据可视化表。&lt;/p&gt;
    
    </summary>
    
      <category term="Python可视化" scheme="https://www.makcyun.top/categories/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="Python可视化" scheme="https://www.makcyun.top/tags/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(4)：图片批量下载-以澎湃网信息图为例</title>
    <link href="https://www.makcyun.top/2018/09/02/web_scraping_withpython4.html"/>
    <id>https://www.makcyun.top/2018/09/02/web_scraping_withpython4.html</id>
    <published>2018-09-02T00:00:57.468Z</published>
    <updated>2018-12-19T03:12:32.961Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。</p><a id="more"></a><p><strong>摘要：</strong> 上一篇文章介绍了单页图片的爬取，但是当爬取多页时，难度会增加。同时，前几篇爬虫文章中的网站有一个明显的特点是：可以通过点击鼠标实现网页的翻页，并且url会发生相应的变化。除了此类网站以外，还有一类非常常见的网站特点是：没有”下一页”这样的按钮，而是”加载更多”或者会不断自动刷新从而呈现出更多的内容，同时网页url也不发生变化。这种类型的网页通常采用的是Ajax技术，要抓取其中的网页内容需要采取一定的技巧。本文以信息图做得非常棒的澎湃”美数课”为例，抓取该栏目至今所有文章的图片。<br>栏目网址：<a href="https://www.thepaper.cn/list_25635" target="_blank" rel="noopener">https://www.thepaper.cn/list_25635</a></p><p><img src="http://media.makcyun.top/18-9-2/30853746.jpg" alt=""><br><img src="http://media.makcyun.top/18-9-2/51553778.jpg" alt=""></p><p><strong>本文知识点：</strong></p><ul><li>Ajax知识</li><li>多页图片爬取</li></ul><h2 id="1-Ajax知识"><a href="#1-Ajax知识" class="headerlink" title="1. Ajax知识"></a>1. Ajax知识</h2><p>在该主页上尝试不断下拉，会发现网页不断地加载出新的文章内容来，而并不需要通过点击”下一页”来实现，而且网址url也保持不变。也就是说在同一个网页中通过下拉源源不断地刷新出了网页内容。这种形式的网页在今天非常常见，它们普遍是采用了<strong>Ajax</strong>技术。</p><blockquote><p>Ajax 全称是 Asynchronous JavaScript and XML（异步 JavaScript 和 XML）。<br>它不是一门编程语言，而是利用 JavaScript 在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。</p></blockquote><p>Ajax更多参考：<br><a href="https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html" target="_blank" rel="noopener">https://germey.gitbooks.io/python3webspider/content/6.2-Ajax%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95.html</a></p><p>采用了Ajax的网页和普通的网页有一定的区别，普通网页的爬虫代码放在这种类型的网页上就行不通了，必须另辟出路。下面我们就来尝试一下如何爬取网易”数读”所有的文章。</p><p>主页<code>右键-检查</code>，然后按<code>f5</code>刷新，会弹出很多链接文件。鼠标上拉回到第一个文件：<strong>list_25635</strong>，在右侧按<code>ctrl+f</code>搜索一下第一篇文章的标题：”娃娃机生意经”，可以看到在html网页中找到了对应的源代码。</p><p><img src="http://media.makcyun.top/18-9-2/67533549.jpg" alt=""></p><p>接着，我们拖动下拉鼠标，显示出更多文章。然后再次搜索一篇文章的标题：”金砖峰会”，会发现搜不到相应的内容了。是不是感觉很奇怪？</p><p><img src="http://media.makcyun.top/18-9-2/72910700.jpg" alt=""></p><p>其实，这里就是用了Ajax的技术，和普通网页翻页是刷新整个网页不同，这种类型网页可以再保持url不变的前提下只刷新部分内容。这就为我们进行爬虫带来了麻烦。因为，我们通过解析网页的url：<code>https://www.thepaper.cn/list_25635</code>只能爬取前面部分的内容而后面通过下拉刷新出来的内容是爬取不到的。这显然不完美，那么怎么才能够爬取到后面不断刷新出来的网页内容呢？</p><h2 id="2-url分析"><a href="#2-url分析" class="headerlink" title="2. url分析"></a>2. url分析</h2><p>我们把右侧的选项卡从<code>ALL</code>切换到<code>Network</code>，然后按再次按<code>f5</code>刷新，可以发现<code>Name</code>列有4个结果。选择第3个链接打开并点击<code>Response</code>，通过滑动可以看到一些文本内容和网页中的文章标题是一一对应的。比如第一个是：<strong>娃娃机生意经｜有没有好奇过抓娃娃机怎么又重新火起来了？</strong>，一直往下拖拽可以看到有很多篇文章。此时，再切换到headers选项卡，复制<code>Request URL</code>后面的链接并打开，会显示一部分文章的标题和图片内容。数一下的话，可以发现一共有20个文章标题，也就是对应着20篇文章。</p><p>这个链接其实和上面的<strong>list_25635</strong>链接的内容是一致的。这样看来，好像发现不了什么东西，不过不要着急。<br><img src="http://media.makcyun.top/18-9-2/40705043.jpg" alt=""><br><img src="http://media.makcyun.top/18-9-2/642179.jpg" alt=""></p><p>接下来，回到<code>Name</code>列，尝试滚动下拉鼠标，会发现弹出好几个新的开头为<code>load_index</code>的链接来。选中第一个<code>load_index</code>的链接，点击<code>Response</code>查看一下html源代码，尝试在网页中搜索一下：<code>十年金砖峰</code>这个文章的标题，惊奇地发现，在网页中找到了对于的文章标题。而前面，我们搜索这个词时，是没有搜索到的。<br><img src="http://media.makcyun.top/18-9-2/83879931.jpg" alt=""></p><p>这说明了什么呢？说明<code>十年金砖峰</code>这篇文章的内容不在第一个<strong>list_25635</strong>链接中，而在这个<code>load_index</code>的链接里。鼠标点击<code>headers</code>，复制<code>Request URL</code>后面的链接并打开，就可以再次看到包括这篇文章在内的新的20篇文章。</p><p><img src="http://media.makcyun.top/18-9-2/4302921.jpg" alt=""></p><p>是不是发现了点了什么？接着，我们继续下拉，会发现弹出更多的<code>load_index</code>的链接。再搜索一个标题：<code>地图湃｜海外港口热</code>，可以发现在网页中也同样找到了文章标题。<br><img src="http://media.makcyun.top/18-9-2/68563785.jpg" alt=""></p><p>回到我们的初衷：<strong>下载所有网页的图片内容</strong>。那么现在就有解决办法礼：一个个地把出现的这些url网址中图片下载下来就大功告成了。</p><p>好，我们先来分析一下这些url，看看有没有相似性，如果有很明显的相似性，那么就可以像普通网页那样，通过构造翻页页数的url，实现for循环就可以批量下载所有网页的图片了。复制前3个链接如下：<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=2&amp;isList=true&amp;lastTime=1533169319712  </span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=3&amp;isList=true&amp;lastTime=1528625875167  </span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;topCids=&amp;pageidx=4&amp;isList=true&amp;lastTime=1525499007926</span><br></pre></td></tr></table></figure><p></p><p>发现<code>pageidx</code>键的值呈现规律的数字递增变化，看起来是个好消息。但同时发现后面的lastTime键的值看起来是随机变化的，这个有没有影响呢？ 来测试一下，复制第一个链接，删掉<code>&amp;lastTime=1533169319712</code>这一串字符，会发现网页一样能够正常打开，就说明着一对参数不影响网页内容，那就太好了。我们可以删除掉，这样所有url的区别只剩<code>pageidx</code>的值了，这时就可以构造url来实现for循环了。构造的url形式如下：<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=2</span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=3</span><br><span class="line">https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=4</span><br></pre></td></tr></table></figure><p></p><p>同时，尝试把数字2改成1并打开链接看看会有什么变化，发现呈现的内容就是第1页的内容。这样，我们就可以从第一页开始构造url循环了。<br><code>https://www.thepaper.cn/load_index.jsp?nodeids=25635&amp;pageidx=1</code></p><p>既然确定了首页，那么也要相应地确定一下尾页。很简单，我们把数字改大然后打开链接看是否有内容即可。比如改为<strong>10 </strong>，打开发现有内容显示，很好。接着，再改为30，发现没有内容了。说明该栏目的页数介于这两个数之间，尝试几次后，发现<code>25</code>是最后一个有内容的网页，也意味着能够爬取的页数一共是25页。</p><p>确定了首页和尾页后，下面我们就可以开始构造链接，先爬取第一篇文章网页里的图片（这个爬取过程，我们上一篇爬取网易”数读”已经尝试过了），然后爬取这一整页的图片，最后循环25页，爬取所有图片，下面开始吧。</p><h2 id="3-程序代码"><a href="#3-程序代码" class="headerlink" title="3. 程序代码"></a>3. 程序代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 获取索引界面网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_index</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment"># 下载1页</span></span><br><span class="line">    <span class="comment"># url = 'https://www.thepaper.cn/newsDetail_forward_2370041'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2下载多页，构造url</span></span><br><span class="line">    paras = &#123;</span><br><span class="line">        <span class="string">'nodeids'</span>: <span class="number">25635</span>,</span><br><span class="line">        <span class="string">'pageidx'</span>: i</span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'https://www.thepaper.cn/load_index.jsp?'</span> + urlencode(paras)</span><br><span class="line"></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="comment"># print(response.text)  # 测试网页内容是否提取成功ok</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 解析索引界面网页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_index</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每页文章数</span></span><br><span class="line">    num = soup.find_all(name = <span class="string">'div'</span>,class_=<span class="string">'news_li'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)):</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="comment"># 获取title</span></span><br><span class="line">        <span class="string">'title'</span>:soup.select(<span class="string">'h2 a'</span>)[i].get_text(),</span><br><span class="line">        <span class="comment"># 获取图片url，需加前缀</span></span><br><span class="line">        <span class="string">'url'</span>:<span class="string">'https://www.thepaper.cn/'</span> + soup.select(<span class="string">'h2 a'</span>)[i].attrs[<span class="string">'href'</span>]</span><br><span class="line">        <span class="comment"># print(url)  # 测试图片链接</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 获取每条文章的详情页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_detail</span><span class="params">(item)</span>:</span></span><br><span class="line">    url = item.get(<span class="string">'url'</span>)</span><br><span class="line">    <span class="comment"># 增加异常捕获语句</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,headers = headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="comment"># print(response.text)  # 测试网页内容是否提取成功</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        print(<span class="string">'网页请求失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 4 解析每条文章的详情页内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page_detail</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># 获取title</span></span><br><span class="line">    <span class="keyword">if</span> soup.h1:  <span class="comment">#有的网页没有h1节点，因此必须要增加判断，否则会报错</span></span><br><span class="line">        title = soup.h1.string</span><br><span class="line">        <span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">        items = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>,<span class="string">'600'</span>])</span><br><span class="line">        <span class="comment"># 有的图片节点用width='100%'表示，有的用600表示，因此用list合并选择</span></span><br><span class="line">        <span class="comment"># https://blog.csdn.net/w_xuechun/article/details/76093950</span></span><br><span class="line">        <span class="comment"># print(items) # 测试返回的img节点ok</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(items)):</span><br><span class="line">            pic = items[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">            <span class="comment"># print(pic) #测试图片链接ok</span></span><br><span class="line">            <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'title'</span>:title,</span><br><span class="line">            <span class="string">'pic'</span>:pic,</span><br><span class="line">            <span class="string">'num'</span>:i  <span class="comment"># 图片添加编号顺序</span></span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment"># 5 下载图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pic</span><span class="params">(pic)</span>:</span></span><br><span class="line">    title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">    <span class="comment"># 标题规范命名：去掉符号非法字符| 等</span></span><br><span class="line">     title = re.sub(<span class="string">'[\/:*?"&lt;&gt;|]'</span>,<span class="string">'-'</span>,title).strip()</span><br><span class="line">    url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line">    <span class="comment"># 设置图片编号顺序</span></span><br><span class="line">    num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">        os.mkdir(title)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片url网页信息</span></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 建立图片存放地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line">            <span class="comment"># 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest()</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="comment"># 开始下载图片</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">                    print(<span class="string">'文章"&#123;0&#125;"的第&#123;1&#125;张图片下载完成'</span> .format(title,num))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'该图片%s 已下载'</span> %title)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e,<span class="string">'图片获取失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="comment"># get_page_index(i) # 测试索引界面网页内容是否获取成功ok</span></span><br><span class="line"></span><br><span class="line">    html = get_page_index(i)</span><br><span class="line">    data = parse_page_index(html)  <span class="comment"># 测试索引界面url是否获取成功ok</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># print(item)  #测试返回的dict</span></span><br><span class="line">        html = get_page_detail(item)</span><br><span class="line">        data = parse_page_detail(html)</span><br><span class="line">        <span class="keyword">for</span> pic <span class="keyword">in</span> data:</span><br><span class="line">            save_pic(pic)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>):</span><br><span class="line">        main(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    pool.map(main,[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">26</span>)])</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://media.makcyun.top/18-9-2/46673865.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-9-3/4372426.jpg" alt=""></p><p>文章代码和栏目从2015年至今437篇文章共1509张图片资源，可在下方链接中得到。<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><p>本文完。</p><p><img src="http://media.makcyun.top/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;澎湃网文章的质量不错，它的”美数课”栏目的信息图做得也很好。图片干货多还能带来ppt和图表制作的技巧。为了更方便浏览所有文章图片，通过分析Ajax爬取栏目至今所有信息图的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫(3)：单页图片下载-网易&quot;数读&quot;信息图</title>
    <link href="https://www.makcyun.top/2018/09/01/web_scraping_withpython3.html"/>
    <id>https://www.makcyun.top/2018/09/01/web_scraping_withpython3.html</id>
    <published>2018-09-01T06:49:30.854Z</published>
    <updated>2018-12-19T03:12:32.960Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。</p><a id="more"></a><p><img src="http://media.makcyun.top/18-9-2/65507772.jpg" alt=""></p><p><strong>本文知识点：</strong></p><ul><li>单张图片下载</li><li>单页图片下载</li><li>Ajax技术介绍</li></ul><h2 id="1-单张图片下载"><a href="#1-单张图片下载" class="headerlink" title="1. 单张图片下载"></a>1. 单张图片下载</h2><p>以一篇最近比较热的涨房价的文章为例：<a href="http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html" target="_blank" rel="noopener">暴涨的房租，正在摧毁中国年轻人的生活</a>，从文章里随意挑选一张<a href="http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png" target="_blank" rel="noopener">北京房租地图图片</a>，通过<strong>Requests的content属性</strong>来实现单张图片的下载。<br><img src="http://media.makcyun.top/18-9-2/14528855.jpg" alt=""><br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'北京房租地图.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure><p></p><p>5行代码就能将这张图片下载到电脑上。不只是该张图片，任意图片都可以下载，只要替换图片的url即可。<br>这里用到了<strong>Requests的content属性</strong>，将图片存储为二进制数据。至于，图片为什么可以用二进制数据进行存储，可以参考这个教程：<br><a href="https://www.zhihu.com/question/36269548/answer/66734582" target="_blank" rel="noopener">https://www.zhihu.com/question/36269548/answer/66734582</a></p><p>5行代码看起来很短，但如果只是下载一张图片显然没有必要写代码，”右键另存为”更快。现在，我们放大一下范围，去下载这篇文章中的所有图片。粗略数一下，网页里有超过15张图片，这时，如果再用”右键另存为”的方法，显然就比较繁琐了。下面，我们用代码来实现下载该网页中的所有图片。</p><h2 id="2-单页图片下载"><a href="#2-单页图片下载" class="headerlink" title="2. 单页图片下载"></a>2. 单页图片下载</h2><h3 id="2-1-Requests获取网页内容"><a href="#2-1-Requests获取网页内容" class="headerlink" title="2.1. Requests获取网页内容"></a>2.1. Requests获取网页内容</h3><p>首先，用堪称python”爬虫利器”的<strong>Requests库</strong>来获取该篇文章的html内容。<br>Requests库可以说是一款python爬虫的利器，它的更多用法，可参考下面的教程：<br><a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/index.html</a><br><a href="https://cuiqingcai.com/2556.html" target="_blank" rel="noopener">https://cuiqingcai.com/2556.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">url = <span class="string">'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'</span></span><br><span class="line"></span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="comment"># return response.text</span></span><br><span class="line">    print(response.text)  <span class="comment"># 测试网页内容是否提取成功ok</span></span><br></pre></td></tr></table></figure><h3 id="2-2-解析网页内容"><a href="#2-2-解析网页内容" class="headerlink" title="2.2. 解析网页内容"></a>2.2. 解析网页内容</h3><p>通过上面方法可以获取到html内容，接下来解析html字符串内容，从中提取出网页内的图片url。解析和提取url的方法有很多种，常见的有5种，分别是：正则表达式、Xpath、BeautifulSoup、CSS、PyQuery。任选一种即可，这里为了再次加强练习，5种方法全部尝试一遍。<br>首先，在网页中定位到图片url所在的位置，如下图所示：<br><img src="http://media.makcyun.top/18-9-2/91340067.jpg" alt=""></p><p>从外到内定位url的位置：<code>&lt;p&gt;节点-&lt;a&gt;节点-&lt;img&gt;节点里的src属性值</code>。</p><h4 id="2-2-1-正则表达式"><a href="#2-2-1-正则表达式" class="headerlink" title="2.2.1. 正则表达式"></a>2.2.1. 正则表达式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern =re.compile(<span class="string">'&lt;p&gt;.*?&lt;img alt="房租".*?src="(.*?)".*?style'</span>,re.S)</span><br><span class="line">    items = re.findall(pattern,html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行结果如下:<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/425eca61322a4f99837988bb78a001ac.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/df39aac05e0b417a80487562cdf6ca40.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/d6cb58a6bb014b8683b232f3c00f0e39.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/88d2e535765a4ed09e03877238647aa5.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/09/01/98d2f9579e9e49aeb76ad6155e8fc4ea.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7410ed4041a94cab8f30e8de53aaaaa1.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/49a0c80a140b4f1aa03724654c5a39af.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3070964278bf4637ba3d92b6bb771cea.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/812b7a51475246a9b57f467940626c5c.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/8bcbc7d180f74397addc74e47eaa1f63.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/e593efca849744489096a77aafd10d3e.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/7653feecbfd94758a8a0ff599915d435.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/edbaa24a17dc4cca9430761bfc557ffb.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/f768d440d9f14b8bb58e3c425345b97e.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/3430043fd305411782f43d3d8635d632.png'&#125;</span><br><span class="line">&#123;'url': 'http://cms-bucket.nosdn.127.net/2018/08/31/111ba73d11084c68b8db85cdd6d474a7.png'&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="2-2-2-Xpath语法"><a href="#2-2-2-Xpath语法" class="headerlink" title="2.2.2. Xpath语法"></a>2.2.2. Xpath语法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'*//p//img[@alt = "房租"]/@src'</span>)</span><br><span class="line">    print(items)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>结果同上。</p><h4 id="2-2-3-CSS选择器"><a href="#2-2-3-CSS选择器" class="headerlink" title="2.2.3. CSS选择器"></a>2.2.3. CSS选择器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = soup.select(<span class="string">'p &gt; a &gt; img'</span>) <span class="comment">#&gt;表示下级绝对节点</span></span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'url'</span>:item[<span class="string">'src'</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h4 id="2-2-4-BeautifulSoup-find-all方法"><a href="#2-2-4-BeautifulSoup-find-all方法" class="headerlink" title="2.2.4. BeautifulSoup find_all方法"></a>2.2.4. BeautifulSoup find_all方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"><span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">item = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(item)):</span><br><span class="line">    url = item[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'url'</span>:url</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># print(pic) #测试图片链接ok</span></span><br></pre></td></tr></table></figure><h4 id="2-2-5-PyQuery"><a href="#2-2-5-PyQuery" class="headerlink" title="2.2.5. PyQuery"></a>2.2.5. PyQuery</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">data = pq(html)</span><br><span class="line">data2 = data(<span class="string">'p &gt; a &gt; img'</span>)</span><br><span class="line"><span class="comment"># print(items)</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> data2.items():   <span class="comment">#注意这里和BeautifulSoup 的css用法不同</span></span><br><span class="line">    <span class="keyword">yield</span>&#123;</span><br><span class="line">    <span class="string">'url'</span>:item.attr(<span class="string">'src'</span>)</span><br><span class="line">    <span class="comment"># 或者'url':item.attr.src</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>以上用了5种方法提取出了该网页的url地址，任选一种即可。这里假设选择了第4种方法，接下来就可以下载图片了。提取出的网址是一个<strong>dict字典</strong>，通过dict的get方法调用里面的键和值。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line"><span class="comment"># 设置图片编号顺序</span></span><br><span class="line">num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">    os.mkdir(title)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图片url网页信息</span></span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立图片存放地址</span></span><br><span class="line">file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line"><span class="comment"># 文件名采用编号方便按顺序查看</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始下载图片</span></span><br><span class="line"><span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">   f.write(response.content)</span><br><span class="line">   print(<span class="string">'该图片已下载完成'</span>,title)</span><br></pre></td></tr></table></figure><p></p><p>很快，15张图片就按着文章的顺序下载下来了。<br><img src="http://media.makcyun.top/18-9-2/94009879.jpg" alt=""></p><p>将上述代码整理一下，增加一点异常处理和图片的标题、编号的代码以让爬虫更健壮，完整的代码如下所示：</p><h3 id="2-3-全部代码"><a href="#2-3-全部代码" class="headerlink" title="2.3. 全部代码"></a>2.3. 全部代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 下载1页</span></span><br><span class="line">    url = <span class="string">'http://data.163.com/18/0901/01/DQJ3D0D9000181IU.html'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加异常捕获语句</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url,headers = headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">            <span class="comment"># print(response.text)  # 测试网页内容是否提取成功</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        print(<span class="string">'网页请求失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># 获取title</span></span><br><span class="line">    title = soup.h1.string</span><br><span class="line">    <span class="comment"># 每个网页只能拥有一个&lt;H1&gt;标签,因此唯一</span></span><br><span class="line">    item = soup.find_all(name=<span class="string">'img'</span>,width =[<span class="string">'100%'</span>])</span><br><span class="line">    <span class="comment"># print(item) # 测试</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(item)):</span><br><span class="line">        pic = item[i].attrs[<span class="string">'src'</span>]</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">        <span class="string">'title'</span>:title,</span><br><span class="line">        <span class="string">'pic'</span>:pic,</span><br><span class="line">        <span class="string">'num'</span>:i  <span class="comment"># 图片添加编号顺序</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># print(pic) #测试图片链接ok</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_pic</span><span class="params">(pic)</span>:</span></span><br><span class="line">    title = pic.get(<span class="string">'title'</span>)</span><br><span class="line">    url = pic.get(<span class="string">'pic'</span>)</span><br><span class="line">    <span class="comment"># 设置图片编号顺序</span></span><br><span class="line">    num = pic.get(<span class="string">'num'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(title):</span><br><span class="line">        os.mkdir(title)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取图片url网页信息</span></span><br><span class="line">    response = requests.get(url,headers = headers)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 建立图片存放地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;\&#123;1&#125;.&#123;2&#125;'</span> .format(title,num,<span class="string">'jpg'</span>)</span><br><span class="line">            <span class="comment"># 文件名采用编号方便按顺序查看，而未采用哈希值md5(response.content).hexdigest()</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="comment"># 开始下载图片</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">                    print(<span class="string">'该图片已下载完成'</span>,title)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'该图片%s 已下载'</span> %title)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e,<span class="string">'图片获取失败'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># get_page() # 测试网页内容是获取成功ok</span></span><br><span class="line">    html = get_page()</span><br><span class="line">    <span class="comment"># parse_page(html) # 测试网页内容是否解析成功ok</span></span><br><span class="line"></span><br><span class="line">    data = parse_page(html)</span><br><span class="line">    <span class="keyword">for</span> pic <span class="keyword">in</span> data:</span><br><span class="line">        <span class="comment"># print(pic) #测试dict</span></span><br><span class="line">        save_pic(pic)</span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>小结</strong><br>上面通过爬虫实现下载一张图片延伸到下载一页图片，相比于手动操作，爬虫的优势逐渐显现。那么，能否实现多页循环批量下载更多的图片呢，当然可以，下一篇文章将进行介绍。</p><p>你也可以尝试一下，这里先放上”福利”：网易”数读”栏目从2012年至今350篇文章的全部图片已下载完成。<br><img src="http://media.makcyun.top/18-9-2/90267464.jpg" alt=""></p><p>如果你需要，可以到我的github下载。<br><a href="https://github.com/makcyun/web_scraping_with_python" target="_blank" rel="noopener">https://github.com/makcyun/web_scraping_with_python</a></p><p>本文完。</p><p><img src="http://media.makcyun.top/Fj4q0NIPPMC5dMshIU9H650qD_Tf" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;下载网络中的图片是件有意思的事，除了”右键另存为”这种方式以外，还可以用Python爬虫来自动下载。本文以信息图做得非常棒的网易”数读”栏目为例，介绍如何下载一个网页中的图片。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="Ajax" scheme="https://www.makcyun.top/tags/Ajax/"/>
    
      <category term="图片下载" scheme="https://www.makcyun.top/tags/%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(2)：10行代码爬取全国所有A股/港股/新三板上市公司信息</title>
    <link href="https://www.makcyun.top/2018/08/27/web_scraping_withpython2.html"/>
    <id>https://www.makcyun.top/2018/08/27/web_scraping_withpython2.html</id>
    <published>2018-08-27T00:26:57.000Z</published>
    <updated>2018-12-19T03:12:32.959Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p><b><em>python爬虫第2篇</em></b><br>利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。</p><a id="more"></a><p><strong>摘要：</strong> 我们平常在浏览网页中会遇到一些表格型的数据信息，除了表格本身体现的内容以外，你可能想透过表格再更进一步地进行汇总、筛选、处理分析等操作从而得到更多有价值的信息，这时可用python爬虫来实现。本文采用pandas库中的read_html方法来快速准确地抓取表格数据。</p><p><strong>本文知识点：</strong></p><ul><li>Table型表格抓取</li><li>DataFrame.read_html函数使用</li><li>爬虫数据存储到mysql数据库</li><li>Navicat数据库的使用</li></ul><h2 id="1-table型表格"><a href="#1-table型表格" class="headerlink" title="1. table型表格"></a>1. table型表格</h2><p>我们在网页上会经常看到这样一些表格，比如：<br><a href="http://ranking.promisingedu.com/qs" target="_blank" rel="noopener">QS2018世界大学排名</a>：<br><img src="http://media.makcyun.top/18-8-27/59439970.jpg" alt=""></p><p><a href="http://www.fortunechina.com/fortune500/c/2018-07/19/content_311046.htm" target="_blank" rel="noopener">财富世界500强企业排名</a>：<br><img src="http://media.makcyun.top/18-8-27/66712901.jpg" alt=""></p><p><a href="https://www.boxofficemojo.com/" target="_blank" rel="noopener">IMDB世界电影票房排行榜</a>：<br><img src="http://media.makcyun.top/18-8-27/76002510.jpg" alt=""></p><p><a href="http://media.makcyun.top/18-8-27/78659021.jpg" target="_blank" rel="noopener">中国上市公司信息</a>：<br><img src="http://media.makcyun.top/18-8-27/78659021.jpg" alt=""></p><p>他们除了都是表格以外，还一个共同点就是当你点击右键-定位时，可以看到他们都是table类型的表格形式。<br><img src="http://media.makcyun.top/18-8-27/87245193.jpg" alt=""><br><img src="http://media.makcyun.top/18-8-27/54573575.jpg" alt=""><br><img src="http://media.makcyun.top/18-8-27/21054545.jpg" alt=""><br><img src="http://media.makcyun.top/18-8-27/30765316.jpg" alt=""></p><p>从中可以看到table类型的表格网页结构大致如下：<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">class</span>=<span class="string">"..."</span> <span class="attr">id</span>=<span class="string">"..."</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">th</span>&gt;</span>...<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">thead</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tbody</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>...<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span>...<span class="tag">&lt;/<span class="name">tr</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">tbody</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p><p>先来简单解释一下上文出现的几种标签含义：<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span>&gt;</span>: 定义表格</span><br><span class="line"><span class="tag">&lt;<span class="name">thead</span>&gt;</span>: 定义表格的页眉</span><br><span class="line"><span class="tag">&lt;<span class="name">tbody</span>&gt;</span>: 定义表格的主体</span><br><span class="line"><span class="tag">&lt;<span class="name">tr</span>&gt;</span>: 定义表格的行</span><br><span class="line"><span class="tag">&lt;<span class="name">th</span>&gt;</span>: 定义表格的表头</span><br><span class="line"><span class="tag">&lt;<span class="name">td</span>&gt;</span>: 定义表格单元</span><br></pre></td></tr></table></figure><p></p><p>这样的表格数据，就可以利用pandas模块里的read_html函数方便快捷地抓取下来。下面我们就来操作一下。</p><h2 id="2-快速抓取"><a href="#2-快速抓取" class="headerlink" title="2. 快速抓取"></a>2. 快速抓取</h2><p>下面以中国上市公司信息这个网页中的表格为例，感受一下read_html函数的强大之处。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">178</span>):  <span class="comment"># 爬取全部177页数据</span></span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=%s'</span> % (str(i))</span><br><span class="line">tb = pd.read_html(url)[<span class="number">3</span>] <span class="comment">#经观察发现所需表格是网页中第4个表格，故为[3]</span></span><br><span class="line">tb.to_csv(<span class="string">r'1.csv'</span>, mode=<span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>, header=<span class="number">1</span>, index=<span class="number">0</span>)</span><br><span class="line">print(<span class="string">'第'</span>+str(i)+<span class="string">'页抓取完成'</span>)</span><br></pre></td></tr></table></figure><p></p><p><img src="http://media.makcyun.top/18-8-27/96662344.jpg" alt=""><br>只需不到十行代码，1分钟左右就可以将全部178页共3536家A股上市公司的信息干净整齐地抓取下来。比采用正则表达式、xpath这类常规方法要省心省力地多。如果采取人工一页页地复制粘贴到excel中，就得操作到猴年马月去了。<br>上述代码除了能爬上市公司表格以外，其他几个网页的表格都可以爬，只需做简单的修改即可。因此，可作为一个简单通用的代码模板。但是，为了让代码更健壮更通用一些，接下来，以爬取177页的A股上市公司信息为目标，讲解一下详细的代码实现步骤。</p><h2 id="3-详细代码实现"><a href="#3-详细代码实现" class="headerlink" title="3. 详细代码实现"></a>3. 详细代码实现</h2><h3 id="3-1-read-html函数"><a href="#3-1-read-html函数" class="headerlink" title="3.1. read_html函数"></a>3.1. read_html函数</h3><p>先来了解一下<strong>read_html</strong>函数的api:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pandas.read_html(io, match=<span class="string">'.+'</span>, flavor=<span class="keyword">None</span>, header=<span class="keyword">None</span>, index_col=<span class="keyword">None</span>, skiprows=<span class="keyword">None</span>, attrs=<span class="keyword">None</span>, parse_dates=<span class="keyword">False</span>, tupleize_cols=<span class="keyword">None</span>, thousands=<span class="string">', '</span>, encoding=<span class="keyword">None</span>, decimal=<span class="string">'.'</span>, converters=<span class="keyword">None</span>, na_values=<span class="keyword">None</span>, keep_default_na=<span class="keyword">True</span>, displayed_only=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">常用的参数：</span><br><span class="line">io:可以是url、html文本、本地文件等；</span><br><span class="line">flavor：解析器；</span><br><span class="line">header：标题行；</span><br><span class="line">skiprows：跳过的行；</span><br><span class="line">attrs：属性，比如 attrs = &#123;<span class="string">'id'</span>: <span class="string">'table'</span>&#125;；</span><br><span class="line">parse_dates：解析日期</span><br><span class="line"></span><br><span class="line">注意：返回的结果是**DataFrame**组成的**list**。</span><br></pre></td></tr></table></figure><p></p><p>参考：</p><blockquote><p>1 <a href="http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html</a><br>2 <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html</a></p></blockquote><h3 id="3-2-分析网页url"><a href="#3-2-分析网页url" class="headerlink" title="3.2. 分析网页url"></a>3.2. 分析网页url</h3><p>首先，观察一下中商情报网第1页和第2页的网址：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=1#QueryCondition</span><br><span class="line">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=2#QueryCondition</span><br></pre></td></tr></table></figure><p></p><p>可以发现，只有<strong>pageNum</strong>的值随着翻页而变化，所以基本可以断定pageNum=1代表第1页，pageNum=10代表第10页，以此类推。这样比较容易用for循环构造爬取的网址。<br>试着把<strong>#QueryCondition</strong>删除，看网页是否同样能够打开，经尝试发现网页依然能正常打开，因此在构造url时，可以使用这样的格式：<br><strong><a href="http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i" target="_blank" rel="noopener">http://s.askci.com/stock/a/?reportTime=2017-12-31&amp;pageNum=i</a></strong><br>再注意一下其他参数：<br><strong>a</strong>：表示A股，把a替换为<strong>h</strong>，表示<strong>港股</strong>；把a替换为<strong>xsb</strong>，则表示<strong>新三板</strong>。那么，在网址分页for循环外部再加一个for循环，就可以爬取这三个股市的股票了。</p><h3 id="3-3-定义函数"><a href="#3-3-定义函数" class="headerlink" title="3.3. 定义函数"></a>3.3. 定义函数</h3><p>将整个爬取分为网页提取、内容解析、数据存储等步骤，依次建立相应的函数。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网页提取函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,   </span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># beatutiful soup解析然后提取表格</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line"></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(tbl)</span><br><span class="line"><span class="comment"># return tbl</span></span><br><span class="line"><span class="comment"># rename将表格15列的中文名改为英文名，便于存储到mysql及后期进行数据分析</span></span><br><span class="line"><span class="comment"># tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):   <span class="comment"># page表示提取页数</span></span><br><span class="line">html = get_one_page(i)</span><br><span class="line">parse_one_page(html)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)   <span class="comment">#共提取n页</span></span><br></pre></td></tr></table></figure><p></p><p>上面两个函数相比于快速抓取的方法代码要多一些，如果需要抓的表格很少或只需要抓一次，那么推荐快速抓取法。如果页数比较多，这种方法就更保险一些。解析函数用了BeautifulSoup和css选择器，这种方法定位提取表格所在的<strong>id为#myTable04</strong>的table代码段，更为准确。</p><h3 id="3-4-存储到MySQL"><a href="#3-4-存储到MySQL" class="headerlink" title="3.4. 存储到MySQL"></a>3.4. 存储到MySQL</h3><p>接下来，我们可以将结果保存到本地csv文件，也可以保存到MySQL数据库中。这里为了练习一下MySQL，因此选择保存到MySQL中。</p><p>首先，需要先在数据库建立存放数据的表格，这里命名为<strong>listed_company</strong>。代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,   <span class="comment"># 本地服务器</span></span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,  <span class="comment"># 你的数据库密码</span></span><br><span class="line">port=<span class="number">3306</span>,          <span class="comment"># 默认端口</span></span><br><span class="line">charset = <span class="string">'utf8'</span>,</span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company2 (serial_number INT(30) NOT NULL,stock_code INT(30) ,stock_abbre VARCHAR(30) ,company_name VARCHAR(30) ,province VARCHAR(30) ,city VARCHAR(30) ,main_bussiness_income VARCHAR(30) ,net_profit VARCHAR(30) ,employees INT(30) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(30) ,financial_report VARCHAR(30) , industry_classification VARCHAR(255) ,industry_type VARCHAR(255) ,main_business VARCHAR(255) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">    <span class="comment"># listed_company是要在wade数据库中建立的表，用于存放数据</span></span><br><span class="line">    </span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line">generate_mysql()</span><br></pre></td></tr></table></figure><p></p><p>上述代码定义了generate_mysql()函数，用于在MySQL中wade数据库下生成一个listed_company的表。表格包含15个列字段。根据每列字段的属性，分别设置为INT整形（长度为30）、VARCHAR字符型(长度为30) 、DATETIME(0) 日期型等。<br>在Navicat中查看建立好之后的表格：<br><img src="http://media.makcyun.top/18-8-28/33076915.jpg" alt=""><br><img src="http://media.makcyun.top/18-8-28/97554452.jpg" alt=""></p><p>接下来就可以往这个表中写入数据，代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="comment"># db = 'wade'表示存储到wade这个数据库中,root后面的*是密码</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># 因为要循环网页不断数据库写入内容，所以if_exists选择append，同时该表要有表头，parse_one_page（）方法中df.rename已设置</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><p></p><p>以上就完成了单个页面的表格爬取和存储工作，接下来只要在main()函数进行for循环，就可以完成所有总共178页表格的爬取和存储，完整代码如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode  <span class="comment"># 编码 URL 字符串</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()  <span class="comment">#计算程序运行时间</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(i)</span>:</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line">paras = &#123;</span><br><span class="line"><span class="string">'reportTime'</span>: <span class="string">'2017-12-31'</span>,</span><br><span class="line"><span class="comment">#可以改报告日期，比如2018-6-30获得的就是该季度的信息</span></span><br><span class="line"><span class="string">'pageNum'</span>: i   <span class="comment">#页码</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'http://s.askci.com/stock/a/?'</span> + urlencode(paras)</span><br><span class="line">response = requests.get(url,headers = headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line"><span class="keyword">return</span> response.text</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">print(<span class="string">'爬取失败'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">content = soup.select(<span class="string">'#myTable04'</span>)[<span class="number">0</span>] <span class="comment">#[0]将返回的list改为bs4类型</span></span><br><span class="line">tbl = pd.read_html(content.prettify(),header = <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># prettify()优化代码,[0]从pd.read_html返回的list中提取出DataFrame</span></span><br><span class="line">tbl.rename(columns = &#123;<span class="string">'序号'</span>:<span class="string">'serial_number'</span>, <span class="string">'股票代码'</span>:<span class="string">'stock_code'</span>, <span class="string">'股票简称'</span>:<span class="string">'stock_abbre'</span>, <span class="string">'公司名称'</span>:<span class="string">'company_name'</span>, <span class="string">'省份'</span>:<span class="string">'province'</span>, <span class="string">'城市'</span>:<span class="string">'city'</span>, <span class="string">'主营业务收入(201712)'</span>:<span class="string">'main_bussiness_income'</span>, <span class="string">'净利润(201712)'</span>:<span class="string">'net_profit'</span>, <span class="string">'员工人数'</span>:<span class="string">'employees'</span>, <span class="string">'上市日期'</span>:<span class="string">'listing_date'</span>, <span class="string">'招股书'</span>:<span class="string">'zhaogushu'</span>, <span class="string">'公司财报'</span>:<span class="string">'financial_report'</span>, <span class="string">'行业分类'</span>:<span class="string">'industry_classification'</span>, <span class="string">'产品类型'</span>:<span class="string">'industry_type'</span>, <span class="string">'主营业务'</span>:<span class="string">'main_business'</span>&#125;,inplace = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(tbl)</span></span><br><span class="line"><span class="keyword">return</span> tbl</span><br><span class="line"><span class="comment"># rename将中文名改为英文名，便于存储到mysql及后期进行数据分析</span></span><br><span class="line"><span class="comment"># tbl = pd.DataFrame(tbl,dtype = 'object') #dtype可统一修改列格式为文本</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mysql</span><span class="params">()</span>:</span></span><br><span class="line">conn = pymysql.connect(</span><br><span class="line">host=<span class="string">'localhost'</span>,</span><br><span class="line">user=<span class="string">'root'</span>,</span><br><span class="line">password=<span class="string">'******'</span>,</span><br><span class="line">port=<span class="number">3306</span>,</span><br><span class="line">charset = <span class="string">'utf8'</span>,  </span><br><span class="line">db = <span class="string">'wade'</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line">sql = <span class="string">'CREATE TABLE IF NOT EXISTS listed_company (serial_number INT(20) NOT NULL,stock_code INT(20) ,stock_abbre VARCHAR(20) ,company_name VARCHAR(20) ,province VARCHAR(20) ,city VARCHAR(20) ,main_bussiness_income VARCHAR(20) ,net_profit VARCHAR(20) ,employees INT(20) ,listing_date DATETIME(0) ,zhaogushu VARCHAR(20) ,financial_report VARCHAR(20) , industry_classification VARCHAR(20) ,industry_type VARCHAR(100) ,main_business VARCHAR(200) ,PRIMARY KEY (serial_number))'</span></span><br><span class="line">    <span class="comment"># listed_company是要在wade数据库中建立的表，用于存放数据</span></span><br><span class="line">    </span><br><span class="line">cursor.execute(sql)</span><br><span class="line">conn.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_sql</span><span class="params">(tbl, db = <span class="string">'wade'</span>)</span>:</span></span><br><span class="line">    engine = create_engine(<span class="string">'mysql+pymysql://root:******@localhost:3306/&#123;0&#125;?charset=utf8'</span>.format(db))</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># df = pd.read_csv(df)</span></span><br><span class="line">    tbl.to_sql(<span class="string">'listed_company2'</span>,con = engine,if_exists=<span class="string">'append'</span>,index=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># append表示在原有表基础上增加，但该表要有表头</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    generate_mysql()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,page):  </span><br><span class="line">html = get_one_page(i)</span><br><span class="line">tbl = parse_one_page(html)</span><br><span class="line">write_to_sql(tbl)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 单进程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">main(<span class="number">178</span>)</span><br><span class="line"></span><br><span class="line">endtime = time.time()-start_time</span><br><span class="line">print(<span class="string">'程序运行了%.2f秒'</span> %endtime)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多进程</span></span><br><span class="line"><span class="comment"># from multiprocessing import Pool</span></span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment"># pool = Pool(4)</span></span><br><span class="line"><span class="comment"># pool.map(main, [i for i in range(1,178)])  #共有178页</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># endtime = time.time()-start_time</span></span><br><span class="line"><span class="comment"># print('程序运行了%.2f秒' %(time.time()-start_time))</span></span><br></pre></td></tr></table></figure><p></p><p>最终，A股所有3535家企业的信息已经爬取到mysql中，如下图：<br><img src="http://media.makcyun.top/18-8-28/63973864.jpg" alt=""></p><p>最后，需说明不是所有表格都可以用这种方法爬取，比如这个网站中的表格，表面是看起来是表格，但在html中不是前面的table格式，而是list列表格式。这种表格则不适用read_html爬取。得用其他的方法，比如selenium，以后再进行介绍。<br><img src="http://media.makcyun.top/18-8-28/3402980.jpg" alt=""></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;b&gt;&lt;em&gt;python爬虫第2篇&lt;/em&gt;&lt;/b&gt;&lt;br&gt;利用pandas库中的read_html方法快速抓取网页中常见的表格型数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="pandas" scheme="https://www.makcyun.top/tags/pandas/"/>
    
      <category term="数据抓取" scheme="https://www.makcyun.top/tags/%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫(1):多种方法爬取猫眼top100电影</title>
    <link href="https://www.makcyun.top/2018/08/20/web_scraping_withpython1.html"/>
    <id>https://www.makcyun.top/2018/08/20/web_scraping_withpython1.html</id>
    <published>2018-08-20T11:18:14.973Z</published>
    <updated>2018-12-19T03:12:32.957Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:48 GMT+0800 (GMT+08:00) --><p><b><em>python爬虫第1篇</em></b><br>利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。</p><a id="more"></a><p><strong>摘要：</strong> 作为小白，<strong>爬虫可以说是入门python最快和最容易获得成就感的途径</strong>。因为初级爬虫的套路相对固定，常见的方法只有几种，比较好上手。最近，跟着崔庆才大佬的书：<em>python3网络爬虫开发实战</em> 学习爬虫。选取网页结构较为简单的猫眼top100电影为案例进行练习。 <strong>重点是用上述所说的4种方法提取出关键内容</strong>。一个问题采用不同的解决方法有助于拓展思维，通过不断练习就能够灵活运用。</p><blockquote><p><strong>本文知识点：</strong><br>Requsts 请求库的使用<br>beautiful+lxml两大解析库使用<br>正则表达式 、xpath、css选择器的使用</p></blockquote><p><img src="http://media.makcyun.top/18-8-19/49818413.jpg" alt=""></p><h2 id="1-为什么爬取该网页？"><a href="#1-为什么爬取该网页？" class="headerlink" title="1. 为什么爬取该网页？"></a>1. 为什么爬取该网页？</h2><ul><li>比较懒，不想一页页地去翻100部电影的介绍，<strong>想在一个页面内进行总体浏览</strong>（比如在excel表格中）；</li></ul><p><img src="http://media.makcyun.top/18-8-19/28479553.jpg" alt=""></p><ul><li>想<strong>深入了解一些比较有意思的信息</strong>，比如：哪部电影的评分最高？哪位演员的作品数量最多？哪个国家/地区上榜的电影数量最多？哪一年上榜的电影作品最多等。这些信息在网页上是不那么容易能直接获得的，所以需要爬虫。</li></ul><p><img src="http://media.makcyun.top/18-8-20/66062822.jpg" alt=""></p><h2 id="2-爬虫目标"><a href="#2-爬虫目标" class="headerlink" title="2. 爬虫目标"></a>2. 爬虫目标</h2><ul><li>从网页中提取出top100电影的电影名称、封面图片、排名、评分、演员、上映国家/地区、评分等信息，并保存为csv文本文件。</li><li>根据爬取结果，进行简单的可视化分析。</li></ul><p>平台：windows7 + SublimeText3</p><h2 id="3-爬取步骤"><a href="#3-爬取步骤" class="headerlink" title="3. 爬取步骤"></a>3. 爬取步骤</h2><h3 id="3-1-网址URL分析"><a href="#3-1-网址URL分析" class="headerlink" title="3.1. 网址URL分析"></a>3.1. 网址URL分析</h3><p>首先，打开猫眼Top100的url网址： <a href="http://maoyan.com/board/4?offset=0" target="_blank" rel="noopener"><strong>http://maoyan.com/board/4?offset=0</strong></a>。页面非常简单，所包含的信息就是上述所说的爬虫目标。下拉页面到底部，点击第2页可以看到网址变为：<strong><a href="http://maoyan.com/board/4?offset=10" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10</a></strong>。因此，可以推断出url的变化规律：offset表示偏移，10代表一个页面的电影偏移数量，即：第一页电影是从0-10，第二页电影是从11-20。因此，获取全部100部电影，只需要构造出10个url，然后依次获取网页内容，再用不同的方法提取出所需内容就可以了。<br>下面，用requests方法获取第一个页面。</p><h3 id="3-2-Requests获取首页数据"><a href="#3-2-Requests获取首页数据" class="headerlink" title="3.2. Requests获取首页数据"></a>3.2. Requests获取首页数据</h3><p>先定义一个获取单个页面的函数：get_one_page()，传入url参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line">        <span class="comment"># 不加headers爬不了</span></span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># try-except语句捕获异常</span></span><br></pre></td></tr></table></figure><p>接下来在main()函数中设置url。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    print(html)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>运行上述程序后，首页的源代码就被爬取下来了。如下图所示：</p><p><img src="http://media.makcyun.top/18-8-19/18415362.jpg" alt=""></p><p>接下来就需要从整个网页中提取出几项我们需要的内容，用到的方法就是上述所说的四种方法，下面分别进行说明。</p><h3 id="3-3-4种内容解析提取方法"><a href="#3-3-4种内容解析提取方法" class="headerlink" title="3.3. 4种内容解析提取方法"></a>3.3. 4种内容解析提取方法</h3><h4 id="3-3-1-正则表达式提取"><a href="#3-3-1-正则表达式提取" class="headerlink" title="3.3.1. 正则表达式提取"></a>3.3.1. 正则表达式提取</h4><p>第一种是利用<strong>正则表达式</strong>提取。<br>什么是正则表达式？ 下面这串看起来乱七八糟的符号就是正则表达式的语法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&apos;</span><br></pre></td></tr></table></figure><p>它是一种强大的字符串处理工具。之所以叫正则表达式，是因为它们可以识别正则字符串（regular string）。可以这么定义：“ 如果你给我的字符串符合规则，我就返回它”；“如果字符串不符合规则，我就忽略它”。通过requests抓取下来的网页是一堆大量的字符串，用它处理后便可提取出我们想要的内容。</p><p>如果还不了解它，可以参考下面的教程：</p><blockquote><p><a href="http://www.runoob.com/regexp/regexp-syntax.html" target="_blank" rel="noopener">http://www.runoob.com/regexp/regexp-syntax.html</a><br><a href="https://www.w3cschool.cn/regexp/zoxa1pq7.html" target="_blank" rel="noopener">https://www.w3cschool.cn/regexp/zoxa1pq7.html</a></p></blockquote><p><strong>正则表达式常用语法：</strong></p><style>table th:nth-of-type(1){width:60px}</style><table><thead><tr><th style="text-align:center">模式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">\w</td><td style="text-align:center">匹配字母数字及下划线</td></tr><tr><td style="text-align:center">\W</td><td style="text-align:center">匹配非字母数字及下划线</td></tr><tr><td style="text-align:center">\s</td><td style="text-align:center">匹配任意空白字符，等价于 [\t\n\r\f]</td></tr><tr><td style="text-align:center">\S</td><td style="text-align:center">匹配任意非空字符</td></tr><tr><td style="text-align:center">\d</td><td style="text-align:center">匹配任意数字，等价于 [0-9]</td></tr><tr><td style="text-align:center">\D</td><td style="text-align:center">匹配任意非数字</td></tr><tr><td style="text-align:center">\n</td><td style="text-align:center">匹配一个换行符</td></tr><tr><td style="text-align:center">\t</td><td style="text-align:center">匹配一个制表符</td></tr><tr><td style="text-align:center">^</td><td style="text-align:center">匹配字符串开始位置的字符</td></tr><tr><td style="text-align:center">$</td><td style="text-align:center">匹配字符串的末尾</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">匹配任意字符，除了换行符</td></tr><tr><td style="text-align:center">[…]</td><td style="text-align:center">用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’</td></tr><tr><td style="text-align:center">[^…]</td><td style="text-align:center">不在 [ ] 中的字符</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">匹配前面的字符、子表达式或括号里的字符 0 次或多次</td></tr><tr><td style="text-align:center">+</td><td style="text-align:center">同上，匹配至少一次</td></tr><tr><td style="text-align:center">?</td><td style="text-align:center">同上，匹配0到1次</td></tr><tr><td style="text-align:center">{n}</td><td style="text-align:center">匹配前面的字符、子表达式或括号里的字符 n 次</td></tr><tr><td style="text-align:center">{n, m}</td><td style="text-align:center">同上，匹配 m 到n 次（包含 m 或 n）</td></tr><tr><td style="text-align:center">( )</td><td style="text-align:center">匹配括号内的表达式，也表示一个组</td></tr></tbody></table><p>下面，开始提取关键内容。右键网页-检查-Network选项，选中左边第一个文件然后定位到电影信息的相应位置，如下图：</p><p><img src="http://media.makcyun.top/18-8-19/84055565.jpg" alt=""></p><p>可以看到每部电影的相关信息都在<strong>dd</strong>这个节点之中。所以就可以从该节点运用正则进行提取。<br>第1个要提取的内容是电影的排名。它位于class=”board-index”的<strong>i</strong>节点内。不需要提取的内容用’.*?’替代，需要提取的数字排名用（）括起来，（）里面的数字表示为（\d+）。正则表达式可以写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;&apos;</span><br></pre></td></tr></table></figure><p>接着，第2个需要提取的是封面图片，图片网址位于img节点的’data-src’属性中，正则表达式可写为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;data-src=&quot;(.*?)&quot;.*?&apos;</span><br></pre></td></tr></table></figure><p>第1和第2个正则之间的代码是不需要的，用’.*?’替代，所以这两部分合起来写就是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;</span><br></pre></td></tr></table></figure><p>同理，可以依次用正则写下主演、上映时间和评分等内容,完整的正则表达式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name&quot;&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&apos;</span><br></pre></td></tr></table></figure><p>正则表达式写好以后，可以定义一个页面解析提取方法：parse_one_page（），用来提取内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(</span><br><span class="line">        <span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>, re.S)</span><br><span class="line">    <span class="comment"># re.S表示匹配任意字符，如果不加，则无法匹配换行符</span></span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'index'</span>: item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(item[<span class="number">1</span>]),  <span class="comment"># 定义get_thumb()方法进一步处理网址</span></span><br><span class="line">            <span class="string">'name'</span>: item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'star'</span>: item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="comment"># 'time': item[4].strip()[5:],</span></span><br><span class="line">            <span class="comment"># 用两个方法分别提取time里的日期和地区</span></span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: item[<span class="number">5</span>].strip() + item[<span class="number">6</span>].strip()</span><br><span class="line">            <span class="comment"># 评分score由整数+小数两部分组成</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>re.S:</strong>匹配任意字符，如果不加，则无法匹配换行符；<br><strong>yield:</strong>使用yield的好处是作为生成器，可以遍历迭代，并且将数据整理形成字典，输出结果美观。具体用法可参考：<a href="https://blog.csdn.net/zhangpinghao/article/details/18716275；" target="_blank" rel="noopener">https://blog.csdn.net/zhangpinghao/article/details/18716275；</a><br><strong>.strip():</strong>用于去掉字符串中的空格。</p></blockquote><p>上面程序为了便于提取内容，又定义了3个方法：get_thumb（）、get_release_time（）和 get_release_area（）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取封面大图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_thumb</span><span class="params">(url)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)@.*?'</span>)</span><br><span class="line">    thumb = re.search(pattern, url)</span><br><span class="line">    <span class="keyword">return</span> thumb.group(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c</span></span><br><span class="line"><span class="comment"># 去掉@160w_220h_1e_1c就是大图    </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取上映时间函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_time</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)(\(|$)'</span>)</span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)  <span class="comment"># 返回匹配到的第一个括号(.*?)中结果即时间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家/地区函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_area</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'.*\((.*)\)'</span>)</span><br><span class="line">    <span class="comment"># $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?)</span></span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>‘r’：</strong>正则前面加上’r’ 是为了告诉编译器这个string是个raw string，不要转意’\’。当一个字符串使用了正则表达式后，最好在前面加上’r’；<br><strong>‘|’ ‘$’：</strong> 正则’|’表示或’，’$’表示匹配一行字符串的结尾；<br><strong>.group(1)</strong>：意思是返回search匹配的第一个括号中的结果，即(.*?)，gropup()则返回所有结果2013-12-18(，group(1)返回’（’。</p></blockquote><p>接下来，修改main()函数来输出爬取的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        print(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>if <strong>name</strong> == ‘_ _main__’:</strong>当.py文件被直接运行时，if <strong>name</strong> == ‘_ <em>main__’之下的代码块将被运行；当.py文件以模块形式被导入时，if <strong>name</strong> == ‘</em> _main__’之下的代码块不被运行。<br>参考：<a href="https://blog.csdn.net/yjk13703623757/article/details/77918633。" target="_blank" rel="noopener">https://blog.csdn.net/yjk13703623757/article/details/77918633。</a></p></blockquote><p>运行程序，就可成功地提取出所需内容，结果如下：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'1'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg'</span>, <span class="string">'name'</span>: <span class="string">'霸王别姬'</span>, <span class="string">'star'</span>: <span class="string">'张国荣,张丰毅,巩俐'</span>, <span class="string">'time'</span>: <span class="string">'1993-01-01'</span>, <span class="string">'area'</span>: <span class="string">'中国香港'</span>, <span class="string">'score'</span>: <span class="string">'9.6'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'2'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/54617769d96807e4d81804284ffe2a27239007.jpg'</span>, <span class="string">'name'</span>: <span class="string">'罗马假日'</span>, <span class="string">'star'</span>: <span class="string">'格利高里·派克,奥黛丽·赫本,埃迪·艾伯特'</span>, <span class="string">'time'</span>: <span class="string">'1953-09-02'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.1'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'3'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/283292171619cdfd5b240c8fd093f1eb255670.jpg'</span>, <span class="string">'name'</span>: <span class="string">'肖申克的救赎'</span>, <span class="string">'star'</span>: <span class="string">'蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿'</span>, <span class="string">'time'</span>: <span class="string">'1994-10-14'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.5'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'4'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p0.meituan.net/movie/e55ec5d18ccc83ba7db68caae54f165f95924.jpg'</span>, <span class="string">'name'</span>: <span class="string">'这个杀手不太冷'</span>, <span class="string">'star'</span>: <span class="string">'让·雷诺,加里·奥德曼,娜塔莉·波特曼'</span>, <span class="string">'time'</span>: <span class="string">'1994-09-14'</span>, <span class="string">'area'</span>: <span class="string">'法国'</span>, <span class="string">'score'</span>: <span class="string">'9.5'</span>&#125;</span><br><span class="line">&#123;<span class="string">'index'</span>: <span class="string">'5'</span>, <span class="string">'thumb'</span>: <span class="string">'http://p1.meituan.net/movie/f5a924f362f050881f2b8f82e852747c118515.jpg'</span>, <span class="string">'name'</span>: <span class="string">'教父'</span>, <span class="string">'star'</span>: <span class="string">'马龙·白兰度,阿尔·帕西诺,詹姆斯·肯恩'</span>, <span class="string">'time'</span>: <span class="string">'1972-03-24'</span>, <span class="string">'area'</span>: <span class="string">'美国'</span>, <span class="string">'score'</span>: <span class="string">'9.3'</span>&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br><span class="line">[Finished <span class="keyword">in</span> <span class="number">1.9</span>s]</span><br></pre></td></tr></table></figure><p>以上是第1种提取方法，如果还不习惯正则表达式这种复杂的语法，可以试试下面的第2种方法。</p><h4 id="3-3-2-lxml结合xpath提取"><a href="#3-3-2-lxml结合xpath提取" class="headerlink" title="3.3.2. lxml结合xpath提取"></a>3.3.2. lxml结合xpath提取</h4><p>该方法需要用到<strong>lxml</strong>这款解析利器，同时搭配xpath语法，利用它的的路径选择表达式，来高效提取所需内容。lxml包为第三方包，需要自行安装。如果对xpath的语法还不太熟悉，可参考下面的教程：<br><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/xpath_syntax.asp</a></p><p><strong>xpath常用的规则</strong></p><table><thead><tr><th style="text-align:center">表达式</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">nodename</td><td style="text-align:center">选取此节点的所有子节点</td></tr><tr><td style="text-align:center">/</td><td style="text-align:center">从当前节点选取直接子节点</td></tr><tr><td style="text-align:center">//</td><td style="text-align:center">从当前节点选取子孙节点</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">选取当前节点</td></tr><tr><td style="text-align:center">..</td><td style="text-align:center">选取当前节点的父节点</td></tr><tr><td style="text-align:center">@</td><td style="text-align:center">选取属性</td></tr></tbody></table><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span> <span class="attr">id</span>=<span class="string">"app"</span> <span class="attr">class</span>=<span class="string">"page-board/index"</span> &gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"content"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"wrapper"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"main"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"update-time"</span>&gt;</span>2018-08-18<span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"has-fresh-text"</span>&gt;</span>已更新<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"board-content"</span>&gt;</span>榜单规则：将猫眼电影库中的经典影片，按照评分和评分人数从高到低综合排序取前100名，每天上午10点更新。相关数据来源于“猫眼电影库”。<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dl</span> <span class="attr">class</span>=<span class="string">"board-wrapper"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">dd</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"board-index board-index-1"</span>&gt;</span>1<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/films/1203"</span> <span class="attr">title</span>=<span class="string">"霸王别姬"</span> <span class="attr">class</span>=<span class="string">"image-link"</span> <span class="attr">data-act</span>=<span class="string">"boarditem-click"</span> <span class="attr">data-val</span>=<span class="string">"&#123;movieId:1203&#125;"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"//ms0.meituan.net/mywww/image/loading_2.e3d934bf.png"</span> <span class="attr">alt</span>=<span class="string">""</span> <span class="attr">class</span>=<span class="string">"poster-default"</span> /&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-src</span>=<span class="string">"http://p1.meituan.net/movie/20803f59291c47e1e116c11963ce019e68711.jpg@160w_220h_1e_1c"</span> <span class="attr">alt</span>=<span class="string">"霸王别姬"</span> <span class="attr">class</span>=<span class="string">"board-img"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"board-item-main"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"board-item-content"</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"movie-item-info"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"name"</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/films/1203"</span> <span class="attr">title</span>=<span class="string">"霸王别姬"</span> <span class="attr">data-act</span>=<span class="string">"boarditem-click"</span> <span class="attr">data-val</span>=<span class="string">"&#123;movieId:1203&#125;"</span>&gt;</span>霸王别姬<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"star"</span>&gt;</span></span><br><span class="line">                主演：张国荣,张丰毅,巩俐</span><br><span class="line">        <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"releasetime"</span>&gt;</span>上映时间：1993-01-01(中国香港)<span class="tag">&lt;/<span class="name">p</span>&gt;</span>    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"movie-item-number score-num"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"score"</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"integer"</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fraction"</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                <span class="tag">&lt;/<span class="name">dd</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">dd</span>&gt;</span></span><br></pre></td></tr></table></figure><p>根据截取的部分html网页，先来提取第1个电影排名信息，有两种方法。<br><strong>第一种：</strong>直接复制。<br>右键-Copy-Copy Xpath，得到xpath路径为：<strong>//*[@id=”app”]/div/div/div[1]/dl/dd[1]/i</strong>,为了能够提取到页面所有的排名信息，需进一步修改为：<strong>//*[@id=”app”]/div/div/div[1]/dl/dd/i/text()</strong>，如果想要再精简一点，可以省去中间部分绝对路径’/‘然后用相对路径’//‘代替，最后进一步修改为：<strong>//*[@id=”app”]//div//dd/i/text()</strong>。<br><img src="http://media.makcyun.top/18-8-19/840042.jpg" alt=""></p><p><strong>第二种：</strong>观察网页结构自己写。<br>首先注意到<strong>id = app</strong>的div节点，因为在整个网页结构id是唯一的不会有第二个相同的，所有可以将该div节点作为xpath语法的起点，然后往下观察分别是3级div节点，可以省略写为：<strong>//div</strong>,再往下分别是是两个并列的<strong>p</strong>节点、<strong>dl</strong>节点、<strong>dd</strong>节点和最后的<strong>i</strong>节点文本。中间可以随意省略，只要保证该路径能够选择到唯一的文本值<strong>‘1’</strong>即可，例如省去p和dl节点，只保留后面的节点。这样，完整路径可以为：<strong>//*[@id=”app”]//div//dd/i/text()</strong>，和上式一样。</p><p><img src="http://media.makcyun.top/18-8-19/87341266.jpg" alt=""></p><p>根据上述思路，可以写下其他内容的xpath路径。观察到路径的前一部分：<strong>//*[@id=”app”]//div//dd</strong>都是一样的，从后面才开始不同，因此为了能够精简代码，将前部分路径赋值为一个变量items，最终提取的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2 用lxml结合xpath提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page2</span><span class="params">(html)</span>:</span></span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'//*[@id="app"]//div//dd'</span>)</span><br><span class="line">    <span class="comment"># 完整的是//*[@id="app"]/div/div/div[1]/dl/dd</span></span><br><span class="line">    <span class="comment"># print(type(items))</span></span><br><span class="line">    <span class="comment"># *代表匹配所有节点，@表示属性</span></span><br><span class="line">    <span class="comment"># 第一个电影是dd[1],要提取页面所有电影则去掉[1]</span></span><br><span class="line">    <span class="comment"># xpath://*[@id="app"]/div/div/div[1]/dl/dd[1]    </span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>: item.xpath(<span class="string">'./i/text()'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="comment">#./i/text()前面的点表示从items节点开始</span></span><br><span class="line">            <span class="comment">#/text()提取文本</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(str(item.xpath(<span class="string">'./a/img[2]/@data-src'</span>)[<span class="number">0</span>].strip())),</span><br><span class="line">            <span class="comment"># 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。</span></span><br><span class="line">            <span class="string">'name'</span>: item.xpath(<span class="string">'./a/@title'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'star'</span>: item.xpath(<span class="string">'.//p[@class = "star"]/text()'</span>)[<span class="number">0</span>].strip(),</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span> : item.xpath(<span class="string">'.//p[@class = "score"]/i[1]/text()'</span>)[<span class="number">0</span>] + \</span><br><span class="line">            item.xpath(<span class="string">'.//p[@class = "score"]/i[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>Tips:</strong><br><strong>[0]：</strong>xpath后面添加了[0]是因为返回的是只有1个字符串的list，添加[0]是将list提取为字符串，使其简洁；<br><strong>Network：</strong>要在最原始的Network选项卡中定位，而不是Elements中，不然提取不到相关内容；<br><strong>class属性：</strong>p[@class = “star”]/text()表示提取class属性为”star”的p节点的文本值；<br><strong>提取属性值：</strong>img[2]/@data-src’：提取img节点的data-src属性值，属性值后面无需添加’/text()’</p></blockquote><p>运行程序，就可成功地提取出所需内容，结果和第一种方法一样。</p><p>以上是第2种提取方法，如果也不太习惯xpath语法，可以试试下面的第3种方法。</p><h4 id="3-3-3-Beautiful-Soup-css选择器"><a href="#3-3-3-Beautiful-Soup-css选择器" class="headerlink" title="3.3.3. Beautiful Soup + css选择器"></a>3.3.3. Beautiful Soup + css选择器</h4><p>Beautiful Soup 同lxml一样，是一个非常强大的python解析库，可以从HTML或XML文件中提取效率非常高。关于它的用法，可参考下面的教程：<br><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></p><p>css选择器选是一种模式，用于选择需要添加样式的元素，使用它的语法同样能够快速定位到所需节点，然后提取相应内容。使用方法可参考下面的教程：<br><a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp</a></p><p><strong>css选择器常用的规则</strong></p><style>table th:nth-of-type(1){width:30%}</style><table><thead><tr><th style="text-align:center">选择器</th><th style="text-align:center">例子</th><th style="text-align:center">例子描述</th></tr></thead><tbody><tr><td style="text-align:center">.class</td><td style="text-align:center">.intro</td><td style="text-align:center">选择 class=”intro” 的所有元素。</td></tr><tr><td style="text-align:center">#id</td><td style="text-align:center">#firstname</td><td style="text-align:center">选择 id=”firstname” 的所有元素。</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">*</td><td style="text-align:center">选择所有元素。</td></tr><tr><td style="text-align:center">element</td><td style="text-align:center">p</td><td style="text-align:center">选择所有p元素。</td></tr><tr><td style="text-align:center">element,element</td><td style="text-align:center">div,p</td><td style="text-align:center">选择所有div元素和所有p元素。</td></tr><tr><td style="text-align:center">element?element</td><td style="text-align:center">div p</td><td style="text-align:center">选择div元素内部的所有p元素。</td></tr><tr><td style="text-align:center">element&gt;element</td><td style="text-align:center">div&gt;p</td><td style="text-align:center">选择父元素为div元素的所有p元素。</td></tr><tr><td style="text-align:center">element+element</td><td style="text-align:center">div+p</td><td style="text-align:center">选择紧接在div元素之后的所有p元素。</td></tr><tr><td style="text-align:center">[attribute]</td><td style="text-align:center">[target]</td><td style="text-align:center">选择带有 target 属性所有元素。</td></tr><tr><td style="text-align:center">[attribute=value]</td><td style="text-align:center">[target=_blank]</td><td style="text-align:center">选择 target=”_blank” 的所有元素。</td></tr></tbody></table><p>下面就利用这种方法进行提取：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3 用beautifulsoup + css选择器提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page3</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    <span class="comment"># print(type(content))</span></span><br><span class="line">    <span class="comment"># print('------------')</span></span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.select(<span class="string">'dd i.board-index'</span>)[item].string,</span><br><span class="line">            <span class="comment"># iclass节点完整地为'board-index board-index-1',写board-index即可</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(soup.select(<span class="string">'a &gt; img.board-img'</span>)[item][<span class="string">"data-src"</span>]),</span><br><span class="line">            <span class="comment"># 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值</span></span><br><span class="line"></span><br><span class="line">            <span class="string">'name'</span>: soup.select(<span class="string">'.name a'</span>)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.select(<span class="string">'.star'</span>)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: soup.select(<span class="string">'.integer'</span>)[item].string + soup.select(<span class="string">'.fraction'</span>)[item].string</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p></p><p>运行上述程序，结果同第1种方法一样。</p><h4 id="3-3-4-Beautiful-Soup-find-all函数提取"><a href="#3-3-4-Beautiful-Soup-find-all函数提取" class="headerlink" title="3.3.4. Beautiful Soup + find_all函数提取"></a>3.3.4. Beautiful Soup + find_all函数提取</h4><p>Beautifulsoup除了和css选择器搭配，还可以直接用它自带的find_all函数进行提取。<br><strong>find_all</strong>，顾名思义，就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，功能十分强大。<br>它的API如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find_all(name , attrs , recursive , text , **kwargs)</span><br></pre></td></tr></table></figure><blockquote><p><strong>常用的语法规则如下：</strong><br>soup.find_all(name=’ul’)： 查找所有<strong>ul</strong>节点，ul节点内还可以嵌套；<br>li.string和li.get_text()：都是获取<strong>li</strong>节点的文本，但推荐使用后者；<br>soup.find_all(attrs={‘id’: ‘list-1’}))：传入 attrs 参数，参数的类型是字典类型，表示查询 <strong>id</strong> 为 <strong>list-1</strong> 的节点；<br>常用的属性比如 id、class 等，可以省略attrs采用更简洁的形式，例如：<br>soup.find_all(id=’list-1’)<br>soup.find_all(class_=’element’)</p></blockquote><p>根据上述常用语法，可以提取网页中所需内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page4</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.find_all(class_=<span class="string">'board-index'</span>)[item].string,</span><br><span class="line">            <span class="string">'thumb'</span>: soup.find_all(class_ = <span class="string">'board-img'</span>)[item].attrs[<span class="string">'data-src'</span>],</span><br><span class="line">            <span class="comment"># 用.get('data-src')获取图片src链接，或者用attrs['data-src']</span></span><br><span class="line">            <span class="string">'name'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span> : <span class="string">'name'</span>&#125;)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'star'</span>&#125;)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>:soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'integer'</span>&#125;)[item].string.strip() + soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'fraction'</span>&#125;)[item].string.strip()</span><br><span class="line"></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>以上就是4种不同的内容提取方法。</p><h3 id="3-4-数据存储"><a href="#3-4-数据存储" class="headerlink" title="3.4. 数据存储"></a>3.4. 数据存储</h3><p>上述输出的结果为字典格式，可利用csv包的DictWriter函数将字典格式数据存储到csv文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储到csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file3</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼top100.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 'a'为追加模式（添加）</span></span><br><span class="line">        <span class="comment"># utf_8_sig格式导出csv不乱码 </span></span><br><span class="line">        fieldnames = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]</span><br><span class="line">        w = csv.DictWriter(f,fieldnames = fieldnames)</span><br><span class="line">        <span class="comment"># w.writeheader()</span></span><br><span class="line">        w.writerow(item)</span><br></pre></td></tr></table></figure><p>然后修改一下main()方法：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset=0'</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line">        write_to_csv(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://media.makcyun.top/18-8-20/66910595.jpg" alt=""></p><p>再将封面的图片下载下来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_thumb</span><span class="params">(name, url,num)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'封面图/'</span> + name + <span class="string">'.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">            print(<span class="string">'第%s部电影封面下载完毕'</span> %num)</span><br><span class="line">            print(<span class="string">'------'</span>)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     <span class="comment"># 不能是w，否则会报错，因为图片是二进制数据所以要用wb</span></span><br></pre></td></tr></table></figure><h3 id="3-5-分页爬取"><a href="#3-5-分页爬取" class="headerlink" title="3.5. 分页爬取"></a>3.5. 分页爬取</h3><p>上面完成了一页电影数据的提取，接下来还需提取剩下9页共90部电影的数据。对网址进行遍历，给网址传入一个offset参数即可，修改如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset='</span> + str(offset)</span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  </span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line">        write_to_csv(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        main(offset = i*<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>这样就完成了所有电影的爬取。结果如下：</p><p><img src="http://media.makcyun.top/18-8-19/28479553.jpg" alt=""></p><p><img src="http://media.makcyun.top/18-8-19/34117279.jpg" alt=""></p><h2 id="4-可视化分析"><a href="#4-可视化分析" class="headerlink" title="4. 可视化分析"></a>4. 可视化分析</h2><p>俗话说“文不如表，表不如图”。下面根据excel的数据结果，进行简单的数据可视化分析，并用图表呈现。</p><h3 id="4-1-电影评分最高top10"><a href="#4-1-电影评分最高top10" class="headerlink" title="4.1. 电影评分最高top10"></a>4.1. 电影评分最高top10</h3><p>首先，想看一看评分最高的前10部电影是哪些？</p><p>程序如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment">#用于修改x轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)   <span class="comment">#默认绘图风格很难看，替换为好看的ggplot风格</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))   <span class="comment">#设置图片大小</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment">#设置图表title、text标注的颜色</span></span><br><span class="line"></span><br><span class="line">columns = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]  <span class="comment">#设置表头</span></span><br><span class="line">df = pd.read_csv(<span class="string">'maoyan_top100.csv'</span>,encoding = <span class="string">"utf-8"</span>,header = <span class="keyword">None</span>,names =columns,index_col = <span class="string">'index'</span>)  <span class="comment">#打开表格</span></span><br><span class="line"><span class="comment"># index_col = 'index' 将索引设为index</span></span><br><span class="line"></span><br><span class="line">df_score = df.sort_values(<span class="string">'score'</span>,ascending = <span class="keyword">False</span>)  <span class="comment">#按得分降序排列</span></span><br><span class="line"></span><br><span class="line">name1 = df_score.name[:<span class="number">10</span>]      <span class="comment">#x轴坐标</span></span><br><span class="line">score1 = df_score.score[:<span class="number">10</span>]    <span class="comment">#y轴坐标  </span></span><br><span class="line">plt.bar(range(<span class="number">10</span>),score1,tick_label = name1)  <span class="comment">#绘制条形图，用range()能搞保持x轴正确顺序</span></span><br><span class="line">plt.ylim ((<span class="number">9</span>,<span class="number">9.8</span>))  <span class="comment">#设置纵坐标轴范围</span></span><br><span class="line">plt.title(<span class="string">'电影评分最高top10'</span>,color = colors1) <span class="comment">#标题</span></span><br><span class="line">plt.xlabel(<span class="string">'电影名称'</span>)      <span class="comment">#x轴标题</span></span><br><span class="line">plt.ylabel(<span class="string">'评分'</span>)          <span class="comment">#y轴标题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个条形图添加数值标签</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(score1)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.01</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line"></span><br><span class="line">pl.xticks(rotation=<span class="number">270</span>)   <span class="comment">#x轴名称太长发生重叠，旋转为纵向显示</span></span><br><span class="line">plt.tight_layout()    <span class="comment">#自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line"><span class="comment"># plt.savefig('电影评分最高top10.png')   #保存图片</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://media.makcyun.top/18-8-20/80083042.jpg" alt=""><br>可以看到：排名最高的分别是两部国产片”霸王别姬”和”大话西游”，其他还包括”肖申克的救赎”、”教父”等。<br>嗯，还好基本上都看过。</p><h3 id="4-2-各国家的电影数量比较"><a href="#4-2-各国家的电影数量比较" class="headerlink" title="4.2. 各国家的电影数量比较"></a>4.2. 各国家的电影数量比较</h3><p>然后，想看看100部电影都是来自哪些国家？<br>程序如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">area_count = df.groupby(by = <span class="string">'area'</span>).area.count().sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图方法1</span></span><br><span class="line">area_count.plot.bar(color = <span class="string">'#4652B1'</span>)  <span class="comment">#设置为蓝紫色</span></span><br><span class="line">pl.xticks(rotation=<span class="number">0</span>)   <span class="comment">#x轴名称太长重叠，旋转为纵向</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图方法2</span></span><br><span class="line"><span class="comment"># plt.bar(range(11),area_count.values,tick_label = area_count.index)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(area_count.values)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.5</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line">plt.title(<span class="string">'各国/地区电影数量排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'国家/地区'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('各国(地区)电影数量排名.png')</span></span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://media.makcyun.top/18-8-21/57684234.jpg" alt=""><br>可以看到，除去网站自身没有显示国家的电影以外，上榜电影被10个国家/地区”承包”了。其中，美国以30部电影的绝对优势占据第1名，其次是8部的日本，韩国第3，居然有7部上榜。<br>不得不说的是香港有5部，而内地一部都没有。。。</p><h3 id="4-3-电影作品数量集中的年份"><a href="#4-3-电影作品数量集中的年份" class="headerlink" title="4.3. 电影作品数量集中的年份"></a>4.3. 电影作品数量集中的年份</h3><p>接下来站在漫长的百年电影史的时间角度上，分析一下哪些年份”贡献了”最多的电影数量，也可以说是”电影大年”。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从日期中提取年份</span></span><br><span class="line">df[<span class="string">'year'</span>] = df[<span class="string">'time'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">'/'</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># print(df.info())</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计各年上映的电影数量</span></span><br><span class="line">grouped_year = df.groupby(<span class="string">'year'</span>)</span><br><span class="line">grouped_year_amount = grouped_year.year.count()</span><br><span class="line">top_year = grouped_year_amount.sort_values(ascending = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">top_year.plot(kind = <span class="string">'bar'</span>,color = <span class="string">'orangered'</span>) <span class="comment">#颜色设置为橙红色</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(list(top_year.values)):</span><br><span class="line">    plt.text(x,y+<span class="number">0.1</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line">plt.title(<span class="string">'电影数量年份排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'年份(年)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line"><span class="comment"># plt.savefig('电影数量年份排名.png')</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://media.makcyun.top/18-8-20/32342735.jpg" alt=""></p><p>可以看到，100部电影来自37个年份。其中2011年上榜电影数量最多，达到9部；其次是前一年的7部。回忆一下，那会儿正是上大学的头两年，可怎么感觉除了阿凡达之外，没有什么其他有印象的电影了。。。<br>另外，网上传的号称”电影史奇迹年”的1994年仅排名第6。这让我进一步对猫眼榜单的权威性产生了质疑。<br>再往后看，发现遥远的1939和1940年也有电影上榜。那会儿应该还是黑白电影时代吧，看来电影的口碑好坏跟外在的技术没有绝对的关系，质量才是王道。</p><h3 id="4-4-拥有电影作品数量最多的演员"><a href="#4-4-拥有电影作品数量最多的演员" class="headerlink" title="4.4 拥有电影作品数量最多的演员"></a>4.4 拥有电影作品数量最多的演员</h3><p>最后，看看前100部电影中哪些演员的作品数量最多。<br>程序如下：<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中</span></span><br><span class="line">starlist = []</span><br><span class="line">star_total = df.star</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df.star.str.replace(<span class="string">' '</span>,<span class="string">''</span>).str.split(<span class="string">','</span>):</span><br><span class="line">    starlist.extend(i)  </span><br><span class="line"><span class="comment"># print(starlist)</span></span><br><span class="line"><span class="comment"># print(len(starlist))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set去除重复的演员名</span></span><br><span class="line">starall = set(starlist)</span><br><span class="line"><span class="comment"># print(starall)</span></span><br><span class="line"><span class="comment"># print(len(starall))</span></span><br><span class="line"></span><br><span class="line">starall2 = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> starall:</span><br><span class="line">    <span class="keyword">if</span> starlist.count(i)&gt;<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 筛选出电影数量超过1部的演员</span></span><br><span class="line">        starall2[i] = starlist.count(i)</span><br><span class="line"></span><br><span class="line">starall2 = sorted(starall2.items(),key = <span class="keyword">lambda</span> starlist:starlist[<span class="number">1</span>] ,reverse = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">starall2 = dict(starall2[:<span class="number">10</span>])  <span class="comment">#将元组转为字典格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">x_star = list(starall2.keys())      <span class="comment">#x轴坐标</span></span><br><span class="line">y_star = list(starall2.values())    <span class="comment">#y轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.bar(range(<span class="number">10</span>),y_star,tick_label = x_star)</span><br><span class="line">pl.xticks(rotation = <span class="number">270</span>)</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> enumerate(y_star):</span><br><span class="line">    plt.text(x,y+<span class="number">0.1</span>,<span class="string">'%s'</span> %round(y,<span class="number">1</span>),ha = <span class="string">'center'</span>,color = colors1)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'演员电影作品数量排名'</span>,color = colors1)</span><br><span class="line">plt.xlabel(<span class="string">'演员'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()    </span><br><span class="line"><span class="comment"># plt.savefig('演员电影作品数量排名.png')</span></span><br></pre></td></tr></table></figure><p></p><p>结果如下图：<br><img src="http://media.makcyun.top/18-8-20/66062822.jpg" alt=""></p><p>张国荣排在了第一位，这是之前没有猜到的。其次是梁朝伟和星爷，再之后是布拉德·皮特。惊奇地发现，前十名影星中，香港影星居然占了6位。有点严重怀疑这是不是香港版的top100电影。。。</p><p>对张国荣以7部影片的巨大优势雄霸榜单第一位感到好奇，想看看是哪7部电影。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'star1'</span>] = df[<span class="string">'star'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">','</span>)[<span class="number">0</span>])  <span class="comment">#提取1号演员</span></span><br><span class="line">df[<span class="string">'star2'</span>] = df[<span class="string">'star'</span>].map(<span class="keyword">lambda</span> x:x.split(<span class="string">','</span>)[<span class="number">1</span>])  <span class="comment">#提取2号演员</span></span><br><span class="line">star_most = df[(df.star1 == <span class="string">'张国荣'</span>) | (df.star2 == <span class="string">'张国荣'</span>)][[<span class="string">'star'</span>,<span class="string">'name'</span>]].reset_index(<span class="string">'index'</span>)</span><br><span class="line"><span class="comment"># |表示两个条件或查询，之后重置索引</span></span><br><span class="line">print(star_most)</span><br></pre></td></tr></table></figure><p></p><p>可以看到包括排名第1的”霸王别姬”、第17名的”春光乍泄”、第27名的”射雕英雄传之东成西就”等。<br>突然发现，好像只看过”英雄本色”。。。有时间，去看看他其他的作品。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">     index        star              name</span><br><span class="line">0      1   张国荣,张丰毅,巩俐        霸王别姬</span><br><span class="line">1     17   张国荣,梁朝伟,张震        春光乍泄</span><br><span class="line">2     27  张国荣,梁朝伟,张学友  射雕英雄传之东成西就</span><br><span class="line">3     37  张国荣,梁朝伟,刘嘉玲        东邪西毒</span><br><span class="line">4     70   张国荣,王祖贤,午马        倩女幽魂</span><br><span class="line">5     99  张国荣,张曼玉,刘德华        阿飞正传</span><br><span class="line">6    100   狄龙,张国荣,周润发        英雄本色</span><br></pre></td></tr></table></figure><p>由于数据量有限，故仅作了上述简要的分析。</p><h2 id="5-完整程序"><a href="#5-完整程序" class="headerlink" title="5. 完整程序"></a>5. 完整程序</h2><p>最后，将前面爬虫的所有代码整理一下，完整的代码如下：<br>一、爬虫部分<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line">        <span class="comment"># 不加headers爬不了</span></span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 用正则提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    pattern = re.compile(</span><br><span class="line">        <span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>, re.S)</span><br><span class="line">    <span class="comment"># re.S表示匹配任意字符，如果不加.无法匹配换行符</span></span><br><span class="line">    items = re.findall(pattern, html)</span><br><span class="line">    <span class="comment"># print(items)</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'index'</span>: item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(item[<span class="number">1</span>]),</span><br><span class="line">            <span class="string">'name'</span>: item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'star'</span>: item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="comment"># 'time': item[4].strip()[5:],</span></span><br><span class="line">            <span class="comment"># 用函数分别提取time里的日期和地区</span></span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item[<span class="number">4</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: item[<span class="number">5</span>].strip() + item[<span class="number">6</span>].strip()</span><br><span class="line">        &#125;</span><br><span class="line">      </span><br><span class="line"><span class="comment"># 2 用lxml结合xpath提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page2</span><span class="params">(html)</span>:</span></span><br><span class="line">    parse = etree.HTML(html)</span><br><span class="line">    items = parse.xpath(<span class="string">'//*[@id="app"]//div//dd'</span>)</span><br><span class="line">    <span class="comment"># 完整的是//*[@id="app"]/div/div/div[1]/dl/dd</span></span><br><span class="line">    <span class="comment"># print(type(items))</span></span><br><span class="line">    <span class="comment"># *代表匹配所有节点，@表示属性</span></span><br><span class="line">    <span class="comment"># 第一个电影是dd[1],要提取页面所有电影则去掉[1]</span></span><br><span class="line">    <span class="comment"># xpath://*[@id="app"]/div/div/div[1]/dl/dd[1]</span></span><br><span class="line">    <span class="comment"># lst = []</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>: item.xpath(<span class="string">'./i/text()'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="comment">#./i/text()前面的点表示从items节点开始</span></span><br><span class="line">            <span class="comment">#/text()提取文本</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(str(item.xpath(<span class="string">'./a/img[2]/@data-src'</span>)[<span class="number">0</span>].strip())),</span><br><span class="line">            <span class="comment"># 'thumb': 要在network中定位，在elements里会写成@src而不是@data-src，从而会报list index out of range错误。</span></span><br><span class="line">            <span class="string">'name'</span>: item.xpath(<span class="string">'./a/@title'</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'star'</span>: item.xpath(<span class="string">'.//p[@class = "star"]/text()'</span>)[<span class="number">0</span>].strip(),</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(item.xpath(</span><br><span class="line">                <span class="string">'.//p[@class = "releasetime"]/text()'</span>)[<span class="number">0</span>].strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span> : item.xpath(<span class="string">'.//p[@class = "score"]/i[1]/text()'</span>)[<span class="number">0</span>] + \</span><br><span class="line">            item.xpath(<span class="string">'.//p[@class = "score"]/i[2]/text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 3 用beautifulsoup + css选择器提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page3</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</span><br><span class="line">    <span class="comment"># print(content)</span></span><br><span class="line">    <span class="comment"># print(type(content))</span></span><br><span class="line">    <span class="comment"># print('------------')</span></span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.select(<span class="string">'dd i.board-index'</span>)[item].string,</span><br><span class="line">            <span class="comment"># iclass节点完整地为'board-index board-index-1',写board-inde即可</span></span><br><span class="line">            <span class="string">'thumb'</span>: get_thumb(soup.select(<span class="string">'a &gt; img.board-img'</span>)[item][<span class="string">"data-src"</span>]),</span><br><span class="line">            <span class="comment"># 表示a节点下面的class = board-img的img节点,注意浏览器eelement里面是src节点，而network里面是data-src节点，要用这个才能正确返回值</span></span><br><span class="line"></span><br><span class="line">            <span class="string">'name'</span>: soup.select(<span class="string">'.name a'</span>)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.select(<span class="string">'.star'</span>)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_area(soup.select(<span class="string">'.releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>: soup.select(<span class="string">'.integer'</span>)[item].string + soup.select(<span class="string">'.fraction'</span>)[item].string</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 用beautifulsoup + find_all提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page4</span><span class="params">(html)</span>:</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">    items = range(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="string">'index'</span>: soup.find_all(class_=<span class="string">'board-index'</span>)[item].string,</span><br><span class="line">            <span class="string">'thumb'</span>: soup.find_all(class_ = <span class="string">'board-img'</span>)[item].attrs[<span class="string">'data-src'</span>],</span><br><span class="line">            <span class="comment"># 用.get('data-src')获取图片src链接，或者用attrs['data-src']</span></span><br><span class="line">            <span class="string">'name'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span> : <span class="string">'name'</span>&#125;)[item].string,</span><br><span class="line">            <span class="string">'star'</span>: soup.find_all(name = <span class="string">'p'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'star'</span>&#125;)[item].string.strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'area'</span>: get_release_time(soup.find_all(class_ =<span class="string">'releasetime'</span>)[item].string.strip()[<span class="number">5</span>:]),</span><br><span class="line">            <span class="string">'score'</span>:soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'integer'</span>&#125;)[item].string.strip() + soup.find_all(name = <span class="string">'i'</span>,attrs = &#123;<span class="string">'class'</span>:<span class="string">'fraction'</span>&#125;)[item].string.strip()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取时间函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_time</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)(\(|$)'</span>)</span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)  <span class="comment"># 返回匹配到的第一个括号(.*?)中结果即时间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取国家/地区函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_release_area</span><span class="params">(data)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'.*\((.*)\)'</span>)</span><br><span class="line">    <span class="comment"># $表示匹配一行字符串的结尾，这里就是(.*?)；\(|$,表示匹配字符串含有(,或者只有(.*?)</span></span><br><span class="line">    items = re.search(pattern, data)</span><br><span class="line">    <span class="keyword">if</span> items <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'未知'</span></span><br><span class="line">    <span class="keyword">return</span> items.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取封面大图</span></span><br><span class="line"><span class="comment"># http://p0.meituan.net/movie/5420be40e3b755ffe04779b9b199e935256906.jpg@160w_220h_1e_1c</span></span><br><span class="line"><span class="comment"># 去掉@160w_220h_1e_1c就是大图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_thumb</span><span class="params">(url)</span>:</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'(.*?)@.*?'</span>)</span><br><span class="line">    thumb = re.search(pattern, url)</span><br><span class="line">    <span class="keyword">return</span> thumb.group(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据存储到csv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file3</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼top100.csv'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf_8_sig'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 'a'为追加模式（添加）</span></span><br><span class="line">        <span class="comment"># utf_8_sig格式导出csv不乱码 </span></span><br><span class="line">        fieldnames = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]</span><br><span class="line">        w = csv.DictWriter(f,fieldnames = fieldnames)</span><br><span class="line">        <span class="comment"># w.writeheader()</span></span><br><span class="line">        w.writerow(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 封面下载</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_thumb</span><span class="params">(name, url,num)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'封面图/'</span> + name + <span class="string">'.jpg'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">            print(<span class="string">'第%s部电影封面下载完毕'</span> %num)</span><br><span class="line">            print(<span class="string">'------'</span>)</span><br><span class="line">    <span class="keyword">except</span> RequestException <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     <span class="comment"># 存储格式是wb,因为图片是二进制数格式，不能用w，否则会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset='</span> + str(offset)</span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    <span class="comment"># print(html)</span></span><br><span class="line">    <span class="comment"># parse_one_page2(html)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> parse_one_page(html):  <span class="comment"># 切换内容提取方法</span></span><br><span class="line">        print(item)</span><br><span class="line">        write_to_file(item)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下载封面图</span></span><br><span class="line">        download_thumb(item[<span class="string">'name'</span>], item[<span class="string">'thumb'</span>],item[<span class="string">'index'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment">#     for i in range(10):</span></span><br><span class="line"><span class="comment">#         main(i * 10)</span></span><br><span class="line">        <span class="comment"># time.sleep(0.5)</span></span><br><span class="line">        <span class="comment"># 猫眼增加了反爬虫，设置0.5s的延迟时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 使用多进程提升抓取效率</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    pool.map(main, [i * <span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p></p><p>二、可视化部分<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 可视化分析</span></span><br><span class="line"><span class="comment"># -------------------------------</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl  <span class="comment"># 用于修改x轴坐标</span></span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)  <span class="comment"># 默认绘图风格很难看，替换为好看的ggplot风格</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))  <span class="comment"># 设置图片大小</span></span><br><span class="line">colors1 = <span class="string">'#6D6D6D'</span>  <span class="comment"># 设置图表title、text标注的颜色</span></span><br><span class="line">columns = [<span class="string">'index'</span>, <span class="string">'thumb'</span>, <span class="string">'name'</span>, <span class="string">'star'</span>, <span class="string">'time'</span>, <span class="string">'area'</span>, <span class="string">'score'</span>]  <span class="comment"># 设置表头</span></span><br><span class="line">df = pd.read_csv(<span class="string">'maoyan_top100.csv'</span>, encoding=<span class="string">"utf-8"</span>,</span><br><span class="line">                 header=<span class="keyword">None</span>, names=columns, index_col=<span class="string">'index'</span>)  <span class="comment"># 打开表格</span></span><br><span class="line"><span class="comment"># index_col = 'index' 将索引设为index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1电影评分最高top10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_1</span><span class="params">()</span>:</span></span><br><span class="line">    df_score = df.sort_values(<span class="string">'score'</span>, ascending=<span class="keyword">False</span>)  <span class="comment"># 按得分降序排列</span></span><br><span class="line"></span><br><span class="line">    name1 = df_score.name[:<span class="number">10</span>]  <span class="comment"># x轴坐标</span></span><br><span class="line">    score1 = df_score.score[:<span class="number">10</span>]  <span class="comment"># y轴坐标</span></span><br><span class="line">    plt.bar(range(<span class="number">10</span>), score1, tick_label=name1)  <span class="comment"># 绘制条形图，用range()能搞保持x轴正确顺序</span></span><br><span class="line">    plt.ylim((<span class="number">9</span>, <span class="number">9.8</span>))  <span class="comment"># 设置纵坐标轴范围</span></span><br><span class="line">    plt.title(<span class="string">'电影评分最高top10'</span>, color=colors1)  <span class="comment"># 标题</span></span><br><span class="line">    plt.xlabel(<span class="string">'电影名称'</span>)  <span class="comment"># x轴标题</span></span><br><span class="line">    plt.ylabel(<span class="string">'评分'</span>)  <span class="comment"># y轴标题</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为每个条形图添加数值标签</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(score1)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.01</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line"></span><br><span class="line">    pl.xticks(rotation=<span class="number">270</span>)  <span class="comment"># x轴名称太长发生重叠，旋转为纵向显示</span></span><br><span class="line">    plt.tight_layout()  <span class="comment"># 自动控制空白边缘，以全部显示x轴名称</span></span><br><span class="line">    <span class="comment"># plt.savefig('电影评分最高top10.png')   #保存图片</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 2各国家的电影数量比较</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_2</span><span class="params">()</span>:</span></span><br><span class="line">    area_count = df.groupby(</span><br><span class="line">        by=<span class="string">'area'</span>).area.count().sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图方法1</span></span><br><span class="line">    area_count.plot.bar(color=<span class="string">'#4652B1'</span>)  <span class="comment"># 设置为蓝紫色</span></span><br><span class="line">    pl.xticks(rotation=<span class="number">0</span>)  <span class="comment"># x轴名称太长重叠，旋转为纵向</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图方法2</span></span><br><span class="line">    <span class="comment"># plt.bar(range(11),area_count.values,tick_label = area_count.index,color</span></span><br><span class="line">    <span class="comment"># = '#4652B1')</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(area_count.values)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.5</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line">    plt.title(<span class="string">'各国/地区电影数量排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'国家/地区'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('各国(地区)电影数量排名.png')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 3电影作品数量集中的年份</span></span><br><span class="line"><span class="comment"># 从日期中提取年份</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_3</span><span class="params">()</span>:</span></span><br><span class="line">    df[<span class="string">'year'</span>] = df[<span class="string">'time'</span>].map(<span class="keyword">lambda</span> x: x.split(<span class="string">'/'</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># print(df.info())</span></span><br><span class="line">    <span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计各年上映的电影数量</span></span><br><span class="line">    grouped_year = df.groupby(<span class="string">'year'</span>)</span><br><span class="line">    grouped_year_amount = grouped_year.year.count()</span><br><span class="line">    top_year = grouped_year_amount.sort_values(ascending=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    top_year.plot(kind=<span class="string">'bar'</span>, color=<span class="string">'orangered'</span>)  <span class="comment"># 颜色设置为橙红色</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(list(top_year.values)):</span><br><span class="line">        plt.text(x, y + <span class="number">0.1</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line">    plt.title(<span class="string">'电影数量年份排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'年份(年)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment"># plt.savefig('电影数量年份排名.png')</span></span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------</span></span><br><span class="line"><span class="comment"># 4拥有电影作品数量最多的演员</span></span><br><span class="line"><span class="comment"># 表中的演员位于同一列，用逗号分割符隔开。需进行分割然后全部提取到list中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annalysis_4</span><span class="params">()</span>:</span></span><br><span class="line">    starlist = []</span><br><span class="line">    star_total = df.star</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> df.star.str.replace(<span class="string">' '</span>, <span class="string">''</span>).str.split(<span class="string">','</span>):</span><br><span class="line">        starlist.extend(i)</span><br><span class="line">    <span class="comment"># print(starlist)</span></span><br><span class="line">    <span class="comment"># print(len(starlist))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># set去除重复的演员名</span></span><br><span class="line">    starall = set(starlist)</span><br><span class="line">    <span class="comment"># print(starall)</span></span><br><span class="line">    <span class="comment"># print(len(starall))</span></span><br><span class="line"></span><br><span class="line">    starall2 = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> starall:</span><br><span class="line">        <span class="keyword">if</span> starlist.count(i) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 筛选出电影数量超过1部的演员</span></span><br><span class="line">            starall2[i] = starlist.count(i)</span><br><span class="line"></span><br><span class="line">    starall2 = sorted(starall2.items(),</span><br><span class="line">                      key=<span class="keyword">lambda</span> starlist: starlist[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    starall2 = dict(starall2[:<span class="number">10</span>])  <span class="comment"># 将元组转为字典格式</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘图</span></span><br><span class="line">    x_star = list(starall2.keys())  <span class="comment"># x轴坐标</span></span><br><span class="line">    y_star = list(starall2.values())  <span class="comment"># y轴坐标</span></span><br><span class="line"></span><br><span class="line">    plt.bar(range(<span class="number">10</span>), y_star, tick_label=x_star)</span><br><span class="line">    pl.xticks(rotation=<span class="number">270</span>)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> enumerate(y_star):</span><br><span class="line">        plt.text(x, y + <span class="number">0.1</span>, <span class="string">'%s'</span> % round(y, <span class="number">1</span>), ha=<span class="string">'center'</span>, color=colors1)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">'演员电影作品数量排名'</span>, color=colors1)</span><br><span class="line">    plt.xlabel(<span class="string">'演员'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'数量(部)'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('演员电影作品数量排名.png')</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    annalysis_1()</span><br><span class="line">    annalysis_2()</span><br><span class="line">    annalysis_3()</span><br><span class="line">    annalysis_4()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:48 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;b&gt;&lt;em&gt;python爬虫第1篇&lt;/em&gt;&lt;/b&gt;&lt;br&gt;利用Request请求库和4种内容提取方法：正则表达式、lxml+xpath、Beatutifulsoup+css选择器、Beatutifulsoup+find_all爬取网页内容。&lt;/p&gt;
    
    </summary>
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="https://www.makcyun.top/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="requests" scheme="https://www.makcyun.top/tags/requests/"/>
    
      <category term="正则表达式" scheme="https://www.makcyun.top/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
      <category term="beautifulsoup" scheme="https://www.makcyun.top/tags/beautifulsoup/"/>
    
      <category term="css" scheme="https://www.makcyun.top/tags/css/"/>
    
      <category term="xpath" scheme="https://www.makcyun.top/tags/xpath/"/>
    
      <category term="lxml" scheme="https://www.makcyun.top/tags/lxml/"/>
    
  </entry>
  
  <entry>
    <title>4块钱,用Github+Hexo搭建你的个人博客：美化篇</title>
    <link href="https://www.makcyun.top/2018/07/17/hexo02.html"/>
    <id>https://www.makcyun.top/2018/07/17/hexo02.html</id>
    <published>2018-07-17T10:17:10.000Z</published>
    <updated>2018-12-19T02:54:55.454Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p>上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。</p><a id="more"></a><p><strong>摘要：</strong>搭建博客相对简单，而美化博客则要复杂一些，因为涉及到修改和增删源代码，对于没有前端基础的人来说，会比较费时间精力。为了尽可能在最短的时间里，打造一个总体看得过去的博客，本文以我的博客为例，介绍一些比较实用的博客美化操作和技巧。</p><h2 id="1-选择新的模板"><a href="#1-选择新的模板" class="headerlink" title="1. 选择新的模板"></a>1. 选择新的模板</h2><p>首先，是要更换非常难看的初始的博客界面。重新挑选一个好看的主题模板，然后在此基础上进行美化。<br><img src="http://media.makcyun.top/18-7-14/90405263.jpg" alt=""></p><p>主题寻找：<br><a href="https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories" target="_blank" rel="noopener">https://github.com/search?o=desc&amp;q=topic%3Ahexo-theme&amp;s=stars&amp;type=Repositories</a></p><p>该网站按照模板的受欢迎程度进行排名，可以看到遥遥领先的第一名是一款叫作：<strong>next</strong>的主题，选用这款即可。进入到这个主题，可以阅读<strong>README.md</strong>模板使用说明，还可以查看模板示例网站。</p><p><img src="http://media.makcyun.top/FsgZW6JnrTdKpylVLyEORZbKJr9f" alt=""></p><p>模板使用：<br>打开博客根目录下的<strong>themes文件夹</strong>(注：后文所说的根目录指：<code>D:\blog</code>)，右键<strong>Git Bash</strong>运行下述命令：<br><code>git clone https://github.com/iissnan/hexo-theme-next themes/next</code><br>就可以把这款主题的安装文件下载到电脑中。接着，打开D:\blog_config.yml文件，找到 theme字段，修改参数为：theme: hexo-theme-next，然后根目录运行下述命令：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo s -g</span><br></pre></td></tr></table></figure><p></p><p>这样，便成功应用新的<strong>next</strong>主题，浏览器访问 :<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>，查看一下新的博客页面。<br><img src="http://media.makcyun.top/FpHLTzWWl-JiakApNlw4nTH4hiin" alt=""><br>可以看到，博客变得非常清爽了，（可能和你实际看到的，略有不同，没有关系）。<br>这款主题包含4种风格，默认的是<strong>Muse</strong>，也可以尝试其他风格。具体操作：<br>打开<code>D:\blog\_config.yml</code>，定位到Schemes，想要哪款主题就取消前面的<strong>#</strong>，我的博客使用的是<strong>Pisces</strong>风格。<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">scheme: Pisces</span><br><span class="line">#scheme: Gemini</span><br></pre></td></tr></table></figure><p></p><p><img src="http://media.makcyun.top/18-8-22/78786445.jpg" alt=""></p><h2 id="2-模板美化"><a href="#2-模板美化" class="headerlink" title="2. 模板美化"></a>2. 模板美化</h2><p>接下来进行模板的美化。<br>根据网页的结构布局，将从以下几个部分进行针对性地美化：</p><ul><li>总体</li><li>侧边栏</li><li>页脚</li><li>文章</li></ul><p><strong>重要的文件</strong><br>美化需要主要是对几个模板文件进行修改和增删。为了便于后续进行操作，先列出文件名和所在的位置：</p><ul><li>站点文件。位于站点文件夹根目录内：<br><strong>D:/blog/_config.yml</strong></li><li>主题文件。位于主题文件夹根目录内：<br><strong>D:/blog/themes/next/_config.yml</strong></li><li>自定义样式文件。位于主题文件夹内：<br><strong>D:\blog\themes\hexo-theme-next\source\css_custom\custom.styl</strong></li></ul><h3 id="2-1-总体布置"><a href="#2-1-总体布置" class="headerlink" title="2.1. 总体布置"></a>2.1. 总体布置</h3><p><img src="http://media.makcyun.top/18-8-22/38494028.jpg" alt=""></p><h4 id="2-1-1-设置中文界面"><a href="#2-1-1-设置中文界面" class="headerlink" title="2.1.1. 设置中文界面"></a>2.1.1. 设置中文界面</h4><p><strong>站点文件:</strong> language: zh-Hans<br>如果中文乱码，记事本另存为utf-8，最好不要用记事本编辑，用notepad。</p><h4 id="2-1-2-动态背景"><a href="#2-1-2-动态背景" class="headerlink" title="2.1.2. 动态背景"></a>2.1.2. 动态背景</h4><p><strong>主题文件：</strong> canvas_nest: true<br>背景的几何线条是采用的nest效果，一个基于html5 canvas绘制的网页背景效果，非常赞！来自github的开源项目canvas-nest：<a href="https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md" target="_blank" rel="noopener">https://github.com/hustcc/canvas-nest.js/blob/master/README-zh.md</a></p><p>如果感觉默认的线条太多的话，可以这么设置：<br>打开 <code>next/layout/_layout.swig</code>，在 &lt; /body&gt;之前添加代码(注意不要放在&lt; /head&gt;的后面)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if theme.canvas_nest %&#125;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;</span><br><span class="line">color=&quot;233,233,233&quot; opacity=&apos;0.9&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>说明：<br>color ：线条颜色, 默认: ‘0,0,0’；三个数字分别为(R,G,B)<br>opacity: 线条透明度（0~1）, 默认: 0.5<br>count: 线条的总数量, 默认: 150<br>zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1</p><h3 id="2-2-侧边栏美化"><a href="#2-2-侧边栏美化" class="headerlink" title="2.2. 侧边栏美化"></a>2.2. 侧边栏美化</h3><p><img src="http://media.makcyun.top/18-8-22/97716423.jpg" alt=""></p><h4 id="2-2-1-添加博客名字和slogan"><a href="#2-2-1-添加博客名字和slogan" class="headerlink" title="2.2.1. 添加博客名字和slogan"></a>2.2.1. 添加博客名字和slogan</h4><p>修改<strong>站点文件</strong>如下：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Hexo Configuration</span><br><span class="line">## Docs: https://hexo.io/docs/configuration.html</span><br><span class="line">## Source: https://github.com/hexojs/hexo/</span><br><span class="line"></span><br><span class="line"># Site</span><br><span class="line">title: 高级农民工            # 更改为你自己的</span><br><span class="line">subtitle: Beginner<span class="string">'s Mind   </span></span><br><span class="line"><span class="string">description:</span></span><br><span class="line"><span class="string">keywords: python,hexo,神器,软件</span></span><br><span class="line"><span class="string">author: 高级农民工</span></span><br><span class="line"><span class="string">language: zh-Hans</span></span><br><span class="line"><span class="string">timezone:</span></span><br></pre></td></tr></table></figure><p></p><h4 id="2-2-2-菜单设置"><a href="#2-2-2-菜单设置" class="headerlink" title="2.2.2. 菜单设置"></a>2.2.2. 菜单设置</h4><p>文件路径：<code>D:\blog\themes\hexo-theme-next\languages\zh-Hans.yml</code><br>修改如下：<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: 首&amp;emsp;&amp;emsp;页</span><br><span class="line">  archives: 归&amp;emsp;&amp;emsp;档</span><br><span class="line">  categories: 分&amp;emsp;&amp;emsp;类</span><br><span class="line">  tags: 标&amp;emsp;&amp;emsp;签</span><br><span class="line">  about: 关于博主</span><br><span class="line">  search: 站内搜索</span><br><span class="line">  top: 最受欢迎</span><br><span class="line">  schedule: 日程表</span><br><span class="line">  sitemap: 站点地图</span><br><span class="line">  # commonweal: 公益404</span><br></pre></td></tr></table></figure><p></p><p>注意：两字的中间添加<code>&amp;emsp;&amp;emsp;</code>可实现列对齐。</p><h4 id="2-2-3-新建标签、分类、关于页面"><a href="#2-2-3-新建标签、分类、关于页面" class="headerlink" title="2.2.3. 新建标签、分类、关于页面"></a>2.2.3. 新建标签、分类、关于页面</h4><p>分别运行命令：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;tags&quot; </span><br><span class="line">hexo new page &quot;categories&quot;  </span><br><span class="line">hexo new page &quot;about&quot;</span><br></pre></td></tr></table></figure><p></p><p>然后，打开<code>D:\blog\source</code>就可以看到上述三个文件夹。<br>要添加关于博主的介绍，只需要在<code>/about/index.md</code>文件中，用markdown书写内容即可，写完后运行：<code>hexo d -g</code>，便可看到效果。</p><h4 id="2-2-4-侧栏社交链接图标设置"><a href="#2-2-4-侧栏社交链接图标设置" class="headerlink" title="2.2.4. 侧栏社交链接图标设置"></a>2.2.4. 侧栏社交链接图标设置</h4><p>可以添加你的github、Email、知乎、简书等社交网站账号。<br><strong>主题文件：</strong><br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># ---------------------------------------------------------------</span><br><span class="line"># Sidebar Settings 侧栏社交链接图标设置</span><br><span class="line"># ---------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"># Social Links.</span><br><span class="line"># Usage: `Key: permalink || icon`</span><br><span class="line"># Key is the link label showing to end users.</span><br><span class="line"># Value before `||` delimeter is the target permalink.</span><br><span class="line"># Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, globe icon will be loaded.</span><br><span class="line">social:</span><br><span class="line">  GitHub: https:<span class="comment">//github.com/makcyun || github</span></span><br><span class="line">  E-Mail: mailto:johnny824lee@gmail.com || envelope</span><br><span class="line">  #Google: https://plus.google.com/yourname || google</span><br><span class="line">  #Twitter: https://twitter.com/yourname || twitter</span><br><span class="line">  #FB Page: https://www.facebook.com/yourname || facebook</span><br><span class="line">  #VK Group: https://vk.com/yourname || vk</span><br><span class="line">  #StackOverflow: https://stackoverflow.com/yourname || stack-overflow</span><br><span class="line">  #YouTube: https://youtube.com/yourname || youtube</span><br><span class="line">  #Instagram: https://instagram.com/yourname || instagram</span><br><span class="line">  #Skype: skype:yourname?call|chat || skype</span><br><span class="line"></span><br><span class="line">social_icons:</span><br><span class="line">  enable: <span class="literal">true</span></span><br><span class="line">  icons_only: <span class="literal">false</span></span><br><span class="line">  transition: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p></p><h4 id="2-2-5-添加头像并美化"><a href="#2-2-5-添加头像并美化" class="headerlink" title="2.2.5. 添加头像并美化"></a>2.2.5. 添加头像并美化</h4><p>博客添加头像有两种方法：第一种是放在本地文件夹中：D:\blog\public\uploads，并且命名为<strong>avatar.jpg</strong>。第二种是将图片放在七牛云中，然后传入链接。推荐这种方式，可以加快网页打开速度。<br><strong>站点文件</strong>任意行添加下面代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 添加头像</span><br><span class="line"># avatar: /uploads/avatar.jpg   #方法1本地图片</span><br><span class="line">avatar: http:<span class="comment">//media.makcyun.top/18-8-3/40685653.jpg  # 方法2网络图片</span></span><br><span class="line"></span><br><span class="line">注意：uppoads文件夹是在主题里的文件夹，没有则新建</span><br><span class="line">D:\blog\themes\hexo-theme-next\source\uploads\avatar.jpg</span><br></pre></td></tr></table></figure><p></p><p><strong>头像变圆形</strong><br>可参考：<br><a href="http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html" target="_blank" rel="noopener">http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html</a><br><code>D:\blog\themes\next\source\css\_common\components\sidebar\sidebar-author.styl</code>，在里面添加如下代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">.site-author-image &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  margin: <span class="number">0</span> auto;</span><br><span class="line">  padding: $site-author-image-padding;</span><br><span class="line">  max-width: $site-author-image-width;</span><br><span class="line">  height: $site-author-image-height;</span><br><span class="line">  border: $site-author-image-border-width solid $site-author-image-border-color;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 头像圆形 */</span></span><br><span class="line">  border-radius: <span class="number">80</span>px;</span><br><span class="line">  -webkit-border-radius: <span class="number">80</span>px;</span><br><span class="line">  -moz-border-radius: <span class="number">80</span>px;</span><br><span class="line">  box-shadow: inset 0 -1px 0 #333sf;</span><br><span class="line">  <span class="comment">/* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 </span></span><br><span class="line"><span class="comment">    (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-transition: -webkit-transform <span class="number">1.0</span>s ease-out;</span><br><span class="line">  -moz-transition: -moz-transform <span class="number">1.0</span>s ease-out;</span><br><span class="line">  transition: transform <span class="number">1.0</span>s ease-out;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*再进一步想点击产生旋转效果，就继续在该文件下方添加代码：*/</span></span><br><span class="line"></span><br><span class="line">img:hover &#123;</span><br><span class="line">  <span class="comment">/* 鼠标经过停止头像旋转 </span></span><br><span class="line"><span class="comment">  -webkit-animation-play-state:paused;</span></span><br><span class="line"><span class="comment">  animation-play-state:paused;*/</span></span><br><span class="line">  <span class="comment">/* 鼠标经过头像旋转360度 */</span></span><br><span class="line">  -webkit-transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">  -moz-transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">  transform: rotateZ(<span class="number">360</span>deg);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/* Z 轴旋转动画 */</span></span><br><span class="line">@-webkit-keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    -webkit-transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@-moz-keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    -moz-transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    -moz-transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">@keyframes play &#123;</span><br><span class="line">  <span class="number">0</span>% &#123;</span><br><span class="line">    transform: rotateZ(<span class="number">0</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">100</span>% &#123;</span><br><span class="line">    transform: rotateZ(<span class="number">-360</span>deg);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3 id="2-3-页脚美化"><a href="#2-3-页脚美化" class="headerlink" title="2.3. 页脚美化"></a>2.3. 页脚美化</h3><p><img src="http://media.makcyun.top/18-8-22/89825453.jpg" alt=""><br><strong>建站时间设置</strong><br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Set rss to false to disable feed link.</span><br><span class="line"># Leave rss as empty to use site's feed link.</span><br><span class="line"># Set rss to specific value if you have burned your feed already.</span><br><span class="line">rss:</span><br><span class="line"></span><br><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  # 建站年份</span><br><span class="line">  since: 2018           #根据实际情况修改</span><br></pre></td></tr></table></figure><p></p><h4 id="2-3-1-隐藏powered-By-Hexo-主题"><a href="#2-3-1-隐藏powered-By-Hexo-主题" class="headerlink" title="2.3.1. 隐藏powered By Hexo/主题"></a>2.3.1. 隐藏powered By Hexo/主题</h4><p>文件路径： D:\blog\themes\hexo-theme-next\layout_partials\ <strong>footer.swig</strong><br>更改该文件下面的代码：<br></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="theme-info"&gt;&#123;#</span><br><span class="line">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line">  #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span><br><span class="line">#&#125;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p></p><p>用注释两行如下语句，也可以直接删除掉这段代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--<span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"theme-info"</span>&gt;</span>&#123;#</span></span><br><span class="line"><span class="xml">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span></span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line"><span class="xml">  #&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span>&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span></span><br><span class="line"><span class="xml">#&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span>--&gt;</span><br></pre></td></tr></table></figure><p></p><h4 id="2-3-2-next版本隐藏"><a href="#2-3-2-next版本隐藏" class="headerlink" title="2.3.2. next版本隐藏"></a>2.3.2. next版本隐藏</h4><p>继续在上面文件中修改代码如下：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 用&lt;!--注释语句--&gt;</span><br><span class="line">&#123;% <span class="keyword">if</span> theme.footer.theme.enable %&#125;</span><br><span class="line">  &lt;!--<span class="xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"theme-info"</span>&gt;</span>&#123;#</span></span><br><span class="line"><span class="xml">  #&#125;&#123;&#123; __('footer.theme') &#125;&#125; &amp;mdash; &#123;#</span></span><br><span class="line">  #&#125;&lt;a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next"&gt;&#123;#</span><br><span class="line">    #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;#</span><br><span class="line"><span class="xml">  #&#125;<span class="tag">&lt;/<span class="name">a</span>&gt;</span>&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;#</span></span><br><span class="line"><span class="xml">#&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span>--&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="2-3-3-时间和用户名之间添加心形"><a href="#2-3-3-时间和用户名之间添加心形" class="headerlink" title="2.3.3. 时间和用户名之间添加心形"></a>2.3.3. 时间和用户名之间添加心形</h4><p><strong>主题文件：</strong>建站时间下面修改<code>icon: heart</code><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup.</span><br><span class="line">  # If not defined, current year will be used.</span><br><span class="line">  # 建站年份</span><br><span class="line">  since: 2018</span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  # 年份后面的图标，为 Font Awesome 图标</span><br><span class="line">  # 自己去纠结 http://fontawesome.io/icons/</span><br><span class="line">  # 然后更改名字就行，下面的有关图标的设置都一样</span><br><span class="line"></span><br><span class="line">  # Icon between year and copyright info.</span><br><span class="line">  #icon: user</span><br><span class="line">  icon: heart</span><br></pre></td></tr></table></figure><p></p><p>如果还想让心变成跳动的红心，则继续在:上面的<strong>footer.swig</strong>文件中修改：<br><code>&lt;span class=&quot;with-love&quot;&gt;</code>为 <code>&lt;span class=&quot;with-love&quot; id=&quot;heart&quot;&gt;</code> #一定要加id=”heart”<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="copyright"&gt;&#123;#</span><br><span class="line">#&#125;&#123;% set current = date(Date.now(), "YYYY") %&#125;&#123;#</span><br><span class="line">#&#125;&amp;copy; &#123;% if theme.footer.since and theme.footer.since != current %&#125;&#123;&#123; theme.footer.since &#125;&#125; &amp;mdash; &#123;% endif %&#125;&#123;#</span><br><span class="line">#&#125;&lt;span itemprop="copyrightYear"&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt;</span><br><span class="line">  &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"with-love"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-&#123;&#123; theme.footer.icon &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">  &lt;span class="author" itemprop="copyrightHolder"&gt;&#123;&#123; theme.footer.copyright || config.author &#125;&#125;&lt;/</span>span&gt;</span><br></pre></td></tr></table></figure><p></p><p>在自定义文件中添加如下代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 页脚加闪烁红心</span></span><br><span class="line"><span class="comment">// 自定义页脚跳动的心样式</span></span><br><span class="line">@keyframes heartAnimate &#123;</span><br><span class="line">    <span class="number">0</span>%,<span class="number">100</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    <span class="number">10</span>%,<span class="number">30</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    <span class="number">20</span>%,<span class="number">40</span>%,<span class="number">60</span>%,<span class="number">80</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    <span class="number">50</span>%,<span class="number">70</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line">#heart &#123;</span><br><span class="line">    animation: heartAnimate <span class="number">1.33</span>s ease-<span class="keyword">in</span>-out infinite;</span><br><span class="line">&#125;</span><br><span class="line">.with-love &#123;</span><br><span class="line">    color: rgb(<span class="number">192</span>, <span class="number">0</span>, <span class="number">39</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>接着在自定义<code>custom.styl</code>文件中，添加以下代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 页脚加闪烁红心</span></span><br><span class="line"><span class="comment">// 自定义页脚跳动的心样式</span></span><br><span class="line">@keyframes heartAnimate &#123;</span><br><span class="line">    <span class="number">0</span>%,<span class="number">100</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1</span>);&#125;</span><br><span class="line">    <span class="number">10</span>%,<span class="number">30</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">0.9</span>);&#125;</span><br><span class="line">    <span class="number">20</span>%,<span class="number">40</span>%,<span class="number">60</span>%,<span class="number">80</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">    <span class="number">50</span>%,<span class="number">70</span>%&#123;<span class="attr">transform</span>:scale(<span class="number">1.1</span>);&#125;</span><br><span class="line">&#125;</span><br><span class="line">#heart &#123;</span><br><span class="line">    animation: heartAnimate <span class="number">1.33</span>s ease-<span class="keyword">in</span>-out infinite;</span><br><span class="line">&#125;</span><br><span class="line">.with-love &#123;</span><br><span class="line">    color: rgb(192, 0, 39);   # rgb可随意修改</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="2-3-4-页脚显示总访客数和总浏览量"><a href="#2-3-4-页脚显示总访客数和总浏览量" class="headerlink" title="2.3.4. 页脚显示总访客数和总浏览量"></a>2.3.4. 页脚显示总访客数和总浏览量</h4><p>首先，在上述<code>footer.swig</code>文件首行添加如下代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;script <span class="keyword">async</span> src=<span class="string">"https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">#接着修改相应代码：</span><br><span class="line"># 添加总访客量</span><br><span class="line">&lt;span id=<span class="string">"busuanzi_container_site_uv"</span>&gt;</span><br><span class="line">  访客数:<span class="xml"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"busuanzi_value_site_uv"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span>人次</span><br><span class="line">&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">&#123;% if theme.footer.powered %&#125;</span></span><br><span class="line"><span class="regexp">  &lt;!--&lt;div class="powered-by"&gt;&#123;#</span></span><br><span class="line"><span class="regexp">  #&#125;&#123;&#123; __('footer.powered', '&lt;a class="theme-link" target="_blank" href="https:/</span><span class="regexp">/hexo.io"&gt;Hexo&lt;/</span>a&gt;<span class="string">') &#125;&#125;&#123;#</span></span><br><span class="line"><span class="string">#&#125;&lt;/div&gt;--&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加'</span>|<span class="string">'符号</span></span><br><span class="line"><span class="string">&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125;</span></span><br><span class="line"><span class="string">  &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;% if theme.footer.custom_text %&#125;</span></span><br><span class="line"><span class="string">  &lt;div class="footer-custom"&gt;&#123;#</span></span><br><span class="line"><span class="string">  #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;#</span></span><br><span class="line"><span class="string">#&#125;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加总访问量</span></span><br><span class="line"><span class="string">&lt;span id="busuanzi_container_site_pv"&gt;</span></span><br><span class="line"><span class="string">   总访问量:&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次</span></span><br><span class="line"><span class="string">&lt;/span&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加'</span>|<span class="string">'符号</span></span><br><span class="line"><span class="string">&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125;</span></span><br><span class="line"><span class="string">  &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</span></span><br><span class="line"><span class="string">&#123;% endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 添加博客全站共：</span></span><br><span class="line"><span class="string">&lt;div class="theme-info"&gt;</span></span><br><span class="line"><span class="string">  &lt;div class="powered-by"&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">  &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br></pre></td></tr></table></figure><p></p><h3 id="2-4-文章美化"><a href="#2-4-文章美化" class="headerlink" title="2.4. 文章美化"></a>2.4. 文章美化</h3><p><img src="http://media.makcyun.top/18-8-22/74366995.jpg" alt=""></p><h4 id="2-4-1-显示统计字数和估计阅读时长"><a href="#2-4-1-显示统计字数和估计阅读时长" class="headerlink" title="2.4.1. 显示统计字数和估计阅读时长"></a>2.4.1. 显示统计字数和估计阅读时长</h4><p>修改<strong>主题文件：</strong><br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Post wordcount display settings</span><br><span class="line"># Dependencies: https://github.com/willin/hexo-wordcount</span><br><span class="line"># 显示统计字数和估计阅读时长</span><br><span class="line"># 注意：这个要安装插件，先进入站点文件夹根目录</span><br><span class="line"># 然后：npm install hexo-wordcount --save</span><br><span class="line">post_wordcount:</span><br><span class="line">  item_text: <span class="literal">true</span></span><br><span class="line">  wordcount: <span class="literal">true</span></span><br><span class="line">  min2read: <span class="literal">true</span></span><br><span class="line">  totalcount: <span class="literal">false</span></span><br><span class="line">  separated_meta: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p></p><p>注意，做了以上修改后，发现字数只显示了数字并没有带相应的单位:<strong>字</strong>和<strong>分钟</strong>。因此，还需做如下修改：<br>打开<code>D:\blog\themes\hexo-theme-next\layout\_macro\ **post.swig**</code>文件，添加单位：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#123;% <span class="keyword">if</span> theme.post_wordcount.wordcount or theme.post_wordcount.min2read %&#125;</span><br><span class="line">            &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-wordcount"</span>&gt;</span><br><span class="line">              &#123;% <span class="keyword">if</span> theme.post_wordcount.wordcount %&#125;</span><br><span class="line">                &#123;% <span class="keyword">if</span> not theme.post_wordcount.separated_meta %&#125;</span><br><span class="line">                  &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-divider"</span>&gt;|<span class="xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-item-icon"</span>&gt;</span><br><span class="line">                  &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-file-word-o"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">                &#123;% if theme.post_wordcount.item_text %&#125;</span></span><br><span class="line"><span class="regexp">                  &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.wordcount') &#125;&#125;&amp;#58;&lt;/</span>span&gt;</span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span title=<span class="string">"&#123;&#123; __('post.wordcount') &#125;&#125;"</span>&gt;</span><br><span class="line">                  &#123;&#123; wordcount(post.content) &#125;&#125; 字</span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">              &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">              &#123;% if theme.post_wordcount.wordcount and theme.post_wordcount.min2read %&#125;</span></span><br><span class="line"><span class="regexp">                &lt;span class="post-meta-divider"&gt;|&lt;/</span>span&gt;</span><br><span class="line">              &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">              &#123;% <span class="keyword">if</span> theme.post_wordcount.min2read %&#125;</span><br><span class="line">                &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-meta-item-icon"</span>&gt;</span><br><span class="line">                  &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-clock-o"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">                &#123;% if theme.post_wordcount.item_text %&#125;</span></span><br><span class="line"><span class="regexp">                  &lt;span class="post-meta-item-text"&gt;&#123;&#123; __('post.min2read') &#125;&#125; &amp;asymp;&lt;/</span>span&gt;</span><br><span class="line">                &#123;% endif %&#125;</span><br><span class="line">                &lt;span title=<span class="string">"&#123;&#123; __('post.min2read') &#125;&#125;"</span>&gt;</span><br><span class="line">                  &#123;&#123; min2read(post.content) &#125;&#125; 分钟 </span><br><span class="line">                &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">              &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;/</span>div&gt;</span><br><span class="line">          &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">          &#123;% <span class="keyword">if</span> post.description and (not theme.excerpt_description or not is_index) %&#125;</span><br><span class="line">              &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-description"</span>&gt;</span><br><span class="line">                  &#123;&#123; post.description &#125;&#125;</span><br><span class="line">              &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">          &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">      &lt;<span class="regexp">/header&gt;</span></span><br><span class="line"><span class="regexp">    &#123;% endif %&#125;</span></span><br></pre></td></tr></table></figure><p></p><h4 id="2-4-2-添加阅读全文"><a href="#2-4-2-添加阅读全文" class="headerlink" title="2.4.2. 添加阅读全文"></a>2.4.2. 添加阅读全文</h4><p>实现在主页只展示部分文字，其他文字隐藏起来，通过点击’阅读更多’来阅读全文。<br>方法就是写每一篇文章的时候，在必要的地方添加<code>&lt;!-- more --&gt;</code>即可。<br>例如：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 4块钱,用Github+Hexo搭建你的个人博客：搭建篇</span><br><span class="line">id: hexo01</span><br><span class="line">images: http://media.makcyun.top/18-8-3/89578286.jpg</span><br><span class="line">categories: hexo博客</span><br><span class="line">tags: [hexo,个人博客,github]</span><br><span class="line">keywords: hexo,搭建博客,github pages,next</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。</span><br><span class="line">  </span><br><span class="line">&lt;!-- more --&gt;</span><br><span class="line"></span><br><span class="line">摘要： 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。</span><br></pre></td></tr></table></figure><p></p><h4 id="2-4-3-显示每篇文章的阅读量"><a href="#2-4-3-显示每篇文章的阅读量" class="headerlink" title="2.4.3. 显示每篇文章的阅读量"></a>2.4.3. 显示每篇文章的阅读量</h4><p>参考这个教程即可：<br><a href="http://www.jeyzhang.com/hexo-next-add-post-views.html" target="_blank" rel="noopener">http://www.jeyzhang.com/hexo-next-add-post-views.html</a></p><p>在这个过程中发现了一个问题：pc端正常显示阅读量，但是移动端没有显示具体的阅读量。解决办法：<br>在leancloud网站上，进入安全中心，检查web安全域名列表中是否添加了<strong>http：开头</strong>的域名，如果没有，则添加上应该就能解决，例如，我的：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://makcyun.top/</span><br></pre></td></tr></table></figure><p></p><h4 id="2-4-4-文章摘要配图"><a href="#2-4-4-文章摘要配图" class="headerlink" title="2.4.4. 文章摘要配图"></a>2.4.4. 文章摘要配图</h4><p>参考这个教程即可：<br><a href="http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/" target="_blank" rel="noopener">http://wellliu.com/2016/12/30/%E3%80%90%E8%BD%AC%E3%80%91Blog%E6%91%98%E8%A6%81%E9%85%8D%E5%9B%BE/</a></p><p>附上我的设置：<br>在自定义文件中添加如下代码：<br></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// img.img-topic &#123;</span></span><br><span class="line"><span class="comment">//    width: 100%;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//图片外部的容器方框</span></span><br><span class="line">.out-img-topic &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  max-height:<span class="number">350</span>px;      <span class="comment">//图片显示高度，如果不设置则每篇文章的图片高度会不一样，看起来不协调</span></span><br><span class="line">  margin-bottom: <span class="number">24</span>px;</span><br><span class="line">  overflow: hidden;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//图片</span></span><br><span class="line">img.img-topic &#123;</span><br><span class="line">  display: block ;</span><br><span class="line">  margin-left: <span class="number">.7</span>em;</span><br><span class="line">  margin-right: <span class="number">.7</span>em;</span><br><span class="line">  padding: <span class="number">0</span>;</span><br><span class="line">  float: right;</span><br><span class="line">  clear: right;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 去掉图片边框</span></span><br><span class="line">.posts-expand .post-body img &#123;</span><br><span class="line">    border: none;</span><br><span class="line">    padding: <span class="number">0</span>px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4 id="2-4-5-添加打赏功能"><a href="#2-4-5-添加打赏功能" class="headerlink" title="2.4.5. 添加打赏功能"></a>2.4.5. 添加打赏功能</h4><p>参考下面的教程：<br><a href="https://www.cnblogs.com/mrwuzs/p/7943337.html" target="_blank" rel="noopener">https://www.cnblogs.com/mrwuzs/p/7943337.html</a><br><a href="https://blog.csdn.net/lcyaiym/article/details/76796545" target="_blank" rel="noopener">https://blog.csdn.net/lcyaiym/article/details/76796545</a></p><p>以上，包括了博客美化的大部分操作。<br>如果，你觉得还不够，想做得更精致一些，那么推荐一个非常详细的美化教程：<br><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl" target="_blank" rel="noopener">https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#附上我的-custom-styl</a></p><p>本文完。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;上一篇文章，介绍了如何搭建个人博客。这一篇文章则以我的博客为例详细介绍博客的美化步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客" scheme="https://www.makcyun.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://www.makcyun.top/tags/hexo/"/>
    
      <category term="个人博客" scheme="https://www.makcyun.top/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github" scheme="https://www.makcyun.top/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>4块钱,用Github+Hexo搭建你的个人博客：搭建篇</title>
    <link href="https://www.makcyun.top/2018/07/06/hexo01.html"/>
    <id>https://www.makcyun.top/2018/07/06/hexo01.html</id>
    <published>2018-07-06T08:44:19.881Z</published>
    <updated>2018-12-19T03:00:38.283Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --><p><b><em>4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。</em></b><br>之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。</p><a id="more"></a><p><strong>【更新于2018/7/14】</strong></p><p><strong>摘要：</strong> 对于一个不懂任何前端的纯小白来说，搭建博客是件很有挑战的事。好在参考了很多大佬的教程后顺利搭建完成，但过程中还是踩了一些坑。这里及时进行总结，作为博客的第一篇文章。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><h3 id="1-网上有很多现成的博客不用，为什么要自己搭建"><a href="#1-网上有很多现成的博客不用，为什么要自己搭建" class="headerlink" title="1 网上有很多现成的博客不用，为什么要自己搭建?"></a>1 网上有很多现成的博客不用，为什么要自己搭建?</h3><p>可能有人会说：很多网站都能写博客，而且博客现在其实都有点过时了，为什么还要自己去搞？</p><p>这里我说一下我想自己搭建的两点原因：<br><strong>一、</strong>网上的多数博客大家都共用一套相同的模板界面，没有特点、界面也杂乱充斥着很多不相关的东西，不论是自己写还是给人看，体验都不好。<br><strong>二、</strong>拥有一个你自己可以起名字的博客网站，里面的任何内容完全由你自己决定，这感觉是件很酷的事。</p><p>这些普通的网站博客和一些个人博客，哪个好看，高下立判吧。</p><p><img src="http://media.makcyun.top/18-7-14/94423333.jpg" alt="新浪博客"></p><p><center><strong>vs</strong></center><br><img src="http://media.makcyun.top/18-7-14/12736339.jpg" alt="个人博客"></p><p>&nbsp;<br><img src="http://media.makcyun.top/18-7-14/26966.jpg" alt="CSDN博客"></p><p><center><strong>vs</strong></center><br><img src="http://media.makcyun.top/18-7-14/73613801.jpg" alt="个人博客"></p><p>更多个人博客：<br><strong>litten</strong> &nbsp; <a href="http://litten.me/" target="_blank" rel="noopener">http://litten.me/</a><br><strong>Ryan</strong> &nbsp; <a href="http://ryane.top/" target="_blank" rel="noopener">http://ryane.top/</a><br><strong>liyin</strong> &nbsp; <a href="https://liyin.date/" target="_blank" rel="noopener">https://liyin.date/</a><br><strong>reuixiy</strong> &nbsp; <a href="https://reuixiy.github.io/" target="_blank" rel="noopener">https://reuixiy.github.io/</a><br><strong>Tranquilpeak</strong> &nbsp; <a href="https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/" target="_blank" rel="noopener">https://louisbarranqueiro.github.io/hexo-theme-tranquilpeak/</a></p><p><br></p><h3 id="2-搭建博客难不难？"><a href="#2-搭建博客难不难？" class="headerlink" title="2 搭建博客难不难？"></a>2 搭建博客难不难？</h3><p>我之前认为搭建博客是一件只有能程序猿才能做出来的高大上的活。其实，只要跟着网上的教程一步步做下去，一个小时不到就可以搭建好你自己的个人博客。所以，搭建博客其实很简单。只不过，如果你想把博客做得好看一些的话，才会花费一些精力。</p><p><br></p><h2 id="二、开始搭建博客"><a href="#二、开始搭建博客" class="headerlink" title="二、开始搭建博客"></a>二、开始搭建博客</h2><p><strong>如果看到上面那些精美的博客，你已经心动了，那就开始动手吧。下面正式开始博客搭建步骤。</strong></p><p><strong>搭建教程参考</strong><br>搭建博客的教程网上一搜一大堆，为了节省你的搜索时间，这里我筛选出了下面几篇很棒的教程，我基本都是跟着一步步做下来的。</p><ol><li><a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">小白独立搭建博客</a></li><li><a href="http://ryane.top/2018/01/10/2018%EF%BC%8C%E4%BD%A0%E8%AF%A5%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E4%BA%86%EF%BC%81/" target="_blank" rel="noopener">2018，你该搭建自己的博客了！</a></li><li><a href="https://blog.csdn.net/gdutxiaoxu/article/details/53576018" target="_blank" rel="noopener">手把手教你用Hexo+Github 搭建属于自己的博客</a></li></ol><p>操作平台:Win7 64位。</p><p><br></p><p><strong>相关名词解释：</strong><br><strong>Hexo：</strong>一种常用的博客框架，有了它建立博客非常简单。你可以认为它是一种博客模板，只不过它比普通网站的那种博客模板要好看地多，并且有很高度的自定义性，只要你愿意，你可以建立一个独一无二的博客来。<br>若想详细了解Hexo的使用，移步 <strong>Hexo官方网站</strong> <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/</a>。</p><p><strong>Github：</strong>一个全世界程序猿聚集的知名网站。免费的远程仓库和开源协作社区。我们需要利用网站里的Github Pages功能来托管需发布到网上的博客的相关文件和代码。</p><p><strong>Git：</strong> 一种版本控制系统。我们在自己的本地电脑写博客，如何把博客同步到Github，然后发布到网上去？就需要用这个软件去写几行代码然后就能搞定，后期用的最多的就是它。</p><p><strong>Node.js：</strong> 提供JavaScript的开发环境，安装好以后就不用跟它再打交道，所以不用太关注它。</p><h3 id="1-软件安装配置"><a href="#1-软件安装配置" class="headerlink" title="1 软件安装配置"></a>1 软件安装配置</h3><p>搭建博客需要先下载2个软件：Git和Nodejs。<br>软件安装过程很简单，一直点击Next默认直到安装完成就行了。</p><p><strong>Git</strong><br>官网：<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">https://git-scm.com/download/win</a><br>安装完，打开cmd窗口运行下面命令，如果有返回版本信息说明安装成功。</p><pre><code>git –version </code></pre><p><strong>Nodejs</strong><br>官网：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a><br>同样，安装完有返回版本信息说明安装成功，见下图。</p><pre><code>node -v  npm -v  </code></pre><p><img src="http://media.makcyun.top/18-7-14/43008634.jpg" alt="cmd命令"></p><p>至此，软件安装步骤完成。</p><h3 id="2-安装Hexo博客框架"><a href="#2-安装Hexo博客框架" class="headerlink" title="2 安装Hexo博客框架"></a>2 安装Hexo博客框架</h3><ul><li>安装hexo</li></ul><p>这里开始就要用到使用频率最高的Git软件了。</p><p>桌面右键点击<strong>git bash here</strong>选项，会打开Git软件界面，输入下面每行命令并回车：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>第一句是安装hexo，第二句是安装hexo部署到git page的deployer。代码命令看不懂没关系，一是这些命令之后几乎不再用到，二是用多了你会慢慢记住。<br><img src="http://media.makcyun.top/18-7-14/18599032.jpg" alt=""></p><ul><li>设置博客存放文件夹</li></ul><p>你需要预想一下你要把博客文件新建在哪个盘下的文件夹里，除了c盘都可以，例如d盘根目录下的blog文件夹，紧接着在桌面输入下面命令并回车：</p><pre><code>hexo init /d/blogcd /d/blognpm install*注：/d/bog可以更改为你自己的文件夹*</code></pre><p>有的教程是先新建博客文件夹，在该文件夹下右键鼠标，点击Git Bash Here，进入Git命令框，再执行以下操作。但是我操作过程中出现过这样的失败提示：<code>hexo:conmand not found</code>，但我执行上面的命令时就没有出现该问题。<br>​<br>hexo init<br>npm install</p><ul><li>查看博客效果</li></ul><p>至此，博客初步搭建好，输入下面一行本地部署生成的命令：</p><pre><code>hexo s -g </code></pre><p>然后打开浏览器在网址栏输入：<code>localhost:4000</code>就可以看到博客的样子，如果无法打开，则继续输入下面命令：</p><pre><code>npm install hexo-deployer-git --savehexo cleanhexo s -g </code></pre><p><img src="http://media.makcyun.top/18-7-14/90405263.jpg" alt=""></p><p>打开该网址，你可以看到第一篇默认的博客：<strong>Hello World</strong>。但看起来很难看，后续会通过重新选择模板来对博客进行美化。</p><div class="note primary"><p>现在你就可以开始写博客了，但是博客只能在你自己的电脑上看到，别人无法在网上看到你的博客。接下来需要利用前面提到的的Github Pages功能进行设置，设置完成之后别人通过搜索就可以看到你的博客。</p></div><h3 id="3-把你的博客部署到Github-Pages上去"><a href="#3-把你的博客部署到Github-Pages上去" class="headerlink" title="3 把你的博客部署到Github Pages上去"></a>3 把你的博客部署到Github Pages上去</h3><p>这是搭建博客相对比较复杂也是容易出错的一部分。</p><p><strong>1. Github账号注册及配置</strong></p><p>如果你没有github帐号，就新建一个，然后去邮箱进行验证；如果你有帐号则直接登录。<br>官网：<a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a></p><p>配置步骤：</p><ul><li>建立new repository</li></ul><p>只填写username.github.io即可，然后点击<code>create repositrory</code>。<br>注意：<code>username.github.io</code> 的<code>username</code>要和用户名保持一致，不然后面会失败。以我的为例：</p><p><img src="http://media.makcyun.top/18-7-14/98355992.jpg" alt="1"></p><p><img src="http://media.makcyun.top/18-7-14/62084844.jpg" alt="2"></p><ul><li>开启gh-pages功能</li></ul><p>点击github主页点击头像下面的profile,找到新建立的username.github.io文件打开，点击settings，往下拉动鼠标到GitHub Pages。<br>如果你看到上方出现以下警告：</p><div class="note warning"><br>GitHub Pages is currently disabled. You must first add content to your repository before you can publish a GitHub Pages site<br></div><p>不用管他，点击选择<code>choose a theme</code>，随便选择一个，（之后我们要更改这些丑陋的模板），然后select theme保存就行了。</p><p><img src="http://media.makcyun.top/18-7-14/79896859.jpg" alt="3"></p><p><img src="http://media.makcyun.top/18-7-14/1269326.jpg" alt="5"></p><p>接下来的几个步骤参考<a href="https://my.oschina.net/ryaneLee/blog/638440" target="_blank" rel="noopener">教程1</a>即可。</p><p>主要步骤包括：</p><ul><li>git创建SSH密钥</li><li>在GitHub账户中添加你的公钥</li><li>测试成功并设置用户信息</li><li>将本地的Hexo文件更新到Github库中</li><li>hexo部署更新博客</li></ul><p>经过以上几步的操作，顺利的话，你的博客可以发布到网上，其他人也可以通过你的网址<code>username.github.io</code>（我的是<code>makcyun.github.io</code>）<br>访问到你的博客。</p><h3 id="4-赶紧新建个博客试试"><a href="#4-赶紧新建个博客试试" class="headerlink" title="4 赶紧新建个博客试试"></a>4 赶紧新建个博客试试</h3><p>接下来你可以自己新建一个文档来写下你的第一篇博客并在网页上测试。</p><p>同样在根目录<code>D:\blog</code>中运行下面命令：</p><pre><code>hexo new 第一篇博客*注：第一篇博客名称可以随便修改*</code></pre><p>然后打开<code>D:\blog\source\_posts</code>文件夹，就可以看到一个<code>第一篇博客.md</code>的文件。用支持markdown语法的软件打开该文件进行编辑即可。</p><p>编辑好以后，运行下述命令：<br>​<br>hexo clean<br>hexo d -g</p><p>然后，在网址中输入<code>username.github.io</code>即可看到你的博客上，出现<strong>第一篇博客</strong>这篇新的文章。</p><p><strong>至此，你的个人博客初步搭建过程就完成了。</strong></p><p><br></p><p>但是，现在还存在两个问题你可能想解决：</p><ul><li>markdown语法是什么，如何用软件编写博客？</li><li>网址是<code>username.github.io</code>，感觉很奇怪，而我的博客网址怎么是<strong>www</strong>开头的？</li></ul><p>好，下面来讲解一下。</p><p><br></p><p><strong>第一个问题</strong></p><p>关于markdown语法介绍：<br><a href="https://www.jianshu.com/p/1e402922ee32/" target="_blank" rel="noopener">markdown——入门指南</a></p><p>当你大致了解markdown语法后，如何用markdown写博客呢？不妨参考这两篇详细教程：</p><blockquote><p><a href="https://markdown.tw/" target="_blank" rel="noopener">Markdown語法說明</a><br><a href="https://www.ofind.cn/archives/" target="_blank" rel="noopener">HEXO下的Markdown语法(GFM)写博客</a></p></blockquote><p>接下来你要一个可以写markdown语法的软件，这里推荐两款软件。</p><p>Windows下使用<a href="http://markdownpad.com/" target="_blank" rel="noopener">Markdown Pad2</a>, Mac下使用<a href="http://25.io/mou/" target="_blank" rel="noopener">Mou</a>。</p><p>我用的是Markdown Pad2，这款软件是付费软件，但网上有很多破解版，我这里将软件上传到了百度网盘，如需请取。<br>MarkdownPad2： <a href="https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA" target="_blank" rel="noopener">https://pan.baidu.com/s/1NHA-E83pxKfm2he0WJxXPA</a> 密码：y9zh</p><p>安装好后，就可以打开刚才的<code>第一篇博客.md</code>，开始尝试写你的第一篇博客了。</p><p>比如这是我用markdownpad写的博客原稿。<br><img src="http://media.makcyun.top/FpU5NFP6QFdTwqjStlUrBNk2GPDK" alt=""></p><p>可以看到文档里面都是字符，没有图片这些。所以只需要用键盘专注于打字就行了，不需要像word那么复杂，还要用鼠标插入标题样式、图片这些操作。</p><p><br></p><p><strong>第二个问题</strong></p><p>我的网址不是默认的<code>username.github.io</code>，是因为我购买了一个域名，然后和<code>username.github.io</code>进行了关联，这样我的博客网址变成了我的域名。</p><p>在哪里购买域名呢？<br>首推去 <a href="https://wanwang.aliyun.com/domain/?spm=5176.383338.1907008.1.LWIFhw" target="_blank" rel="noopener">阿里云官网</a> 购买。</p><p>你可以随意起你喜欢的名字，然后在该网站进行搜索，没有人占用的话你就可以购买该域名。不同的后缀价格不同。可以看到<strong>.com</strong>、 <strong>.net</strong>等会比较贵，最便宜的这两年新出的<strong>.top域名</strong>，只要4块钱一年，我购买的就是这种。</p><p>购买完域名以后，需要做以下几个步骤：</p><ul><li>实名认证</li><li>修改DNS</li><li>域名解析</li><li>新建CNAME文件</li></ul><p><strong>1 实名认证</strong><br>在修改DNS之前，必须要阿里云官网实名认证成功，用淘宝账户登录然后填写相关信息即可。</p><p><strong>2 修改DNS</strong><br>实名认证成功后，进入管理界面，依次点击：<br><img src="http://media.makcyun.top/Ft9CnDVTNm1WFZMegkca8SaOokfW" alt=""></p><p><img src="http://media.makcyun.top/FtOks38CUKdJga4Q-rlSXjcdezPs" alt=""></p><p>修改DNS为：<br><strong>f1g1ns1.dnspod.net<br>f1g1ns2.dnspod.net</strong></p><p><strong>3 域名解析</strong><br>DNS修改好以后，到<strong>DNSPOD</strong>这个网站去解析你的域名。</p><p>首先，微信登录并注册 <a href="https://www.dnspod.cn/" target="_blank" rel="noopener">https://www.dnspod.cn/</a>，点击域名解析，添加上你的域名。<br><img src="http://media.makcyun.top/FuS2HLF9F9v9wvYiloqF8kpWMh8V" alt=""></p><p>接着，添加以下两条记录即可。</p><p><img src="http://media.makcyun.top/FojJP59gDAOuk41RMtCvEkuKijo2" alt=""></p><p>注意：<strong>makcyun.github.io.</strong>需换成你自己的名称，另外最后有一个<strong>“.”</strong></p><p><strong>4 新建CNAME文件</strong><br>在博客根目录文件夹下,例如我的<code>D:\blog\source</code>，新建名为<strong>CNAME</strong>的记事本文件，去掉后缀。<br>在里面输入你的域名，例如我的：<strong><a href="http://www.makcyun.top">www.makcyun.top</a></strong>即可，保存并关闭。</p><p><img src="http://media.makcyun.top/FmmCR8YMx-heNEiN9RFb4MkEbFA0" alt=""></p><p><strong>注意：</strong><br>这里填不填写<strong>www</strong>前缀都是可以的，区别在于填写www，那么博客网址就会以www开头，例如：<strong><a href="http://www.makcyun.top">www.makcyun.top</a></strong>；如果不填写，博客网址是：<strong>makcyun.top</strong>，二者都可以，看你喜欢。</p><p>完成以上4步之后，根目录下再次运行：</p><pre><code>hexo d -g  </code></pre><p>这时，输入你在记事本里的域名网址，即可打开你的博客。<br>至此，你的博客就换成了你想要的网址，别人也可以通过这个网址访问到你的博客。</p><p><br></p><p>到这里，博客的初步搭建就算完成了，顺利的话不到1小时就能完成，如果中间出现差错，保持耐心多试几次应该就没问题。</p><p>此时，还有一个比较重要的问题就是，你可能会觉得你的博客不够美观，不如我博客里前面提到的那几个博客，甚至也没有我的好看。</p><p>如果你还愿意折腾的话，下一篇文章，我会以我的博客为例，讲一讲如何进行博客的美化。</p><hr><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun May 19 2019 19:15:47 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;b&gt;&lt;em&gt;4块钱,你就能够在茫茫互联网中拥有一处专属于你的小天地，丈量你走过的每一个脚印。&lt;/em&gt;&lt;/b&gt;&lt;br&gt;之前，在网上看到过很多人拥有很酷的个人博客，很是羡慕，但感觉很难所以一直没敢去尝试。最近捣鼓了几天，发现搭建博客其实没有想象中的难。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo博客" scheme="https://www.makcyun.top/categories/hexo%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="https://www.makcyun.top/tags/hexo/"/>
    
      <category term="个人博客" scheme="https://www.makcyun.top/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="github" scheme="https://www.makcyun.top/tags/github/"/>
    
  </entry>
  
</feed>
